{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model Main Functions and Controller \n",
    "\n",
    "> For an in-depth tutorial, click [here for Roberta language model](https://anhquan0412.github.io/that-nlp-library/model_lm_roberta_tutorial.html), and [here for GPT language model](https://anhquan0412.github.io/that-nlp-library/model_lm_gpt2_tutorial.html)\n",
    "\n",
    "- skip_showdoc: true\n",
    "- skip_exec: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model_lm_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import os, sys\n",
    "from transformers import Trainer, TrainingArguments, AutoConfig\n",
    "from datasets import DatasetDict,Dataset\n",
    "import torch\n",
    "import gc\n",
    "import math\n",
    "from functools import partial\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from that_nlp_library.utils import *\n",
    "from that_nlp_library.text_main_lm import TextDataLMController\n",
    "from that_nlp_library.text_main_lm_streaming import TextDataLMControllerStreaming\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def language_model_init(model_class, # Model's class object, e.g. AutoModelForMaskedLM\n",
    "                        cpoint_path=None, # Either model string name on HuggingFace, or the path to model checkpoint. Put `None` to train from scratch\n",
    "                        config=None, # Model config. If not provided, AutoConfig is used to load config from cpoint_path                        \n",
    "                        device=None, # Device to train on\n",
    "                        seed=None, # Random seed\n",
    "                       ):\n",
    "    \"\"\"\n",
    "    To initialize a language model, either masked or casual\n",
    "    \"\"\"\n",
    "    if device is None: device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if config is None:\n",
    "        config = AutoConfig.from_pretrained(cpoint_path)\n",
    "\n",
    "    if seed:\n",
    "        seed_everything(seed)\n",
    "    if cpoint_path:\n",
    "        model = model_class.from_pretrained(cpoint_path,config=config).to(device)\n",
    "    else:\n",
    "        print('Initiate a new language model from scratch')\n",
    "        model = model_class.from_config(config)\n",
    "    \n",
    "    print(f'Total parameters: {sum(param.numel() for param in model.parameters())}')\n",
    "    print(f'Total trainable parameters: {sum(param.numel() for param in model.parameters() if param.requires_grad)}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/model_lm_main.py#L24){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### language_model_init\n",
       "\n",
       ">      language_model_init (model_class, cpoint_path=None, config=None,\n",
       ">                           device=None, seed=None)\n",
       "\n",
       "To initialize a language model, either masked or casual\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| model_class |  |  | Model's class object, e.g. AutoModelForMaskedLM |\n",
       "| cpoint_path | NoneType | None | Either model string name on HuggingFace, or the path to model checkpoint. Put `None` to train from scratch |\n",
       "| config | NoneType | None | Model config. If not provided, AutoConfig is used to load config from cpoint_path |\n",
       "| device | NoneType | None | Device to train on |\n",
       "| seed | NoneType | None | Random seed |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/model_lm_main.py#L24){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### language_model_init\n",
       "\n",
       ">      language_model_init (model_class, cpoint_path=None, config=None,\n",
       ">                           device=None, seed=None)\n",
       "\n",
       "To initialize a language model, either masked or casual\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| model_class |  |  | Model's class object, e.g. AutoModelForMaskedLM |\n",
       "| cpoint_path | NoneType | None | Either model string name on HuggingFace, or the path to model checkpoint. Put `None` to train from scratch |\n",
       "| config | NoneType | None | Model config. If not provided, AutoConfig is used to load config from cpoint_path |\n",
       "| device | NoneType | None | Device to train on |\n",
       "| seed | NoneType | None | Random seed |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(language_model_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 124697433\n",
      "Total trainable parameters: 124697433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_model1 = language_model_init(AutoModelForMaskedLM,\n",
    "                              'roberta-base')\n",
    "_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 70764377\n",
      "Total trainable parameters: 70764377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(59993, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=59993, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_model1 = language_model_init(AutoModelForMaskedLM,\n",
    "                              'nguyenvulebinh/envibert')\n",
    "_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 124439808\n",
      "Total trainable parameters: 124439808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_model2 = language_model_init(AutoModelForCausalLM,\n",
    "                              'gpt2')\n",
    "_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_lm_accuracy(eval_preds, # An EvalPrediction object from HuggingFace \n",
    "                        is_mlm, # if this is masked language model, set to `True`. If this is casual language model, set to `False`\n",
    "                     ):\n",
    "    \n",
    "    \"\"\"    \n",
    "    Reference: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py#L592C35-L592C35\n",
    "    \"\"\"\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    preds, labels = eval_preds\n",
    "    # preds have the same shape as the labels, after the argmax(-1) has been calculated\n",
    "    # by preprocess_logits_for_metrics\n",
    "    if is_mlm:\n",
    "        labels = labels.reshape(-1)\n",
    "        preds = preds.reshape(-1)\n",
    "        mask = labels != -100\n",
    "        labels = labels[mask]\n",
    "        preds = preds[mask]\n",
    "        return metric.compute(predictions=preds, references=labels)\n",
    "    else:\n",
    "        # we need to shift the labels\n",
    "        labels = labels[:, 1:].reshape(-1)\n",
    "        preds = preds[:, :-1].reshape(-1)\n",
    "        return metric.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def preprocess_lm_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        # Depending on the model and config, logits may contain extra tensors,\n",
    "        # like past_key_values, but logits always come first\n",
    "        logits = logits[0]\n",
    "    return logits.argmax(dim=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def finetune_lm(lr, # Learning rate\n",
    "                bs, # Batch size\n",
    "                wd, # Weight decay\n",
    "                epochs, # Number of epochs\n",
    "                ddict, # The HuggingFace datasetdict\n",
    "                tokenizer,# HuggingFace tokenizer\n",
    "                o_dir = './tmp_weights', # Directory to save weights\n",
    "                save_checkpoint=False, # Whether to save weights (checkpoints) to o_dir\n",
    "                model=None, # NLP model\n",
    "                model_init=None, # A function to initialize model\n",
    "                data_collator=None, # HuggingFace data collator\n",
    "                compute_metrics=None, # A function to compute metric, default to `compute_lm_accuracy`\n",
    "                grad_accum_steps=2, # The batch at each step will be divided by this integer and gradient will be accumulated over gradient_accumulation_steps steps.\n",
    "                lr_scheduler_type='cosine',  # The scheduler type to use. Including: linear, cosine, cosine_with_restarts, polynomial, constant, constant_with_warmup\n",
    "                warmup_ratio=0.1, # The warmup ratio for some lr scheduler\n",
    "                no_valid=False, # Whether there is a validation set or not\n",
    "                seed=None, # Random seed\n",
    "                report_to='none', # The list of integrations to report the results and logs to. Supported platforms are \"azure_ml\", \"comet_ml\", \"mlflow\", \"neptune\", \"tensorboard\",\"clearml\" and \"wandb\". Use \"all\" to report to all integrations installed, \"none\" for no integrations.\n",
    "                trainer_class=None, # You can include the class name of your custom trainer here\n",
    "            ):\n",
    "    \"The main model training/finetuning function\"\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    if seed:\n",
    "        seed_everything(seed)\n",
    "        \n",
    "    training_args = TrainingArguments(o_dir, \n",
    "                                     learning_rate=lr, \n",
    "                                     warmup_ratio=warmup_ratio,\n",
    "                                     lr_scheduler_type=lr_scheduler_type, \n",
    "                                     fp16=True,\n",
    "                                     do_train=True,\n",
    "                                     do_eval= not no_valid,\n",
    "                                     evaluation_strategy=\"no\" if no_valid else \"epoch\", \n",
    "                                     save_strategy=\"epoch\" if save_checkpoint else 'no',\n",
    "                                     overwrite_output_dir=True,\n",
    "                                     gradient_accumulation_steps=grad_accum_steps,\n",
    "                                     per_device_train_batch_size=bs, \n",
    "                                     per_device_eval_batch_size=bs,\n",
    "                                     num_train_epochs=epochs, weight_decay=wd,\n",
    "                                     report_to=report_to,\n",
    "                                     logging_dir=os.path.join(o_dir, 'log') if report_to!='none' else None,\n",
    "                                     logging_steps = len(ddict[\"train\"]) // bs,\n",
    "                                     )\n",
    "\n",
    "    # instantiate trainer\n",
    "    trainer_class = Trainer if trainer_class is None else trainer_class\n",
    "    trainer = trainer_class(\n",
    "        model=model,\n",
    "        model_init=model_init if model is None else None,\n",
    "        args=training_args,\n",
    "        train_dataset=ddict['train'],#.shard(200, 0)\n",
    "        eval_dataset=ddict['validation'] if not no_valid else None,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics if not no_valid else None,\n",
    "        preprocess_logits_for_metrics=preprocess_lm_logits_for_metrics if not no_valid else None\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/model_lm_main.py#L85){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### finetune_lm\n",
       "\n",
       ">      finetune_lm (lr, bs, wd, epochs, ddict, tokenizer, o_dir='./tmp_weights',\n",
       ">                   save_checkpoint=False, model=None, model_init=None,\n",
       ">                   data_collator=None, compute_metrics=None,\n",
       ">                   grad_accum_steps=2, lr_scheduler_type='cosine',\n",
       ">                   warmup_ratio=0.1, no_valid=False, seed=None,\n",
       ">                   report_to='none', trainer_class=None)\n",
       "\n",
       "The main model training/finetuning function\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| lr |  |  | Learning rate |\n",
       "| bs |  |  | Batch size |\n",
       "| wd |  |  | Weight decay |\n",
       "| epochs |  |  | Number of epochs |\n",
       "| ddict |  |  | The HuggingFace datasetdict |\n",
       "| tokenizer |  |  | HuggingFace tokenizer |\n",
       "| o_dir | str | ./tmp_weights | Directory to save weights |\n",
       "| save_checkpoint | bool | False | Whether to save weights (checkpoints) to o_dir |\n",
       "| model | NoneType | None | NLP model |\n",
       "| model_init | NoneType | None | A function to initialize model |\n",
       "| data_collator | NoneType | None | HuggingFace data collator |\n",
       "| compute_metrics | NoneType | None | A function to compute metric, default to `compute_lm_accuracy` |\n",
       "| grad_accum_steps | int | 2 | The batch at each step will be divided by this integer and gradient will be accumulated over gradient_accumulation_steps steps. |\n",
       "| lr_scheduler_type | str | cosine | The scheduler type to use. Including: linear, cosine, cosine_with_restarts, polynomial, constant, constant_with_warmup |\n",
       "| warmup_ratio | float | 0.1 | The warmup ratio for some lr scheduler |\n",
       "| no_valid | bool | False | Whether there is a validation set or not |\n",
       "| seed | NoneType | None | Random seed |\n",
       "| report_to | str | none | The list of integrations to report the results and logs to. Supported platforms are \"azure_ml\", \"comet_ml\", \"mlflow\", \"neptune\", \"tensorboard\",\"clearml\" and \"wandb\". Use \"all\" to report to all integrations installed, \"none\" for no integrations. |\n",
       "| trainer_class | NoneType | None | You can include the class name of your custom trainer here |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/model_lm_main.py#L85){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### finetune_lm\n",
       "\n",
       ">      finetune_lm (lr, bs, wd, epochs, ddict, tokenizer, o_dir='./tmp_weights',\n",
       ">                   save_checkpoint=False, model=None, model_init=None,\n",
       ">                   data_collator=None, compute_metrics=None,\n",
       ">                   grad_accum_steps=2, lr_scheduler_type='cosine',\n",
       ">                   warmup_ratio=0.1, no_valid=False, seed=None,\n",
       ">                   report_to='none', trainer_class=None)\n",
       "\n",
       "The main model training/finetuning function\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| lr |  |  | Learning rate |\n",
       "| bs |  |  | Batch size |\n",
       "| wd |  |  | Weight decay |\n",
       "| epochs |  |  | Number of epochs |\n",
       "| ddict |  |  | The HuggingFace datasetdict |\n",
       "| tokenizer |  |  | HuggingFace tokenizer |\n",
       "| o_dir | str | ./tmp_weights | Directory to save weights |\n",
       "| save_checkpoint | bool | False | Whether to save weights (checkpoints) to o_dir |\n",
       "| model | NoneType | None | NLP model |\n",
       "| model_init | NoneType | None | A function to initialize model |\n",
       "| data_collator | NoneType | None | HuggingFace data collator |\n",
       "| compute_metrics | NoneType | None | A function to compute metric, default to `compute_lm_accuracy` |\n",
       "| grad_accum_steps | int | 2 | The batch at each step will be divided by this integer and gradient will be accumulated over gradient_accumulation_steps steps. |\n",
       "| lr_scheduler_type | str | cosine | The scheduler type to use. Including: linear, cosine, cosine_with_restarts, polynomial, constant, constant_with_warmup |\n",
       "| warmup_ratio | float | 0.1 | The warmup ratio for some lr scheduler |\n",
       "| no_valid | bool | False | Whether there is a validation set or not |\n",
       "| seed | NoneType | None | Random seed |\n",
       "| report_to | str | none | The list of integrations to report the results and logs to. Supported platforms are \"azure_ml\", \"comet_ml\", \"mlflow\", \"neptune\", \"tensorboard\",\"clearml\" and \"wandb\". Use \"all\" to report to all integrations installed, \"none\" for no integrations. |\n",
       "| trainer_class | NoneType | None | You can include the class name of your custom trainer here |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(finetune_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "def extract_hidden_states(batch,\n",
    "                          model=None, # NLP model\n",
    "                          model_input_names=['input_ids', 'token_type_ids', 'attention_mask'], # Model required inputs, from tokenizer.model_input_names\n",
    "                          data_collator=None, # HuggingFace data collator\n",
    "                          state_name='last_hidden_state', # Name of the state to extract\n",
    "                          state_idx=0, # The index (or indices) of the state to extract\n",
    "                          device = None, # device that the model is trained on\n",
    "                          ):\n",
    "    state_idx = val2iterable(state_idx)\n",
    "    \n",
    "    if data_collator is not None:    \n",
    "# --- Convert from  \n",
    "# {'input_ids': [tensor([    0, 10444,   244, 14585,   125,  2948,  5925,   368,     2]), \n",
    "#                tensor([    0, 16098,  2913,   244,   135,   198, 34629,  6356,     2])]\n",
    "# 'attention_mask': [tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), \n",
    "#                    tensor([1, 1, 1, 1, 1, 1, 1, 1, 1])]\n",
    "#                    }\n",
    "# --- to\n",
    "# [{'input_ids': tensor([    0, 10444,   244, 14585,   125,  2948,  5925,   368,     2]),\n",
    "#   'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1])},\n",
    "#  {'input_ids': tensor([    0, 16098,  2913,   244,   135,   198, 34629,  6356,     2]),\n",
    "#   'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1])}]\n",
    "\n",
    "        # remove string text, due to transformer new version       \n",
    "        collator_inp = []\n",
    "        ks = [k for k in batch.keys() if k in model_input_names]\n",
    "        vs = [batch[k] for k in ks]\n",
    "        for pair in zip(*vs):\n",
    "            collator_inp.append({k:v for k,v in zip(ks,pair)})\n",
    "        \n",
    "        batch = data_collator(collator_inp)\n",
    "    \n",
    "    inputs = {k:v.to(device) for k,v in batch.items()\n",
    "              if k in model_input_names}\n",
    "            \n",
    "    # switch to eval mode for evaluation\n",
    "    if model.training:\n",
    "        model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        states = outputs[state_name]\n",
    "        for i in state_idx:\n",
    "            if isinstance(states,tuple):\n",
    "                states = states[i]\n",
    "            else:\n",
    "                states = states[:,i]\n",
    "    # Switch back to train mode\n",
    "    if not model.training:\n",
    "        model.train()\n",
    "\n",
    "    return {state_name:states.cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ModelLMController():\n",
    "    def __init__(self,\n",
    "                 model, # NLP language model\n",
    "                 data_store=None, # a TextDataLMController/TextDataLMControllerStreaming object\n",
    "                 seed=None, # Random seed\n",
    "                ):\n",
    "        self.model = model\n",
    "        self.data_store = data_store\n",
    "        self.seed = seed\n",
    "        \n",
    "    def fit(self,\n",
    "            epochs, # Number of epochs\n",
    "            learning_rate, # Learning rate\n",
    "            ddict=None, # DatasetDict to fit (will override data_store)\n",
    "            compute_metrics=None, # A function to compute metric, default to `compute_lm_accuracy`\n",
    "            batch_size=16, # Batch size\n",
    "            weight_decay=0.01, # Weight decay\n",
    "            lr_scheduler_type='cosine', # The scheduler type to use. Including: linear, cosine, cosine_with_restarts, polynomial, constant, constant_with_warmup\n",
    "            warmup_ratio=0.1, # The warmup ratio for some lr scheduler\n",
    "            o_dir = './tmp_weights', # Directory to save weights\n",
    "            save_checkpoint=False, # Whether to save weights (checkpoints) to o_dir\n",
    "            hf_report_to='none', # The list of HuggingFace-allowed integrations to report the results and logs to\n",
    "            grad_accum_steps=2, # Gradient will be accumulated over gradient_accumulation_steps steps.\n",
    "            tokenizer=None, # Tokenizer (to override one in ```data_store```)\n",
    "            data_collator=None, # Data Collator (to override one in ```data_store```)\n",
    "            is_mlm=None, # Whether this is masked LM or casual LM\n",
    "            trainer_class=None, # You can include the class name of your custom trainer here\n",
    "           ):\n",
    "        \n",
    "        if tokenizer is None: tokenizer=check_and_get_attribute(self.data_store,'tokenizer')\n",
    "        if data_collator is None: data_collator=check_and_get_attribute(self.data_store,'data_collator')\n",
    "        if ddict is None: ddict = check_and_get_attribute(self.data_store,'main_ddict')\n",
    "        if is_mlm is None: is_mlm = check_and_get_attribute(self.data_store,'is_mlm')\n",
    "            \n",
    "        if compute_metrics is None:\n",
    "            compute_metrics=partial(compute_lm_accuracy,is_mlm=is_mlm)\n",
    "        \n",
    "        if len(set(ddict.keys()) & set(['train','training']))==0:\n",
    "            raise ValueError(\"Missing the following key for DatasetDict: train/training\")\n",
    "        no_valid = len(set(ddict.keys()) & set(['validation','val','valid']))==0\n",
    "        \n",
    "        trainer = finetune_lm(learning_rate,\n",
    "                              batch_size,\n",
    "                              weight_decay,\n",
    "                              epochs,\n",
    "                              ddict,\n",
    "                              tokenizer,\n",
    "                              o_dir,\n",
    "                              save_checkpoint=save_checkpoint,\n",
    "                              model=self.model,\n",
    "                              data_collator=data_collator,\n",
    "                              compute_metrics=compute_metrics,\n",
    "                              grad_accum_steps=grad_accum_steps,\n",
    "                              lr_scheduler_type=lr_scheduler_type,\n",
    "                              warmup_ratio=warmup_ratio,\n",
    "                              no_valid=no_valid,\n",
    "                              seed=self.seed,\n",
    "                              trainer_class=trainer_class,\n",
    "                              report_to=hf_report_to)\n",
    "        \n",
    "        if not no_valid:\n",
    "            eval_results = trainer.evaluate()\n",
    "            try:\n",
    "                perplexity = math.exp(eval_results[\"eval_loss\"])\n",
    "                print(f'Perplexity on validation set: {perplexity:.3f}')\n",
    "            except OverflowError:\n",
    "                perplexity = float(\"inf\")\n",
    "                print(f'Perplexity on validation set: {perplexity}')\n",
    "            eval_results['perplexity'] = perplexity\n",
    "        \n",
    "        self.trainer = trainer\n",
    "        \n",
    "    def predict_raw_text(self,\n",
    "                         content:dict|list|str, # Either a single sentence, list of sentence or a dictionary where keys are metadata, values are list\n",
    "                         print_result=True, # To whether print the result in readable format, or get the result returned\n",
    "                         **kwargs, # keyword arguments for HuggingFace's text-generation (for clm) or fill-mask (for mlm)\n",
    "                        ):\n",
    "        # Example of kwargs for text-generation:\n",
    "        # https://huggingface.co/docs/transformers/v4.33.2/en/main_classes/text_generation#transformers.GenerationMixin.generate\n",
    "        \n",
    "        if not isinstance(self.data_store,(TextDataLMController,TextDataLMControllerStreaming)) or not self.data_store._processed_call:\n",
    "            raise ValueError('This functionality needs a TextDataController object which has processed some training data')\n",
    "        test_dset = self.data_store.prepare_test_dataset_from_raws(content)\n",
    "        result = self.predict_ddict(test_dset,**kwargs)\n",
    "        if print_result:\n",
    "            is_mlm = check_and_get_attribute(self.data_store,'is_mlm')\n",
    "            for preds in result:\n",
    "                for pred in preds:\n",
    "                    if is_mlm:\n",
    "                        print(f\"Score: {pred['score']:.3f} >>> {pred['sequence']}\")\n",
    "                    else:\n",
    "                        print(f\">>> {pred['generated_text']}\")\n",
    "                print('-'*20)\n",
    "        else:\n",
    "            return result\n",
    "    \n",
    "    def predict_ddict(self,\n",
    "                      dset:Dataset, # A processed and tokenized Dataset\n",
    "                      **kwargs, # keyword arguments for HuggingFace's text-generation (for clm) or fill-mask (for mlm)\n",
    "                     ):\n",
    "        is_mlm = check_and_get_attribute(self.data_store,'is_mlm')\n",
    "        tokenizer=check_and_get_attribute(self.data_store,'tokenizer')\n",
    "        main_text=check_and_get_attribute(self.data_store,'main_text')\n",
    "        _task = 'fill-mask' if is_mlm else 'text-generation'\n",
    "        pipeline_obj = pipeline(_task,model=self.model,tokenizer=tokenizer,device=self.model.device)\n",
    "        str_list = dset[main_text]\n",
    "        if _task=='fill-mask':\n",
    "            all_tfms = self.data_store.content_tfms \n",
    "            all_tfms = partial(func_all,functions=all_tfms) if len(all_tfms) else lambda x: x\n",
    "            mask_str = all_tfms(tokenizer.mask_token)\n",
    "            str_list = [str(s).replace(mask_str,tokenizer.mask_token) for s in str_list]\n",
    "        return [pipeline_obj(s,**kwargs) for s in str_list]\n",
    "    \n",
    "    def get_hidden_states_from_raw_text(self,\n",
    "                                        content:dict|list|str, # Either a single sentence, list of sentence or a dictionary where keys are metadata, values are list\n",
    "                                        state_name, # Name of the (hidden) state to extract\n",
    "                                        state_idx=0, # The index (or indices) of the state to extract. For `hidden_states`, accept multiple values\n",
    "                                       ):\n",
    "        if not isinstance(self.data_store,(TextDataLMController,TextDataLMControllerStreaming)) or not self.data_store._processed_call:\n",
    "            raise ValueError('This functionality needs a TextDataController object which has processed some training data')\n",
    "        dset = self.data_store.prepare_test_dataset_from_raws(content,do_tokenize=True)\n",
    "        return self.get_hidden_states(dset,\n",
    "                                      state_name=state_name,\n",
    "                                      state_idx=state_idx,\n",
    "                                      batch_size=1\n",
    "                                     )\n",
    "\n",
    "        \n",
    "    def get_hidden_states_from_raw_dset(self,\n",
    "                                        dset: Dataset, # A raw HuggingFace dataset\n",
    "                                        state_name, # Name of the (hidden) state to extract\n",
    "                                        state_idx=0, # The index (or indices) of the state to extract. For `hidden_states`, accept multiple values\n",
    "                                        batch_size=16, # GPU batch size\n",
    "                                       ):\n",
    "        if not isinstance(self.data_store,(TextDataLMController,TextDataLMControllerStreaming)) or not self.data_store._processed_call:\n",
    "            raise ValueError('This functionality needs a TextDataController object which has processed some training data')\n",
    "        dset = self.data_store.prepare_test_dataset(dset,do_tokenize=True)\n",
    "        return self.get_hidden_states(dset,\n",
    "                                      state_name=state_name,\n",
    "                                      state_idx=state_idx,\n",
    "                                      batch_size=batch_size\n",
    "                                     )\n",
    "        \n",
    "        \n",
    "    def get_hidden_states(self,\n",
    "                          ddict:DatasetDict|Dataset=None, # A processed and tokenized DatasetDict/Dataset (will override one in ```data_store```)\n",
    "                          ds_type='test', # The split of DatasetDict to predict\n",
    "                          state_name='last_hidden_state', # Name of the (hidden) state to extract\n",
    "                          state_idx=0, # The index (or indices) of the state to extract. For `hidden_states`, accept multiple values\n",
    "                          batch_size=16, # GPU batch size\n",
    "                         ):\n",
    "        \n",
    "        tokenizer=check_and_get_attribute(self.data_store,'tokenizer')\n",
    "        if ddict is None: ddict = check_and_get_attribute(self.data_store,'main_ddict')\n",
    "        if isinstance(ddict,DatasetDict):\n",
    "            if ds_type not in ddict.keys():\n",
    "                raise ValueError(f'{ds_type} is not in the given DatasetDict')\n",
    "            ddict = ddict[ds_type]\n",
    "        \n",
    "        ddict.set_format(\"torch\",\n",
    "                        columns=tokenizer.model_input_names)\n",
    "        \n",
    "        results = ddict.map(partial(extract_hidden_states,\n",
    "                                 model=self.model,\n",
    "                                 model_input_names = tokenizer.model_input_names,\n",
    "                                 state_name=state_name,\n",
    "                                 state_idx=state_idx,\n",
    "                                 device = self.model.device),\n",
    "                           batched=True,\n",
    "                           batch_size=batch_size\n",
    "                          )\n",
    "        results.set_format('numpy')\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/model_lm_main.py#L202){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ModelLMController\n",
       "\n",
       ">      ModelLMController (model, data_store=None, seed=None)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| model |  |  | NLP language model |\n",
       "| data_store | NoneType | None | a TextDataLMController/TextDataLMControllerStreaming object |\n",
       "| seed | NoneType | None | Random seed |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/model_lm_main.py#L202){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ModelLMController\n",
       "\n",
       ">      ModelLMController (model, data_store=None, seed=None)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| model |  |  | NLP language model |\n",
       "| data_store | NoneType | None | a TextDataLMController/TextDataLMControllerStreaming object |\n",
       "| seed | NoneType | None | Random seed |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ModelLMController)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/model_lm_main.py#L212){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ModelLMController.fit\n",
       "\n",
       ">      ModelLMController.fit (epochs, learning_rate, ddict=None,\n",
       ">                             compute_metrics=None, batch_size=16,\n",
       ">                             weight_decay=0.01, lr_scheduler_type='cosine',\n",
       ">                             warmup_ratio=0.1, o_dir='./tmp_weights',\n",
       ">                             save_checkpoint=False, hf_report_to='none',\n",
       ">                             grad_accum_steps=2, tokenizer=None,\n",
       ">                             data_collator=None, is_mlm=None,\n",
       ">                             trainer_class=None)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| epochs |  |  | Number of epochs |\n",
       "| learning_rate |  |  | Learning rate |\n",
       "| ddict | NoneType | None | DatasetDict to fit (will override data_store) |\n",
       "| compute_metrics | NoneType | None | A function to compute metric, default to `compute_lm_accuracy` |\n",
       "| batch_size | int | 16 | Batch size |\n",
       "| weight_decay | float | 0.01 | Weight decay |\n",
       "| lr_scheduler_type | str | cosine | The scheduler type to use. Including: linear, cosine, cosine_with_restarts, polynomial, constant, constant_with_warmup |\n",
       "| warmup_ratio | float | 0.1 | The warmup ratio for some lr scheduler |\n",
       "| o_dir | str | ./tmp_weights | Directory to save weights |\n",
       "| save_checkpoint | bool | False | Whether to save weights (checkpoints) to o_dir |\n",
       "| hf_report_to | str | none | The list of HuggingFace-allowed integrations to report the results and logs to |\n",
       "| grad_accum_steps | int | 2 | Gradient will be accumulated over gradient_accumulation_steps steps. |\n",
       "| tokenizer | NoneType | None | Tokenizer (to override one in ```data_store```) |\n",
       "| data_collator | NoneType | None | Data Collator (to override one in ```data_store```) |\n",
       "| is_mlm | NoneType | None | Whether this is masked LM or casual LM |\n",
       "| trainer_class | NoneType | None | You can include the class name of your custom trainer here |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/model_lm_main.py#L212){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ModelLMController.fit\n",
       "\n",
       ">      ModelLMController.fit (epochs, learning_rate, ddict=None,\n",
       ">                             compute_metrics=None, batch_size=16,\n",
       ">                             weight_decay=0.01, lr_scheduler_type='cosine',\n",
       ">                             warmup_ratio=0.1, o_dir='./tmp_weights',\n",
       ">                             save_checkpoint=False, hf_report_to='none',\n",
       ">                             grad_accum_steps=2, tokenizer=None,\n",
       ">                             data_collator=None, is_mlm=None,\n",
       ">                             trainer_class=None)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| epochs |  |  | Number of epochs |\n",
       "| learning_rate |  |  | Learning rate |\n",
       "| ddict | NoneType | None | DatasetDict to fit (will override data_store) |\n",
       "| compute_metrics | NoneType | None | A function to compute metric, default to `compute_lm_accuracy` |\n",
       "| batch_size | int | 16 | Batch size |\n",
       "| weight_decay | float | 0.01 | Weight decay |\n",
       "| lr_scheduler_type | str | cosine | The scheduler type to use. Including: linear, cosine, cosine_with_restarts, polynomial, constant, constant_with_warmup |\n",
       "| warmup_ratio | float | 0.1 | The warmup ratio for some lr scheduler |\n",
       "| o_dir | str | ./tmp_weights | Directory to save weights |\n",
       "| save_checkpoint | bool | False | Whether to save weights (checkpoints) to o_dir |\n",
       "| hf_report_to | str | none | The list of HuggingFace-allowed integrations to report the results and logs to |\n",
       "| grad_accum_steps | int | 2 | Gradient will be accumulated over gradient_accumulation_steps steps. |\n",
       "| tokenizer | NoneType | None | Tokenizer (to override one in ```data_store```) |\n",
       "| data_collator | NoneType | None | Data Collator (to override one in ```data_store```) |\n",
       "| is_mlm | NoneType | None | Whether this is masked LM or casual LM |\n",
       "| trainer_class | NoneType | None | You can include the class name of your custom trainer here |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ModelLMController.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/model_lm_main.py#L274){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ModelLMController.predict_raw_text\n",
       "\n",
       ">      ModelLMController.predict_raw_text (content:dict|list|str,\n",
       ">                                          print_result=True, **kwargs)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| content | dict \\| list \\| str |  | Either a single sentence, list of sentence or a dictionary where keys are metadata, values are list |\n",
       "| print_result | bool | True | To whether print the result in readable format, or get the result returned |\n",
       "| kwargs |  |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/model_lm_main.py#L274){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ModelLMController.predict_raw_text\n",
       "\n",
       ">      ModelLMController.predict_raw_text (content:dict|list|str,\n",
       ">                                          print_result=True, **kwargs)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| content | dict \\| list \\| str |  | Either a single sentence, list of sentence or a dictionary where keys are metadata, values are list |\n",
       "| print_result | bool | True | To whether print the result in readable format, or get the result returned |\n",
       "| kwargs |  |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ModelLMController.predict_raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
