{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Main For Language Model - Streaming\n",
    "\n",
    "> This module contains the main Python class for the **streaming** version of `TextDataLMController`\n",
    "\n",
    "\n",
    "- skip_showdoc: true\n",
    "- skip_exec: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp text_main_lm_streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from datasets import Dataset,IterableDataset\n",
    "from that_nlp_library.utils import *\n",
    "from that_nlp_library.text_main import tokenize_function\n",
    "from that_nlp_library.text_main_streaming import *\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from that_nlp_library.text_transformation import *\n",
    "from that_nlp_library.text_augmentation import *\n",
    "from importlib.machinery import SourceFileLoader\n",
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class TextDataLMControllerStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TextDataLMControllerStreaming(TextDataControllerStreaming):\n",
    "    def __init__(self,\n",
    "                 inp, # HuggingFainpce Dataset or DatasetDict\n",
    "                 main_text:str, # Name of the main text column\n",
    "                 filter_dict={}, # A dictionary: {feature: filtering_function_for_that_feature}\n",
    "                 metadatas=[], # Names of the metadata columns\n",
    "                 process_metas=True, # Whether to do simple text processing on the chosen metadatas\n",
    "                 content_transformations=[], # A list of text transformations\n",
    "                 seed=None, # Random seed\n",
    "                 batch_size=1024, # Transformation + Tokenization batch size\n",
    "                 num_proc=1, # Number of process for multiprocessing\n",
    "                 cols_to_keep=None, # Columns to keep after all processings\n",
    "                 verbose=True, # Whether to prdint processing information\n",
    "                ):\n",
    "        \n",
    "        super().__init__(inp=inp,\n",
    "                         main_text=main_text,\n",
    "                         filter_dict=filter_dict,\n",
    "                         metadatas=metadatas,\n",
    "                         process_metas=process_metas,\n",
    "                         content_transformations=content_transformations,\n",
    "                         seed=seed,\n",
    "                         batch_size=batch_size,\n",
    "                         num_proc=num_proc,\n",
    "                         cols_to_keep=cols_to_keep,\n",
    "                         verbose=verbose\n",
    "                        )\n",
    "            \n",
    "    \n",
    "    def _do_label_transformation(self):\n",
    "        raise NotImplementedError(\"There's no classification/regression label in text processing for Language Model\")\n",
    "        \n",
    "    def _encode_labels(self):\n",
    "        raise NotImplementedError(\"There's no classification/regression label in text processing for Language Model\")\n",
    "    \n",
    "    def _do_transformation_augmentation_tokenization(self):\n",
    "        raise NotImplementedError(\"There's no augmentation in text processing for Language Model\")\n",
    "\n",
    "\n",
    "    def save_as_pickles(self,\n",
    "                        fname, # Name of the pickle file\n",
    "                        parent='pickle_files', # Parent folder\n",
    "                       ):\n",
    "        \n",
    "        save_to_pickle(self,fname,parent=parent)\n",
    "        \n",
    "    def _group_texts_with_stride(self,examples):\n",
    "        max_length = self.max_length\n",
    "        if max_length is None: \n",
    "            max_length = self.tokenizer.model_max_length\n",
    "        stride = self.stride\n",
    "        if stride is None: stride=max_length\n",
    "        else: stride = max_length-stride\n",
    "        if stride==0: raise ValueError(f'Stride cannot be equal to max length of {max_length}')\n",
    "            \n",
    "        # Concatenate all texts.\n",
    "        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        result_all={}\n",
    "        for k,t in concatenated_examples.items():\n",
    "            result=[]\n",
    "            i=0\n",
    "            while i+max_length<=total_length:\n",
    "                result.append(t[i:i+max_length])\n",
    "                i+=stride\n",
    "            result_all[k]=result\n",
    "        \n",
    "        return result_all  \n",
    "    \n",
    "    \n",
    "    def _do_transformation_tokenization(self,dtrain):             \n",
    "        if len(self.content_tfms):            \n",
    "            for tfm in self.content_tfms:\n",
    "                _func = partial(lambda_map_batch,\n",
    "                                feature=self.main_text,\n",
    "                                func=tfm,\n",
    "                                is_batched=self.is_batched)\n",
    "                dtrain = hf_map_dset(dtrain,_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "\n",
    "        tok_func = partial(tokenize_function,\n",
    "                           tok=self.tokenizer,\n",
    "                           max_length=self.max_length if self.line_by_line else -1,\n",
    "                           return_special_tokens_mask=True\n",
    "                          )\n",
    "        \n",
    "        # Tokenization\n",
    "        _func = partial(lambda_map_batch,\n",
    "                        feature=self.main_text,\n",
    "                        func=tok_func,\n",
    "                        output_feature=None,\n",
    "                        is_batched=self.is_batched)\n",
    "        \n",
    "        dtrain = hf_map_dset(dtrain,_func,self.is_batched,self.batch_size,self.tok_num_proc)\n",
    "        if not self.line_by_line: dtrain = dtrain.remove_columns(self.cols_to_keep)   \n",
    "        \n",
    "        # Token concatenation\n",
    "        if not self.line_by_line: \n",
    "            dtrain = hf_map_dset(dtrain,\n",
    "                                 self._group_texts_with_stride,\n",
    "                                 is_batched=True,\n",
    "                                 batch_size=self.batch_size,\n",
    "                                 num_proc=self.tok_num_proc)\n",
    "        return dtrain\n",
    "    \n",
    "    \n",
    "    def _construct_generator_with_batch(self,dset):        \n",
    "        def _get_generator(dset):\n",
    "            for v in dset: yield v\n",
    "            \n",
    "        final_dict = defaultdict(list)\n",
    "        for inp in dset: # dset is generator\n",
    "            # inp[text_name] will be a single item\n",
    "            for k,v in inp.items():\n",
    "                final_dict[k].append(v)\n",
    "            \n",
    "            if len(final_dict[self.main_text])==self.batch_size:\n",
    "                # a full batch (self.batch_size) is created\n",
    "                dtrain = Dataset.from_dict(final_dict)\n",
    "                dtrain = self._do_transformation_tokenization(dtrain)\n",
    "                yield from _get_generator(dtrain)\n",
    "                final_dict=defaultdict(list)            \n",
    "            \n",
    "        if len(final_dict[self.main_text]):\n",
    "            # hasn't reached batch_size (of last batch)\n",
    "            dtrain = Dataset.from_dict(final_dict)\n",
    "            dtrain = self._do_transformation_tokenization(dtrain)\n",
    "            yield from _get_generator(dtrain)\n",
    "\n",
    "    def _do_transformation_tokenization_generator(self):\n",
    "        _tmp1 = self.num_proc\n",
    "        _tmp2 = self.tok_num_proc\n",
    "        self.num_proc=1\n",
    "        self_tok_num_proc=1\n",
    "        self.main_ddict['train'] = IterableDataset.from_generator(self._construct_generator_with_batch,\n",
    "                                                                  gen_kwargs={'dset': self.main_ddict['train']}\n",
    "                                                                 )\n",
    "        self.num_proc = _tmp1\n",
    "        self.tok_num_proc = _tmp2\n",
    "    \n",
    "    def _do_transformation_tokenization_generator_fast(self):\n",
    "        # only use for line-by-line tokenization with no padding\n",
    "        def _get_generator(dset,tok_func,all_tfms):\n",
    "            for inp in dset:\n",
    "                # inp[text_name] will be a single item\n",
    "                results = tok_func(all_tfms(inp[self.main_text]))\n",
    "                # add back cols_to_keep in inp\n",
    "                results = dict(inp,**results)\n",
    "                yield results\n",
    "        \n",
    "        # no padding for tokenization\n",
    "        tok_func = partial(tokenize_function,\n",
    "                           tok=self.tokenizer,\n",
    "                           max_length=-1,\n",
    "                           return_special_tokens_mask=True\n",
    "                          )\n",
    "        all_tfms = self.content_tfms \n",
    "        all_tfms = partial(func_all,functions=all_tfms) if len(all_tfms) else lambda x: x\n",
    "           \n",
    "        self.main_ddict['train'] = IterableDataset.from_generator(_get_generator,\n",
    "                                                   gen_kwargs={'dset': self.main_ddict['train'],\n",
    "                                                               'tok_func':tok_func,\n",
    "                                                               'all_tfms': all_tfms\n",
    "                                                              }\n",
    "                                                                 )\n",
    "\n",
    "    \n",
    "    def process_and_tokenize(self,\n",
    "                             tokenizer, # Tokenizer (preferably from HuggingFace)\n",
    "                             max_length=None, # pad to model's allowed max length (default is max_sequence_length). Use -1 for no padding at all\n",
    "                             tok_num_proc=None, # Number of processes for tokenization\n",
    "                             line_by_line=True, # To whether tokenize each sentence separately, or concatenate them\n",
    "                             stride=None, # option to do striding when line_by_line is False\n",
    "                            ):\n",
    "        if self._processed_call:\n",
    "            warnings.warn('Your dataset has already been processed. Returning the previous processed DatasetDict...')\n",
    "            return self.main_ddict\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.line_by_line = line_by_line\n",
    "        if not self.line_by_line and self.batch_size==1:\n",
    "            raise ValueError('Cannot perform token concatenation with batch size of 1')\n",
    "        self.stride = stride        \n",
    "        self.tok_num_proc = tok_num_proc if tok_num_proc else self.num_proc\n",
    "        \n",
    "        # Filtering\n",
    "        print_msg('Data Filtering',20,verbose=self.verbose)\n",
    "        for k in self.main_ddict.keys():   \n",
    "            self.main_ddict[k] = self._do_filtering(self.main_ddict[k])\n",
    "        self.verboseprint('Done')\n",
    "\n",
    "        \n",
    "        # Process metadatas\n",
    "        print_msg('Metadata Simple Processing & Concatenating to Main Content',verbose=self.verbose)\n",
    "        for k in self.main_ddict.keys():   \n",
    "            self.main_ddict[k] = self._process_metadatas(self.main_ddict[k])\n",
    "        self.verboseprint('Done')\n",
    "\n",
    "        # Dropping unused columns\n",
    "        self._simplify_ddict()\n",
    "\n",
    "        if self.seed:\n",
    "            seed_everything(self.seed)\n",
    "            \n",
    "        # Content transformation + tokenization for validation\n",
    "        if 'validation' in self.main_ddict.keys():\n",
    "            print_msg('Performing Content Transformation and Tokenization on Validation Set',verbose=self.verbose)\n",
    "            self.main_ddict['validation'] = self._do_transformation_tokenization(self.main_ddict['validation'])\n",
    "            self.verboseprint('Done')\n",
    "        \n",
    "        # Content transformation + tokenization for train\n",
    "        print_msg('Creating a generator for content transformation and tokenization on Train set',verbose=self.verbose)\n",
    "        if line_by_line and max_length is not None and max_length<0: # line-by-line tokenization with no padding\n",
    "            self._do_transformation_tokenization_generator_fast()\n",
    "        else:\n",
    "            self._do_transformation_tokenization_generator()\n",
    "        self.verboseprint('Done')\n",
    "        \n",
    "        self._processed_call=True\n",
    "    \n",
    "    def set_data_collator(self,\n",
    "                          is_mlm=True, # Is this masked language model (True) or causal language model (False)\n",
    "                          mlm_prob=0.15, # Mask probability for masked language model\n",
    "                         ):\n",
    "        if not hasattr(self,'max_length'):\n",
    "            raise ValueError(\"Please call `process_and_tokenize' or `do_tokenization` to tokenize your dataset\")\n",
    "            \n",
    "        pad_to_multiple_of_8 = (self.max_length<0) # get data collator to pad when tokenizer does not apply padding\n",
    "        self.data_collator = DataCollatorForLanguageModeling(tokenizer=self.tokenizer,\n",
    "                                                             mlm=is_mlm,\n",
    "                                                             mlm_probability=mlm_prob,\n",
    "                                                             pad_to_multiple_of=8 if pad_to_multiple_of_8 else None\n",
    "                                                            )\n",
    "                                               \n",
    "    def prepare_test_dataset_from_raws(self,\n",
    "                                       content, # Either a single sentence, list of sentence or a dictionary with keys are metadata columns and values are list\n",
    "                                       do_tokenize=False, # Whether to tokenize text\n",
    "                                      ):\n",
    "        if len(self.metadatas) and not isinstance(content,dict):\n",
    "            raise ValueError(f'There is/are metadatas in the preprocessing step. Please include a dictionary including these keys for metadatas: {self.metadatas}, and texture content: {self.main_text}')\n",
    "            \n",
    "        _dic = {self.main_text:[content]} if isinstance(content,str) else content\n",
    "        for k in _dic.keys():\n",
    "            _dic[k] = val2iterable(_dic[k])\n",
    "        \n",
    "        test_dict = Dataset.from_dict(_dic)\n",
    "        \n",
    "        # set num_proc to 1 for small data processing\n",
    "        _tmp1 = self.num_proc\n",
    "        _tmp2 = self.tok_num_proc\n",
    "        self.num_proc=1\n",
    "        self.tok_num_proc=1\n",
    "        results = self.prepare_test_dataset(test_dict,do_tokenize)\n",
    "        self.num_proc = _tmp1\n",
    "        self.tok_num_proc=_tmp2\n",
    "        return results\n",
    "        \n",
    "    def prepare_test_dataset(self,\n",
    "                             test_dset, # The HuggingFace Dataset as Test set\n",
    "                             do_tokenize, # Whether to tokenize text\n",
    "                            ):\n",
    "            \n",
    "        test_cols = set(get_dset_col_names(test_dset))\n",
    "        missing_cols = set(self.cols_to_keep) - test_cols\n",
    "        if len(missing_cols):\n",
    "            raise ValueError(f'Test set does not have these columns required for preprocessings: {missing_cols}')\n",
    "            \n",
    "        print_msg('Start Test Set Transformation',20,verbose=self.verbose)\n",
    "        \n",
    "        # Process metadatas\n",
    "        test_dset = self._process_metadatas(test_dset)\n",
    "        \n",
    "        # Content transformation\n",
    "        test_dset = self._do_transformation(test_dset)\n",
    "        \n",
    "        # Drop unused columns\n",
    "        cols_to_remove = test_cols - set(self.cols_to_keep)\n",
    "        test_dset = test_dset.remove_columns(list(cols_to_remove))\n",
    "        \n",
    "        if do_tokenize:\n",
    "            print_msg('Tokenization',20,verbose=self.verbose)\n",
    "            tok_func = partial(tokenize_function,\n",
    "                           tok=self.tokenizer,\n",
    "                           max_length=self.max_length if self.line_by_line else -1,\n",
    "                           return_special_tokens_mask=True\n",
    "                          )\n",
    "            \n",
    "            _func = partial(lambda_map_batch,\n",
    "                        feature=self.main_text,\n",
    "                        func=tok_func,\n",
    "                        output_feature=None,\n",
    "                        is_batched=self.is_batched)\n",
    "            test_dset = hf_map_dset(test_dset,_func,self.is_batched,self.batch_size,self.tok_num_proc)\n",
    "            \n",
    "        self.verboseprint('Done')\n",
    "        return test_dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_lm_streaming.py#L18){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataLMControllerStreaming\n",
       "\n",
       ">      TextDataLMControllerStreaming (inp, main_text:str, filter_dict={},\n",
       ">                                     metadatas=[], process_metas=True,\n",
       ">                                     content_transformations=[], seed=None,\n",
       ">                                     batch_size=1024, num_proc=1,\n",
       ">                                     cols_to_keep=None, verbose=True)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | HuggingFainpce Dataset or DatasetDict |\n",
       "| main_text | str |  | Name of the main text column |\n",
       "| filter_dict | dict | {} | A dictionary: {feature: filtering_function_for_that_feature} |\n",
       "| metadatas | list | [] | Names of the metadata columns |\n",
       "| process_metas | bool | True | Whether to do simple text processing on the chosen metadatas |\n",
       "| content_transformations | list | [] | A list of text transformations |\n",
       "| seed | NoneType | None | Random seed |\n",
       "| batch_size | int | 1024 | Transformation + Tokenization batch size |\n",
       "| num_proc | int | 1 | Number of process for multiprocessing |\n",
       "| cols_to_keep | NoneType | None | Columns to keep after all processings |\n",
       "| verbose | bool | True | Whether to prdint processing information |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_lm_streaming.py#L18){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataLMControllerStreaming\n",
       "\n",
       ">      TextDataLMControllerStreaming (inp, main_text:str, filter_dict={},\n",
       ">                                     metadatas=[], process_metas=True,\n",
       ">                                     content_transformations=[], seed=None,\n",
       ">                                     batch_size=1024, num_proc=1,\n",
       ">                                     cols_to_keep=None, verbose=True)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | HuggingFainpce Dataset or DatasetDict |\n",
       "| main_text | str |  | Name of the main text column |\n",
       "| filter_dict | dict | {} | A dictionary: {feature: filtering_function_for_that_feature} |\n",
       "| metadatas | list | [] | Names of the metadata columns |\n",
       "| process_metas | bool | True | Whether to do simple text processing on the chosen metadatas |\n",
       "| content_transformations | list | [] | A list of text transformations |\n",
       "| seed | NoneType | None | Random seed |\n",
       "| batch_size | int | 1024 | Transformation + Tokenization batch size |\n",
       "| num_proc | int | 1 | Number of process for multiprocessing |\n",
       "| cols_to_keep | NoneType | None | Columns to keep after all processings |\n",
       "| verbose | bool | True | Whether to prdint processing information |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataLMControllerStreaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data + Basic use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset source: https://www.kaggle.com/datasets/kavita5/review_ecommerce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With line-by-line tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: IterableDataset({\n",
       "        features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'],\n",
       "        n_shards: 1\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'],\n",
       "        num_rows: 2349\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict_with_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc = TextDataLMControllerStreaming(ddict_with_val,\n",
    "                                    filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                    main_text='Review Text',\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_lm_streaming.py#L181){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataLMControllerStreaming.process_and_tokenize\n",
       "\n",
       ">      TextDataLMControllerStreaming.process_and_tokenize (tokenizer,\n",
       ">                                                          max_length=None,\n",
       ">                                                          tok_num_proc=None,\n",
       ">                                                          line_by_line=True,\n",
       ">                                                          stride=None)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| max_length | NoneType | None | pad to model's allowed max length (default is max_sequence_length). Use -1 for no padding at all |\n",
       "| tok_num_proc | NoneType | None | Number of processes for tokenization |\n",
       "| line_by_line | bool | True | To whether tokenize each sentence separately, or concatenate them |\n",
       "| stride | NoneType | None | option to do striding when line_by_line is False |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_lm_streaming.py#L181){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataLMControllerStreaming.process_and_tokenize\n",
       "\n",
       ">      TextDataLMControllerStreaming.process_and_tokenize (tokenizer,\n",
       ">                                                          max_length=None,\n",
       ">                                                          tok_num_proc=None,\n",
       ">                                                          line_by_line=True,\n",
       ">                                                          stride=None)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| max_length | NoneType | None | pad to model's allowed max length (default is max_sequence_length). Use -1 for no padding at all |\n",
       "| tok_num_proc | NoneType | None | Number of processes for tokenization |\n",
       "| line_by_line | bool | True | To whether tokenize each sentence separately, or concatenate them |\n",
       "| stride | NoneType | None | option to do striding when line_by_line is False |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataLMControllerStreaming.process_and_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Data Filtering --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f081ebfd4384914b451a1493781e79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "----- Performing Content Transformation and Tokenization on Validation Set -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383188fc105e406f9ced13f64effca76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Creating a generator for content transformation and tokenization on Train set -----\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc.process_and_tokenize(tokenizer,line_by_line=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: IterableDataset({\n",
       "        features: Unknown,\n",
       "        n_shards: 1\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 2270\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ids: [0, 243, 21, 657, 23, 78, 6112, 13, 162, 328, 939, 524, 4716, 1459, 195, 108, 176, 59, 12312, 23246, 8, 5, 650, 1006, 157, 13, 162, 4, 939, 2333, 645, 3023, 29, 11, 144, 1964, 11, 6215, 6, 53, 142, 51, 129, 56, 10, 650, 314, 11, 5, 1400, 6, 939, 802, 24, 74, 173, 25, 939, 109, 101, 127, 23734, 10, 410, 2671, 4, 77, 939, 1381, 24, 15, 6, 24, 21, 5, 1969, 1836, 328, 101, 5, 97, 37102, 26, 6, 939, 115, 192, 596, 24, 189, 45, 173, 13, 167, 19, 10, 35682, 7050, 443, 4, 939, 524, 15, 5, 2735, 526, 11, 14, 443, 8, 5, 754, 14, 939, 439, 62, 7, 10, 650, 1386, 9, 3023, 29, 189, 2489, 705, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Decoded: <s>It was love at first sight for me! i am petite 5'2 about 115 lbs and the small worked well for me. i usually order xs in most items in retailer, but because they only had a small left in the store, i thought it would work as i do like my coats a little bigger. when i tried it on, it was the perfect size! like the other reviewer said, i could see why it may not work for those with a fuller chest area. i am on the smaller side in that area and the fact that i went up to a small instead of xs may hav</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(tdc.main_ddict['validation']):\n",
    "    if i==1:break\n",
    "    print(f\"Input ids: {v['input_ids']}\\nDecoded: {tokenizer.decode(v['input_ids'])}\\nAttention Mask: {v['attention_mask']}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1fc7e9aaca49869459eb6aff661364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1024 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ids: [0, 713, 16, 30, 444, 5, 847, 990, 6, 144, 18842, 65, 2125, 939, 33, 655, 1381, 15, 328, 24, 10698, 6683, 8, 16, 34203, 11, 5, 235, 2127, 6, 24, 630, 75, 311, 350, 203, 12479, 25571, 8, 16, 6473, 615, 13, 284, 1061, 4, 3668, 657, 42, 3235, 11, 13504, 4, 938, 75, 11, 657, 19, 5, 1079, 9, 5, 8117, 6, 2818, 51, 283, 66, 11, 55, 2705, 8089, 23090, 1010, 328, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Decoded: <s>This is by far the cutest, most glamorous one piece i have ever tried on! it fits perfectly and is flattering in the right places, it doesn't show too much cleavage and is modest enough for family events. absolutely love this suit in navy. wasn't in love with the rest of the patterns, hoping they come out in more solid colors someday soon!</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    if i==1:break\n",
    "    print(f\"Input ids: {v['input_ids']}\\n\\nDecoded: {tokenizer.decode(v['input_ids'])}\\n\\nAttention Mask: {v['attention_mask']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering + Metadatas + Content Transformation + Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "from underthesea import text_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from that_nlp_library.text_main_lm import TextDataLMController"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Tokenize our corpus line-by-line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With no padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc = TextDataLMControllerStreaming(ddict_with_val,\n",
    "                                    main_text='Review Text',\n",
    "                                    filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                    metadatas=['Title','Division Name'],\n",
    "                                    content_transformations=[text_normalize,str.lower],\n",
    "                                    cols_to_keep=['Clothing ID','Review Text'],\n",
    "                                    seed=42,\n",
    "                                    batch_size=1024,\n",
    "                                    verbose=False\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.process_and_tokenize(tokenizer,line_by_line=True,max_length=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>general petite. beautiful top, worth the necessary tailoring. the beautiful bold print drew me to this top and it did not disappoint upon receipt. however, the bottom ruffle belled so far out on each side that it was laughable! the actual fit is nothing like the picture ; clearly the model's arms are placed in front of all the extra fabric to hold the ruffle back. however, the fabric is beautiful, the fit was perfect ( size 2, 5'4 \", 106 lbs. ), the quality is great and i love the print so i decided to take it to my tailor to \" sew away \" the \" wings \" on both si</s>\n",
      "\n",
      "<s>general. soft, feminine and fun pockets!. i love this tunic. purchased the dark orange in medium ( i am 5'9 and 140 lbs ). tried the small and almost kept it but i felt seams around my arm pits a tad, so went with the medium and glad i did - this top should be comfortable. feels very fall and perfect for casual get-togethers and running around town. only comment is that it is rayon... and for me anyway rayon doesn't wash too well - so we shall see how this one fairs.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(next(iter(tdc.main_ddict['train']))['input_ids']))\n",
    "print()\n",
    "print(tokenizer.decode(tdc.main_ddict['validation']['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "CPU times: user 961 ms, sys: 95 µs, total: 961 ms\n",
      "Wall time: 954 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    if i%100==0:\n",
    "        print(i)\n",
    "    if i==1024-1:\n",
    "        break\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to non-streamed version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset2 = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val2 = dset2.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val2['validation'] = ddict_with_val2['test']\n",
    "del ddict_with_val2['test']\n",
    "\n",
    "tdc2 = TextDataLMController(ddict_with_val2,\n",
    "                            main_text='Review Text',\n",
    "                            filter_dict={'Review Text': lambda x: x is not None},\n",
    "                            metadatas=['Title','Division Name'],\n",
    "                            content_transformations=[text_normalize,str.lower],\n",
    "                            cols_to_keep=['Clothing ID','Review Text'],\n",
    "                            seed=42,\n",
    "                            batch_size=1024,\n",
    "                            verbose=False\n",
    "                            )\n",
    "tdc2.process_and_tokenize(tokenizer,line_by_line=True,max_length=-1,shuffle_trn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether train sets are the same\n",
    "assert len(list(tdc.main_ddict['train']))==len(tdc2.main_ddict['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter1 = iter(tdc.main_ddict['train'])\n",
    "iter2 = iter(tdc2.main_ddict['train'])\n",
    "for a,b in zip(iter1,iter2):\n",
    "    assert a['input_ids']==b['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Clothing ID': 1056, 'Review Text': 'general . perfect pant . I picked these up the other day looking for a good jeans alternative. i love them. they are the perfect fit of slim but not skinny. i went with my normal size (26) and so far after one wear, they are still in good shape. a little bit of stretch, but not too much. the moss color is so crisp and goes with a lot. they will be perfect for transitioning into fall.', 'input_ids': [0, 15841, 479, 1969, 16259, 479, 939, 2738, 209, 62, 5, 97, 183, 546, 13, 10, 205, 10844, 3626, 479, 939, 657, 106, 479, 51, 32, 5, 1969, 2564, 9, 11875, 53, 45, 22877, 479, 939, 439, 19, 127, 2340, 1836, 36, 973, 4839, 8, 98, 444, 71, 65, 3568, 2156, 51, 32, 202, 11, 205, 3989, 479, 10, 410, 828, 9, 4140, 2156, 53, 45, 350, 203, 479, 5, 40711, 3195, 16, 98, 17766, 8, 1411, 19, 10, 319, 479, 51, 40, 28, 1969, 13, 26135, 88, 1136, 479, 2], 'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "--------------------\n",
      "{'Clothing ID': 1056, 'Review Text': 'general . perfect pant . i picked these up the other day looking for a good jeans alternative . i love them . they are the perfect fit of slim but not skinny . i went with my normal size ( 26 ) and so far after one wear , they are still in good shape . a little bit of stretch , but not too much . the moss color is so crisp and goes with a lot . they will be perfect for transitioning into fall .', 'input_ids': [0, 15841, 479, 1969, 16259, 479, 939, 2738, 209, 62, 5, 97, 183, 546, 13, 10, 205, 10844, 3626, 479, 939, 657, 106, 479, 51, 32, 5, 1969, 2564, 9, 11875, 53, 45, 22877, 479, 939, 439, 19, 127, 2340, 1836, 36, 973, 4839, 8, 98, 444, 71, 65, 3568, 2156, 51, 32, 202, 11, 205, 3989, 479, 10, 410, 828, 9, 4140, 2156, 53, 45, 350, 203, 479, 5, 40711, 3195, 16, 98, 17766, 8, 1411, 19, 10, 319, 479, 51, 40, 28, 1969, 13, 26135, 88, 1136, 479, 2], 'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print('-'*20)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether validation set is the same\n",
    "assert len(list(tdc.main_ddict['validation']))==len(tdc2.main_ddict['validation'])\n",
    "\n",
    "iter1 = iter(tdc.main_ddict['validation'])\n",
    "iter2 = iter(tdc2.main_ddict['validation'])\n",
    "for a,b in zip(iter1,iter2):\n",
    "    assert a==b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With padding \n",
    "\n",
    "(set `max_length` to `None` if you want to pad to model's maximum sequence length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Data Filtering --------------------\n",
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "----- Performing Content Transformation and Tokenization on Validation Set -----\n",
      "Done\n",
      "----- Creating a generator for content transformation and tokenization on Train set -----\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']\n",
    "\n",
    "tdc = TextDataLMControllerStreaming(ddict_with_val,\n",
    "                                    main_text='Review Text',\n",
    "                                    filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                    metadatas=['Title','Division Name'],\n",
    "                                    content_transformations=[text_normalize,str.lower],\n",
    "                                    cols_to_keep=['Clothing ID','Review Text'],\n",
    "                                    seed=42,\n",
    "                                    batch_size=1024,\n",
    "                                    verbose=True\n",
    "                                    )\n",
    "tdc.process_and_tokenize(tokenizer,line_by_line=True,max_length=256,tok_num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e66e492525d046ee80d5d245ba18a06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1024 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01fc8e2777c48faab555b5864d611fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1024 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72145bcf74bc4687866dcc8f827f2633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1024 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>general petite. beautiful top, worth the necessary tailoring. the beautiful bold print drew me to this top and it did not disappoint upon receipt. however, the bottom ruffle belled so far out on each side that it was laughable! the actual fit is nothing like the picture ; clearly the model's arms are placed in front of all the extra fabric to hold the ruffle back. however, the fabric is beautiful, the fit was perfect ( size 2, 5'4 \", 106 lbs. ), the quality is great and i love the print so i decided to take it to my tailor to \" sew away \" the \" wings \" on both si</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "\n",
      "<s>general. soft, feminine and fun pockets!. i love this tunic. purchased the dark orange in medium ( i am 5'9 and 140 lbs ). tried the small and almost kept it but i felt seams around my arm pits a tad, so went with the medium and glad i did - this top should be comfortable. feels very fall and perfect for casual get-togethers and running around town. only comment is that it is rayon... and for me anyway rayon doesn't wash too well - so we shall see how this one fairs.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(next(iter(tdc.main_ddict['train']))['input_ids']))\n",
    "print()\n",
    "print(tokenizer.decode(tdc.main_ddict['validation']['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to non-streamed version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset2 = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val2 = dset2.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val2['validation'] = ddict_with_val2['test']\n",
    "del ddict_with_val2['test']\n",
    "\n",
    "tdc2 = TextDataLMController(ddict_with_val2,\n",
    "                            main_text='Review Text',\n",
    "                            filter_dict={'Review Text': lambda x: x is not None},\n",
    "                            metadatas=['Title','Division Name'],\n",
    "                            content_transformations=[text_normalize,str.lower],\n",
    "                            cols_to_keep=['Clothing ID','Review Text'],\n",
    "                            seed=42,\n",
    "                            batch_size=1024,\n",
    "                            verbose=False\n",
    "                            )\n",
    "tdc2.process_and_tokenize(tokenizer,line_by_line=True,max_length=256,shuffle_trn=False,tok_num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether train sets are the same\n",
    "assert len(list(tdc.main_ddict['train']))==len(tdc2.main_ddict['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter1 = iter(tdc.main_ddict['train'])\n",
    "iter2 = iter(tdc2.main_ddict['train'])\n",
    "for a,b in zip(iter1,iter2):\n",
    "    assert a==b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether validation set is the same\n",
    "assert len(list(tdc.main_ddict['validation']))==len(tdc2.main_ddict['validation'])\n",
    "\n",
    "iter1 = iter(tdc.main_ddict['validation'])\n",
    "iter2 = iter(tdc2.main_ddict['validation'])\n",
    "for a,b in zip(iter1,iter2):\n",
    "    assert a==b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Tokenize every text, then concatenate them together before splitting them in smaller parts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']\n",
    "\n",
    "tdc = TextDataLMControllerStreaming(ddict_with_val,\n",
    "                                    main_text='Review Text',\n",
    "                                    filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                    metadatas=['Title','Division Name'],\n",
    "                                    content_transformations=[text_normalize,str.lower],\n",
    "                                    seed=42,\n",
    "                                    batch_size=1024,\n",
    "                                    verbose=False\n",
    "                                    )\n",
    "tdc.process_and_tokenize(tokenizer,line_by_line=False,max_length=256,tok_num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>general petite. beautiful top, worth the necessary tailoring. the beautiful bold print drew me to this top and it did not disappoint upon receipt. however, the bottom ruffle belled so far out on each side that it was laughable! the actual fit is nothing like the picture ; clearly the model's arms are placed in front of all the extra fabric to hold the ruffle back. however, the fabric is beautiful, the fit was perfect ( size 2, 5'4 \", 106 lbs. ), the quality is great and i love the print so i decided to take it to my tailor to \" sew away \" the \" wings \" on both si</s><s>general. not as short on me ( petite ). i ordered the xxs p as this dress is not a fitted dress, and that was the right size for me. only thing is the length is a bit linger still 9 lower on calf for me ), the straps are almost tight, so i would say the dress is a reversed taper shape. color is beautiful, i ordered green as the other color ( plum ) doesn't have petite available. green is rich, and classy, the fabric is surprisingly soft. i love the little details in the velvet. definitely need a\n",
      "\n",
      "<s>general. soft, feminine and fun pockets!. i love this tunic. purchased the dark orange in medium ( i am 5'9 and 140 lbs ). tried the small and almost kept it but i felt seams around my arm pits a tad, so went with the medium and glad i did - this top should be comfortable. feels very fall and perfect for casual get-togethers and running around town. only comment is that it is rayon... and for me anyway rayon doesn't wash too well - so we shall see how this one fairs.</s><s>general petite. a new staple!. tried these on out of sheer curiosity -- i've got a long torso & was pleasantly surprised how flattering they are! they manage to look flowing & sleek without shortening the legs. took a size 6 with my 27 \" waist, 37 \" hips. it's a bit of a generous fit, especially around the waist, but they're extremely comfortable & have room to tuck tops into. i have the cowled sweater tank in gray & it looks fantastic over these! couldn't resist getting both the rust and black. perfect for a dressy casual look</s><s>general. maybe swing is for me!. i love swing dresses but they never seem\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(next(iter(tdc.main_ddict['train']))['input_ids']))\n",
    "print()\n",
    "print(tokenizer.decode(tdc.main_ddict['validation']['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "CPU times: user 11.8 s, sys: 23.9 ms, total: 11.8 s\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    if i%100==0:\n",
    "        print(i)\n",
    "    if i==1024-1:\n",
    "        break\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to non-streamed version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset2 = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val2 = dset2.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val2['validation'] = ddict_with_val2['test']\n",
    "del ddict_with_val2['test']\n",
    "\n",
    "tdc2 = TextDataLMController(ddict_with_val2,\n",
    "                            main_text='Review Text',\n",
    "                            filter_dict={'Review Text': lambda x: x is not None},\n",
    "                            metadatas=['Title','Division Name'],\n",
    "                            content_transformations=[text_normalize,str.lower],\n",
    "                            seed=42,\n",
    "                            batch_size=1024,\n",
    "                            verbose=False\n",
    "                            )\n",
    "tdc2.process_and_tokenize(tokenizer,line_by_line=False,max_length=256,shuffle_trn=False,tok_num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether train sets are the same\n",
    "assert len(list(tdc.main_ddict['train']))==len(tdc2.main_ddict['train'])\n",
    "\n",
    "iter1 = iter(tdc.main_ddict['train'])\n",
    "iter2 = iter(tdc2.main_ddict['train'])\n",
    "for a,b in zip(iter1,iter2):\n",
    "    assert a==b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether validation set is the same\n",
    "assert len(list(tdc.main_ddict['validation']))==len(tdc2.main_ddict['validation'])\n",
    "\n",
    "iter1 = iter(tdc.main_ddict['validation'])\n",
    "iter2 = iter(tdc2.main_ddict['validation'])\n",
    "for a,b in zip(iter1,iter2):\n",
    "    assert a==b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Striding (For Concatenation of tokens)\n",
    "\n",
    "If your sentences (or paragraphs) are larger than `max_length`, after concatenation, they will be broken apart; your long paragraph will be incompleted in terms of meaning. **Striding** is a way to somewhat preserve the sentence's meaning, by getting part of the sentence back. We will demonstrate it with an example, and you can compare it with the previous one (without striding) to see the differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']\n",
    "\n",
    "tdc = TextDataLMControllerStreaming(ddict_with_val,\n",
    "                                    main_text='Review Text',\n",
    "                                    filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                    metadatas=['Title','Division Name'],\n",
    "                                    content_transformations=[text_normalize,str.lower],\n",
    "                                    seed=42,\n",
    "                                    batch_size=1024,\n",
    "                                    verbose=False\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.process_and_tokenize(tokenizer,line_by_line=False,max_length=100,stride=20,tok_num_proc=1)\n",
    "# Stride is 20, meaning for the next entry, we go back 20 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>general petite. beautiful top, worth the necessary tailoring. the beautiful bold print drew me to this top and it did not disappoint upon receipt. however, the bottom ruffle belled so far out on each side that it was laughable! the actual fit is nothing like the picture ; clearly the model's arms are placed in front of all the extra fabric to hold the ruffle back. however, the fabric is beautiful, the fit was perfect ( size 2, 5'4 \",\n",
      "--------------------\n",
      " however, the fabric is beautiful, the fit was perfect ( size 2, 5'4 \", 106 lbs. ), the quality is great and i love the print so i decided to take it to my tailor to \" sew away \" the \" wings \" on both si</s><s>general. not as short on me ( petite ). i ordered the xxs p as this dress is not a fitted dress, and that was the right size for me. only thing is the length is a\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    if i==2: break\n",
    "    print(tokenizer.decode(v['input_ids']))\n",
    "    print('-'*20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>general. soft, feminine and fun pockets!. i love this tunic. purchased the dark orange in medium ( i am 5'9 and 140 lbs ). tried the small and almost kept it but i felt seams around my arm pits a tad, so went with the medium and glad i did - this top should be comfortable. feels very fall and perfect for casual get-togethers and running around town. only comment is that it is rayon... and for me anyway rayon doesn\n",
      " running around town. only comment is that it is rayon... and for me anyway rayon doesn't wash too well - so we shall see how this one fairs.</s><s>general petite. a new staple!. tried these on out of sheer curiosity -- i've got a long torso & was pleasantly surprised how flattering they are! they manage to look flowing & sleek without shortening the legs. took a size 6 with my 27 \" waist, 37 \" hips. it's a bit\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tdc.main_ddict['validation']['input_ids'][0]))\n",
    "print(tokenizer.decode(tdc.main_ddict['validation']['input_ids'][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second entry, we can see it starts with the last 20 tokens of the previous entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to non-streamed version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset2 = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val2 = dset2.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val2['validation'] = ddict_with_val2['test']\n",
    "del ddict_with_val2['test']\n",
    "\n",
    "tdc2 = TextDataLMController(ddict_with_val2,\n",
    "                            main_text='Review Text',\n",
    "                            filter_dict={'Review Text': lambda x: x is not None},\n",
    "                            metadatas=['Title','Division Name'],\n",
    "                            content_transformations=[text_normalize,str.lower],\n",
    "                            seed=42,\n",
    "                            batch_size=1024,\n",
    "                            verbose=False\n",
    "                            )\n",
    "tdc2.process_and_tokenize(tokenizer,line_by_line=False,max_length=100,shuffle_trn=False,\n",
    "                          stride=20,tok_num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether train sets are the same\n",
    "assert len(list(tdc.main_ddict['train']))==len(tdc2.main_ddict['train'])\n",
    "\n",
    "iter1 = iter(tdc.main_ddict['train'])\n",
    "iter2 = iter(tdc2.main_ddict['train'])\n",
    "for a,b in zip(iter1,iter2):\n",
    "    assert a==b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether validation set is the same\n",
    "assert len(list(tdc.main_ddict['validation']))==len(tdc2.main_ddict['validation'])\n",
    "\n",
    "iter1 = iter(tdc.main_ddict['validation'])\n",
    "iter2 = iter(tdc2.main_ddict['validation'])\n",
    "for a,b in zip(iter1,iter2):\n",
    "    assert a==b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import text_normalize\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For masked language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our text controller first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']\n",
    "\n",
    "tdc = TextDataLMControllerStreaming(ddict_with_val,\n",
    "                                    main_text='Review Text',\n",
    "                                    filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                    metadatas=['Title','Division Name'],\n",
    "                                    content_transformations=[text_normalize,str.lower],\n",
    "                                    cols_to_keep=['Clothing ID','Review Text'],\n",
    "                                    seed=42,\n",
    "                                    batch_size=1024,\n",
    "                                    verbose=False\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will tokenize our corpus line-by-line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.process_and_tokenize(tokenizer,line_by_line=True,max_length=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.set_data_collator(is_mlm=True,mlm_prob=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForLanguageModeling(tokenizer=RobertaTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}, mlm=True, mlm_probability=0.15, pad_to_multiple_of=8, tf_experimental_compile=False, return_tensors='pt')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying the collator...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Clothing ID': 841, 'Review Text': 'general petite . beautiful top, worth the necessary tailoring . The beautiful bold print drew me to this top and it did not disappoint upon receipt. however, the bottom ruffle belled so far out on each side that it was laughable! the actual fit is nothing like the picture; clearly the model\\'s arms are placed in front of all the extra fabric to hold the ruffle back.\\r\\nhowever, the fabric is beautiful, the fit was perfect (size 2, 5\\'4\", 106 lbs.), the quality is great and i love the print so i decided to take it to my tailor to \"sew away\" the \"wings\" on both si', 'input_ids': [0, 15841, 4716, 1459, 479, 2721, 299, 2156, 966, 5, 2139, 7886, 5137, 479, 5, 2721, 7457, 5780, 4855, 162, 7, 42, 299, 8, 24, 222, 45, 17534, 2115, 18245, 479, 959, 2156, 5, 2576, 910, 15315, 28, 9970, 98, 444, 66, 15, 349, 526, 14, 24, 21, 38677, 27785, 5, 3031, 2564, 16, 1085, 101, 5, 2170, 25606, 2563, 5, 1421, 18, 3701, 32, 2325, 11, 760, 9, 70, 5, 1823, 10199, 7, 946, 5, 910, 15315, 124, 479, 959, 2156, 5, 10199, 16, 2721, 2156, 5, 2564, 21, 1969, 36, 1836, 132, 2156, 195, 128, 204, 22, 2156, 13442, 23246, 479, 4839, 2156, 5, 1318, 16, 372, 8, 939, 657, 5, 5780, 98, 939, 1276, 7, 185, 24, 7, 127, 26090, 7, 22, 35043, 409, 22, 5, 22, 11954, 22, 15, 258, 3391, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}\n",
      "Length of input_ids: 136\n",
      "--------------------\n",
      "{'Clothing ID': 1110, 'Review Text': \"general . not as short on me (petite) . I ordered the xxs p as this dress is not a fitted dress, and that was the right size for me. only thing is the length is a bit linger still 9lower on calf for me), the straps are almost tight, so i would say the dress is a reversed taper shape. color is beautiful, i ordered green as the other color (plum) doesn't have petite available. green is rich, and classy, the fabric is surprisingly soft. i love the little details in the velvet. definitely need a strapless bra for this one.\\r\\n\\r\\n115 lbsm 30d\", 'input_ids': [0, 15841, 479, 45, 25, 765, 15, 162, 36, 4716, 1459, 4839, 479, 939, 2740, 5, 37863, 29, 181, 25, 42, 3588, 16, 45, 10, 15898, 3588, 2156, 8, 14, 21, 5, 235, 1836, 13, 162, 479, 129, 631, 16, 5, 5933, 16, 10, 828, 18277, 202, 361, 795, 15, 16701, 13, 162, 4839, 2156, 5, 31622, 32, 818, 3229, 2156, 98, 939, 74, 224, 5, 3588, 16, 10, 13173, 326, 15888, 3989, 479, 3195, 16, 2721, 2156, 939, 2740, 2272, 25, 5, 97, 3195, 36, 36838, 4839, 630, 75, 33, 4716, 1459, 577, 479, 2272, 16, 4066, 2156, 8, 30228, 2156, 5, 10199, 16, 10262, 3793, 479, 939, 657, 5, 410, 1254, 11, 5, 29986, 479, 2299, 240, 10, 18052, 16979, 11689, 13, 42, 65, 479, 12312, 23246, 119, 389, 385, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}\n",
      "Length of input_ids: 133\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    if i==2: break\n",
    "    print(v)\n",
    "    print(f\"Length of input_ids: {len(v['input_ids'])}\")\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the length of each token list is different from each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only the required keys\n",
    "inp_keys = tokenizer.model_input_names\n",
    "iter1 = iter(tdc.main_ddict['train'])\n",
    "_result=[]\n",
    "for i in range(5):\n",
    "    _inp = next(iter1)\n",
    "    _result.append({k:_inp[k] for k in inp_keys})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tdc.data_collator(_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all token lists have the same length, which is a multiple of 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 136])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 15841,  4716,  1459,   479,  2721,   299,  2156,   966, 50264,\n",
       "          2139,  7886,  5137,   479,     5,  2721,  7457,  5780,  4855,   162,\n",
       "             7,    42,   299,     8, 50264,   222,    45, 17534,  2115, 50264,\n",
       "           479, 50264,  2156,     5,  2576,   910, 15315,    28,  9970,    98,\n",
       "           444,    66,    15,   349,   526,    14,    24,    21, 38677, 27785,\n",
       "         17138,  3031, 50264,    16,  1085,   101,     5,  2170, 25606,  2563,\n",
       "             5,  1421,    18,  3701,    32,  2325,    11,   760,     9,    70,\n",
       "             5,  1823, 10199, 50264, 29261, 50264,   910, 15315,   124,   479,\n",
       "           959,  2156,     5, 10199,    16,  2721,  2156, 50264,  2564,    21,\n",
       "          1969,    36,  1836,   132, 50264,   195,   128,   204,    22, 50264,\n",
       "         13442, 23246,   479, 50264,  2156,     5, 50264,    16, 50264,     8,\n",
       "           939,   657,     5, 50264,    98,   939,  1276, 50264,   185,    24,\n",
       "             7,   127, 26090,     7,    22, 35043,   409, 50264,     5,    22,\n",
       "         11954,    22, 50264,   258,  3391,     2],\n",
       "        [    0, 15841,   479,    45,    25, 13055,    15, 50264,    36,  4716,\n",
       "          1459,  4839,   479, 50264,  2740,     5, 37863, 50264,   181, 50264,\n",
       "            42,  3588,    16,    45,    10, 15898,  3588, 50264,     8,    14,\n",
       "            21,     5,   235,  1836,    13,   162,   479,   129,   631,    16,\n",
       "             5,  5933,    16, 38152,   828, 18277,   202,   361,   795,    15,\n",
       "         16701,    13,   162,  4839,  2156,     5, 31622,    32,   818,  3229,\n",
       "          2156,    98,   939,    74, 50264,     5,  3588,    16,    10, 13173,\n",
       "           326, 50264,  3989,   479,  3195,    16, 50264,  2156,   939,  2740,\n",
       "          2272,    25,     5,    97,  3195,    36, 36838,  4839, 50264,    75,\n",
       "            33,  4716,  1459, 50264, 50264,  2272,    16,  4066,  2156,     8,\n",
       "         30228, 50264, 50264, 50264,    16, 10262,  3793,   479,   939, 50264,\n",
       "             5,   410,  1254,    11,     5, 50264,   479,  2299,   240, 50264,\n",
       "         18052, 16979, 11689,    13,    42, 50264,   479, 12312, 23246,   119,\n",
       "           389,   385,     2,     1,     1,     1],\n",
       "        [    0, 15841,   479,  1969, 50264,    13,    80, 50264,   479, 15983,\n",
       "         10717, 39574,    47,   240, 50264,   216,    14,    42,  1907,     9,\n",
       "         50264, 50264,     5,    65,    14,    40,   120,  6538,    36,   939,\n",
       "          2162,     5, 50264,    65,  4839,   479,    24,    16,  2422,  7174,\n",
       "         50264,  9869,  2156,    53,   939,    21,   129,   441,     7,   120,\n",
       "            80, 15033,    66, 50264,    24, 50264, 50264,   222, 10397,    24,\n",
       "         50264, 50264,  4925, 50264,    18,  1836,   142,   939,  1079, 47904,\n",
       "            24, 50264, 50264,   172, 10601,     7,  3841,   479,   939,    21,\n",
       "          2422,  5779, 50264,     5,  3568,    53, 10874,   145,   441,     7,\n",
       "         50264,    24,   396,   864,    23, 50264, 50264,  6215,   479,     2,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['input_ids'][:3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `labels` have also been constructed, which shows the \"mask\" tokens (non -100) in which the model has to predict. To increase the amount of masked tokens, increase the `mlm_prob`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,     5,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,    24,  -100,  -100,  -100,  -100, 18245,\n",
       "          -100,   959,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "             5,  -100,  2564,  -100,  -100,  -100,  -100,  -100,  -100,  2563,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,     7,   946,     5,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,     5,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  2156,  -100,  -100,  -100,  -100,  2156,\n",
       "          -100,  -100,  -100,  4839,  -100,  -100,  1318,  -100,   372,  -100,\n",
       "          -100,  -100,  -100,  5780,  -100,  -100,  -100,     7,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,    22,  -100,  -100,\n",
       "          -100,  -100,    15,  -100,  -100,  -100],\n",
       "        [ -100, 15841,  -100,  -100,  -100,   765,  -100,   162,  -100,  -100,\n",
       "          -100,  -100,  -100,   939,  -100,  -100,  -100,    29,  -100,    25,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  2156,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "             5,  -100,  -100,    10,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,   224,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100, 15888,  -100,  -100,  -100,  -100,  2721,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   630,  -100,\n",
       "          -100,  -100,  -100,   577,   479,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  2156,     5, 10199,  -100,  -100,  -100,  -100,  -100,   657,\n",
       "          -100,  -100,  -100,  -100,  -100, 29986,  -100,  -100,  -100,    10,\n",
       "          -100,  -100,  -100,  -100,  -100,    65,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [ -100,  -100,  -100,  -100, 39574,  -100,  -100, 15033,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,     7,  -100,  -100,  -100,  -100,  -100,\n",
       "         10199,    16,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  1104,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "             8,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,     9,  -100,   479,   939,  -100,  -100,  -100,\n",
       "             8,    24,  -100,    24,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,   150,  7727,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,    59,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           671,  -100,  -100,  -100,  -100,   127,   400,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['labels'][:3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you apply padding in the tokenization step (by adjusting the `max_length` argument), no matter whether it's line-by-line tokenization or not, the data collator will skip the padding step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']\n",
    "\n",
    "tdc = TextDataLMControllerStreaming(ddict_with_val,\n",
    "                                    main_text='Review Text',\n",
    "                                    filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                    metadatas=['Title','Division Name'],\n",
    "                                    content_transformations=[text_normalize,str.lower],\n",
    "                                    cols_to_keep=['Clothing ID','Review Text'],\n",
    "                                    seed=42,\n",
    "                                    batch_size=1024,\n",
    "                                    verbose=False\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.process_and_tokenize(tokenizer,line_by_line=False,max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.set_data_collator(is_mlm=True,mlm_prob=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only the required keys\n",
    "inp_keys = tokenizer.model_input_names\n",
    "iter1 = iter(tdc.main_ddict['train'])\n",
    "_result=[]\n",
    "for i in range(5):\n",
    "    _inp = next(iter1)\n",
    "    _result.append({k:_inp[k] for k in inp_keys})\n",
    "        \n",
    "\n",
    "out = tdc.data_collator(_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 100])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 15841,  4716,  1459,   479,  2721,   299,  2156,   966, 50264,\n",
       "          2139,  7886,  5137,   479,     5,  2721,  7457,  5780,  4855,   162,\n",
       "             7,    42,   299,     8,    24,   222,    45, 17534,  2115, 50264,\n",
       "           479, 50264,  2156,     5,  2576,   910, 15315,    28,  9970,    98,\n",
       "           444,    66,    15,   349,   526,    14,    24,    21, 38677, 27785,\n",
       "         50264,  3031, 50264,    16,  1085,   101,     5,  2170, 25606, 41316,\n",
       "             5,  1421,    18,  3701,    32,  2325,    11,   760,     9,    70,\n",
       "             5,  1823, 10199, 50264, 17204, 50264,   910, 15315,   124,   479,\n",
       "           959,  2156,     5, 10199,    16,  2721,  2156, 50264,  2564,    21,\n",
       "          1969,    36,  1836,   132, 50264,   195,   128,   204,    22, 50264],\n",
       "        [13442, 23246,   479, 50264,  2156,     5, 50264,    16, 23781,     8,\n",
       "           939,   657,     5,  5780,    98,   939,  1276, 50264,   185,    24,\n",
       "             7,   127, 26090,     7,    22, 35043,   409, 50264,     5,    22,\n",
       "         11954,    22, 50264,   258,  3391,     2,     0, 50264,   479,    45,\n",
       "            25, 50264,    15, 44224,    36,  4716,  1459,  4839,   479, 50264,\n",
       "          2740,     5, 37863, 50264,   181,    25,    42,  3588,    16,    45,\n",
       "            10, 15898,  3588, 50264,     8,    14,    21,     5,   235,  1836,\n",
       "            13,   162,   479,   129,   631,    16, 50264,  5933,    16, 50264,\n",
       "           828, 18277,   202,   361,   795,    15, 16701,    13,   162,  4839,\n",
       "          2156,     5, 31622,    32,   818,  3229,  2156,    98,   939,    74]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['input_ids'][:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,     5,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,    24,  -100,  -100,  -100,  -100, 18245,\n",
       "          -100,   959,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "             5,  -100,  2564,  -100,  -100,  -100,  -100,  -100,  -100,  2563,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,     7,   946,     5,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,     5,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  2156,  -100,  -100,  -100,  -100,  2156],\n",
       "        [ -100,  -100,  -100,  4839,  -100,  -100,  1318,  -100,   372,  -100,\n",
       "          -100,  -100,  -100,  5780,  -100,  -100,  -100,     7,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,    22,  -100,  -100,\n",
       "          -100,  -100,    15,  -100,  -100,  -100,  -100, 15841,  -100,  -100,\n",
       "          -100,   765,  -100,   162,  -100,  -100,  -100,  -100,  -100,   939,\n",
       "          -100,  -100,  -100,    29,  -100,    25,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  2156,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,     5,  -100,  -100,    10,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['labels'][:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using the concatenation-of-tokenization technique, one smart thing that the HuggingFace's `DataCollatorForLanguageModeling` (which is the data collator we use) does is to allow maskings at every position, at opposed to to the previous cases (with line-by-line tokenization), there's no masking near the end tokens of each list, because those end tokens are padding tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For causal language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from tokenizers import processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our GPT2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT2 does not use start/end-of-sentence token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'Ġis', 'Ġa', 'Ġtext', '.', 'ĠThat', 'Ġis', 'Ġa', 'Ġsecond', 'Ġtext', '.', 'But', 'Ġthere', \"'s\", 'Ġa', 'Ġthird', 'Ġone']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(tokenizer(\"this is a text. That is a second text.But there's a third one\")['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to perform concatenation-of-token, and you want your causal LM to differentiate between sentences, you can add a special token to separate sentences, as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"$A \" + tokenizer.eos_token,\n",
    "    special_tokens=[(tokenizer.eos_token, tokenizer.eos_token_id)],\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'Ġis', 'Ġa', 'Ġtext', '.', 'ĠThat', 'Ġis', 'Ġa', 'Ġsecond', 'Ġtext', '.', 'But', 'Ġthere', \"'s\", 'Ġa', 'Ġthird', 'Ġone', '<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(tokenizer(\"this is a text. That is a second text.But there's a third one\")['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this modified tokenizer, let's perform concatenation-of-tokenization using GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']\n",
    "\n",
    "tdc = TextDataLMControllerStreaming(ddict_with_val,\n",
    "                                    main_text='Review Text',\n",
    "                                    filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                    metadatas=['Title','Division Name'],\n",
    "                                    content_transformations=[text_normalize,str.lower],\n",
    "                                    seed=42,\n",
    "                                    batch_size=1024,\n",
    "                                    verbose=False\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.process_and_tokenize(tokenizer,line_by_line=False,max_length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's casual language modeling, let's turn off `is_mlm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.set_data_collator(is_mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "iter1 = iter(tdc.main_ddict['train'])\n",
    "out = tdc.data_collator([next(iter1) for i in range(5)]) # simulation with batch size 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 100])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[24622,  4273,   578,   764,  4950,  1353,   837,  2861,   262,  3306,\n",
       "          7894,  3255,   764,   262,  4950, 10758,  3601,  9859,   502,   284,\n",
       "           428,  1353,   290,   340,   750,   407,  6703,  2402, 14507,   764,\n",
       "          2158,   837,   262,  4220,   374, 18137,   307,  3353,   523,  1290,\n",
       "           503,   319,  1123,  1735,   326,   340,   373, 47623,  5145,   262,\n",
       "          4036,  4197,   318,  2147,   588,   262,  4286,  2162,  4084,   262,\n",
       "          2746,   338,  5101,   389,  4624,   287,  2166,   286,   477,   262,\n",
       "          3131,  9664,   284,  1745,   262,   374, 18137,   736,   764,  2158,\n",
       "           837,   262,  9664,   318,  4950,   837,   262,  4197,   373,  2818,\n",
       "           357,  2546,   362,   837,   642,   705,   604,   366,   837, 15696],\n",
       "        [15785,   764,  1267,   837,   262,  3081,   318,  1049,   290,  1312,\n",
       "          1842,   262,  3601,   523,  1312,  3066,   284,  1011,   340,   284,\n",
       "           616, 35280,   284,   366, 34249,  1497,   366,   262,   366, 12098,\n",
       "           366,   319,  1111, 33721, 50256, 24622,   764,   407,   355,  1790,\n",
       "           319,   502,   357,  4273,   578,  1267,   764,  1312,  6149,   262,\n",
       "         31383,    82,   279,   355,   428,  6576,   318,   407,   257, 18235,\n",
       "          6576,   837,   290,   326,   373,   262,   826,  2546,   329,   502,\n",
       "           764,   691,  1517,   318,   262,  4129,   318,   257,  1643, 31402,\n",
       "           991,   860,  2793,   319, 31134,   329,   502,  1267,   837,   262,\n",
       "         29552,   389,  2048,  5381,   837,   523,  1312,   561,   910,   262]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['input_ids'][:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[24622,  4273,   578,   764,  4950,  1353,   837,  2861,   262,  3306,\n",
       "          7894,  3255,   764,   262,  4950, 10758,  3601,  9859,   502,   284,\n",
       "           428,  1353,   290,   340,   750,   407,  6703,  2402, 14507,   764,\n",
       "          2158,   837,   262,  4220,   374, 18137,   307,  3353,   523,  1290,\n",
       "           503,   319,  1123,  1735,   326,   340,   373, 47623,  5145,   262,\n",
       "          4036,  4197,   318,  2147,   588,   262,  4286,  2162,  4084,   262,\n",
       "          2746,   338,  5101,   389,  4624,   287,  2166,   286,   477,   262,\n",
       "          3131,  9664,   284,  1745,   262,   374, 18137,   736,   764,  2158,\n",
       "           837,   262,  9664,   318,  4950,   837,   262,  4197,   373,  2818,\n",
       "           357,  2546,   362,   837,   642,   705,   604,   366,   837, 15696],\n",
       "        [15785,   764,  1267,   837,   262,  3081,   318,  1049,   290,  1312,\n",
       "          1842,   262,  3601,   523,  1312,  3066,   284,  1011,   340,   284,\n",
       "           616, 35280,   284,   366, 34249,  1497,   366,   262,   366, 12098,\n",
       "           366,   319,  1111, 33721,  -100, 24622,   764,   407,   355,  1790,\n",
       "           319,   502,   357,  4273,   578,  1267,   764,  1312,  6149,   262,\n",
       "         31383,    82,   279,   355,   428,  6576,   318,   407,   257, 18235,\n",
       "          6576,   837,   290,   326,   373,   262,   826,  2546,   329,   502,\n",
       "           764,   691,  1517,   318,   262,  4129,   318,   257,  1643, 31402,\n",
       "           991,   860,  2793,   319, 31134,   329,   502,  1267,   837,   262,\n",
       "         29552,   389,  2048,  5381,   837,   523,  1312,   561,   910,   262]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['labels'][:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CLM, the `labels` are essentially the same as `input_ids`. From HuggingFace documentation:\n",
    "```\n",
    "`DataCollatorForLanguageModeling` will take care of creating the language model labels — in causal language modeling the inputs serve as labels too (just shifted by one element), and this data collator creates them on the fly during training.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load TextDataController"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_lm_streaming.py#L57){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataLMControllerStreaming.save_as_pickles\n",
       "\n",
       ">      TextDataLMControllerStreaming.save_as_pickles (fname,\n",
       ">                                                     parent='pickle_files')\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_lm_streaming.py#L57){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataLMControllerStreaming.save_as_pickles\n",
       "\n",
       ">      TextDataLMControllerStreaming.save_as_pickles (fname,\n",
       ">                                                     parent='pickle_files')\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataLMControllerStreaming.save_as_pickles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.from_pickle\n",
       "\n",
       ">      TextDataControllerStreaming.from_pickle (fname, parent='pickle_files')\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.from_pickle\n",
       "\n",
       ">      TextDataControllerStreaming.from_pickle (fname, parent='pickle_files')\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataLMControllerStreaming.from_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextDataLMControllerStreaming object can be saved and loaded with ease. This is especially useful after text processing and/or tokenization have been done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import disable_caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']\n",
    "\n",
    "tdc = TextDataLMControllerStreaming(ddict_with_val,\n",
    "                                    main_text='Review Text',\n",
    "                                    filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                    metadatas=['Title','Division Name'],\n",
    "                                    content_transformations=[text_normalize,str.lower],\n",
    "                                    seed=42,\n",
    "                                    batch_size=1024,\n",
    "                                    verbose=False\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.process_and_tokenize(tokenizer,line_by_line=True,max_length=-1)\n",
    "\n",
    "tdc.set_data_collator(is_mlm=True,mlm_prob=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.save_as_pickles('my_lm_tdc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load back our object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc2 = TextDataLMController.from_pickle('my_lm_tdc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can still access all its attributes, data, preprocessings, transformations ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: IterableDataset({\n",
       "        features: Unknown,\n",
       "        n_shards: 1\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Title', 'Review Text', 'Division Name', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 2253\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc2.main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Review Text': <function __main__.<lambda>(x)>},\n",
       " [<function underthesea.pipeline.text_normalize.text_normalize(text, tokenizer='underthesea')>,\n",
       "  <method 'lower' of 'str' objects>])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc2.filter_dict,tdc2.content_tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
