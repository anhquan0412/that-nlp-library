{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Main For Language Model - Streaming\n",
    "\n",
    "> This module contains the main Python class for the **streaming** version of `TextDataLMController`\n",
    "\n",
    "\n",
    "- skip_showdoc: true\n",
    "- skip_exec: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp text_main_lm_streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from datasets import DatasetDict,Dataset,IterableDataset\n",
    "from pathlib import Path\n",
    "from that_nlp_library.utils import *\n",
    "from that_nlp_library.text_main import tokenize_function\n",
    "from that_nlp_library.text_main_streaming import *\n",
    "from functools import partial\n",
    "import warnings\n",
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from that_nlp_library.text_transformation import *\n",
    "from that_nlp_library.text_augmentation import *\n",
    "from importlib.machinery import SourceFileLoader\n",
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class TextDataLMControllerStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TextDataLMControllerStreaming(TextDataControllerStreaming):\n",
    "    def __init__(self,\n",
    "                 inp, # HuggingFainpce Dataset or DatasetDict\n",
    "                 main_text:str, # Name of the main text column\n",
    "                 filter_dict={}, # A dictionary: {feature: filtering_function_for_that_feature}\n",
    "                 metadatas=[], # Names of the metadata columns\n",
    "                 process_metas=True, # Whether to do simple text processing on the chosen metadatas\n",
    "                 content_transformations=[], # A list of text transformations\n",
    "                 seed=None, # Random seed\n",
    "                 batch_size=1024, # Transformation + Tokenization batch size\n",
    "                 num_proc=1, # Number of process for multiprocessing\n",
    "                 cols_to_keep=None, # Columns to keep after all processings\n",
    "                 verbose=True, # Whether to prdint processing information\n",
    "                ):\n",
    "        \n",
    "        super().__init__(inp=inp,\n",
    "                         main_text=main_text,\n",
    "                         filter_dict=filter_dict,\n",
    "                         metadatas=metadatas,\n",
    "                         process_metas=process_metas,\n",
    "                         content_transformations=content_transformations,\n",
    "                         seed=seed,\n",
    "                         batch_size=batch_size,\n",
    "                         num_proc=num_proc,\n",
    "                         cols_to_keep=cols_to_keep,\n",
    "                         verbose=verbose\n",
    "                        )\n",
    "            \n",
    "    \n",
    "    def _do_label_transformation(self):\n",
    "        raise NotImplementedError(\"There's no classification/regression label in text processing for Language Model\")\n",
    "        \n",
    "    def _encode_labels(self):\n",
    "        raise NotImplementedError(\"There's no classification/regression label in text processing for Language Model\")\n",
    "    \n",
    "    def _do_transformation_augmentation_tokenization(self):\n",
    "        raise NotImplementedError(\"There's no augmentation in text processing for Language Model\")\n",
    "\n",
    "\n",
    "    def save_as_pickles(self,\n",
    "                        fname, # Name of the pickle file\n",
    "                        parent='pickle_files', # Parent folder\n",
    "                       ):\n",
    "        \n",
    "        save_to_pickle(self,fname,parent=parent)\n",
    "        \n",
    "    def _group_texts_with_stride(self,examples):\n",
    "        max_length = self.max_length\n",
    "        if max_length is None: \n",
    "            max_length = self.tokenizer.model_max_length\n",
    "        stride = self.stride\n",
    "        if stride is None: stride=max_length\n",
    "        else: stride = max_length-stride\n",
    "        if stride==0: raise ValueError(f'Stride cannot be equal to max length of {max_length}')\n",
    "            \n",
    "        # Concatenate all texts.\n",
    "        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        result_all={}\n",
    "        for k,t in concatenated_examples.items():\n",
    "            result=[]\n",
    "            i=0\n",
    "            while i+max_length<=total_length:\n",
    "                result.append(t[i:i+max_length])\n",
    "                i+=stride\n",
    "            result_all[k]=result\n",
    "        \n",
    "        return result_all  \n",
    "    \n",
    "    \n",
    "    def _do_transformation_tokenization(self,dtrain):             \n",
    "        if len(self.content_tfms):            \n",
    "            for tfm in self.content_tfms:\n",
    "                _func = partial(lambda_map_batch,\n",
    "                                feature=self.main_text,\n",
    "                                func=tfm,\n",
    "                                is_batched=self.is_batched)\n",
    "                dtrain = hf_map_dset(dtrain,_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "\n",
    "        tok_func = partial(tokenize_function,\n",
    "                           tok=self.tokenizer,\n",
    "                           max_length=self.max_length if self.line_by_line else -1,\n",
    "                           return_special_tokens_mask=True\n",
    "                          )\n",
    "        _func = partial(lambda_map_batch,\n",
    "                        feature=self.main_text,\n",
    "                        func=tok_func,\n",
    "                        output_feature=None,\n",
    "                        is_batched=self.is_batched)\n",
    "        \n",
    "        dtrain = hf_map_dset(dtrain,_func,self.is_batched,self.batch_size,self.tok_num_proc)\n",
    "        dtrain = dtrain.remove_columns(self.cols_to_keep)   \n",
    "        \n",
    "        if not self.line_by_line: # string concatenation\n",
    "            dtrain = hf_map_dset(dtrain,\n",
    "                                 self._group_texts_with_stride,\n",
    "                                 is_batched=True,\n",
    "                                 batch_size=self.batch_size if self.batch_size>1 else 1024,\n",
    "                                 num_proc=self.tok_num_proc)\n",
    "        return dtrain\n",
    "    \n",
    "\n",
    "    def _construct_generator_with_batch(self,dset,text_name,tok_func,func):        \n",
    "        def _get_generator(d):\n",
    "            num_iterations = len(next(iter(d.values())))\n",
    "            for i in range(num_iterations):\n",
    "                yield {key: value[i] for key, value in d.items()}\n",
    "            \n",
    "        batch_size = self.batch_size if self.batch_size>1 else 1024\n",
    "        str_list=[] \n",
    "        for inp in dset: # dset is generator\n",
    "            # inp[text_name] will be a single item\n",
    "            str_list.append(func(inp[text_name]))\n",
    "            if len(str_list)==batch_size:\n",
    "                # tokenize\n",
    "                result_dict = tok_func(str_list)\n",
    "                if not self.line_by_line:\n",
    "                    # token concatenation\n",
    "                    result_dict = self._group_texts_with_stride(result_dict)\n",
    "                str_list=[]\n",
    "                yield from _get_generator(result_dict)\n",
    "                \n",
    "            \n",
    "        if len(str_list):\n",
    "            # str_list length hasn't reached batch_size (last batch)\n",
    "            # tokenize\n",
    "            result_dict = tok_func(str_list)\n",
    "            if not self.line_by_line:\n",
    "                # token concatenation\n",
    "                result_dict = self._group_texts_with_stride(result_dict)\n",
    "            str_list=[]\n",
    "            yield from _get_generator(result_dict)\n",
    "            \n",
    "    def _do_transformation_tokenization_generator(self):\n",
    "        tok_func = partial(tokenize_function,\n",
    "                           tok=self.tokenizer,\n",
    "                           max_length=self.max_length if self.line_by_line else -1,\n",
    "                           return_special_tokens_mask=True\n",
    "                          )\n",
    "    \n",
    "        all_tfms = self.content_tfms\n",
    "        all_tfms = partial(func_all,functions=all_tfms) if len(all_tfms) else lambda x: x\n",
    "        if self.seed:\n",
    "            seed_everything(self.seed)\n",
    "           \n",
    "        self.main_ddict['train'] = IterableDataset.from_generator(self._construct_generator_with_batch,\n",
    "                                                   gen_kwargs={'dset': self.main_ddict['train'],\n",
    "                                                               'text_name':self.main_text,\n",
    "                                                               'tok_func':tok_func,\n",
    "                                                               'func': all_tfms\n",
    "                                                              }\n",
    "                                                                 )\n",
    "\n",
    "    \n",
    "    def process_and_tokenize(self,\n",
    "                             tokenizer, # Tokenizer (preferably from HuggingFace)\n",
    "                             max_length=None, # pad to model's allowed max length (default is max_sequence_length). Use -1 for no padding at all\n",
    "                             num_proc=None, # Number of processes for tokenization\n",
    "                             line_by_line=True, # To whether tokenize each sentence separately, or concatenate them\n",
    "                             stride=None, # option to do striding when line_by_line is False\n",
    "                            ):\n",
    "        if self._processed_call:\n",
    "            warnings.warn('Your dataset has already been processed. Returning the previous processed DatasetDict...')\n",
    "            return self.main_ddict\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.line_by_line = line_by_line\n",
    "        self.stride = stride        \n",
    "        self.tok_num_proc = num_proc if num_proc else self.num_proc\n",
    "        \n",
    "        # Filtering\n",
    "        print_msg('Data Filtering',20,verbose=self.verbose)\n",
    "        for k in self.main_ddict.keys():   \n",
    "            self.main_ddict[k] = self._do_filtering(self.main_ddict[k])\n",
    "        self.verboseprint('Done')\n",
    "\n",
    "        \n",
    "        # Process metadatas\n",
    "        print_msg('Metadata Simple Processing & Concatenating to Main Content',verbose=self.verbose)\n",
    "        for k in self.main_ddict.keys():   \n",
    "            self.main_ddict[k] = self._process_metadatas(self.main_ddict[k])\n",
    "        self.verboseprint('Done')\n",
    "\n",
    "        # Dropping unused columns\n",
    "        self._simplify_ddict()\n",
    "\n",
    "           \n",
    "        # Content transformation + tokenization for validation\n",
    "        if 'validation' in self.main_ddict.keys():\n",
    "            print_msg('Performing Content Transformation and Tokenization on Validation Set',verbose=self.verbose)\n",
    "            self.main_ddict['validation'] = self._do_transformation_tokenization(self.main_ddict['validation'])\n",
    "            self.verboseprint('Done')\n",
    "        \n",
    "        # Content transformation + tokenization for train\n",
    "        print_msg('Creating a generator for content transformation and tokenization on Train set',verbose=self.verbose)\n",
    "        self._do_transformation_tokenization_generator()\n",
    "        self.verboseprint('Done')\n",
    "        \n",
    "        self._processed_call=True\n",
    "    \n",
    "    def set_data_collator(self,\n",
    "                          is_mlm=True, # Is this masked language model (True) or causal language model (False)\n",
    "                          mlm_prob=0.15, # Mask probability for masked language model\n",
    "                         ):\n",
    "        if not hasattr(self,'max_length'):\n",
    "            raise ValueError(\"Please call `process_and_tokenize' or `do_tokenization` to tokenize your dataset\")\n",
    "            \n",
    "        pad_to_multiple_of_8 = (self.max_length<0) # get data collator to pad\n",
    "        self.data_collator = DataCollatorForLanguageModeling(tokenizer=self.tokenizer,\n",
    "                                                             mlm=is_mlm,\n",
    "                                                             mlm_probability=mlm_prob,\n",
    "                                                             pad_to_multiple_of=8 if pad_to_multiple_of_8 else None\n",
    "                                                            )\n",
    "                                               \n",
    "        \n",
    "    def prepare_test_dataset(self,\n",
    "                             test_dset, # The HuggingFace Dataset as Test set\n",
    "                             do_filtering=False, # whether to perform data filtering on this test set\n",
    "                            ):\n",
    "        raise NotImplementedError(\"There's no test set preparation for Language Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### TextDataLMControllerStreaming\n",
       "\n",
       ">      TextDataLMControllerStreaming (inp, main_text:str, filter_dict={},\n",
       ">                                     metadatas=[], process_metas=True,\n",
       ">                                     content_transformations=[], seed=None,\n",
       ">                                     batch_size=1024, num_proc=1,\n",
       ">                                     cols_to_keep=None, verbose=True)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | HuggingFainpce Dataset or DatasetDict |\n",
       "| main_text | str |  | Name of the main text column |\n",
       "| filter_dict | dict | {} | A dictionary: {feature: filtering_function_for_that_feature} |\n",
       "| metadatas | list | [] | Names of the metadata columns |\n",
       "| process_metas | bool | True | Whether to do simple text processing on the chosen metadatas |\n",
       "| content_transformations | list | [] | A list of text transformations |\n",
       "| seed | NoneType | None | Random seed |\n",
       "| batch_size | int | 1024 | Transformation + Tokenization batch size |\n",
       "| num_proc | int | 1 | Number of process for multiprocessing |\n",
       "| cols_to_keep | NoneType | None | Columns to keep after all processings |\n",
       "| verbose | bool | True | Whether to prdint processing information |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### TextDataLMControllerStreaming\n",
       "\n",
       ">      TextDataLMControllerStreaming (inp, main_text:str, filter_dict={},\n",
       ">                                     metadatas=[], process_metas=True,\n",
       ">                                     content_transformations=[], seed=None,\n",
       ">                                     batch_size=1024, num_proc=1,\n",
       ">                                     cols_to_keep=None, verbose=True)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | HuggingFainpce Dataset or DatasetDict |\n",
       "| main_text | str |  | Name of the main text column |\n",
       "| filter_dict | dict | {} | A dictionary: {feature: filtering_function_for_that_feature} |\n",
       "| metadatas | list | [] | Names of the metadata columns |\n",
       "| process_metas | bool | True | Whether to do simple text processing on the chosen metadatas |\n",
       "| content_transformations | list | [] | A list of text transformations |\n",
       "| seed | NoneType | None | Random seed |\n",
       "| batch_size | int | 1024 | Transformation + Tokenization batch size |\n",
       "| num_proc | int | 1 | Number of process for multiprocessing |\n",
       "| cols_to_keep | NoneType | None | Columns to keep after all processings |\n",
       "| verbose | bool | True | Whether to prdint processing information |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataLMControllerStreaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data + Basic use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset source: https://www.kaggle.com/datasets/kavita5/review_ecommerce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With line-by-line tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'],\n",
       "        num_rows: 2349\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict_with_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc = TextDataLMControllerStreaming(ddict_with_val,\n",
    "                                    filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                    main_text='Review Text',\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Data Filtering --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b5d40fabe84981a75cb214e340b62d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "----- Performing Content Transformation and Tokenization on Validation Set -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e810c758f41648b88b38ae64ef4682b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2267 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Creating a generator for content transformation and tokenization on Train set -----\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc.process_and_tokenize(tokenizer,line_by_line=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 2267\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ids: [0, 35299, 45, 224, 615, 372, 383, 59, 42, 8443, 4, 2579, 1318, 8, 385, 31766, 8, 215, 10, 23541, 6, 3473, 1973, 25, 4340, 7, 10, 55, 16697, 3089, 14609, 4, 2579, 6, 1528, 7, 1836, 2564, 36, 13713, 1459, 650, 21, 1969, 15, 127, 195, 108, 176, 113, 12312, 23246, 5120, 5579, 3654, 350, 3229, 8, 45, 3298, 4740, 6, 21764, 202, 155, 73, 306, 1592, 40, 645, 41, 4716, 1459, 3023, 29, 13, 127, 1354, 6, 350, 4, 6215, 6, 939, 74, 2299, 907, 97, 8089, 114, 47, 2845, 7, 645, 106, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Decoded: <s>Could not say enough great things about this jacket. nice quality and drape and such a wearable, comfortable option as opposed to a more structured blazer. nice, true to size fit (petite small was perfect on my 5'2\" 115 lbs frame--not too tight and not baggy, sleeves still 3/4.) will order an petite xs for my daughter, too. retailer, i would definitely buy other colors if you decide to order them.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(tdc.main_ddict['validation']):\n",
    "    if i==1:break\n",
    "    print(f\"Input ids: {v['input_ids']}\\nDecoded: {tokenizer.decode(v['input_ids'])}\\nAttention Mask: {v['attention_mask']}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ids: [0, 6785, 300, 42, 3588, 11, 41, 3023, 462, 4, 939, 348, 393, 2740, 31, 6215, 137, 98, 939, 421, 49, 5418, 7, 422, 650, 4, 939, 21, 1593, 4, 24, 18, 10, 2721, 239, 1318, 3588, 53, 939, 206, 24, 18, 95, 350, 380, 4, 939, 189, 253, 62, 28149, 24, 13, 10, 784, 4, 2299, 1836, 159, 11, 42, 3588, 328, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Decoded: <s>Just got this dress in an xl. i've never ordered from retailer before so i expected their clothes to run small. i was wrong. it's a beautiful high quality dress but i think it's just too big. i may end up exchanging it for a l. definitely size down in this dress!</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    if i==1:break\n",
    "    print(f\"Input ids: {v['input_ids']}\\n\\nDecoded: {tokenizer.decode(v['input_ids'])}\\n\\nAttention Mask: {v['attention_mask']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering + Metadatas + Content Transformation + Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "from underthesea import text_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from that_nlp_library.text_main_lm import TextDataLMController"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Tokenize our corpus line-by-line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With no padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc = TextDataLMControllerStreaming(ddict_with_val,\n",
    "                                    main_text='Review Text',\n",
    "                                    filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                    metadatas=['Title','Division Name'],\n",
    "                                    content_transformations=[text_normalize,str.lower],\n",
    "                                    seed=42,\n",
    "                                    batch_size=1024,\n",
    "                                    verbose=False\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c9e2a4516404bc7bd52ebd95038f4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36210e26210d4dd8a62ed61932afd5ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28a9c5e07234ce7aef3cd29f886c8d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tdc.process_and_tokenize(tokenizer,line_by_line=True,max_length=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>general petite. beautiful top, worth the necessary tailoring. the beautiful bold print drew me to this top and it did not disappoint upon receipt. however, the bottom ruffle belled so far out on each side that it was laughable! the actual fit is nothing like the picture ; clearly the model's arms are placed in front of all the extra fabric to hold the ruffle back. however, the fabric is beautiful, the fit was perfect ( size 2, 5'4 \", 106 lbs. ), the quality is great and i love the print so i decided to take it to my tailor to \" sew away \" the \" wings \" on both si</s>\n",
      "\n",
      "<s>general. soft, feminine and fun pockets!. i love this tunic. purchased the dark orange in medium ( i am 5'9 and 140 lbs ). tried the small and almost kept it but i felt seams around my arm pits a tad, so went with the medium and glad i did - this top should be comfortable. feels very fall and perfect for casual get-togethers and running around town. only comment is that it is rayon... and for me anyway rayon doesn't wash too well - so we shall see how this one fairs.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(next(iter(tdc.main_ddict['train']))['input_ids']))\n",
    "print()\n",
    "print(tokenizer.decode(tdc.main_ddict['validation']['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to non-streamed version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8360488f1bc4a199a8954b106dfdfbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/20388 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12bc9f364ab4300ae9cba23fb35db3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dset2 = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val2 = dset2.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val2['validation'] = ddict_with_val2['test']\n",
    "del ddict_with_val2['test']\n",
    "\n",
    "tdc2 = TextDataLMController(ddict_with_val2,\n",
    "                            main_text='Review Text',\n",
    "                            filter_dict={'Review Text': lambda x: x is not None},\n",
    "                            metadatas=['Title','Division Name'],\n",
    "                            content_transformations=[text_normalize,str.lower],\n",
    "                            seed=42,\n",
    "                            batch_size=1024,\n",
    "                            verbose=False\n",
    "                            )\n",
    "tdc2.process_and_tokenize(tokenizer,line_by_line=True,max_length=-1,shuffle_trn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether train sets are the same\n",
    "assert len(list(tdc.main_ddict['train']))==len(tdc2.main_ddict['train'])\n",
    "\n",
    "iter1 = iter(tdc.main_ddict['train'])\n",
    "iter2 = iter(tdc2.main_ddict['train'])\n",
    "for a,b in zip(iter1,iter2):\n",
    "    assert a==b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether validation set is the same\n",
    "assert len(list(tdc.main_ddict['validation']))==len(tdc2.main_ddict['validation'])\n",
    "\n",
    "iter1 = iter(tdc.main_ddict['validation'])\n",
    "iter2 = iter(tdc2.main_ddict['validation'])\n",
    "for a,b in zip(iter1,iter2):\n",
    "    assert a==b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With padding \n",
    "\n",
    "(set `max_length` to `None` if you want to pad to model's maximum sequence length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba73f3b9504a49588a2e05b4f97890a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']\n",
    "\n",
    "tdc = TextDataLMControllerStreaming(ddict_with_val,\n",
    "                                    main_text='Review Text',\n",
    "                                    filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                    metadatas=['Title','Division Name'],\n",
    "                                    content_transformations=[text_normalize,str.lower],\n",
    "                                    seed=42,\n",
    "                                    batch_size=1024,\n",
    "                                    verbose=False\n",
    "                                    )\n",
    "tdc.process_and_tokenize(tokenizer,line_by_line=True,max_length=256,num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>general petite. beautiful top, worth the necessary tailoring. the beautiful bold print drew me to this top and it did not disappoint upon receipt. however, the bottom ruffle belled so far out on each side that it was laughable! the actual fit is nothing like the picture ; clearly the model's arms are placed in front of all the extra fabric to hold the ruffle back. however, the fabric is beautiful, the fit was perfect ( size 2, 5'4 \", 106 lbs. ), the quality is great and i love the print so i decided to take it to my tailor to \" sew away \" the \" wings \" on both si</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "\n",
      "<s>general. soft, feminine and fun pockets!. i love this tunic. purchased the dark orange in medium ( i am 5'9 and 140 lbs ). tried the small and almost kept it but i felt seams around my arm pits a tad, so went with the medium and glad i did - this top should be comfortable. feels very fall and perfect for casual get-togethers and running around town. only comment is that it is rayon... and for me anyway rayon doesn't wash too well - so we shall see how this one fairs.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(next(iter(tdc.main_ddict['train']))['input_ids']))\n",
    "print()\n",
    "print(tokenizer.decode(tdc.main_ddict['validation']['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to non-streamed version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d98c543fae4bd9a495c46a5536f1a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20388 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1eb8b0f2ece4af38eae91f8aefec2ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dset2 = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val2 = dset2.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val2['validation'] = ddict_with_val2['test']\n",
    "del ddict_with_val2['test']\n",
    "\n",
    "tdc2 = TextDataLMController(ddict_with_val2,\n",
    "                            main_text='Review Text',\n",
    "                            filter_dict={'Review Text': lambda x: x is not None},\n",
    "                            metadatas=['Title','Division Name'],\n",
    "                            content_transformations=[text_normalize,str.lower],\n",
    "                            seed=42,\n",
    "                            batch_size=1024,\n",
    "                            verbose=False\n",
    "                            )\n",
    "tdc2.process_and_tokenize(tokenizer,line_by_line=True,max_length=256,shuffle_trn=False,num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether train sets are the same\n",
    "assert len(list(tdc.main_ddict['train']))==len(tdc2.main_ddict['train'])\n",
    "\n",
    "iter1 = iter(tdc.main_ddict['train'])\n",
    "iter2 = iter(tdc2.main_ddict['train'])\n",
    "for a,b in zip(iter1,iter2):\n",
    "    assert a==b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether validation set is the same\n",
    "assert len(list(tdc.main_ddict['validation']))==len(tdc2.main_ddict['validation'])\n",
    "\n",
    "iter1 = iter(tdc.main_ddict['validation'])\n",
    "iter2 = iter(tdc2.main_ddict['validation'])\n",
    "for a,b in zip(iter1,iter2):\n",
    "    assert a==b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Tokenize every text, then concatenate them together before splitting them in smaller parts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3954594faf4b278ba74d2359e48a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb35ad92706406494ec91a7a059fab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']\n",
    "\n",
    "tdc = TextDataLMControllerStreaming(ddict_with_val,\n",
    "                                    main_text='Review Text',\n",
    "                                    filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                    metadatas=['Title','Division Name'],\n",
    "                                    content_transformations=[text_normalize,str.lower],\n",
    "                                    seed=42,\n",
    "                                    batch_size=1024,\n",
    "                                    verbose=False\n",
    "                                    )\n",
    "tdc.process_and_tokenize(tokenizer,line_by_line=False,max_length=256,num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>general petite. beautiful top, worth the necessary tailoring. the beautiful bold print drew me to this top and it did not disappoint upon receipt. however, the bottom ruffle belled so far out on each side that it was laughable! the actual fit is nothing like the picture ; clearly the model's arms are placed in front of all the extra fabric to hold the ruffle back. however, the fabric is beautiful, the fit was perfect ( size 2, 5'4 \", 106 lbs. ), the quality is great and i love the print so i decided to take it to my tailor to \" sew away \" the \" wings \" on both si</s><s>general. not as short on me ( petite ). i ordered the xxs p as this dress is not a fitted dress, and that was the right size for me. only thing is the length is a bit linger still 9 lower on calf for me ), the straps are almost tight, so i would say the dress is a reversed taper shape. color is beautiful, i ordered green as the other color ( plum ) doesn't have petite available. green is rich, and classy, the fabric is surprisingly soft. i love the little details in the velvet. definitely need a\n",
      "\n",
      "<s>general. soft, feminine and fun pockets!. i love this tunic. purchased the dark orange in medium ( i am 5'9 and 140 lbs ). tried the small and almost kept it but i felt seams around my arm pits a tad, so went with the medium and glad i did - this top should be comfortable. feels very fall and perfect for casual get-togethers and running around town. only comment is that it is rayon... and for me anyway rayon doesn't wash too well - so we shall see how this one fairs.</s><s>general petite. a new staple!. tried these on out of sheer curiosity -- i've got a long torso & was pleasantly surprised how flattering they are! they manage to look flowing & sleek without shortening the legs. took a size 6 with my 27 \" waist, 37 \" hips. it's a bit of a generous fit, especially around the waist, but they're extremely comfortable & have room to tuck tops into. i have the cowled sweater tank in gray & it looks fantastic over these! couldn't resist getting both the rust and black. perfect for a dressy casual look</s><s>general. maybe swing is for me!. i love swing dresses but they never seem\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(next(iter(tdc.main_ddict['train']))['input_ids']))\n",
    "print()\n",
    "print(tokenizer.decode(tdc.main_ddict['validation']['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to non-streamed version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca63b3ada38742da8c5f52f1a88db9ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20388 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dde2340597e4f3aaa7e3ee0bc490909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c26128ae4f4fd9883e954c367c789e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20388 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c6ae60c9874f1b9f2f607e83b00bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dset2 = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val2 = dset2.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val2['validation'] = ddict_with_val2['test']\n",
    "del ddict_with_val2['test']\n",
    "\n",
    "tdc2 = TextDataLMController(ddict_with_val2,\n",
    "                            main_text='Review Text',\n",
    "                            filter_dict={'Review Text': lambda x: x is not None},\n",
    "                            metadatas=['Title','Division Name'],\n",
    "                            content_transformations=[text_normalize,str.lower],\n",
    "                            seed=42,\n",
    "                            batch_size=1024,\n",
    "                            verbose=False\n",
    "                            )\n",
    "tdc2.process_and_tokenize(tokenizer,line_by_line=False,max_length=256,shuffle_trn=False,num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether train sets are the same\n",
    "assert len(list(tdc.main_ddict['train']))==len(tdc2.main_ddict['train'])\n",
    "\n",
    "iter1 = iter(tdc.main_ddict['train'])\n",
    "iter2 = iter(tdc2.main_ddict['train'])\n",
    "for a,b in zip(iter1,iter2):\n",
    "    assert a==b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether validation set is the same\n",
    "assert len(list(tdc.main_ddict['validation']))==len(tdc2.main_ddict['validation'])\n",
    "\n",
    "iter1 = iter(tdc.main_ddict['validation'])\n",
    "iter2 = iter(tdc2.main_ddict['validation'])\n",
    "for a,b in zip(iter1,iter2):\n",
    "    assert a==b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Striding (For Concatenation of tokens)\n",
    "\n",
    "If your sentences (or paragraphs) are larger than `max_length`, after concatenation, they will be broken apart; your long paragraph will be incompleted in terms of meaning. **Striding** is a way to somewhat preserve the sentence's meaning, by getting part of the sentence back. We will demonstrate it with an example, and you can compare it with the previous one (without striding) to see the differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']\n",
    "\n",
    "tdc = TextDataLMControllerStreaming(ddict_with_val,\n",
    "                                    main_text='Review Text',\n",
    "                                    filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                    metadatas=['Title','Division Name'],\n",
    "                                    content_transformations=[text_normalize,str.lower],\n",
    "                                    seed=42,\n",
    "                                    batch_size=1024,\n",
    "                                    verbose=False\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e1864e3f6f438a9f7452e196a431e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tdc.process_and_tokenize(tokenizer,line_by_line=False,max_length=100,stride=20,num_proc=1)\n",
    "# Stride is 20, meaning for the next entry, we go back 20 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>general petite. beautiful top, worth the necessary tailoring. the beautiful bold print drew me to this top and it did not disappoint upon receipt. however, the bottom ruffle belled so far out on each side that it was laughable! the actual fit is nothing like the picture ; clearly the model's arms are placed in front of all the extra fabric to hold the ruffle back. however, the fabric is beautiful, the fit was perfect ( size 2, 5'4 \",\n",
      "--------------------\n",
      " however, the fabric is beautiful, the fit was perfect ( size 2, 5'4 \", 106 lbs. ), the quality is great and i love the print so i decided to take it to my tailor to \" sew away \" the \" wings \" on both si</s><s>general. not as short on me ( petite ). i ordered the xxs p as this dress is not a fitted dress, and that was the right size for me. only thing is the length is a\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    if i==2: break\n",
    "    print(tokenizer.decode(v['input_ids']))\n",
    "    print('-'*20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>general. soft, feminine and fun pockets!. i love this tunic. purchased the dark orange in medium ( i am 5'9 and 140 lbs ). tried the small and almost kept it but i felt seams around my arm pits a tad, so went with the medium and glad i did - this top should be comfortable. feels very fall and perfect for casual get-togethers and running around town. only comment is that it is rayon... and for me anyway rayon doesn\n",
      " running around town. only comment is that it is rayon... and for me anyway rayon doesn't wash too well - so we shall see how this one fairs.</s><s>general petite. a new staple!. tried these on out of sheer curiosity -- i've got a long torso & was pleasantly surprised how flattering they are! they manage to look flowing & sleek without shortening the legs. took a size 6 with my 27 \" waist, 37 \" hips. it's a bit\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tdc.main_ddict['validation']['input_ids'][0]))\n",
    "print(tokenizer.decode(tdc.main_ddict['validation']['input_ids'][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second entry, we can see it starts with the last 20 tokens of the previous entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to non-streamed version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8301904d70487f88a893c30a664eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20388 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3b59d0c56645dabe7595a4ff5d35ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dset2 = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val2 = dset2.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val2['validation'] = ddict_with_val2['test']\n",
    "del ddict_with_val2['test']\n",
    "\n",
    "tdc2 = TextDataLMController(ddict_with_val2,\n",
    "                            main_text='Review Text',\n",
    "                            filter_dict={'Review Text': lambda x: x is not None},\n",
    "                            metadatas=['Title','Division Name'],\n",
    "                            content_transformations=[text_normalize,str.lower],\n",
    "                            seed=42,\n",
    "                            batch_size=1024,\n",
    "                            verbose=False\n",
    "                            )\n",
    "tdc2.process_and_tokenize(tokenizer,line_by_line=False,max_length=100,shuffle_trn=False,stride=20,num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether train sets are the same\n",
    "assert len(list(tdc.main_ddict['train']))==len(tdc2.main_ddict['train'])\n",
    "\n",
    "iter1 = iter(tdc.main_ddict['train'])\n",
    "iter2 = iter(tdc2.main_ddict['train'])\n",
    "for a,b in zip(iter1,iter2):\n",
    "    assert a==b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether validation set is the same\n",
    "assert len(list(tdc.main_ddict['validation']))==len(tdc2.main_ddict['validation'])\n",
    "\n",
    "iter1 = iter(tdc.main_ddict['validation'])\n",
    "iter2 = iter(tdc2.main_ddict['validation'])\n",
    "for a,b in zip(iter1,iter2):\n",
    "    assert a==b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import text_normalize\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For masked language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our text controller first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']\n",
    "\n",
    "tdc = TextDataLMControllerStreaming(ddict_with_val,\n",
    "                                    main_text='Review Text',\n",
    "                                    filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                    metadatas=['Title','Division Name'],\n",
    "                                    content_transformations=[text_normalize,str.lower],\n",
    "                                    seed=42,\n",
    "                                    batch_size=1024,\n",
    "                                    verbose=False\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will tokenize our corpus line-by-line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2272d328aef439c93fe55a2a5458968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tdc.process_and_tokenize(tokenizer,line_by_line=True,max_length=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.set_data_collator(is_mlm=True,mlm_prob=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForLanguageModeling(tokenizer=RobertaTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)}, clean_up_tokenization_spaces=True), mlm=True, mlm_probability=0.15, pad_to_multiple_of=8, tf_experimental_compile=False, return_tensors='pt')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying the collator...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 15841, 4716, 1459, 479, 2721, 299, 2156, 966, 5, 2139, 7886, 5137, 479, 5, 2721, 7457, 5780, 4855, 162, 7, 42, 299, 8, 24, 222, 45, 17534, 2115, 18245, 479, 959, 2156, 5, 2576, 910, 15315, 28, 9970, 98, 444, 66, 15, 349, 526, 14, 24, 21, 38677, 27785, 5, 3031, 2564, 16, 1085, 101, 5, 2170, 25606, 2563, 5, 1421, 18, 3701, 32, 2325, 11, 760, 9, 70, 5, 1823, 10199, 7, 946, 5, 910, 15315, 124, 479, 959, 2156, 5, 10199, 16, 2721, 2156, 5, 2564, 21, 1969, 36, 1836, 132, 2156, 195, 128, 204, 22, 2156, 13442, 23246, 479, 4839, 2156, 5, 1318, 16, 372, 8, 939, 657, 5, 5780, 98, 939, 1276, 7, 185, 24, 7, 127, 26090, 7, 22, 35043, 409, 22, 5, 22, 11954, 22, 15, 258, 3391, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}\n",
      "--------------------\n",
      "{'input_ids': [0, 15841, 479, 45, 25, 765, 15, 162, 36, 4716, 1459, 4839, 479, 939, 2740, 5, 37863, 29, 181, 25, 42, 3588, 16, 45, 10, 15898, 3588, 2156, 8, 14, 21, 5, 235, 1836, 13, 162, 479, 129, 631, 16, 5, 5933, 16, 10, 828, 18277, 202, 361, 795, 15, 16701, 13, 162, 4839, 2156, 5, 31622, 32, 818, 3229, 2156, 98, 939, 74, 224, 5, 3588, 16, 10, 13173, 326, 15888, 3989, 479, 3195, 16, 2721, 2156, 939, 2740, 2272, 25, 5, 97, 3195, 36, 36838, 4839, 630, 75, 33, 4716, 1459, 577, 479, 2272, 16, 4066, 2156, 8, 30228, 2156, 5, 10199, 16, 10262, 3793, 479, 939, 657, 5, 410, 1254, 11, 5, 29986, 479, 2299, 240, 10, 18052, 16979, 11689, 13, 42, 65, 479, 12312, 23246, 119, 389, 385, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    if i==2: break\n",
    "    print(v)\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the length of each token list is different from each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "iter1 = iter(tdc.main_ddict['train'])\n",
    "out = tdc.data_collator([next(iter1) for i in range(5)]) # simulation with batch size 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all token lists have the same length, which is a multiple of 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 136])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 15841,  4716,  1459,   479,  2721,   299,  2156,   966, 50264,\n",
       "          2139,  7886,  5137,   479,     5,  2721,  7457,  5780,  4855,   162,\n",
       "             7,    42,   299,     8, 50264,   222,    45, 17534,  2115, 50264,\n",
       "           479, 50264,  2156,     5,  2576,   910, 15315,    28,  9970,    98,\n",
       "           444,    66,    15,   349,   526,    14,    24,    21, 38677, 27785,\n",
       "         17138,  3031, 50264,    16,  1085,   101,     5,  2170, 25606,  2563,\n",
       "             5,  1421,    18,  3701,    32,  2325,    11,   760,     9,    70,\n",
       "             5,  1823, 10199, 50264, 29261, 50264,   910, 15315,   124,   479,\n",
       "           959,  2156,     5, 10199,    16,  2721,  2156, 50264,  2564,    21,\n",
       "          1969,    36,  1836,   132, 50264,   195,   128,   204,    22, 50264,\n",
       "         13442, 23246,   479, 50264,  2156,     5, 50264,    16, 50264,     8,\n",
       "           939,   657,     5, 50264,    98,   939,  1276, 50264,   185,    24,\n",
       "             7,   127, 26090,     7,    22, 35043,   409, 50264,     5,    22,\n",
       "         11954,    22, 50264,   258,  3391,     2],\n",
       "        [    0, 15841,   479,    45,    25, 13055,    15, 50264,    36,  4716,\n",
       "          1459,  4839,   479, 50264,  2740,     5, 37863, 50264,   181, 50264,\n",
       "            42,  3588,    16,    45,    10, 15898,  3588, 50264,     8,    14,\n",
       "            21,     5,   235,  1836,    13,   162,   479,   129,   631,    16,\n",
       "             5,  5933,    16, 38152,   828, 18277,   202,   361,   795,    15,\n",
       "         16701,    13,   162,  4839,  2156,     5, 31622,    32,   818,  3229,\n",
       "          2156,    98,   939,    74, 50264,     5,  3588,    16,    10, 13173,\n",
       "           326, 50264,  3989,   479,  3195,    16, 50264,  2156,   939,  2740,\n",
       "          2272,    25,     5,    97,  3195,    36, 36838,  4839, 50264,    75,\n",
       "            33,  4716,  1459, 50264, 50264,  2272,    16,  4066,  2156,     8,\n",
       "         30228, 50264, 50264, 50264,    16, 10262,  3793,   479,   939, 50264,\n",
       "             5,   410,  1254,    11,     5, 50264,   479,  2299,   240, 50264,\n",
       "         18052, 16979, 11689,    13,    42, 50264,   479, 12312, 23246,   119,\n",
       "           389,   385,     2,     1,     1,     1],\n",
       "        [    0, 15841,   479,  1969, 50264,    13,    80, 50264,   479, 15983,\n",
       "         10717, 39574,    47,   240, 50264,   216,    14,    42,  1907,     9,\n",
       "         50264, 50264,     5,    65,    14,    40,   120,  6538,    36,   939,\n",
       "          2162,     5, 50264,    65,  4839,   479,    24,    16,  2422,  7174,\n",
       "         50264,  9869,  2156,    53,   939,    21,   129,   441,     7,   120,\n",
       "            80, 15033,    66, 50264,    24, 50264, 50264,   222, 10397,    24,\n",
       "         50264, 50264,  4925, 50264,    18,  1836,   142,   939,  1079, 47904,\n",
       "            24, 50264, 50264,   172, 10601,     7,  3841,   479,   939,    21,\n",
       "          2422,  5779, 50264,     5,  3568,    53, 10874,   145,   441,     7,\n",
       "         50264,    24,   396,   864,    23, 50264, 50264,  6215,   479,     2,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['input_ids'][:3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `labels` have also been constructed, which shows the \"mask\" tokens (non -100) in which the model has to predict. To increase the amount of masked tokens, increase the `mlm_prob`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,     5,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,    24,  -100,  -100,  -100,  -100, 18245,\n",
       "          -100,   959,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "             5,  -100,  2564,  -100,  -100,  -100,  -100,  -100,  -100,  2563,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,     7,   946,     5,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,     5,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  2156,  -100,  -100,  -100,  -100,  2156,\n",
       "          -100,  -100,  -100,  4839,  -100,  -100,  1318,  -100,   372,  -100,\n",
       "          -100,  -100,  -100,  5780,  -100,  -100,  -100,     7,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,    22,  -100,  -100,\n",
       "          -100,  -100,    15,  -100,  -100,  -100],\n",
       "        [ -100, 15841,  -100,  -100,  -100,   765,  -100,   162,  -100,  -100,\n",
       "          -100,  -100,  -100,   939,  -100,  -100,  -100,    29,  -100,    25,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  2156,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "             5,  -100,  -100,    10,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,   224,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100, 15888,  -100,  -100,  -100,  -100,  2721,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   630,  -100,\n",
       "          -100,  -100,  -100,   577,   479,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  2156,     5, 10199,  -100,  -100,  -100,  -100,  -100,   657,\n",
       "          -100,  -100,  -100,  -100,  -100, 29986,  -100,  -100,  -100,    10,\n",
       "          -100,  -100,  -100,  -100,  -100,    65,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [ -100,  -100,  -100,  -100, 39574,  -100,  -100, 15033,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,     7,  -100,  -100,  -100,  -100,  -100,\n",
       "         10199,    16,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  1104,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "             8,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,     9,  -100,   479,   939,  -100,  -100,  -100,\n",
       "             8,    24,  -100,    24,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,   150,  7727,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,    59,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           671,  -100,  -100,  -100,  -100,   127,   400,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['labels'][:3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you apply padding in the tokenization step (by adjusting the `max_length` argument), no matter whether it's line-by-line tokenization or not, the data collator will skip the padding step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']\n",
    "\n",
    "tdc = TextDataLMControllerStreaming(ddict_with_val,\n",
    "                                    main_text='Review Text',\n",
    "                                    filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                    metadatas=['Title','Division Name'],\n",
    "                                    content_transformations=[text_normalize,str.lower],\n",
    "                                    seed=42,\n",
    "                                    batch_size=1024,\n",
    "                                    verbose=False\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d1e3e72f1f4407b44a2ec283ee9050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517a91b7b9cd4fc4922c6b0ca2836660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tdc.process_and_tokenize(tokenizer,line_by_line=False,max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.set_data_collator(is_mlm=True,mlm_prob=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter1 = iter(tdc.main_ddict['train'])\n",
    "out = tdc.data_collator([next(iter1) for i in range(5)]) # simulation with batch size 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 100])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 15841,  4716,  1459,   479,  2721,   299,  2156,   966, 50264,\n",
       "          2139,  7886,  5137,   479,     5,  2721,  7457,  5780,  4855,   162,\n",
       "             7,    42,   299,     8,    24,   222,    45, 17534,  2115, 50264,\n",
       "           479, 50264,  2156,     5,  2576,   910, 15315,    28,  9970,    98,\n",
       "           444,    66,    15,   349,   526,    14,    24,    21, 38677, 27785,\n",
       "         50264,  3031, 50264,    16,  1085,   101,     5,  2170, 25606, 41316,\n",
       "             5,  1421,    18,  3701,    32,  2325,    11,   760,     9,    70,\n",
       "             5,  1823, 10199, 50264, 17204, 50264,   910, 15315,   124,   479,\n",
       "           959,  2156,     5, 10199,    16,  2721,  2156, 50264,  2564,    21,\n",
       "          1969,    36,  1836,   132, 50264,   195,   128,   204,    22, 50264],\n",
       "        [13442, 23246,   479, 50264,  2156,     5, 50264,    16, 23781,     8,\n",
       "           939,   657,     5,  5780,    98,   939,  1276, 50264,   185,    24,\n",
       "             7,   127, 26090,     7,    22, 35043,   409, 50264,     5,    22,\n",
       "         11954,    22, 50264,   258,  3391,     2,     0, 50264,   479,    45,\n",
       "            25, 50264,    15, 44224,    36,  4716,  1459,  4839,   479, 50264,\n",
       "          2740,     5, 37863, 50264,   181,    25,    42,  3588,    16,    45,\n",
       "            10, 15898,  3588, 50264,     8,    14,    21,     5,   235,  1836,\n",
       "            13,   162,   479,   129,   631,    16, 50264,  5933,    16, 50264,\n",
       "           828, 18277,   202,   361,   795,    15, 16701,    13,   162,  4839,\n",
       "          2156,     5, 31622,    32,   818,  3229,  2156,    98,   939,    74]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['input_ids'][:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,     5,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,    24,  -100,  -100,  -100,  -100, 18245,\n",
       "          -100,   959,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "             5,  -100,  2564,  -100,  -100,  -100,  -100,  -100,  -100,  2563,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,     7,   946,     5,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,     5,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  2156,  -100,  -100,  -100,  -100,  2156],\n",
       "        [ -100,  -100,  -100,  4839,  -100,  -100,  1318,  -100,   372,  -100,\n",
       "          -100,  -100,  -100,  5780,  -100,  -100,  -100,     7,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,    22,  -100,  -100,\n",
       "          -100,  -100,    15,  -100,  -100,  -100,  -100, 15841,  -100,  -100,\n",
       "          -100,   765,  -100,   162,  -100,  -100,  -100,  -100,  -100,   939,\n",
       "          -100,  -100,  -100,    29,  -100,    25,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  2156,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,     5,  -100,  -100,    10,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['labels'][:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using the concatenation-of-tokenization technique, one smart thing that the HuggingFace's `DataCollatorForLanguageModeling` (which is the data collator we use) does is to allow maskings at every position, at opposed to to the previous cases (with line-by-line tokenization), there's no masking near the end tokens of each list, because those end tokens are padding tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For causal language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from tokenizers import processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our GPT2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT2 does not use start/end-of-sentence token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'Ġis', 'Ġa', 'Ġtext', '.', 'ĠThat', 'Ġis', 'Ġa', 'Ġsecond', 'Ġtext', '.', 'But', 'Ġthere', \"'s\", 'Ġa', 'Ġthird', 'Ġone']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(tokenizer(\"this is a text. That is a second text.But there's a third one\")['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to perform concatenation-of-token, and you want your causal LM to differentiate between sentences, you can add a special token to separate sentences, as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"$A \" + tokenizer.eos_token,\n",
    "    special_tokens=[(tokenizer.eos_token, tokenizer.eos_token_id)],\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'Ġis', 'Ġa', 'Ġtext', '.', 'ĠThat', 'Ġis', 'Ġa', 'Ġsecond', 'Ġtext', '.', 'But', 'Ġthere', \"'s\", 'Ġa', 'Ġthird', 'Ġone', '<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(tokenizer(\"this is a text. That is a second text.But there's a third one\")['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this modified tokenizer, let's perform concatenation-of-tokenization using GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']\n",
    "\n",
    "tdc = TextDataLMControllerStreaming(ddict_with_val,\n",
    "                                    main_text='Review Text',\n",
    "                                    filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                    metadatas=['Title','Division Name'],\n",
    "                                    content_transformations=[text_normalize,str.lower],\n",
    "                                    seed=42,\n",
    "                                    batch_size=1024,\n",
    "                                    verbose=False\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "233030ebcf894910822a45b59dc127ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f293838bada542519345675d1dd53f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tdc.process_and_tokenize(tokenizer,line_by_line=False,max_length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's casual language modeling, let's turn off `is_mlm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.set_data_collator(is_mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "iter1 = iter(tdc.main_ddict['train'])\n",
    "out = tdc.data_collator([next(iter1) for i in range(5)]) # simulation with batch size 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 100])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[24622,  4273,   578,   764,  4950,  1353,   837,  2861,   262,  3306,\n",
       "          7894,  3255,   764,   262,  4950, 10758,  3601,  9859,   502,   284,\n",
       "           428,  1353,   290,   340,   750,   407,  6703,  2402, 14507,   764,\n",
       "          2158,   837,   262,  4220,   374, 18137,   307,  3353,   523,  1290,\n",
       "           503,   319,  1123,  1735,   326,   340,   373, 47623,  5145,   262,\n",
       "          4036,  4197,   318,  2147,   588,   262,  4286,  2162,  4084,   262,\n",
       "          2746,   338,  5101,   389,  4624,   287,  2166,   286,   477,   262,\n",
       "          3131,  9664,   284,  1745,   262,   374, 18137,   736,   764,  2158,\n",
       "           837,   262,  9664,   318,  4950,   837,   262,  4197,   373,  2818,\n",
       "           357,  2546,   362,   837,   642,   705,   604,   366,   837, 15696],\n",
       "        [15785,   764,  1267,   837,   262,  3081,   318,  1049,   290,  1312,\n",
       "          1842,   262,  3601,   523,  1312,  3066,   284,  1011,   340,   284,\n",
       "           616, 35280,   284,   366, 34249,  1497,   366,   262,   366, 12098,\n",
       "           366,   319,  1111, 33721, 50256, 24622,   764,   407,   355,  1790,\n",
       "           319,   502,   357,  4273,   578,  1267,   764,  1312,  6149,   262,\n",
       "         31383,    82,   279,   355,   428,  6576,   318,   407,   257, 18235,\n",
       "          6576,   837,   290,   326,   373,   262,   826,  2546,   329,   502,\n",
       "           764,   691,  1517,   318,   262,  4129,   318,   257,  1643, 31402,\n",
       "           991,   860,  2793,   319, 31134,   329,   502,  1267,   837,   262,\n",
       "         29552,   389,  2048,  5381,   837,   523,  1312,   561,   910,   262]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['input_ids'][:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[24622,  4273,   578,   764,  4950,  1353,   837,  2861,   262,  3306,\n",
       "          7894,  3255,   764,   262,  4950, 10758,  3601,  9859,   502,   284,\n",
       "           428,  1353,   290,   340,   750,   407,  6703,  2402, 14507,   764,\n",
       "          2158,   837,   262,  4220,   374, 18137,   307,  3353,   523,  1290,\n",
       "           503,   319,  1123,  1735,   326,   340,   373, 47623,  5145,   262,\n",
       "          4036,  4197,   318,  2147,   588,   262,  4286,  2162,  4084,   262,\n",
       "          2746,   338,  5101,   389,  4624,   287,  2166,   286,   477,   262,\n",
       "          3131,  9664,   284,  1745,   262,   374, 18137,   736,   764,  2158,\n",
       "           837,   262,  9664,   318,  4950,   837,   262,  4197,   373,  2818,\n",
       "           357,  2546,   362,   837,   642,   705,   604,   366,   837, 15696],\n",
       "        [15785,   764,  1267,   837,   262,  3081,   318,  1049,   290,  1312,\n",
       "          1842,   262,  3601,   523,  1312,  3066,   284,  1011,   340,   284,\n",
       "           616, 35280,   284,   366, 34249,  1497,   366,   262,   366, 12098,\n",
       "           366,   319,  1111, 33721,  -100, 24622,   764,   407,   355,  1790,\n",
       "           319,   502,   357,  4273,   578,  1267,   764,  1312,  6149,   262,\n",
       "         31383,    82,   279,   355,   428,  6576,   318,   407,   257, 18235,\n",
       "          6576,   837,   290,   326,   373,   262,   826,  2546,   329,   502,\n",
       "           764,   691,  1517,   318,   262,  4129,   318,   257,  1643, 31402,\n",
       "           991,   860,  2793,   319, 31134,   329,   502,  1267,   837,   262,\n",
       "         29552,   389,  2048,  5381,   837,   523,  1312,   561,   910,   262]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['labels'][:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CLM, the `labels` are essentially the same as `input_ids`. From HuggingFace documentation:\n",
    "```\n",
    "`DataCollatorForLanguageModeling` will take care of creating the language model labels — in causal language modeling the inputs serve as labels too (just shifted by one element), and this data collator creates them on the fly during training.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load TextDataController"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### TextDataLMControllerStreaming.save_as_pickles\n",
       "\n",
       ">      TextDataLMControllerStreaming.save_as_pickles (fname,\n",
       ">                                                     parent='pickle_files')\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### TextDataLMControllerStreaming.save_as_pickles\n",
       "\n",
       ">      TextDataLMControllerStreaming.save_as_pickles (fname,\n",
       ">                                                     parent='pickle_files')\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataLMControllerStreaming.save_as_pickles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.from_pickle\n",
       "\n",
       ">      TextDataControllerStreaming.from_pickle (fname, parent='pickle_files')\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.from_pickle\n",
       "\n",
       ">      TextDataControllerStreaming.from_pickle (fname, parent='pickle_files')\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataLMControllerStreaming.from_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextDataLMControllerStreaming object can be saved and loaded with ease. This is especially useful after text processing and/or tokenization have been done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import disable_caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']\n",
    "\n",
    "tdc = TextDataLMControllerStreaming(ddict_with_val,\n",
    "                                    main_text='Review Text',\n",
    "                                    filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                    metadatas=['Title','Division Name'],\n",
    "                                    content_transformations=[text_normalize,str.lower],\n",
    "                                    seed=42,\n",
    "                                    batch_size=1024,\n",
    "                                    verbose=False\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f8bba3a6494060bacf845c49c2fb6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87cf803055394c0587cdfb360177f6fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8efe0f54f28a447cb4011db245df330b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030d708e1c27478da7327c5bc3acc20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62be6bd12fd402cb4ba4ecdb6ec2f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tdc.process_and_tokenize(tokenizer,line_by_line=True,max_length=-1)\n",
    "\n",
    "tdc.set_data_collator(is_mlm=True,mlm_prob=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.save_as_pickles('my_lm_tdc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load back our object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc2 = TextDataLMController.from_pickle('my_lm_tdc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can still access all its attributes, data, preprocessings, transformations ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 2253\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc2.main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Review Text': <function __main__.<lambda>(x)>},\n",
       " [<function underthesea.pipeline.text_normalize.text_normalize(text, tokenizer='underthesea')>,\n",
       "  <method 'lower' of 'str' objects>])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc2.filter_dict,tdc2.content_tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
