{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roberta Classifiers\n",
    "\n",
    "> This module contains code to build a text classification model using Roberta-related model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.roberta.classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import torch\n",
    "from transformers.models.roberta.configuration_roberta import RobertaConfig\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import AutoConfig\n",
    "from that_nlp_library.model_main import loss_for_classification\n",
    "from that_nlp_library.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ConcatHeadExtended(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Concatenated head for Roberta Classification Model. \n",
    "    This head takes the last n hidden states of [CLS], and concatenate them before passing through the classifier head\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 config, # HuggingFace model configuration\n",
    "                 classifier_dropout=0.1, # Dropout ratio (for dropout layer right before the last nn.Linear)\n",
    "                 last_hidden_size=768, # Last hidden size (before the last nn.Linear)\n",
    "                 layer2concat=4, # number of hidden layer to concatenate (counting from top)\n",
    "                 num_labels=None, # Number of label output. Overwrite config.num_labels \n",
    "                 **kwargs\n",
    "                ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.last_hidden_size=last_hidden_size\n",
    "        self.dropout = torch.nn.Dropout(classifier_dropout)\n",
    "        self.pre_classifier = torch.nn.Linear(layer2concat*config.hidden_size,last_hidden_size)\n",
    "        num_labels=num_labels if num_labels is not None else config.num_labels\n",
    "        self.out_proj = torch.nn.Linear(last_hidden_size, num_labels)\n",
    "    \n",
    "    def forward(self, inp, **kwargs):\n",
    "        x = inp\n",
    "        x = self.dropout(x)\n",
    "        x = self.pre_classifier(x)\n",
    "        x = torch.tanh(x)\n",
    "#         x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/models/roberta/classifiers.py#L19){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ConcatHeadExtended\n",
       "\n",
       ">      ConcatHeadExtended (config, classifier_dropout=0.1, last_hidden_size=768,\n",
       ">                          layer2concat=4, num_labels=None, **kwargs)\n",
       "\n",
       "Concatenated head for Roberta Classification Model. \n",
       "This head takes the last n hidden states of [CLS], and concatenate them before passing through the classifier head\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| config |  |  | HuggingFace model configuration |\n",
       "| classifier_dropout | float | 0.1 | Dropout ratio (for dropout layer right before the last nn.Linear) |\n",
       "| last_hidden_size | int | 768 | Last hidden size (before the last nn.Linear) |\n",
       "| layer2concat | int | 4 | number of hidden layer to concatenate (counting from top) |\n",
       "| num_labels | NoneType | None | Number of label output. Overwrite config.num_labels |\n",
       "| kwargs |  |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/models/roberta/classifiers.py#L19){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ConcatHeadExtended\n",
       "\n",
       ">      ConcatHeadExtended (config, classifier_dropout=0.1, last_hidden_size=768,\n",
       ">                          layer2concat=4, num_labels=None, **kwargs)\n",
       "\n",
       "Concatenated head for Roberta Classification Model. \n",
       "This head takes the last n hidden states of [CLS], and concatenate them before passing through the classifier head\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| config |  |  | HuggingFace model configuration |\n",
       "| classifier_dropout | float | 0.1 | Dropout ratio (for dropout layer right before the last nn.Linear) |\n",
       "| last_hidden_size | int | 768 | Last hidden size (before the last nn.Linear) |\n",
       "| layer2concat | int | 4 | number of hidden layer to concatenate (counting from top) |\n",
       "| num_labels | NoneType | None | Number of label output. Overwrite config.num_labels |\n",
       "| kwargs |  |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ConcatHeadExtended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ConcatHeadSimple(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Concatenated head for Roberta Classification Model, the simpler version (no hidden linear layer)\n",
    "    This head takes the last n hidden states of [CLS], and concatenate them before passing through the classifier head\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 config, # HuggingFace model configuration\n",
    "                 classifier_dropout=0.1, # Dropout ratio (for dropout layer right before the last nn.Linear)\n",
    "                 layer2concat=4, # number of hidden layer to concatenate (counting from top)\n",
    "                 num_labels=None, # Number of label output. Overwrite config.num_labels \n",
    "                 **kwargs\n",
    "                ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(classifier_dropout)\n",
    "        num_labels=num_labels if num_labels is not None else config.num_labels\n",
    "        self.out_proj = torch.nn.Linear(layer2concat*config.hidden_size, num_labels)\n",
    "    def forward(self, inp, **kwargs):\n",
    "        x = inp\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/models/roberta/classifiers.py#L52){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ConcatHeadSimple\n",
       "\n",
       ">      ConcatHeadSimple (config, classifier_dropout=0.1, layer2concat=4,\n",
       ">                        num_labels=None, **kwargs)\n",
       "\n",
       "Concatenated head for Roberta Classification Model, the simpler version (no hidden linear layer)\n",
       "This head takes the last n hidden states of [CLS], and concatenate them before passing through the classifier head\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| config |  |  | HuggingFace model configuration |\n",
       "| classifier_dropout | float | 0.1 | Dropout ratio (for dropout layer right before the last nn.Linear) |\n",
       "| layer2concat | int | 4 | number of hidden layer to concatenate (counting from top) |\n",
       "| num_labels | NoneType | None | Number of label output. Overwrite config.num_labels |\n",
       "| kwargs |  |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/models/roberta/classifiers.py#L52){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ConcatHeadSimple\n",
       "\n",
       ">      ConcatHeadSimple (config, classifier_dropout=0.1, layer2concat=4,\n",
       ">                        num_labels=None, **kwargs)\n",
       "\n",
       "Concatenated head for Roberta Classification Model, the simpler version (no hidden linear layer)\n",
       "This head takes the last n hidden states of [CLS], and concatenate them before passing through the classifier head\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| config |  |  | HuggingFace model configuration |\n",
       "| classifier_dropout | float | 0.1 | Dropout ratio (for dropout layer right before the last nn.Linear) |\n",
       "| layer2concat | int | 4 | number of hidden layer to concatenate (counting from top) |\n",
       "| num_labels | NoneType | None | Number of label output. Overwrite config.num_labels |\n",
       "| kwargs |  |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ConcatHeadSimple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RobertaClassificationHeadCustom(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Same as RobertaClassificationHead, but you can freely adjust dropout\n",
    "    \n",
    "    Reference: https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py#L1424\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 config, # HuggingFace model configuration\n",
    "                 classifier_dropout=0.1, # Dropout ratio (for dropout layer right before the last nn.Linear)\n",
    "                 num_labels=None, # Number of label output. Overwrite config.num_labels \n",
    "                 **kwargs\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.dense = torch.nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = torch.nn.Dropout(classifier_dropout)\n",
    "        num_labels=num_labels if num_labels is not None else config.num_labels\n",
    "        self.out_proj = torch.nn.Linear(config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/models/roberta/classifiers.py#L77){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RobertaClassificationHeadCustom\n",
       "\n",
       ">      RobertaClassificationHeadCustom (config, classifier_dropout=0.1,\n",
       ">                                       num_labels=None, **kwargs)\n",
       "\n",
       "Same as RobertaClassificationHead, but you can freely adjust dropout\n",
       "\n",
       "Reference: https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py#L1424\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| config |  |  | HuggingFace model configuration |\n",
       "| classifier_dropout | float | 0.1 | Dropout ratio (for dropout layer right before the last nn.Linear) |\n",
       "| num_labels | NoneType | None | Number of label output. Overwrite config.num_labels |\n",
       "| kwargs |  |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/models/roberta/classifiers.py#L77){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RobertaClassificationHeadCustom\n",
       "\n",
       ">      RobertaClassificationHeadCustom (config, classifier_dropout=0.1,\n",
       ">                                       num_labels=None, **kwargs)\n",
       "\n",
       "Same as RobertaClassificationHead, but you can freely adjust dropout\n",
       "\n",
       "Reference: https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py#L1424\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| config |  |  | HuggingFace model configuration |\n",
       "| classifier_dropout | float | 0.1 | Dropout ratio (for dropout layer right before the last nn.Linear) |\n",
       "| num_labels | NoneType | None | Number of label output. Overwrite config.num_labels |\n",
       "| kwargs |  |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RobertaClassificationHeadCustom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main classification architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RobertaBaseForSequenceClassification(RobertaPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Base Roberta Architecture for Sequence Classification task\n",
    "    \n",
    "    Based on: https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py#L1155C35-L1155C35\n",
    "    \"\"\"\n",
    "    # make sure standard XLM-R are used\n",
    "    config_class = RobertaConfig\n",
    "\n",
    "    def __init__(self,\n",
    "                 config, # HuggingFace model configuration\n",
    "                 is_multilabel=False, # Whether this is a multilabel classification\n",
    "                 is_multihead=False, # Whether this is a multihead (multi-level) classification\n",
    "                 head_class_sizes=[], # Class size for each head\n",
    "                 head_weights=[], # loss weight for each head. This will be multiplied to the loss of each head's output\n",
    "                 head_class=None, # The class object of the head. You can use RobertaClassificationHeadCustom as default\n",
    "                 **head_class_kwargs, # Keyword arguments for the head class\n",
    "                ):\n",
    "        super().__init__(config)\n",
    "        self.is_multilabel = is_multilabel\n",
    "        self.is_multihead = is_multihead\n",
    "        self.head_class_sizes = val2iterable(head_class_sizes)\n",
    "        self.head_weights = val2iterable(head_weights,lsize=len(self.head_class_sizes))\n",
    "        \n",
    "        # set num_labels for config\n",
    "        num_labels = sum(self.head_class_sizes)\n",
    "        config.num_labels = num_labels\n",
    "        \n",
    "        # add_pooling_layer to False to ensure all hidden states are returned and not only the one associated with the [CLS] token.\n",
    "        self.body_model = RobertaModel(config, add_pooling_layer=False)\n",
    "        # Set up classification head\n",
    "        self.classification_head = head_class(config=config,**head_class_kwargs)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids= None,\n",
    "        attention_mask= None,\n",
    "        token_type_ids= None,\n",
    "        position_ids= None,\n",
    "        head_mask= None,\n",
    "        inputs_embeds= None,\n",
    "        labels= None,\n",
    "        output_attentions= None,\n",
    "        output_hidden_states= None,\n",
    "        return_dict= None,\n",
    "        ):\n",
    "        \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "\n",
    "        outputs = self.body_model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = sequence_output[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "        logits = self.classification_head(sequence_output) # (bs,sum of all class sizes)\n",
    "        \n",
    "        # Calculate losses\n",
    "        if labels is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            loss = loss_for_classification(logits, labels, \n",
    "                                       self.is_multilabel,\n",
    "                                       self.is_multihead, \n",
    "                                       self.head_class_sizes,\n",
    "                                       self.head_weights)\n",
    "        \n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss, logits=logits,\n",
    "#                                      hidden_states=outputs.hidden_states,\n",
    "                                        hidden_states=None,\n",
    "                                        attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/models/roberta/classifiers.py#L105){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RobertaBaseForSequenceClassification\n",
       "\n",
       ">      RobertaBaseForSequenceClassification (config, is_multilabel=False,\n",
       ">                                            is_multihead=False,\n",
       ">                                            head_class_sizes=[],\n",
       ">                                            head_weights=[], head_class=None,\n",
       ">                                            **head_class_kwargs)\n",
       "\n",
       "Base Roberta Architecture for Sequence Classification task\n",
       "\n",
       "Based on: https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py#L1155C35-L1155C35\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| config |  |  | HuggingFace model configuration |\n",
       "| is_multilabel | bool | False | Whether this is a multilabel classification |\n",
       "| is_multihead | bool | False | Whether this is a multihead (multi-level) classification |\n",
       "| head_class_sizes | list | [] | Class size for each head |\n",
       "| head_weights | list | [] | loss weight for each head. This will be multiplied to the loss of each head's output |\n",
       "| head_class | NoneType | None | The class object of the head. You can use RobertaClassificationHeadCustom as default |\n",
       "| head_class_kwargs |  |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/models/roberta/classifiers.py#L105){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RobertaBaseForSequenceClassification\n",
       "\n",
       ">      RobertaBaseForSequenceClassification (config, is_multilabel=False,\n",
       ">                                            is_multihead=False,\n",
       ">                                            head_class_sizes=[],\n",
       ">                                            head_weights=[], head_class=None,\n",
       ">                                            **head_class_kwargs)\n",
       "\n",
       "Base Roberta Architecture for Sequence Classification task\n",
       "\n",
       "Based on: https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py#L1155C35-L1155C35\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| config |  |  | HuggingFace model configuration |\n",
       "| is_multilabel | bool | False | Whether this is a multilabel classification |\n",
       "| is_multihead | bool | False | Whether this is a multihead (multi-level) classification |\n",
       "| head_class_sizes | list | [] | Class size for each head |\n",
       "| head_weights | list | [] | loss weight for each head. This will be multiplied to the loss of each head's output |\n",
       "| head_class | NoneType | None | The class object of the head. You can use RobertaClassificationHeadCustom as default |\n",
       "| head_class_kwargs |  |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RobertaBaseForSequenceClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RobertaHiddenStateConcatForSequenceClassification(RobertaPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Roberta Architecture with Hidden-State-Concatenation for Sequence Classification task\n",
    "    \"\"\"\n",
    "    \n",
    "    config_class = RobertaConfig\n",
    "\n",
    "    def __init__(self,config, # HuggingFace model configuration\n",
    "                 layer2concat=4, # number of hidden layer to concatenate (counting from top)\n",
    "                 is_multilabel=False, # Whether this is a multilabel classification\n",
    "                 is_multihead=False, # Whether this is a multihead (multi-level) classification\n",
    "                 head_class_sizes=[], # Class size for each head\n",
    "                 head_weights=[], # loss weight for each head. This will be multiplied to the loss of each head's output\n",
    "                 head_class=None, # The class object of the head. You can use RobertaClassificationHeadCustom as default\n",
    "                 **head_class_kwargs, # Keyword arguments for the head class\n",
    "                ):\n",
    "        super().__init__(config)\n",
    "        self.is_multilabel = is_multilabel\n",
    "        self.is_multihead = is_multihead\n",
    "        self.head_class_sizes = val2iterable(head_class_sizes)\n",
    "        self.head_weights = val2iterable(head_weights,lsize=len(self.head_class_sizes))\n",
    "        self.layer2concat=layer2concat\n",
    "        # set num_labels for config\n",
    "        num_labels = sum(self.head_class_sizes)\n",
    "        config.num_labels = num_labels\n",
    "        \n",
    "        # Load model body\n",
    "        # add_pooling_layer to False to ensure all hidden states are returned  and not only the one associated with the [CLS] token.\n",
    "        self.body_model = RobertaModel(config, add_pooling_layer=False)\n",
    "        \n",
    "        # Set up classification head\n",
    "        self.classification_head = head_class(config=config,layer2concat=layer2concat,\n",
    "                                              **head_class_kwargs)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids= None,\n",
    "        attention_mask= None,\n",
    "        token_type_ids= None,\n",
    "        position_ids= None,\n",
    "        head_mask= None,\n",
    "        inputs_embeds= None,\n",
    "        labels= None,\n",
    "        output_attentions= None,\n",
    "        output_hidden_states= None,\n",
    "        return_dict= None,\n",
    "        ):\n",
    "        \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "\n",
    "        outputs = self.body_model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        hidden_states = outputs['hidden_states'] # tuples with len 13 (number of layer/block)\n",
    "        # each with shape: (bs,seq_len,hidden_size_len), e.g. for phobert: (bs,256, 768)\n",
    "        # Note: hidden_size_len = embedding_size\n",
    "        hidden_concat = torch.cat([hidden_states[i][:,0] for i in range(-1,-self.layer2concat-1,-1)],\n",
    "                                  -1) \n",
    "        logits = self.classification_head(hidden_concat) # (bs,sum of all class sizes)\n",
    "        \n",
    "        # Calculate losses\n",
    "        if labels is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            loss = loss_for_classification(logits, labels, \n",
    "                                       self.is_multilabel,\n",
    "                                       self.is_multihead, \n",
    "                                       self.head_class_sizes,\n",
    "                                       self.head_weights)\n",
    "\n",
    "            \n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss, logits=logits,\n",
    "#                                      hidden_states=outputs.hidden_states,\n",
    "                                        hidden_states=None,\n",
    "                                        attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/models/roberta/classifiers.py#L191){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RobertaHiddenStateConcatForSequenceClassification\n",
       "\n",
       ">      RobertaHiddenStateConcatForSequenceClassification (config,\n",
       ">                                                         layer2concat=4,\n",
       ">                                                         is_multilabel=False,\n",
       ">                                                         is_multihead=False,\n",
       ">                                                         head_class_sizes=[],\n",
       ">                                                         head_weights=[],\n",
       ">                                                         head_class=None,\n",
       ">                                                         **head_class_kwargs)\n",
       "\n",
       "Roberta Architecture with Hidden-State-Concatenation for Sequence Classification task\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| config |  |  | HuggingFace model configuration |\n",
       "| layer2concat | int | 4 | number of hidden layer to concatenate (counting from top) |\n",
       "| is_multilabel | bool | False | Whether this is a multilabel classification |\n",
       "| is_multihead | bool | False | Whether this is a multihead (multi-level) classification |\n",
       "| head_class_sizes | list | [] | Class size for each head |\n",
       "| head_weights | list | [] | loss weight for each head. This will be multiplied to the loss of each head's output |\n",
       "| head_class | NoneType | None | The class object of the head. You can use RobertaClassificationHeadCustom as default |\n",
       "| head_class_kwargs |  |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/models/roberta/classifiers.py#L191){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RobertaHiddenStateConcatForSequenceClassification\n",
       "\n",
       ">      RobertaHiddenStateConcatForSequenceClassification (config,\n",
       ">                                                         layer2concat=4,\n",
       ">                                                         is_multilabel=False,\n",
       ">                                                         is_multihead=False,\n",
       ">                                                         head_class_sizes=[],\n",
       ">                                                         head_weights=[],\n",
       ">                                                         head_class=None,\n",
       ">                                                         **head_class_kwargs)\n",
       "\n",
       "Roberta Architecture with Hidden-State-Concatenation for Sequence Classification task\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| config |  |  | HuggingFace model configuration |\n",
       "| layer2concat | int | 4 | number of hidden layer to concatenate (counting from top) |\n",
       "| is_multilabel | bool | False | Whether this is a multilabel classification |\n",
       "| is_multihead | bool | False | Whether this is a multihead (multi-level) classification |\n",
       "| head_class_sizes | list | [] | Class size for each head |\n",
       "| head_weights | list | [] | loss weight for each head. This will be multiplied to the loss of each head's output |\n",
       "| head_class | NoneType | None | The class object of the head. You can use RobertaClassificationHeadCustom as default |\n",
       "| head_class_kwargs |  |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RobertaHiddenStateConcatForSequenceClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
