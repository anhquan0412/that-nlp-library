{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Hierarchical Classifiers\n",
    "\n",
    "> This module contains code to build a deep hierarchical classifier, which is a reimplementation of this paper: [https://arxiv.org/ftp/arxiv/papers/2005/2005.06692.pdf](https://arxiv.org/ftp/arxiv/papers/2005/2005.06692.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.roberta.deep_hierarchical_classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import torch\n",
    "from transformers.models.roberta.configuration_roberta import RobertaConfig\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def build_DHC_conditional_mask(df_labels,\n",
    "                                  label1,label2):\n",
    "    \"\"\"\n",
    "    This is really similar to `build_standard_condition_mask`, \n",
    "    but we don't concatenate l1 1-hot matrix and l2 1-hot matrix\n",
    "    \"\"\"\n",
    "    df_labels = df_labels.drop_duplicates().sort_values([label1,label2])\n",
    "    _d = df_labels.groupby([label1])[label2].apply(list).to_dict()\n",
    "    \n",
    "    mlb = MultiLabelBinarizer()\n",
    "    results = mlb.fit_transform([v for k,v in sorted(_d.items())])\n",
    "    return torch.from_numpy(results).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col_1  col_2\n",
       "0      0      0\n",
       "1      0      1\n",
       "2      0      2\n",
       "3      1      3\n",
       "4      1      4\n",
       "5      2      5\n",
       "6      2      6\n",
       "7      2      7"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df_labels=pd.DataFrame({\n",
    "    'col_1':[0,0,0,1,1,2,2,2],\n",
    "    'col_2':[0,1,2,3,4,5,6,7]\n",
    "})\n",
    "_df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1l2_tensor = build_DHC_conditional_mask(_df_labels,'col_1','col_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 1., 1.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1l2_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "def _check_hierarchy(l1_pred,l2_pred,dhc_mask,size_l2):\n",
    "    # Check if the predicted class at L2 is a children of the class predicted at L1 for the entire batch\n",
    "    # if is a children return 0, else 1\n",
    "    _l1_pred = dhc_mask[l1_pred] # bs,size_l2\n",
    "    _l2_pred = torch.nn.functional.one_hot(l2_pred, num_classes=size_l2) # bs, size_l2\n",
    "    return (~torch.mul(_l1_pred,_l2_pred).sum(axis=1).bool()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def loss_for_DHC(l1_repr_logits, # Head 1's logit output\n",
    "                 l2_repr_logits, # Head 2's logit output\n",
    "                 labels_l1, # True label for head 1\n",
    "                 labels_l2, # True label for head 2\n",
    "                 dhc_mask, # A one-hot matrix between classes of head 1 and 2\n",
    "                 lloss_weight=1.0, # Weight for Layer Loss (lloss)\n",
    "                 dloss_weight=0.8 # Weight for Dependence Loss (dloss)\n",
    "                ):\n",
    "    \"Reference: https://github.com/Ugenteraan/Deep_Hierarchical_Classification/blob/main/model/hierarchical_loss.py\"\n",
    "    size_l2 = dhc_mask.shape[1]\n",
    "    \n",
    "    # calculate lloss\n",
    "    lloss = 0\n",
    "    lloss += torch.nn.CrossEntropyLoss()(l1_repr_logits, labels_l1)\n",
    "    lloss += torch.nn.CrossEntropyLoss()(l2_repr_logits, labels_l2)\n",
    "    lloss = lloss_weight*lloss\n",
    "\n",
    "\n",
    "    # calculate dloss\n",
    "    l1_pred = torch.argmax(l1_repr_logits, dim=1)\n",
    "    l2_pred = torch.argmax(l2_repr_logits, dim=1)\n",
    "    D_l = _check_hierarchy(l1_pred,l2_pred,dhc_mask,size_l2)\n",
    "    l1_not_match_true_label = (~(l1_pred == labels_l1)).float()\n",
    "    l2_not_match_true_label = (~(l2_pred == labels_l2)).float()\n",
    "    dloss_l1 = torch.nn.CrossEntropyLoss()(l1_repr_logits*D_l[:,None]*l1_not_match_true_label[:,None], labels_l1)\n",
    "    dloss_l2 = torch.nn.CrossEntropyLoss()(l2_repr_logits*D_l[:,None]*l2_not_match_true_label[:,None], labels_l2)\n",
    "    dloss = dloss_weight*(dloss_l1 + dloss_l2)  \n",
    "    \n",
    "    \n",
    "    loss = lloss + dloss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/models/roberta/deep_hierarchical_classifiers.py#L39){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### loss_for_DHC\n",
       "\n",
       ">      loss_for_DHC (l1_repr_logits, l2_repr_logits, labels_l1, labels_l2,\n",
       ">                    dhc_mask, lloss_weight=1.0, dloss_weight=0.8)\n",
       "\n",
       "Reference: https://github.com/Ugenteraan/Deep_Hierarchical_Classification/blob/main/model/hierarchical_loss.py\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| l1_repr_logits |  |  | Head 1's logit output |\n",
       "| l2_repr_logits |  |  | Head 2's logit output |\n",
       "| labels_l1 |  |  | True label for head 1 |\n",
       "| labels_l2 |  |  | True label for head 2 |\n",
       "| dhc_mask |  |  | A one-hot matrix between classes of head 1 and 2 |\n",
       "| lloss_weight | float | 1.0 | Weight for Layer Loss (lloss) |\n",
       "| dloss_weight | float | 0.8 | Weight for Dependence Loss (dloss) |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/models/roberta/deep_hierarchical_classifiers.py#L39){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### loss_for_DHC\n",
       "\n",
       ">      loss_for_DHC (l1_repr_logits, l2_repr_logits, labels_l1, labels_l2,\n",
       ">                    dhc_mask, lloss_weight=1.0, dloss_weight=0.8)\n",
       "\n",
       "Reference: https://github.com/Ugenteraan/Deep_Hierarchical_Classification/blob/main/model/hierarchical_loss.py\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| l1_repr_logits |  |  | Head 1's logit output |\n",
       "| l2_repr_logits |  |  | Head 2's logit output |\n",
       "| labels_l1 |  |  | True label for head 1 |\n",
       "| labels_l2 |  |  | True label for head 2 |\n",
       "| dhc_mask |  |  | A one-hot matrix between classes of head 1 and 2 |\n",
       "| lloss_weight | float | 1.0 | Weight for Layer Loss (lloss) |\n",
       "| dloss_weight | float | 0.8 | Weight for Dependence Loss (dloss) |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(loss_for_DHC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RobertaConcatHeadDHCRoot(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Concatenated head for Roberta DHC Classification Model. \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 config, # HuggingFace model configuration\n",
    "                 classifier_dropout=0.1, # Dropout ratio (for dropout layer right before the last nn.Linear)\n",
    "                 last_hidden_size=768, # Last hidden size (before the last nn.Linear)\n",
    "                 layer2concat=4, # number of hidden layer to concatenate (counting from top)\n",
    "                 **kwargs\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.classifier_dropout=classifier_dropout\n",
    "        self.last_hidden_size=last_hidden_size\n",
    "        self.dropout = torch.nn.Dropout(classifier_dropout)\n",
    "        self.pre_classifier = torch.nn.Linear(layer2concat*config.hidden_size,\n",
    "                                              last_hidden_size)    \n",
    "    def forward(self, inp, **kwargs):\n",
    "        x = inp\n",
    "        x = self.dropout(x)\n",
    "        x = self.pre_classifier(x)\n",
    "        x = torch.tanh(x)\n",
    "#         x = torch.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/models/roberta/deep_hierarchical_classifiers.py#L73){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RobertaConcatHeadDHCRoot\n",
       "\n",
       ">      RobertaConcatHeadDHCRoot (config, classifier_dropout=0.1,\n",
       ">                                last_hidden_size=768, layer2concat=4, **kwargs)\n",
       "\n",
       "Concatenated head for Roberta DHC Classification Model.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| config |  |  | HuggingFace model configuration |\n",
       "| classifier_dropout | float | 0.1 | Dropout ratio (for dropout layer right before the last nn.Linear) |\n",
       "| last_hidden_size | int | 768 | Last hidden size (before the last nn.Linear) |\n",
       "| layer2concat | int | 4 | number of hidden layer to concatenate (counting from top) |\n",
       "| kwargs |  |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/models/roberta/deep_hierarchical_classifiers.py#L73){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RobertaConcatHeadDHCRoot\n",
       "\n",
       ">      RobertaConcatHeadDHCRoot (config, classifier_dropout=0.1,\n",
       ">                                last_hidden_size=768, layer2concat=4, **kwargs)\n",
       "\n",
       "Concatenated head for Roberta DHC Classification Model.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| config |  |  | HuggingFace model configuration |\n",
       "| classifier_dropout | float | 0.1 | Dropout ratio (for dropout layer right before the last nn.Linear) |\n",
       "| last_hidden_size | int | 768 | Last hidden size (before the last nn.Linear) |\n",
       "| layer2concat | int | 4 | number of hidden layer to concatenate (counting from top) |\n",
       "| kwargs |  |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RobertaConcatHeadDHCRoot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RobertaSimpleHSCDHCSequenceClassification(RobertaPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Roberta Simple-DHC Architecture with Hidden-State-Concatenation for Sequence Classification task\n",
    "    \"\"\"\n",
    "    config_class = RobertaConfig\n",
    "\n",
    "    def __init__(self, \n",
    "                 config, # HuggingFace model configuration\n",
    "                 dhc_mask, # A one-hot matrix between classes of head 1 and 2\n",
    "                 lloss_weight=1.0, # Weight for Layer Loss (lloss)\n",
    "                 dloss_weight=0.8, # Weight for Dependence Loss (dloss)\n",
    "                 layer2concat=4, # number of hidden layer to concatenate (counting from top)\n",
    "                 device=None # CPU or GPU\n",
    "                ):\n",
    "        \n",
    "        super().__init__(config)\n",
    "        self.training_device = device if device is not None else torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.size_l1 = dhc_mask.shape[0]\n",
    "        self.size_l2 = dhc_mask.shape[1]\n",
    "        self.head_class_sizes=[self.size_l1,self.size_l2] # will be useful for metric calculation later\n",
    "\n",
    "        self.dhc_mask = dhc_mask.to(self.training_device)\n",
    "        self.layer2concat=layer2concat\n",
    "    \n",
    "        self.lloss_weight = lloss_weight\n",
    "        self.dloss_weight = dloss_weight\n",
    "        \n",
    "        self.body_model = RobertaModel(config, add_pooling_layer=False)\n",
    "        \n",
    "        self.linear_L1_logit = torch.nn.Linear(layer2concat*config.hidden_size,self.size_l1)\n",
    "\n",
    "        self.linear_L2 = torch.nn.Linear(layer2concat*config.hidden_size, abs(self.size_l2-self.size_l1))\n",
    "        self.linear_L2_logit = torch.nn.Linear(self.size_l2,self.size_l2)\n",
    "        \n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n",
    "                labels=None, **kwargs):\n",
    "        # Use model body to get encoder representations\n",
    "        # the only ones we need for now are input_ids and attention_mask\n",
    "        outputs = self.body_model(input_ids, attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids, **kwargs)\n",
    "        \n",
    "        hidden_states = outputs['hidden_states'] # tuples with len 13 (number of layer/block)\n",
    "        # each with shape: (bs,seq_len,hidden_size_len), e.g. for phobert: (bs,256, 768)\n",
    "        # Note: hidden_size_len = embedding_size\n",
    "        \n",
    "        hidden_concat = torch.cat([hidden_states[i][:,0] for i in range(-1,-self.layer2concat-1,-1)],\n",
    "                                  -1) # (bs,768*4)\n",
    "        \n",
    "        # root_representation\n",
    "        root_repr = hidden_concat # (bs,768*4) \n",
    "        \n",
    "        l1_repr_logits = self.linear_L1_logit(root_repr) # (bs,size_l1)\n",
    "        \n",
    "        l2_repr = self.linear_L2(root_repr) # (bs,size_l2-size_l1)\n",
    "        l2_repr = torch.cat((l2_repr,l1_repr_logits),dim=1) # (bs, size_l2)\n",
    "        l2_repr_logits = self.linear_L2_logit(l2_repr) # (bs, size_l2) \n",
    "        \n",
    "        loss = None\n",
    "        \n",
    "        if labels is not None:\n",
    "            # labels shape: (bs,2), first is L1, second is L2\n",
    "            labels_l1 = labels[:,0].view(-1) #(bs,)\n",
    "            labels_l2 = labels[:,1].view(-1) #(bs,)\n",
    "\n",
    "            loss = loss_for_DHC(l1_repr_logits, l2_repr_logits, \n",
    "                                labels_l1, labels_l2, \n",
    "                                self.dhc_mask,\n",
    "                                self.lloss_weight,self.dloss_weight)\n",
    "            \n",
    "        # Return model output object\n",
    "        return SequenceClassifierOutput(loss=loss, logits=(l1_repr_logits,l2_repr_logits),\n",
    "                                     hidden_states=None,\n",
    "                                     attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/models/roberta/deep_hierarchical_classifiers.py#L99){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RobertaSimpleHSCDHCSequenceClassification\n",
       "\n",
       ">      RobertaSimpleHSCDHCSequenceClassification (config, dhc_mask,\n",
       ">                                                 lloss_weight=1.0,\n",
       ">                                                 dloss_weight=0.8,\n",
       ">                                                 layer2concat=4, device=None)\n",
       "\n",
       "Roberta Simple-DHC Architecture with Hidden-State-Concatenation for Sequence Classification task\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| config |  |  | HuggingFace model configuration |\n",
       "| dhc_mask |  |  | A one-hot matrix between classes of head 1 and 2 |\n",
       "| lloss_weight | float | 1.0 | Weight for Layer Loss (lloss) |\n",
       "| dloss_weight | float | 0.8 | Weight for Dependence Loss (dloss) |\n",
       "| layer2concat | int | 4 | number of hidden layer to concatenate (counting from top) |\n",
       "| device | NoneType | None | CPU or GPU |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/models/roberta/deep_hierarchical_classifiers.py#L99){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RobertaSimpleHSCDHCSequenceClassification\n",
       "\n",
       ">      RobertaSimpleHSCDHCSequenceClassification (config, dhc_mask,\n",
       ">                                                 lloss_weight=1.0,\n",
       ">                                                 dloss_weight=0.8,\n",
       ">                                                 layer2concat=4, device=None)\n",
       "\n",
       "Roberta Simple-DHC Architecture with Hidden-State-Concatenation for Sequence Classification task\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| config |  |  | HuggingFace model configuration |\n",
       "| dhc_mask |  |  | A one-hot matrix between classes of head 1 and 2 |\n",
       "| lloss_weight | float | 1.0 | Weight for Layer Loss (lloss) |\n",
       "| dloss_weight | float | 0.8 | Weight for Dependence Loss (dloss) |\n",
       "| layer2concat | int | 4 | number of hidden layer to concatenate (counting from top) |\n",
       "| device | NoneType | None | CPU or GPU |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RobertaSimpleHSCDHCSequenceClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RobertaHSCDHCSequenceClassification(RobertaPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Roberta DHC Architecture with Hidden-State-Concatenation for Sequence Classification task\n",
    "    \"\"\"    \n",
    "    config_class = RobertaConfig\n",
    "\n",
    "    def __init__(self, \n",
    "                 config, # HuggingFace model configuration\n",
    "                 dhc_mask, # A one-hot matrix between classes of head 1 and 2\n",
    "                 classifier_dropout=0.1, # Dropout ratio (for dropout layer right before the last nn.Linear)\n",
    "                 last_hidden_size=768, # Last hidden size (before the last nn.Linear)\n",
    "                 linear_l1_size=None, # last hidden size for head 1\n",
    "                 linear_l2_size=None, # last hidden size for head 2\n",
    "                 lloss_weight=1.0, # Weight for Layer Loss (lloss)\n",
    "                 dloss_weight=0.8, # Weight for Dependence Loss (dloss)\n",
    "                 layer2concat=4, # number of hidden layer to concatenate (counting from top)\n",
    "                 device=None # CPU or GPU\n",
    "                ):\n",
    "        \n",
    "        super().__init__(config)\n",
    "        self.training_device = device if device is not None else torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.size_l1 = dhc_mask.shape[0]\n",
    "        self.size_l2 = dhc_mask.shape[1]\n",
    "        self.head_class_sizes=[self.size_l1,self.size_l2] # will be useful for metric calculation later\n",
    "\n",
    "        self.dhc_mask = dhc_mask.to(self.training_device)\n",
    "        self.layer2concat=layer2concat\n",
    "\n",
    "        \n",
    "        if linear_l1_size is None: linear_l1_size = (last_hidden_size+self.size_l1)//2 # 389\n",
    "        if linear_l2_size is None: linear_l2_size = (last_hidden_size+self.size_l2)//2 # 417\n",
    "            \n",
    "        self.lloss_weight = lloss_weight\n",
    "        self.dloss_weight = dloss_weight\n",
    "        \n",
    "        self.body_model = RobertaModel(config, add_pooling_layer=False)\n",
    "        self.root_representation = RobertaConcatHeadDHCRoot(config=config,\n",
    "                                                            classifier_dropout=classifier_dropout,\n",
    "                                                            last_hidden_size=last_hidden_size)\n",
    "        \n",
    "        self.linear_L1 = torch.nn.Linear(last_hidden_size, linear_l1_size)\n",
    "        self.linear_L2 = torch.nn.Linear(last_hidden_size, linear_l2_size)\n",
    "        \n",
    "        self.linear_L1_logit = torch.nn.Linear(linear_l1_size,self.size_l1)\n",
    "        self.linear_L2_logit = torch.nn.Linear(linear_l1_size + linear_l2_size,self.size_l2)\n",
    "        \n",
    "\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n",
    "                labels=None, **kwargs):\n",
    "        # Use model body to get encoder representations\n",
    "        # the only ones we need for now are input_ids and attention_mask\n",
    "        outputs = self.body_model(input_ids, attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids, **kwargs)\n",
    "        \n",
    "        hidden_states = outputs['hidden_states'] # tuples with len 13 (number of layer/block)\n",
    "        # each with shape: (bs,seq_len,hidden_size_len), e.g. for phobert: (bs,256, 768)\n",
    "        # Note: hidden_size_len = embedding_size\n",
    "        \n",
    "        hidden_concat = torch.cat([hidden_states[i][:,0] for i in range(-1,-self.layer2concat-1,-1)],\n",
    "                                  -1) # (bs,768*4)\n",
    "        \n",
    "        # root_representation\n",
    "        root_repr = self.root_representation(hidden_concat) # (bs,768) \n",
    "        \n",
    "        l1_repr = self.linear_L1(root_repr) # (bs,linear_L1_size)\n",
    "        l1_repr_logits = self.linear_L1_logit(l1_repr) # (bs,size_l1)\n",
    "        l2_repr = self.linear_L2(root_repr) # (bs,linear_L2_size)\n",
    "        l2_repr = torch.cat((l2_repr,l1_repr),dim=1) # (bs, linear_l1_size + linear_l2_size)\n",
    "        l2_repr_logits = self.linear_L2_logit(l2_repr) # (bs, size_l2) \n",
    "\n",
    "        \n",
    "        loss = None\n",
    "        \n",
    "        if labels is not None:\n",
    "            # labels shape: (bs,2), first is L3, second is L4\n",
    "            labels_l1 = labels[:,0].view(-1) #(bs,)\n",
    "            labels_l2 = labels[:,1].view(-1) #(bs,)\n",
    "\n",
    "            loss = loss_for_DHC(l1_repr_logits, l2_repr_logits, \n",
    "                                labels_l1, labels_l2, \n",
    "                                self.dhc_mask,\n",
    "                                self.lloss_weight,self.dloss_weight)\n",
    "            \n",
    "        # Return model output object\n",
    "        return SequenceClassifierOutput(loss=loss, logits=(l1_repr_logits,l2_repr_logits),\n",
    "                                     hidden_states=None,\n",
    "                                     attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/models/roberta/deep_hierarchical_classifiers.py#L175){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RobertaHSCDHCSequenceClassification\n",
       "\n",
       ">      RobertaHSCDHCSequenceClassification (config, dhc_mask,\n",
       ">                                           classifier_dropout=0.1,\n",
       ">                                           last_hidden_size=768,\n",
       ">                                           linear_l1_size=None,\n",
       ">                                           linear_l2_size=None,\n",
       ">                                           lloss_weight=1.0, dloss_weight=0.8,\n",
       ">                                           layer2concat=4, device=None)\n",
       "\n",
       "Roberta DHC Architecture with Hidden-State-Concatenation for Sequence Classification task\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| config |  |  | HuggingFace model configuration |\n",
       "| dhc_mask |  |  | A one-hot matrix between classes of head 1 and 2 |\n",
       "| classifier_dropout | float | 0.1 | Dropout ratio (for dropout layer right before the last nn.Linear) |\n",
       "| last_hidden_size | int | 768 | Last hidden size (before the last nn.Linear) |\n",
       "| linear_l1_size | NoneType | None | last hidden size for head 1 |\n",
       "| linear_l2_size | NoneType | None | last hidden size for head 2 |\n",
       "| lloss_weight | float | 1.0 | Weight for Layer Loss (lloss) |\n",
       "| dloss_weight | float | 0.8 | Weight for Dependence Loss (dloss) |\n",
       "| layer2concat | int | 4 | number of hidden layer to concatenate (counting from top) |\n",
       "| device | NoneType | None | CPU or GPU |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/models/roberta/deep_hierarchical_classifiers.py#L175){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RobertaHSCDHCSequenceClassification\n",
       "\n",
       ">      RobertaHSCDHCSequenceClassification (config, dhc_mask,\n",
       ">                                           classifier_dropout=0.1,\n",
       ">                                           last_hidden_size=768,\n",
       ">                                           linear_l1_size=None,\n",
       ">                                           linear_l2_size=None,\n",
       ">                                           lloss_weight=1.0, dloss_weight=0.8,\n",
       ">                                           layer2concat=4, device=None)\n",
       "\n",
       "Roberta DHC Architecture with Hidden-State-Concatenation for Sequence Classification task\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| config |  |  | HuggingFace model configuration |\n",
       "| dhc_mask |  |  | A one-hot matrix between classes of head 1 and 2 |\n",
       "| classifier_dropout | float | 0.1 | Dropout ratio (for dropout layer right before the last nn.Linear) |\n",
       "| last_hidden_size | int | 768 | Last hidden size (before the last nn.Linear) |\n",
       "| linear_l1_size | NoneType | None | last hidden size for head 1 |\n",
       "| linear_l2_size | NoneType | None | last hidden size for head 2 |\n",
       "| lloss_weight | float | 1.0 | Weight for Layer Loss (lloss) |\n",
       "| dloss_weight | float | 0.8 | Weight for Dependence Loss (dloss) |\n",
       "| layer2concat | int | 4 | number of hidden layer to concatenate (counting from top) |\n",
       "| device | NoneType | None | CPU or GPU |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RobertaHSCDHCSequenceClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
