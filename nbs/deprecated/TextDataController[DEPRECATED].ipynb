{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9b93d97",
   "metadata": {},
   "source": [
    "# Text Data Controller\n",
    "\n",
    "> This module contains the old Python class for TextDataController\n",
    "\n",
    "- skip_showdoc: true\n",
    "- skip_exec: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00d819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataController():\n",
    "    def __init__(self,\n",
    "                 inp, # HuggingFainpce Dataset or DatasetDict\n",
    "                 main_text:str, # Name of the main text column\n",
    "                 label_names=None, # Names of the label (dependent variable) columns\n",
    "                 class_names_predefined=None, # List of names associated with the labels (same index order)\n",
    "                 filter_dict={}, # A dictionary: {feature: filtering_function_based_on_the_feature}\n",
    "                 metadatas=[], # Names of the metadata columns\n",
    "                 process_metas=True, # Whether to do simple text processing on the chosen metadatas\n",
    "                 content_transformations=[], # A list of text transformations\n",
    "                 val_ratio:int|float|None=0.2, # Ratio of data for validation set\n",
    "                 stratify_cols=[], # Column(s) needed to do stratified shuffle split\n",
    "                 upsampling_list={}, # A list of tuple. Each tuple: (feature,upsampling_function_based_on_the_feature)\n",
    "                 content_augmentations=[], # A list of text augmentations\n",
    "                 seed=None, # Random seed\n",
    "                 batch_size=1000, # CPU batch size\n",
    "                 num_proc=4, # Number of process for multiprocessing\n",
    "                 cols_to_keep=None, # Columns to keep after all processings\n",
    "                 buffer_size=10000, # For shuffling data\n",
    "                 num_shards=64, # Number of shards. Stream datasets can be made out of multiple shards\n",
    "                 convert_training_to_iterable=True, # Whether to convert training Dataset to IterableDataset\n",
    "                 verbose=True, # Whether to print processing information\n",
    "                ):\n",
    "            \n",
    "        self.main_text = main_text\n",
    "        self.metadatas = val2iterable(metadatas)\n",
    "        self.process_metas = process_metas\n",
    "        self.label_names = val2iterable(label_names) if label_names is not None else None\n",
    "        self.label_lists = class_names_predefined\n",
    "        self.filter_dict = filter_dict\n",
    "        self.content_tfms = val2iterable(content_transformations)\n",
    "        self.upsampling_list = upsampling_list\n",
    "        self.aug_tfms = val2iterable(content_augmentations)\n",
    "        self.val_ratio = val_ratio\n",
    "        self.stratify_cols = val2iterable(stratify_cols)\n",
    "        self.seed = seed\n",
    "        self.is_batched = batch_size>1\n",
    "        self.batch_size = batch_size\n",
    "        self.num_proc = num_proc\n",
    "        self.cols_to_keep = cols_to_keep\n",
    "        self.buffer_size = buffer_size\n",
    "        self.num_shards = num_shards\n",
    "        self.ddict_rest = DatasetDict()\n",
    "        self.convert_training_to_iterable = convert_training_to_iterable\n",
    "        self.verbose = verbose\n",
    "        self.verboseprint = print if verbose else lambda *a, **k: None\n",
    "        \n",
    "        if hasattr(inp,'keys'):\n",
    "            if 'train' in inp.keys(): # is datasetdict\n",
    "                self.ddict_rest = inp\n",
    "                self.dset = self.ddict_rest.pop('train')\n",
    "            else:\n",
    "                raise ValueError('The given DatasetDict has no \"train\" split')\n",
    "        else: # is dataset\n",
    "            self.dset = inp\n",
    "            \n",
    "        self.is_streamed=isinstance(self.dset,IterableDataset)\n",
    "        \n",
    "        self.all_cols = get_dset_col_names(self.dset)\n",
    "        if self.is_streamed and self.label_names is not None and self.label_lists is None:\n",
    "            raise ValueError('All class labels must be provided when streaming')\n",
    "        \n",
    "        if self.is_streamed and len(self.upsampling_list):\n",
    "            warnings.warn(\"Upsampling requires dataset concatenation, which can be extremely slow (x2) for streamed dataset\")\n",
    "            \n",
    "        self._processed_call=False\n",
    "        \n",
    "        self._determine_multihead_multilabel()\n",
    "        \n",
    "            \n",
    "    @classmethod\n",
    "    def from_csv(cls,file_path,**kwargs):\n",
    "        file_path = Path(file_path)\n",
    "        ds = load_dataset(str(file_path.parent),\n",
    "                                  data_files=file_path.name,\n",
    "                                  split='train')\n",
    "        return TextDataController(ds,**kwargs)\n",
    "        \n",
    "    \n",
    "    @classmethod\n",
    "    def from_df(cls,df,validate=True,**kwargs):\n",
    "        if validate:\n",
    "            check_input_validation(df)\n",
    "        ds = Dataset.from_pandas(df)\n",
    "        return TextDataController(ds,**kwargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pickle(cls,\n",
    "                    fname, # Name of the pickle file\n",
    "                    parent='pickle_files' # Parent folder\n",
    "                   ):\n",
    "        return load_pickle(fname,parent=parent)\n",
    "    \n",
    "    def _determine_multihead_multilabel(self):\n",
    "        self.is_multilabel=False\n",
    "        self.is_multihead=False\n",
    "        if self.label_names is None: return\n",
    "        \n",
    "        if len(self.label_names)>1:\n",
    "            self.is_multihead=True\n",
    "        # get label of first row\n",
    "        first_label = self.dset[self.label_names[0]][0] if not self.is_streamed else next(iter(self.dset))[self.label_names[0]]\n",
    "        if isinstance(first_label,(list,set,tuple)):\n",
    "            # This is multi-label. Ignore self.label_names[1:]\n",
    "            self.label_names = [self.label_names[0]]\n",
    "            self.is_multihead=False\n",
    "            self.is_multilabel=True\n",
    "                     \n",
    "    def validate_input(self):\n",
    "        if self.is_streamed:\n",
    "            self.verboseprint('Input validation check is disabled when data is streamed')\n",
    "            return\n",
    "        _df = self.dset.to_pandas()\n",
    "        check_input_validation(_df)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def save_as_pickles(self,\n",
    "                        fname, # Name of the pickle file\n",
    "                        parent='pickle_files', # Parent folder\n",
    "                        drop_data_attributes=False # Whether to drop all large-size data attributes\n",
    "                       ):\n",
    "        if drop_data_attributes:\n",
    "            if hasattr(self, 'main_ddict'):\n",
    "                del self.main_ddict\n",
    "            if hasattr(self, 'ddict_rest'):\n",
    "                del self.ddict_rest\n",
    "        save_to_pickle(self,fname,parent=parent)\n",
    "    \n",
    "        \n",
    "    def _check_validation_leaking(self):\n",
    "        if self.val_ratio is None or self.is_streamed:\n",
    "            return\n",
    "        \n",
    "        trn_txt = self.main_ddict['train'][self.main_text]\n",
    "        val_txt = self.main_ddict['validation'][self.main_text]        \n",
    "        val_txt_leaked = check_text_leaking(trn_txt,val_txt,verbose=self.verbose)\n",
    "        \n",
    "        if len(val_txt_leaked)==0: return\n",
    "        \n",
    "        # filter train dataset to get rid of leaks\n",
    "        self.verboseprint('Filtering leaked data out of training set...')\n",
    "        _func = partial(lambda_batch,\n",
    "                        feature=self.main_text,\n",
    "                        func=lambda x: x.strip().lower() not in val_txt_leaked,\n",
    "                        is_batched=self.is_batched)\n",
    "        self.main_ddict['train'] = hf_filter_dset(self.main_ddict['train'],_func,self.is_batched,self.batch_size,self.num_proc)   \n",
    "        self.verboseprint('Done')\n",
    "           \n",
    "    def _train_test_split(self):\n",
    "        print_msg('Train Test Split',20,verbose=self.verbose)\n",
    "        val_key = list(set(self.ddict_rest.keys()) & set(['val','validation','valid']))\n",
    "        if len(val_key)==1: # val split exists\n",
    "            self.verboseprint('Validation split already exists')\n",
    "            self.main_ddict=DatasetDict({'train':self.dset,\n",
    "                                         'validation':self.ddict_rest.pop(val_key[0])})\n",
    "            \n",
    "    \n",
    "        elif self.val_ratio is None: # use all data\n",
    "            self.verboseprint('No validation split defined')\n",
    "            self.main_ddict=DatasetDict({'train':self.dset})\n",
    "            \n",
    "        elif (isinstance(self.val_ratio,float) or isinstance(self.val_ratio,int)) and not len(self.stratify_cols):\n",
    "            self.verboseprint('Validation split based on val_ratio')\n",
    "            if self.is_streamed:\n",
    "                if isinstance(self.val_ratio,float):\n",
    "                    warnings.warn(\"Length of streamed dataset is unknown to use float validation ratio. Default to the first 1000 data points for validation\")\n",
    "                    self.val_ratio=1000  \n",
    "                trn_dset = self.dset.skip(self.val_ratio)  \n",
    "                val_datas = list(self.dset.take(self.val_ratio))\n",
    "                val_dict={k: [v[k] for v in val_datas] for k in val_datas[0].keys()}   \n",
    "                val_dset = Dataset.from_dict(val_dict) \n",
    "                self.main_ddict=DatasetDict({'train':trn_dset,\n",
    "                                         'validation':val_dset})\n",
    "            else:\n",
    "                # train val split\n",
    "                self.main_ddict = self.dset.train_test_split(test_size=self.val_ratio,shuffle=True,seed=self.seed)\n",
    "                self.main_ddict['validation']=self.main_ddict['test']\n",
    "                del self.main_ddict['test']\n",
    "        \n",
    "        else: # val_ratio split with stratifying\n",
    "            if self.is_streamed: raise ValueError('Stratified split is not supported for streamed data')                \n",
    "            if self.is_multilabel and self.label_names[0] in self.stratify_cols:\n",
    "                raise ValueError('For MultiLabel classification, you cannot choose the label as your stratified column')\n",
    "            self.verboseprint('Validation split based on val_ratio, with stratifying')\n",
    "            # Create a new feature 'stratified', which is a concatenation of values in stratify_cols\n",
    "            if self.is_batched:\n",
    "                stratified_creation = lambda x: {'stratified':\n",
    "                                     ['_'.join(list(map(str,[x[v][i] for v in self.stratify_cols]))) \n",
    "                                      for i in range(len(x[self.stratify_cols[0]]))]}\n",
    "            else:\n",
    "                stratified_creation = lambda x: {'stratified':\n",
    "                                     '_'.join(list(map(str,[x[v] for v in self.stratify_cols]))) \n",
    "                                      }\n",
    "            self.dset = self.dset.map(stratified_creation,\n",
    "                                      batched=self.is_batched,\n",
    "                                      batch_size=self.batch_size,\n",
    "                                      num_proc=self.num_proc)\n",
    "            self.dset=self.dset.class_encode_column(\"stratified\")\n",
    "            # train val split\n",
    "            self.main_ddict = self.dset.train_test_split(test_size=self.val_ratio,\n",
    "                                                         shuffle=True,seed=self.seed,\n",
    "                                                        stratify_by_column='stratified')\n",
    "            self.main_ddict['validation']=self.main_ddict['test']\n",
    "            del self.main_ddict['test']\n",
    "            self.main_ddict=self.main_ddict.remove_columns(['stratified'])\n",
    "            \n",
    "        \n",
    "        del self.dset\n",
    "        self.verboseprint('Done')\n",
    "\n",
    "                             \n",
    "    def _create_label_mapping_func(self,encoder_classes):\n",
    "        if self.is_multihead:\n",
    "            label2idxs = [{v:i for i,v in enumerate(l_classes)} for l_classes in encoder_classes]\n",
    "                    \n",
    "            _func = lambda inp: {'label': [[label2idxs[i][v] for i,v in enumerate(vs)] for vs in zip(*[inp[l] for l in self.label_names])] \\\n",
    "                                    if self.is_batched else [label2idxs[i][v] for i,v in enumerate([inp[l] for l in self.label_names])]\n",
    "                              }\n",
    "            \n",
    "        else:\n",
    "            label2idx = {v:i for i,v in enumerate(encoder_classes[0])}\n",
    "            _func = partial(lambda_map_batch,\n",
    "                           feature=self.label_names[0],\n",
    "                           func=lambda x: label2idx[x],\n",
    "                           output_feature='label',\n",
    "                           is_batched=self.is_batched)\n",
    "        return _func\n",
    "        \n",
    "    def _encode_labels(self):\n",
    "        if self.label_names is None: return\n",
    "        print_msg('Label Encoding',verbose=self.verbose)\n",
    "        \n",
    "        if self.label_lists is not None and not isinstance(self.label_lists[0],list):\n",
    "            self.label_lists = [self.label_lists]\n",
    "                    \n",
    "        encoder_classes=[]\n",
    "        if not self.is_multilabel:\n",
    "            for idx,l in enumerate(self.label_names):\n",
    "                if self.label_lists is None:\n",
    "                    l_encoder = LabelEncoder()\n",
    "                    _ = l_encoder.fit(self.dset[l])\n",
    "                    l_classes = list(l_encoder.classes_)\n",
    "                else:\n",
    "                    l_classes = sorted(list(self.label_lists[idx]))\n",
    "                encoder_classes.append(l_classes)\n",
    "            \n",
    "            _func = self._create_label_mapping_func(encoder_classes)\n",
    "                \n",
    "            self.dset = hf_map_dset(self.dset,_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "\n",
    "            val_key = list(set(self.ddict_rest.keys()) & set(['val','validation','valid']))\n",
    "            if len(val_key)>1: raise ValueError('Your DatasetDict has more than 1 validation split')\n",
    "            if len(val_key)==1:\n",
    "                val_key=val_key[0]\n",
    "                self.ddict_rest[val_key] = hf_map_dset(self.ddict_rest[val_key],_func,\n",
    "                                                       self.is_batched,self.batch_size,self.num_proc)\n",
    "                    \n",
    "        else:\n",
    "            # For MultiLabel, we transform the label itself to one-hot (or actually, few-hot)\n",
    "            if self.label_lists is None:\n",
    "                l_encoder = MultiLabelBinarizer()\n",
    "                _ = l_encoder.fit(self.dset[self.label_names[0]])\n",
    "                l_classes = list(l_encoder.classes_)\n",
    "            else:\n",
    "                l_classes = sorted(list(self.label_lists[0]))\n",
    "            \n",
    "            encoder_classes.append(l_classes)\n",
    "            \n",
    "            l_encoder = MultiLabelBinarizer(classes=encoder_classes[0])\n",
    "            _ = l_encoder.fit(None)\n",
    "            _func = partial(lambda_map_batch,\n",
    "                            feature=self.label_names[0],\n",
    "                            func=lambda x: l_encoder.transform(x),\n",
    "                            output_feature='label',\n",
    "                            is_batched=self.is_batched,\n",
    "                            is_func_batched=True)\n",
    "            self.dset = hf_map_dset(self.dset,_func,self.is_batched,self.batch_size,self.num_proc)                                                 \n",
    "            \n",
    "            val_key = list(set(self.ddict_rest.keys()) & set(['val','validation','valid']))\n",
    "            if len(val_key)>1: raise ValueError('Your DatasetDict has more than 1 validation dataset')\n",
    "            if len(val_key)==1:\n",
    "                val_key=val_key[0]\n",
    "                self.ddict_rest[val_key] = hf_map_dset(self.ddict_rest[val_key],_func,\n",
    "                                                       self.is_batched,self.batch_size,self.num_proc)\n",
    "            \n",
    "        self.label_lists = encoder_classes\n",
    "        self.verboseprint('Done')\n",
    "        \n",
    "    def _process_metadatas(self,dset,ddict_rest=None):\n",
    "        if len(self.metadatas)>0:\n",
    "            print_msg('Metadata Simple Processing & Concatenating to Main Content',verbose=self.verbose)\n",
    "            map_func = partial(concat_metadatas,\n",
    "                               main_text=self.main_text,\n",
    "                               metadatas=self.metadatas,\n",
    "                               process_metas=self.process_metas,\n",
    "                               is_batched=self.is_batched)\n",
    "            dset = hf_map_dset(dset,map_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "            if ddict_rest is not None:\n",
    "                ddict_rest = hf_map_dset(ddict_rest,map_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "            self.verboseprint('Done')\n",
    "        return dset if ddict_rest is None else (dset,ddict_rest)\n",
    "            \n",
    "            \n",
    "    \n",
    "    def _simplify_ddict(self):\n",
    "        print_msg('Dropping unused features',20,verbose=self.verbose)\n",
    "        if self.cols_to_keep is None:\n",
    "            self.cols_to_keep= [self.main_text] + self.metadatas\n",
    "            if self.label_names is not None: self.cols_to_keep+=self.label_names\n",
    "        cols_to_remove = set(self.all_cols) - set(self.cols_to_keep)\n",
    "        self.main_ddict['train']=self.main_ddict['train'].remove_columns(list(cols_to_remove))\n",
    "        if 'validation' in self.main_ddict.keys():\n",
    "            self.main_ddict['validation']=self.main_ddict['validation'].remove_columns(list(cols_to_remove))\n",
    "        self.verboseprint('Done')\n",
    "    \n",
    "    def _do_transformation(self,dset,ddict_rest=None):\n",
    "        if len(self.content_tfms):\n",
    "            print_msg('Text Transformation',20,verbose=self.verbose)\n",
    "            for tfm in self.content_tfms:\n",
    "                print_msg(callable_name(tfm),verbose=self.verbose)\n",
    "                _func = partial(lambda_map_batch,\n",
    "                               feature=self.main_text,\n",
    "                               func=tfm,\n",
    "                               is_batched=self.is_batched)\n",
    "                dset = hf_map_dset(dset,_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "                if ddict_rest is not None:\n",
    "                    ddict_rest = hf_map_dset(ddict_rest,_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "            self.verboseprint('Done')\n",
    "        return dset if ddict_rest is None else (dset,ddict_rest)\n",
    " \n",
    "    def _do_filtering(self,dset,ddict_rest=None):\n",
    "        if len(self.filter_dict):\n",
    "            print_msg('Data Filtering',20,verbose=self.verbose)\n",
    "            col_names = get_dset_col_names(dset)\n",
    "            for f,tfm in self.filter_dict.items():\n",
    "                if f in col_names:\n",
    "                    print_msg(f'Do {callable_name(tfm)} on {f}',verbose=self.verbose)\n",
    "                    _func = partial(lambda_batch,\n",
    "                                    feature=f,\n",
    "                                    func=tfm,\n",
    "                                    is_batched=self.is_batched)\n",
    "                    dset = hf_filter_dset(dset,_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "                if ddict_rest is not None: # assuming ddict_rest has the column to filter, always\n",
    "                    ddict_rest = hf_filter_dset(ddict_rest,_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "            self.verboseprint('Done')\n",
    "        return dset if ddict_rest is None else (dset,ddict_rest)\n",
    "    \n",
    "    def _upsampling(self):\n",
    "        if len(self.upsampling_list):\n",
    "            print_msg('Upsampling data',20,verbose=self.verbose)\n",
    "            results=[]\n",
    "            for f,tfm in self.upsampling_list:\n",
    "                print_msg(f'Do {callable_name(tfm)} on {f}',verbose=self.verbose)\n",
    "                _func = partial(lambda_batch,\n",
    "                                feature=f,\n",
    "                                func=tfm,\n",
    "                                is_batched=self.is_batched)\n",
    "                new_dset = hf_filter_dset(self.main_ddict['train'],_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "                results.append(new_dset)\n",
    "            # slow concatenation for iterable dataset    \n",
    "            self.main_ddict['train'] = concatenate_datasets(results+[self.main_ddict['train']])\n",
    "            self.verboseprint('Done')\n",
    "      \n",
    "    def _do_augmentation(self):\n",
    "        \n",
    "        if len(self.aug_tfms):\n",
    "            print_msg('Text Augmentation',20,verbose=self.verbose)\n",
    "\n",
    "            seed_notorch(self.seed)\n",
    "#             self.main_ddict['train'] = self.main_ddict['train'].with_transform(partial(augmentation_helper,\n",
    "#                                                                    text_name=self.main_text,\n",
    "#                                                                    func=partial(func_all,functions=self.aug_tfms)))  \n",
    "            if not self.is_streamed:  \n",
    "                for tfm in self.aug_tfms:\n",
    "                    print_msg(callable_name(tfm),verbose=self.verbose)\n",
    "            \n",
    "                    bs = self.batch_size\n",
    "                    is_func_batched=False\n",
    "                    num_proc = self.num_proc\n",
    "                    is_batched = self.is_batched\n",
    "                    if hasattr(tfm, \"run_on_gpu\") and getattr(tfm,'run_on_gpu')==True:\n",
    "                        bs = 32 if not hasattr(tfm, \"batch_size\") else getattr(tfm,'batch_size')\n",
    "                        is_func_batched=True\n",
    "                        is_batched=True\n",
    "                        num_proc=1\n",
    "                        \n",
    "                    _func = partial(lambda_map_batch,\n",
    "                                   feature=self.main_text,\n",
    "                                   func=tfm,\n",
    "                                   is_batched=is_batched,\n",
    "                                   is_func_batched=is_func_batched\n",
    "                                   )\n",
    "                    self.main_ddict['train'] = hf_map_dset(self.main_ddict['train'],_func,\n",
    "                                                              is_batched=is_batched,\n",
    "                                                              batch_size=bs,\n",
    "                                                              num_proc=num_proc\n",
    "                                                             )\n",
    "\n",
    "            else: \n",
    "                self.main_ddict['train'] = IterableDataset.from_generator(augmentation_stream_generator,\n",
    "#                                                features = self.main_ddict['train'].features,\n",
    "                                               gen_kwargs={'dset': self.main_ddict['train'],\n",
    "                                                           'text_name':self.main_text,\n",
    "                                                           'func':partial(func_all,functions=self.aug_tfms)\n",
    "                                                          })\n",
    "            self.verboseprint('Done')\n",
    "        \n",
    "    def _convert_to_iterable(self):\n",
    "        if (not self.is_streamed) and self.convert_training_to_iterable:\n",
    "            print_msg('Converting train set to iterable',20,verbose=self.verbose)\n",
    "            self.main_ddict['train'] = self.main_ddict['train'].to_iterable_dataset(num_shards=self.num_shards)\n",
    "            self.is_streamed=True\n",
    "            self.verboseprint('Done')\n",
    "\n",
    "            \n",
    "    def _do_train_shuffling(self):\n",
    "        print_msg('Shuffling train set',20,verbose=self.verbose)\n",
    "        if self.is_streamed:\n",
    "            self.main_ddict['train'] = self.main_ddict['train'].shuffle(seed=self.seed, buffer_size=self.buffer_size)\n",
    "        else:\n",
    "            self.main_ddict['train'] = self.main_ddict['train'].shuffle(seed=self.seed)\n",
    "        self.verboseprint('Done')\n",
    "        \n",
    "    def do_all_preprocessing(self,\n",
    "                             shuffle_trn=True # To shuffle the train set before tokenization\n",
    "                            ):\n",
    "        if self._processed_call:\n",
    "            warnings.warn('Your dataset has already been processed. Returning the previous processed DatasetDict...')\n",
    "            return self.main_ddict\n",
    "            \n",
    "        print_msg('Start Main Text Processing',20,verbose=self.verbose)\n",
    "        \n",
    "        # Filtering\n",
    "        self.dset,self.ddict_rest = self._do_filtering(self.dset,self.ddict_rest)\n",
    "        \n",
    "        # Process metadatas\n",
    "        self.dset,self.ddict_rest = self._process_metadatas(self.dset,self.ddict_rest)\n",
    "        \n",
    "        # Process labels\n",
    "        self._encode_labels()\n",
    "        \n",
    "        # Content transformation\n",
    "        self.dset,self.ddict_rest = self._do_transformation(self.dset,self.ddict_rest)\n",
    "         \n",
    "        # Train Test Split.\n",
    "        ### self.main_ddict is created here\n",
    "        self._train_test_split()\n",
    "        \n",
    "        # Dropping unused columns\n",
    "        self._simplify_ddict()\n",
    "        \n",
    "        # Check validation leaking\n",
    "        self._check_validation_leaking()\n",
    "        \n",
    "        ### The rest of these functions applies only to the train dataset\n",
    "        # Upsampling\n",
    "        self._upsampling()\n",
    "        \n",
    "        # Augmentation\n",
    "        self._do_augmentation()\n",
    "           \n",
    "        # Convert train set to iterable\n",
    "        self._convert_to_iterable()\n",
    "        \n",
    "        # Shuffle train\n",
    "        if shuffle_trn:\n",
    "            self._do_train_shuffling()\n",
    "        \n",
    "        self._processed_call=True\n",
    "        \n",
    "        return self.main_ddict\n",
    "    \n",
    "        \n",
    "    def do_tokenization(self,\n",
    "                        tokenizer, # Tokenizer (preferably from HuggingFace)\n",
    "                        max_length=None, # pad to model's allowed max length (default is max_sequence_length)\n",
    "                        trn_size=None, # The number of training data to be tokenized\n",
    "                       ):\n",
    "        print_msg('Tokenization',20,verbose=self.verbose)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        tok_func = partial(tokenize_function,tok=tokenizer,max_length=max_length)\n",
    "        _func = partial(lambda_map_batch,\n",
    "                        feature=self.main_text,\n",
    "                        func=tok_func,\n",
    "                        output_feature=None,\n",
    "                        is_batched=self.is_batched)\n",
    "        \n",
    "        if trn_size is not None:\n",
    "            self.main_ddict['train'] = self.main_ddict['train'].take(trn_size)\n",
    "        \n",
    "        for k in self.main_ddict.keys():\n",
    "            self.main_ddict[k] = hf_map_dset(self.main_ddict[k],_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "\n",
    "        self.verboseprint('Done')\n",
    "        return self.main_ddict\n",
    "        \n",
    "    def process_and_tokenize(self,\n",
    "                             tokenizer, # Tokenizer (preferably from HuggingFace)\n",
    "                             max_length=None, # pad to model's allowed max length (default is max_sequence_length)\n",
    "                             trn_size=None, # The number of training data to be tokenized\n",
    "                             shuffle_trn=True, # To shuffle the train set before tokenization\n",
    "                            ):\n",
    "        \"\"\"\n",
    "        This will perform `do_all_processing` then `do_tokenization`\n",
    "        \"\"\"\n",
    "        _ = self.do_all_preprocessing(shuffle_trn)\n",
    "        _ = self.do_tokenization(tokenizer,max_length,trn_size)\n",
    "        \n",
    "    \n",
    "    def set_data_collator(self,data_collator):\n",
    "        self.data_collator = data_collator\n",
    "        \n",
    "    \n",
    "    def prepare_test_dataset_from_csv(self,\n",
    "                                      file_path, # path to csv file\n",
    "                                      do_filtering=False # whether to perform data filtering on this test set\n",
    "                                     ):\n",
    "        file_path = Path(file_path)\n",
    "        ds = load_dataset(str(file_path.parent),\n",
    "                          data_files=file_path.name,\n",
    "                          split='train')\n",
    "        return self.prepare_test_dataset(ds,do_filtering)\n",
    "    \n",
    "    def prepare_test_dataset_from_df(self,\n",
    "                                     df, # Pandas Dataframe\n",
    "                                     validate=True, # whether to perform input data validation\n",
    "                                     do_filtering=False # whether to perform data filtering on this test set \n",
    "                                    ):\n",
    "        if validate:\n",
    "            check_input_validation(df)\n",
    "        ds = Dataset.from_pandas(df)\n",
    "        return self.prepare_test_dataset(ds,do_filtering)\n",
    "    \n",
    "    def prepare_test_dataset_from_raws(self,\n",
    "                                       content, # Either a single sentence, list of sentence or a dictionary with keys are metadata columns and values are list\n",
    "                                      ):\n",
    "        if len(self.metadatas)!=0 and not isinstance(content,dict):\n",
    "            raise ValueError(f'There is/are metadatas in the preprocessing step. Please include a dictionary including these keys for metadatas: {self.metadatas}, and texture content: {self.main_text}')\n",
    "            \n",
    "        _dic = {self.main_text:[content]} if isinstance(content,str) else content\n",
    "        for k in _dic.keys():\n",
    "            _dic[k] = val2iterable(_dic[k])\n",
    "        \n",
    "        test_dict = Dataset.from_dict(_dic)\n",
    "        return self.prepare_test_dataset(test_dict,do_filtering=False)\n",
    "    \n",
    "    def prepare_test_dataset(self,\n",
    "                             test_dset, # The HuggingFace Dataset as Test set\n",
    "                             do_filtering=False # whether to perform data filtering on this test set\n",
    "                            ):\n",
    "        test_cols = set(get_dset_col_names(test_dset))\n",
    "        test_cols = test_cols - set(self.label_names)\n",
    "        missing_cols = set(self.cols_to_keep) - set(self.label_names) - set(test_cols)\n",
    "        if len(missing_cols):\n",
    "            raise ValueError(f'Test set does not have these columns required for preprocessings: {missing_cols}')\n",
    "            \n",
    "        print_msg('Start Test Set Transformation',20,verbose=self.verbose)\n",
    "\n",
    "        # Filtering\n",
    "        if do_filtering:\n",
    "            test_dset = self._do_filtering(test_dset)\n",
    "        \n",
    "        # Process metadatas\n",
    "        test_dset = self._process_metadatas(test_dset)\n",
    "        \n",
    "        # Content transformation\n",
    "        test_dset = self._do_transformation(test_dset)\n",
    "        \n",
    "        # Drop unused columns\n",
    "        cols_to_remove = test_cols - set(self.cols_to_keep)\n",
    "        test_dset=test_dset.remove_columns(list(cols_to_remove))\n",
    "        \n",
    "        # Tokenization\n",
    "        print_msg('Tokenization',20,verbose=self.verbose)\n",
    "        _func = partial(lambda_batch,\n",
    "                        feature=self.main_text,\n",
    "                        func=partial(tokenize_function,tok=self.tokenizer,max_length=self.max_length),\n",
    "                        output_feature=None,\n",
    "                        is_batched=self.is_batched)\n",
    "        \n",
    "        test_dset = hf_map_dset(test_dset,_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "        \n",
    "        self.verboseprint('Done')\n",
    "        return test_dset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
