{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Feature Extraction From Hidden States\n",
    "\n",
    "> This notebook shows how to extract and utilize the hidden states of a Roberta language model for a variety of tasks\n",
    "\n",
    "- skip_showdoc: true\n",
    "- skip_exec: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will specify a (or a list) of GPUs for training\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "from that_nlp_library.text_transformation import *\n",
    "from that_nlp_library.text_augmentation import *\n",
    "from that_nlp_library.text_main_lm import *\n",
    "from that_nlp_library.utils import seed_everything\n",
    "from that_nlp_library.model_lm_main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import text_normalize\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForMaskedLM, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune a Roberta Language Model (with line-by-line tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a TextDataLMController object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reuse the data and the preprocessings in [this tutorial](https://anhquan0412.github.io/that-nlp-library/text_main_lm.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataLMController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None},\n",
    "                         metadatas='Title',\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our tokenizer for Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenizer = AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process and tokenize our dataset (using line-by-line tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size=112\n",
    "tdc.process_and_tokenize(_tokenizer,line_by_line=True,max_length=block_size) \n",
    "# set max_length=-1 if you want the data collator to pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And set the data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.set_data_collator(is_mlm=True,mlm_prob=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and train Roberta Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_config = AutoConfig.from_pretrained('roberta-base',vocab_size=len(_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 124697433\n",
      "Total trainable parameters: 124697433\n"
     ]
    }
   ],
   "source": [
    "_model = language_model_init(AutoModelForMaskedLM,\n",
    "                             config=_config,\n",
    "                             cpoint_path='roberta-base',\n",
    "                             seed=42\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller = ModelLMController(_model,data_store=tdc,seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can start training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1698' max='1698' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1698/1698 07:37, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.531281</td>\n",
       "      <td>0.653480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.702300</td>\n",
       "      <td>1.501062</td>\n",
       "      <td>0.658028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.702300</td>\n",
       "      <td>1.421605</td>\n",
       "      <td>0.672822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.468500</td>\n",
       "      <td>1.352898</td>\n",
       "      <td>0.684757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.468500</td>\n",
       "      <td>1.311555</td>\n",
       "      <td>0.692361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.304600</td>\n",
       "      <td>1.296340</td>\n",
       "      <td>0.696147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='142' max='142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [142/142 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on validation set: 3.616\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "bs=32\n",
    "wd=0.01\n",
    "epochs= 6\n",
    "warmup_ratio=0.25\n",
    "controller.fit(epochs,lr,\n",
    "               batch_size=bs,\n",
    "               weight_decay=wd,\n",
    "               warmup_ratio=warmup_ratio,\n",
    "               save_checkpoint=False,\n",
    "              )\n",
    "\n",
    "\n",
    "# Epoch\tTraining Loss\tValidation Loss\tAccuracy\n",
    "# 1\tNo log\t1.571658\t0.646139\n",
    "# 2\t1.695500\t1.465111\t0.663383\n",
    "# 3\t1.695500\t1.357892\t0.681303\n",
    "# 4\t1.403900\t1.323384\t0.689879\n",
    "#  [142/142 00:05]\n",
    "# Perplexity on validation set: 3.743\n",
    "\n",
    "\n",
    "# Epoch\tTraining Loss\tValidation Loss\tAccuracy\n",
    "# 1\tNo log\t1.580835\t0.645475\n",
    "# 2\t1.741400\t1.626058\t0.635716\n",
    "# 3\t1.741400\t1.575994\t0.646871\n",
    "# 4\t1.622400\t1.494095\t0.659850\n",
    "# 5\t1.622400\t1.398662\t0.676862\n",
    "# 6\t1.384200\t1.343984\t0.686286\n",
    "# 7\t1.384200\t1.317516\t0.688876\n",
    "#  [142/142 00:06]\n",
    "# Perplexity on validation set: 3.759"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuning from a pretrained model results in a massive improvement in terms of metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.trainer.model.save_pretrained('./sample_weights/roberta_lm_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract hidden states from model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From raw texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp1 = {'Title':'Flattering',\n",
    "        'Review Text': \"Love this skirt. The detail is amazing. Runs small I ordered a 12 I'm usually a 10, but still a little snug\"\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_config = AutoConfig.from_pretrained('./sample_weights/lm_model',output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 124697433\n",
      "Total trainable parameters: 124697433\n"
     ]
    }
   ],
   "source": [
    "trained_model = language_model_init(AutoModelForCausalLM,\n",
    "                                    cpoint_path='./sample_weights/lm_model',\n",
    "                                    config=_config\n",
    "                                   )\n",
    "\n",
    "controller2 = ModelLMController(trained_model,data_store=tdc,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_from_ip1 = controller2.get_hidden_states_from_raw_text(inp1,\n",
    "                                                              state_name='hidden_states',\n",
    "                                                              state_idx=[-1,0]\n",
    "                                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Title', 'Review Text', 'input_ids', 'attention_mask', 'special_tokens_mask', 'hidden_states'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_from_ip1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_from_ip1['hidden_states'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From validation (or even train) set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_from_vals = controller2.get_hidden_states(ds_type='validation',\n",
    "                                                 state_name='hidden_states',\n",
    "                                                 state_idx=[-1,0]\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'special_tokens_mask', 'hidden_states'],\n",
       "    num_rows: 4529\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_from_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4529, 768)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_from_vals['hidden_states'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
