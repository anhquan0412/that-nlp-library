{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Main Streaming\n",
    "\n",
    "> This module contains the main Python class for data control for streaming data: `TextDataControllerStreaming`\n",
    "\n",
    "- skip_showdoc: true\n",
    "- skip_exec: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp text_main_streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from datasets import DatasetDict,Dataset,IterableDataset,load_dataset,Value\n",
    "from pathlib import Path\n",
    "from that_nlp_library.utils import *\n",
    "from that_nlp_library.text_main import tokenize_function,concat_metadatas\n",
    "from functools import partial\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from that_nlp_library.text_transformation import *\n",
    "from that_nlp_library.text_augmentation import *\n",
    "from importlib.machinery import SourceFileLoader\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class TextDataControllerStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TextDataControllerStreaming():\n",
    "    def __init__(self,\n",
    "                 inp, # HuggingFainpce Dataset or DatasetDict\n",
    "                 main_text:str, # Name of the main text column\n",
    "                 label_names=[], # Names of the label (dependent variable) columns\n",
    "                 sup_types=[], # Type of supervised learning for each label name ('classification' or 'regression')\n",
    "                 class_names_predefined=[], # List of names associated with the labels (same index order)\n",
    "                 filter_dict={}, # A dictionary: {feature: filtering_function_based_on_the_feature}\n",
    "                 label_tfm_dict={}, # A dictionary: {label_name: transform_function_for_that_label}\n",
    "                 metadatas=[], # Names of the metadata columns\n",
    "                 process_metas=True, # Whether to do simple text processing on the chosen metadatas\n",
    "                 content_transformations=[], # A list of text transformations\n",
    "                 content_augmentations=[], # A list of text augmentations\n",
    "                 seed=None, # Random seed\n",
    "                 batch_size=100, # CPU batch size\n",
    "                 num_proc=1, # Number of process for multiprocessing. This will be applied on non-streamed validation set\n",
    "                 cols_to_keep=None, # Columns to keep after all processings\n",
    "                 verbose=True, # Whether to print processing information\n",
    "                ):\n",
    "            \n",
    "        self.main_text = main_text\n",
    "        \n",
    "        self.label_names = val2iterable(label_names)\n",
    "        self.sup_types = val2iterable(sup_types)\n",
    "        self._check_sup_types()\n",
    "        self.label_lists = class_names_predefined\n",
    "        \n",
    "        self.filter_dict = filter_dict\n",
    "        self.label_tfm_dict = label_tfm_dict\n",
    "        self.metadatas = val2iterable(metadatas)\n",
    "        self.process_metas = process_metas\n",
    "\n",
    "        self.content_tfms = val2iterable(content_transformations)\n",
    "        self.aug_tfms = val2iterable(content_augmentations)\n",
    "        \n",
    "        self.seed = seed\n",
    "        self.is_batched = batch_size>1\n",
    "        self.batch_size = batch_size\n",
    "        self.num_proc = num_proc\n",
    "        self.cols_to_keep = cols_to_keep\n",
    "\n",
    "        self.main_ddict=DatasetDict()\n",
    "        self.verbose = verbose\n",
    "        self.verboseprint = print if verbose else lambda *a, **k: None\n",
    "        \n",
    "        if hasattr(inp,'keys'): # is datasetdict\n",
    "            if 'train' not in inp.keys(): \n",
    "                raise ValueError('The given DatasetDict has no \"train\" split')\n",
    "            else:\n",
    "                self.main_ddict['train'] = inp['train']\n",
    "            val_key = list(set(inp.keys()) & set(['val','validation','valid']))\n",
    "            if len(val_key)>1: raise ValueError('Your DatasetDict has more than 1 validation split')\n",
    "            if len(val_key)==1:\n",
    "                self.main_ddict['validation'] = inp[val_key[0]]\n",
    "        else: # is dataset\n",
    "            self.main_ddict['train'] = inp\n",
    "          \n",
    "        is_streamed=isinstance(self.main_ddict['train'],IterableDataset)\n",
    "        if not is_streamed: raise Exception('This Text Data Controller only handles streamed dataset')\n",
    "        \n",
    "        self.all_cols = get_dset_col_names(self.main_ddict['train'])\n",
    "        \n",
    "        if is_streamed and ('classification' in self.sup_types) and len(self.label_lists)==0:\n",
    "            raise ValueError('All classification labels must be provided when streaming')\n",
    "            \n",
    "        self._determine_multihead_multilabel()\n",
    "        self._convert_regression_to_float()\n",
    "        self._processed_call=False\n",
    "        \n",
    "            \n",
    "    @classmethod\n",
    "    def from_pickle(cls,\n",
    "                    fname, # Name of the pickle file\n",
    "                    parent='pickle_files' # Parent folder\n",
    "                   ):\n",
    "        return load_pickle(fname,parent=parent)\n",
    "    \n",
    "    def set_verbose(self,verbose):\n",
    "        self.verbose = verbose\n",
    "        self.verboseprint = print if verbose else lambda *a, **k: None\n",
    "    \n",
    "    def _convert_regression_to_float(self):\n",
    "        if len(self.sup_types)==0: return\n",
    "        # convert regression labels to float64\n",
    "        reg_idxs = [i for i,v in enumerate(self.sup_types) if v=='regression']\n",
    "        for i in reg_idxs:\n",
    "            self.main_ddict['train'] = self.main_ddict['train'].cast_column(self.label_names[i],Value(\"float64\"))\n",
    "            if 'validation' in self.main_ddict.keys():\n",
    "                self.main_ddict['validation'] = self.main_ddict['validation'].cast_column(self.label_names[i],Value(\"float64\"))\n",
    "        \n",
    "    def _check_sup_types(self):\n",
    "        assert len(self.label_names)==len(self.sup_types), \"The number of supervised learning declaration must equal to the number of label\"\n",
    "        assert len(set(self.sup_types) - set(['classification','regression']))==0, 'Accepted inputs for `sup_types` are `classification` and `regression`'\n",
    "        \n",
    "    def _determine_multihead_multilabel(self):\n",
    "        self.is_multilabel=False\n",
    "        self.is_multihead=False\n",
    "        if len(self.label_names)==0: return\n",
    "        \n",
    "        if len(self.label_names)>1:\n",
    "            self.is_multihead=True\n",
    "        # get label of first row\n",
    "        first_label = next(iter(self.main_ddict['train']))[self.label_names[0]]\n",
    "        if isinstance(first_label,(list,set,tuple)):\n",
    "            # This is multi-label. Ignore self.label_names[1:]\n",
    "            self.label_names = [self.label_names[0]]\n",
    "            self.is_multihead=False\n",
    "            self.is_multilabel=True\n",
    "                     \n",
    "    \n",
    "    def save_as_pickles(self,\n",
    "                        fname, # Name of the pickle file\n",
    "                        parent='pickle_files', # Parent folder\n",
    "                        drop_attributes=False # Whether to drop large-size attributes\n",
    "                       ):\n",
    "        if drop_attributes:\n",
    "            if hasattr(self, 'main_ddict'):\n",
    "                del self.main_ddict\n",
    "            if hasattr(self, 'ddict_rest'):\n",
    "                del self.ddict_rest\n",
    "            if hasattr(self, 'aug_tfms'):\n",
    "                del self.aug_tfms\n",
    "        save_to_pickle(self,fname,parent=parent)\n",
    "    \n",
    "    def _process_metadatas(self,dtrain):\n",
    "        if len(self.metadatas):\n",
    "            map_func = partial(concat_metadatas,\n",
    "                               main_text=self.main_text,\n",
    "                               metadatas=self.metadatas,\n",
    "                               process_metas=self.process_metas,\n",
    "                               is_batched=self.is_batched)\n",
    "            dtrain = hf_map_dset(dtrain,map_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "        return dtrain\n",
    "    \n",
    "    def _do_label_transformation(self):\n",
    "        if len(self.label_names)==0 or len(self.label_tfm_dict)==0: return\n",
    "        print_msg('Label Transformation',20,verbose=self.verbose)\n",
    "        for f,tfm in self.label_tfm_dict.items():\n",
    "            if f in self.label_names:\n",
    "                _func = partial(lambda_map_batch,\n",
    "                                feature=f,\n",
    "                                func=tfm,\n",
    "                                is_batched=self.is_batched\n",
    "                               )                \n",
    "                self.main_ddict['train'] = hf_map_dset(self.main_ddict['train'],_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "                if 'validation' in self.main_ddict.keys():\n",
    "                    self.main_ddict['validation'] = hf_map_dset(self.main_ddict['validation'],\n",
    "                                                                _func,\n",
    "                                                                self.is_batched,\n",
    "                                                                self.batch_size,\n",
    "                                                                self.num_proc)\n",
    "        self.verboseprint('Done')\n",
    "                    \n",
    "                      \n",
    "                \n",
    "    def _create_label_mapping_func(self,encoder_classes):\n",
    "        if self.is_multihead:\n",
    "            label2idxs = [{v:i for i,v in enumerate(l_classes)} for l_classes in encoder_classes]\n",
    "            _func = lambda inp: {'label': [[label2idxs[i][v] if len(label2idxs[i]) else v for i,v in enumerate(vs)] \\\n",
    "                                           for vs in zip(*[inp[l] for l in self.label_names])] if self.is_batched \\\n",
    "                                 else [label2idxs[i][v] if len(label2idxs[i]) else v for i,v in enumerate([inp[l] for l in self.label_names])]\n",
    "                                }\n",
    "            \n",
    "        else: # single-head\n",
    "            if self.sup_types[0]=='regression':\n",
    "                _func1 = lambda x: x\n",
    "            else:\n",
    "                label2idx = {v:i for i,v in enumerate(encoder_classes[0])}\n",
    "                _func1 = lambda x: label2idx[x]\n",
    "                \n",
    "            _func = partial(lambda_map_batch,\n",
    "                           feature=self.label_names[0],\n",
    "                           func=_func1,\n",
    "                           output_feature='label',\n",
    "                           is_batched=self.is_batched)\n",
    "        return _func\n",
    "        \n",
    "    def _encode_labels(self):\n",
    "        if len(self.label_names)==0: return\n",
    "        print_msg('Label Encoding',verbose=self.verbose)\n",
    "        \n",
    "        if len(self.label_lists) and not isinstance(self.label_lists[0],list):\n",
    "            self.label_lists = [self.label_lists]\n",
    "                    \n",
    "        encoder_classes=[]\n",
    "        if not self.is_multilabel:\n",
    "            for idx,l in enumerate(self.label_names):\n",
    "                if self.sup_types[idx]=='regression':\n",
    "                    l_classes=[]\n",
    "                else: # classification\n",
    "                    l_classes = sorted(list(self.label_lists[idx]))\n",
    "                encoder_classes.append(l_classes)\n",
    "                \n",
    "            _func = self._create_label_mapping_func(encoder_classes)\n",
    "            \n",
    "            self.main_ddict['train'] = hf_map_dset(self.main_ddict['train'],_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "            if 'validation' in self.main_ddict.keys():\n",
    "                self.main_ddict['validation'] = hf_map_dset(self.main_ddict['validation'],_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "                    \n",
    "        else:\n",
    "            # For MultiLabel, we transform the label itself to one-hot (or actually, few-hot)\n",
    "            l_classes = sorted(list(self.label_lists[0]))   \n",
    "            encoder_classes.append(l_classes)\n",
    "            \n",
    "            l_encoder = MultiLabelBinarizer(classes=encoder_classes[0])\n",
    "            _ = l_encoder.fit(None)\n",
    "            _func = partial(lambda_map_batch,\n",
    "                            feature=self.label_names[0],\n",
    "                            func=lambda x: l_encoder.transform(x),\n",
    "                            output_feature='label',\n",
    "                            is_batched=self.is_batched,\n",
    "                            is_func_batched=True)\n",
    "            self.main_ddict['train'] = hf_map_dset(self.main_ddict['train'],_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "            if 'validation' in self.main_ddict.keys():\n",
    "                self.main_ddict['validation'] = hf_map_dset(self.main_ddict['validation'],_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "            \n",
    "        self.label_lists = encoder_classes\n",
    "        self.verboseprint('Done')\n",
    "        \n",
    "            \n",
    "            \n",
    "    def _simplify_ddict(self):\n",
    "        print_msg('Dropping unused features',20,verbose=self.verbose)\n",
    "        if self.cols_to_keep is None:\n",
    "            self.cols_to_keep= [self.main_text] + self.metadatas + self.label_names\n",
    "        cols_to_remove = set(self.all_cols) - set(self.cols_to_keep)\n",
    "        self.main_ddict['train']=self.main_ddict['train'].remove_columns(list(cols_to_remove))\n",
    "        if 'validation' in self.main_ddict.keys():\n",
    "            self.main_ddict['validation']=self.main_ddict['validation'].remove_columns(list(cols_to_remove))\n",
    "        self.verboseprint('Done')\n",
    "\n",
    "    def _do_filtering(self,dtrain):\n",
    "        if len(self.filter_dict):\n",
    "            col_names = get_dset_col_names(dtrain)\n",
    "            for f,tfm in self.filter_dict.items():\n",
    "                if f in col_names:\n",
    "                    _func = partial(lambda_batch,\n",
    "                                    feature=f,\n",
    "                                    func=tfm,\n",
    "                                    is_batched=self.is_batched)\n",
    "                    dtrain = hf_filter_dset(dtrain,_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "        return dtrain\n",
    "        \n",
    "\n",
    "    def _do_transformation_tokenization(self,dtrain,tokenizer,max_length,):\n",
    "        tok_func = partial(tokenize_function,tok=tokenizer,max_length=max_length)\n",
    "        if len(self.content_tfms):            \n",
    "            for tfm in self.content_tfms:\n",
    "                _func = partial(lambda_map_batch,\n",
    "                                feature=self.main_text,\n",
    "                                func=tfm,\n",
    "                                is_batched=self.is_batched)\n",
    "                dtrain = hf_map_dset(dtrain,_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "        \n",
    "        _func = partial(lambda_map_batch,\n",
    "                        feature=self.main_text,\n",
    "                        func=tok_func,\n",
    "                        output_feature=None,\n",
    "                        is_batched=self.is_batched)\n",
    "        dtrain = hf_map_dset(dtrain,_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "            \n",
    "        return dtrain \n",
    " \n",
    "    def _do_transformation_augmentation_tokenization(self,tokenizer,max_length):\n",
    "        tok_func = partial(tokenize_function,tok=tokenizer,max_length=max_length)\n",
    "        all_tfms = self.content_tfms + self.aug_tfms\n",
    "        all_tfms = partial(func_all,functions=all_tfms) if len(all_tfms) else None\n",
    "        seed_everything(self.seed)\n",
    "           \n",
    "        self.main_ddict['train'] = IterableDataset.from_generator(aug_and_tok_stream_generator,\n",
    "                                                   gen_kwargs={'dset': self.main_ddict['train'],\n",
    "                                                               'text_name':self.main_text,\n",
    "                                                               'tok_func':tok_func,\n",
    "                                                               'func': all_tfms\n",
    "                                                              }\n",
    "                                                                 )\n",
    "        \n",
    "        \n",
    "    def process_and_tokenize(self,\n",
    "                             tokenizer, # Tokenizer (preferably from HuggingFace)\n",
    "                             max_length=None, # pad to model's allowed max length (default is max_sequence_length)\n",
    "                            ):\n",
    "        if self._processed_call:\n",
    "            warnings.warn('Your dataset has already been processed. Returning the previous processed DatasetDict...')\n",
    "            return self.main_ddict\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "                             \n",
    "        # Filtering\n",
    "        print_msg('Data Filtering',20,verbose=self.verbose)\n",
    "        for k in self.main_ddict.keys():   \n",
    "            self.main_ddict[k] = self._do_filtering(self.main_ddict[k])\n",
    "        self.verboseprint('Done')\n",
    "\n",
    "        \n",
    "        # Process metadatas\n",
    "        print_msg('Metadata Simple Processing & Concatenating to Main Content',verbose=self.verbose)\n",
    "        for k in self.main_ddict.keys():   \n",
    "            self.main_ddict[k] = self._process_metadatas(self.main_ddict[k])\n",
    "        self.verboseprint('Done')\n",
    "        \n",
    "        # Label transformation\n",
    "        self._do_label_transformation()\n",
    "        \n",
    "        # Process labels\n",
    "        self._encode_labels()\n",
    "\n",
    "        # Dropping unused columns\n",
    "        self._simplify_ddict()\n",
    "\n",
    "        \n",
    "        # Content transformation + tokenization for validation\n",
    "        if 'validation' in self.main_ddict.keys():\n",
    "            print_msg('Performing content transformation and tokenization on validation set',verbose=self.verbose)\n",
    "            self.main_ddict['validation'] = self._do_transformation_tokenization(self.main_ddict['validation'],\n",
    "                                                                                 tokenizer,\n",
    "                                                                                 max_length,\n",
    "                                                                                )\n",
    "            self.verboseprint('Done')\n",
    " \n",
    "        # Content transformation + augmentation + tokenization for train\n",
    "        print_msg('Creating a generator for content transformation, augmentation and tokenization on train set',verbose=self.verbose)\n",
    "        self._do_transformation_augmentation_tokenization(tokenizer,max_length)\n",
    "        self.verboseprint('Done')\n",
    "        \n",
    "        self._processed_call=True\n",
    "    \n",
    "        \n",
    "    \n",
    "    def set_data_collator(self,data_collator):\n",
    "        self.data_collator = data_collator\n",
    "        \n",
    "    \n",
    "    def prepare_test_dataset_from_csv(self,\n",
    "                                      file_path, # path to csv file\n",
    "                                      do_filtering=False # whether to perform data filtering on this test set\n",
    "                                     ):\n",
    "        file_path = Path(file_path)\n",
    "        ds = load_dataset(str(file_path.parent),\n",
    "                          data_files=file_path.name,\n",
    "                          split='train')\n",
    "        return self.prepare_test_dataset(ds,do_filtering)\n",
    "    \n",
    "    def prepare_test_dataset_from_df(self,\n",
    "                                     df, # Pandas Dataframe\n",
    "                                     validate=True, # whether to perform input data validation\n",
    "                                     do_filtering=False # whether to perform data filtering on this test set \n",
    "                                    ):\n",
    "        if validate:\n",
    "            check_input_validation(df)\n",
    "        ds = Dataset.from_pandas(df)\n",
    "        return self.prepare_test_dataset(ds,do_filtering)\n",
    "    \n",
    "    def prepare_test_dataset_from_raws(self,\n",
    "                                       content, # Either a single sentence, list of sentence or a dictionary with keys are metadata columns and values are list\n",
    "                                      ):\n",
    "        if len(self.metadatas)!=0 and not isinstance(content,dict):\n",
    "            raise ValueError(f'There is/are metadatas in the preprocessing step. Please include a dictionary including these keys for metadatas: {self.metadatas}, and texture content: {self.main_text}')\n",
    "            \n",
    "        _dic = {self.main_text:[content]} if isinstance(content,str) else content\n",
    "        for k in _dic.keys():\n",
    "            _dic[k] = val2iterable(_dic[k])\n",
    "        \n",
    "        test_dict = Dataset.from_dict(_dic)\n",
    "        \n",
    "        # set num_proc to 1 for small data processing\n",
    "        _tmp = self.num_proc\n",
    "        self.num_proc=1\n",
    "        results = self.prepare_test_dataset(test_dict,do_filtering=False)\n",
    "        self.num_proc = _tmp\n",
    "        return results\n",
    "    \n",
    "    def prepare_test_dataset(self,\n",
    "                             test_dset, # The HuggingFace Dataset as Test set\n",
    "                             do_filtering=False, # whether to perform data filtering on this test set\n",
    "                            ):\n",
    "        test_cols = set(get_dset_col_names(test_dset))\n",
    "        label_names_set = set(self.label_names)\n",
    "        test_cols = test_cols - label_names_set\n",
    "        missing_cols = set(self.cols_to_keep) - label_names_set - test_cols\n",
    "        if len(missing_cols):\n",
    "            raise ValueError(f'Test set does not have these columns required for preprocessings: {missing_cols}')\n",
    "            \n",
    "        print_msg('Start Test Set Transformation',20,verbose=self.verbose)\n",
    "\n",
    "        # Filtering\n",
    "        if do_filtering:\n",
    "            print_msg('Data Filtering',20,verbose=self.verbose)\n",
    "            test_dset = self._do_filtering(test_dset)\n",
    "            self.verboseprint('Done')\n",
    "        \n",
    "        # Process metadatas\n",
    "        print_msg('Metadata Simple Processing & Concatenating to Main Content',verbose=self.verbose)    \n",
    "        test_dset = self._process_metadatas(test_dset)\n",
    "        self.verboseprint('Done')\n",
    "        \n",
    "        # Drop unused columns\n",
    "        print_msg('Dropping unused features',20,verbose=self.verbose)\n",
    "        cols_to_remove = test_cols - set(self.cols_to_keep)\n",
    "        test_dset=test_dset.remove_columns(list(cols_to_remove))\n",
    "        self.verboseprint('Done')\n",
    "        \n",
    "        \n",
    "        # Content transformation and tokenization\n",
    "        print_msg('Performing content transformation and tokenization on test set',verbose=self.verbose)\n",
    "        test_dset = self._do_transformation_tokenization(test_dset,self.tokenizer,self.max_length)\n",
    "        self.verboseprint('Done')\n",
    "        \n",
    "        return test_dset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### TextDataControllerStreaming\n",
       "\n",
       ">      TextDataControllerStreaming (inp, main_text:str, label_names=[],\n",
       ">                                   sup_types=[], class_names_predefined=[],\n",
       ">                                   filter_dict={}, label_tfm_dict={},\n",
       ">                                   metadatas=[], process_metas=True,\n",
       ">                                   content_transformations=[],\n",
       ">                                   content_augmentations=[], seed=None,\n",
       ">                                   batch_size=100, num_proc=1,\n",
       ">                                   cols_to_keep=None, verbose=True)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | HuggingFainpce Dataset or DatasetDict |\n",
       "| main_text | str |  | Name of the main text column |\n",
       "| label_names | list | [] | Names of the label (dependent variable) columns |\n",
       "| sup_types | list | [] | Type of supervised learning for each label name ('classification' or 'regression') |\n",
       "| class_names_predefined | list | [] | List of names associated with the labels (same index order) |\n",
       "| filter_dict | dict | {} | A dictionary: {feature: filtering_function_based_on_the_feature} |\n",
       "| label_tfm_dict | dict | {} | A dictionary: {label_name: transform_function_for_that_label} |\n",
       "| metadatas | list | [] | Names of the metadata columns |\n",
       "| process_metas | bool | True | Whether to do simple text processing on the chosen metadatas |\n",
       "| content_transformations | list | [] | A list of text transformations |\n",
       "| content_augmentations | list | [] | A list of text augmentations |\n",
       "| seed | NoneType | None | Random seed |\n",
       "| batch_size | int | 100 | CPU batch size |\n",
       "| num_proc | int | 1 | Number of process for multiprocessing. This will be applied on non-streamed validation set |\n",
       "| cols_to_keep | NoneType | None | Columns to keep after all processings |\n",
       "| verbose | bool | True | Whether to print processing information |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### TextDataControllerStreaming\n",
       "\n",
       ">      TextDataControllerStreaming (inp, main_text:str, label_names=[],\n",
       ">                                   sup_types=[], class_names_predefined=[],\n",
       ">                                   filter_dict={}, label_tfm_dict={},\n",
       ">                                   metadatas=[], process_metas=True,\n",
       ">                                   content_transformations=[],\n",
       ">                                   content_augmentations=[], seed=None,\n",
       ">                                   batch_size=100, num_proc=1,\n",
       ">                                   cols_to_keep=None, verbose=True)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | HuggingFainpce Dataset or DatasetDict |\n",
       "| main_text | str |  | Name of the main text column |\n",
       "| label_names | list | [] | Names of the label (dependent variable) columns |\n",
       "| sup_types | list | [] | Type of supervised learning for each label name ('classification' or 'regression') |\n",
       "| class_names_predefined | list | [] | List of names associated with the labels (same index order) |\n",
       "| filter_dict | dict | {} | A dictionary: {feature: filtering_function_based_on_the_feature} |\n",
       "| label_tfm_dict | dict | {} | A dictionary: {label_name: transform_function_for_that_label} |\n",
       "| metadatas | list | [] | Names of the metadata columns |\n",
       "| process_metas | bool | True | Whether to do simple text processing on the chosen metadatas |\n",
       "| content_transformations | list | [] | A list of text transformations |\n",
       "| content_augmentations | list | [] | A list of text augmentations |\n",
       "| seed | NoneType | None | Random seed |\n",
       "| batch_size | int | 100 | CPU batch size |\n",
       "| num_proc | int | 1 | Number of process for multiprocessing. This will be applied on non-streamed validation set |\n",
       "| cols_to_keep | NoneType | None | Columns to keep after all processings |\n",
       "| verbose | bool | True | Whether to print processing information |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataControllerStreaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### TextDataControllerStreaming.process_and_tokenize\n",
       "\n",
       ">      TextDataControllerStreaming.process_and_tokenize (tokenizer,\n",
       ">                                                        max_length=None)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| max_length | NoneType | None | pad to model's allowed max length (default is max_sequence_length) |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### TextDataControllerStreaming.process_and_tokenize\n",
       "\n",
       ">      TextDataControllerStreaming.process_and_tokenize (tokenizer,\n",
       ">                                                        max_length=None)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| max_length | NoneType | None | pad to model's allowed max length (default is max_sequence_length) |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataControllerStreaming.process_and_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Capability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of streaming capability of `TextDataControllerStreaming` is adapted from [HuggingFace's stream](https://huggingface.co/docs/datasets/stream)\n",
    "\n",
    "Streaming is a method to let you work with data without having it in your hard drive. This is especially helpful when the dataset size exceeds the amount of disk space you have on your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few things to be aware of when using `TextDataControllerStreaming` streaming functionality (versus `TextDataController`)\n",
    "\n",
    "- The list of label names must be available beforehand (except for regression label)\n",
    "- To avoid out-of-memory error, reduce batch_size argument.\n",
    "- There will NOT be any validation split functionality. If you want to include a validation set, provide a `validation` split in your HuggingFace DatasetDict beforehand\n",
    "- There's no upsampling, and there's no shuffling the training set\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To stream, you must provide a streamed HuggingFace dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat few examples mentioned in [this tutorial](https://anhquan0412.github.io/that-nlp-library/text_main.html), but with a streaming dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering + Metadatas + Label Transformation +  Content Transformation + Content Augmentation (for Single Head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import text_normalize\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_aug_stochastic(x,aug=None,p=0.5):\n",
    "    if not isinstance(x,list): \n",
    "        if random.random()<p: return aug.augment(x)[0]\n",
    "        return x\n",
    "    news=[]\n",
    "    originals=[]\n",
    "    for _x in x:\n",
    "        if random.random()<p: news.append(_x)\n",
    "        else: originals.append(_x)\n",
    "    # only perform augmentation when needed\n",
    "    if len(news): news = aug.augment(news)\n",
    "    return news+originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug2 = naw.ContextualWordEmbsAug(model_path='roberta-base', \n",
    "                                device='cuda:0', # if you don't have gpu, change to 'cpu'\n",
    "                                action=\"substitute\",\n",
    "                                top_k=10,\n",
    "                               aug_p=0.07)\n",
    "\n",
    "contextual_aug_func = partial(nlp_aug_stochastic,aug=aug2,p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Data Filtering --------------------\n",
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cede394e998c46938a12e36586b6f0a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Label Transformation --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58fead973bb24f7ca2c5de8592596d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1e5f91a35648b5b4111b64842885a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "----- Performing content transformation and tokenization on validation set -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb3792c3b8d4f6fb0854389decf7321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a78c76dc904fb989251930bb1c3d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013908a51abe4f779181bf16b1f02b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Creating a generator for content transformation, augmentation and tokenization on train set -----\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataControllerStreaming(ddict_with_val,\n",
    "                                  main_text='Review Text',\n",
    "                                  label_names='Department Name',\n",
    "                                  sup_types='classification',\n",
    "                                  class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trending'],\n",
    "                                  filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                              },\n",
    "                                  label_tfm_dict={'Department Name': lambda x: x if x!='Trend' else 'Trending'},\n",
    "                                  metadatas=['Title','Division Name'],\n",
    "                                  content_transformations=[text_normalize,str.lower],\n",
    "                                  content_augmentations= contextual_aug_func, \n",
    "                                  process_metas=True,\n",
    "                                  batch_size=1000,\n",
    "                                  num_proc=4,\n",
    "                                  seed=42\n",
    "                                 )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Title', 'Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2253\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "CPU times: user 21.6 s, sys: 900 ms, total: 22.5 s\n",
      "Wall time: 22.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    if i%100==0:\n",
    "        print(i)\n",
    "    if i==1000:\n",
    "        break\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: general petite . beautiful top , worth the necessary tailoring . the beautiful bold print drew me to this top and it did not disappoint upon receipt . however , the bottom ruffle belled so far out on each side that it was laughable ! the actual fit is nothing like the picture ; clearly the model's arms are placed in front of all the extra fabric to hold the ruffle back . however , the fabric is beautiful , the fit was perfect ( size 2 , 5 ' 4 \" , 106 lbs . ) , the quality is great and i love the print so i decided to take it to my tailor to \" sew away \" the \" wings \" on both si\n",
      "Label: Tops => 4\n",
      "----------\n",
      "Text: general . not as short on me ( petite ) . i ordered the xxs p as this dress is not a fitted dress , and that was the right size for me . only thing is the length is a bit linger still 9 lower on calf for me ) , the straps are almost tight , so i would say the dress is a reversed taper shape . color is beautiful , i ordered green as the other color ( plum ) doesn't have petite available . green is rich , and classy , the fabric is surprisingly soft . i love the little details in the velvet . definitely need a strapless bra for this one . 115 lbsm 30 d\n",
      "Label: Dresses => 1\n",
      "----------\n",
      "Text: general. perfect.... for two wears. ok ladies.... you need to know that this type of fabric is the one that will get holes ( i bought the white one ). everything is super thin and lovely, but i was only expecting to get two wears out of it. i only wash it and it maintained it's size because i restretched it while i then hung to dry. i was super disappointed for the wear. appreciated finally able to return it without question at my local retailer.\n",
      "Label: Tops => 4\n",
      "----------\n",
      "Text: initmates . . i love this dress . it is so soft and comfortable , perfect for summer ! ! i wish it came in more colors because i would buy everyone ! !\n",
      "Label: Intimate => 2\n",
      "----------\n",
      "Text: general petite . adorable and excellent quality . this is such a clean and cute printed dress and i knew that i had to try the dress when i first saw it online . after reading other reviews , i sized up . i am normally a 0 or 2 in retailer . i ordered the 2 and it fits nicely and looks great . however , i feel like the 2 buttons at the lowered rib cage area gape slightly . the tie covers it and holds it in place , unless i sit , then it gapes freely . i am a 32 a so not big chested at all , and yet this fit snug in the chest area . i would worry about\n",
      "Label: Dresses => 1\n",
      "----------\n",
      "Text: general . one of a kind skirt . unique and well-made skirt , sure to turn heads . embroidery is lovely . def a statement piece for any wardrobe .\n",
      "Label: Bottoms => 0\n",
      "----------\n",
      "Text: general petite . super huge top . i have to return this top . the armholes are huge . the material is on the thin side and not what i expected . i can't size down . i got an xxs . maybe if i had been able to order a petite , it would have fit better . i ordered the red and i liked the color .\n",
      "Label: Tops => 4\n",
      "----------\n",
      "Text: general petite. great fit and flow. love this flowing. cute top! casual, but can be easily dressed with. great fit.\n",
      "Label: Tops => 4\n",
      "----------\n",
      "Text: initmates. definitely bring back more!!! I love this tank, great shade of yellow in person. love the strap width and length. wish they'd bring back more in lots of colors. i sized up one from my usual size and will have to shorten the length with the strap myself, but it's an easy alteration and is totally worth it and especially good at sale price. i really hope they make more in the style in lots of flavors. great find!\n",
      "Label: Intimate => 2\n",
      "----------\n",
      "Text: general petite,, and flows. comfortable and flows well, does run large, order one size smaller.\n",
      "Label: Tops => 4\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    if i==10:break\n",
    "    print(f\"Text: {v['Review Text']}\\nLabel: {v['Department Name']} => {v['label']}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tops        956\n",
       "Dresses     627\n",
       "Bottoms     375\n",
       "Intimate    187\n",
       "Jackets      97\n",
       "Trending     11\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(tdc.main_ddict['validation']['Department Name']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: general . soft , feminine and fun pockets ! . i love this tunic . purchased the dark orange in medium ( i am 5 ' 9 and 140 lbs ) . tried the small and almost kept it but i felt seams around my arm pits a tad , so went with the medium and glad i did - this top should be comfortable . feels very fall and perfect for casual get-togethers and running around town . only comment is that it is rayon ... and for me anyway rayon doesn't wash too well - so we shall see how this one fairs .\n",
      "Label: Tops => 4\n",
      "----------\n",
      "Text: general petite . a new staple ! . tried these on out of sheer curiosity -- i've got a long torso & was pleasantly surprised how flattering they are ! they manage to look flowing & sleek without shortening the legs . took a size 6 with my 27 \" waist , 37 \" hips . it's a bit of a generous fit , especially around the waist , but they're extremely comfortable & have room to tuck tops into . i have the cowled sweater tank in gray & it looks fantastic over these ! couldn't resist getting both the rust and black . perfect for a dressy casual look\n",
      "Label: Bottoms => 0\n",
      "----------\n",
      "Text: general . maybe swing is for me ! . i love swing dresses but they never seem to work out for me . however , lately i have been trying on swing tops like this one and they are super scores ! i love this top ! in my store , they had a rack of test materials where they don't have the full line but they have a look at some online features or clothes that are very new releases . this was on the rack . i knew it wasn't my size but i tried it on anyway and i am absolutely in love . i am waiting for a sale ( as always ) but i am going to get this i\n",
      "Label: Tops => 4\n",
      "----------\n",
      "Text: general . too flare . too small ... too flare ... nice thick fabric . not my favorite pant .\n",
      "Label: Bottoms => 0\n",
      "----------\n",
      "Text: general . love . i love this top it is easy to wear fun and very comfortable . i was thinking about it for weeks and kept coming back to it after i read a review about going up a size i decided to go for it and i am very happy i did ! ! ! my new favorite ! ! !\n",
      "Label: Tops => 4\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Text: {tdc.main_ddict['validation']['Review Text'][i]}\")\n",
    "    print(f\"Label: {tdc.main_ddict['validation']['Department Name'][i]} => {tdc.main_ddict['validation']['label'][i]}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering + Metadatas + Label Transformation +  Content Transformation + Content Augmentation (for Multi Head: Classification + Regression + Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug2 = naw.ContextualWordEmbsAug(model_path='roberta-base', \n",
    "                                device='cuda:0', # if you don't have gpu, change to 'cpu'\n",
    "                                action=\"substitute\",\n",
    "                                top_k=10,\n",
    "                               aug_p=0.07)\n",
    "\n",
    "contextual_aug_func = partial(nlp_aug_stochastic,aug=aug2,p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95193319211d4a84a9d1edebcfe1b95e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Data Filtering --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ccfb2996b24e14bbe7a771c4c0a85e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f1741c1aa04657ad23aea2d9e82498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610ae53991194e46b8e592782367d32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Label Transformation --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029409571fcf4459a3c34481b25cb81e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d4062b65524d86914508ff6b45c24a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "----- Performing content transformation and tokenization on validation set -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aab982ed9fa4134ac01e94320975cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4723031d8aa64312a89099fa39b33679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4d118200db42509d14992a1b390a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Creating a generator for content transformation, augmentation and tokenization on train set -----\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataControllerStreaming(ddict_with_val,\n",
    "                                  main_text='Review Text',\n",
    "                                  label_names=['Division Name','Rating','Department Name'],\n",
    "                                  sup_types=['classification','regression','classification'],\n",
    "                                  class_names_predefined=[['General', 'General Petite', 'Initmates'],\n",
    "                                                          [], # empty list for regression\n",
    "                                                          ['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trending']],\n",
    "                                  filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                               'Department Name': lambda x: x is not None,\n",
    "                                              },\n",
    "                                  metadatas=['Title'],\n",
    "                                  label_tfm_dict={'Department Name': lambda x: x if x!='Trend' else 'Trending'},\n",
    "                                  content_transformations=[text_normalize,str.lower],\n",
    "                                  content_augmentations=contextual_aug_func,\n",
    "                                  process_metas=True,\n",
    "                                  batch_size=1000,\n",
    "                                  num_proc=4,\n",
    "                                  seed=42\n",
    "                                 )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: beautiful top , worth the necessary tailoring . the beautiful bold print drew me to this top and it did not disappoint upon receipt . however , the bottom ruffle belled so far out on each side that it was laughable ! the actual fit is nothing like the picture ; clearly the model's arms are placed in front of all the extra fabric to hold the ruffle back . however , the fabric is beautiful , the fit was perfect ( size 2 , 5 ' 4 \" , 106 lbs . ) , the quality is great and i love the print so i decided to take it to my tailor to \" sew away \" the \" wings \" on both si\n",
      "Label: ('General Petite', 4.0, 'Tops') => [1, 4.0, 4]\n",
      "----------\n",
      "Text: not as short on me ( petite ). i ordered the big p, this dress is also a fitted dress, and that was the right size on me. main thing is the skirt is a bit linger still 9 lower on calf for me ), the straps are almost tight, so i would say the dress is a reversed taper shape. color is bright, i ordered green as the other color ( plum ) doesn't have petite available. pattern is rich, and classy, the cotton is surprisingly soft. i love the little details in the velvet. definitely need a strapless bra for these one. 115 lbsm 30 d\n",
      "Label: ('General', 5.0, 'Dresses') => [0, 5.0, 1]\n",
      "----------\n",
      "Text: perfect fabric for two wears. ok ladies.... you need to know that this type of fabric is the one it will get holes ( i bought the white one ). it is super nice and lovely, but i was only able to get two wears out of it. i did cut it and only maintained it's size because i restretched it while wet then hung to dry. i was super sorry about the wear but appreciated being able to return it without question at another local retailer.\n",
      "Label: ('General', 1.0, 'Tops') => [0, 1.0, 4]\n",
      "----------\n",
      "Text: . i love this dress. it is so soft and comfortable! perfect for summer!! i wish this came in more colors because i would wow everyone!!\n",
      "Label: ('Initmates', 5.0, 'Intimate') => [2, 5.0, 2]\n",
      "----------\n",
      "Text: adorable and excellent quality . this is such a clean and cute printed dress and i knew that i had to try the dress when i first saw it online . after reading other reviews , i sized up . i am normally a 0 or 2 in retailer . i ordered the 2 and it fits nicely and looks great . however , i feel like the 2 buttons at the lowered rib cage area gape slightly . the tie covers it and holds it in place , unless i sit , then it gapes freely . i am a 32 a so not big chested at all , and yet this fit snug in the chest area . i would worry about\n",
      "Label: ('General Petite', 4.0, 'Dresses') => [1, 4.0, 1]\n",
      "----------\n",
      "Text: one of a kind skirt . unique and well-made skirt , sure to turn heads . embroidery is lovely . def a statement piece for any wardrobe .\n",
      "Label: ('General', 5.0, 'Bottoms') => [0, 5.0, 0]\n",
      "----------\n",
      "Text: super huge top. i have no return this one. the armholes are huge. the material is on the thin side and not as i expected. i can't size down. i got an xxs! maybe if i had been able to order something petite, it would have fit better. i ordered the red and i liked the color.\n",
      "Label: ('General Petite', 2.0, 'Tops') => [1, 2.0, 4]\n",
      "----------\n",
      "Text: great fit and flow. love this flowing, cute top! casual, but may be easily dressed up. perfect fit.\n",
      "Label: ('General Petite', 5.0, 'Tops') => [1, 5.0, 4]\n",
      "----------\n",
      "Text: please bring back more!!!. love this strap! prettier shade of yellow in person. love the strap width and length. wish u bring back more in lots of colors : i sized up one from my usual size and will have to shorten the length of the strap myself, but it's an easy alteration and is totally worth it! especially good at sale price. i really hope they make another in the style in alot of colors and great sale!\n",
      "Label: ('Initmates', 5.0, 'Intimate') => [2, 5.0, 2]\n",
      "----------\n",
      "Text: roomy and flows . comfortable and flows well , does run large , order one size smaller .\n",
      "Label: ('General Petite', 5.0, 'Tops') => [1, 5.0, 4]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    if i==10:break\n",
    "    print(f\"Text: {v['Review Text']}\\nLabel: {v['Division Name'],v['Rating'],v['Department Name']} => {v['label']}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: soft , feminine and fun pockets ! . i love this tunic . purchased the dark orange in medium ( i am 5 ' 9 and 140 lbs ) . tried the small and almost kept it but i felt seams around my arm pits a tad , so went with the medium and glad i did - this top should be comfortable . feels very fall and perfect for casual get-togethers and running around town . only comment is that it is rayon ... and for me anyway rayon doesn't wash too well - so we shall see how this one fairs .\n",
      "Label: ('General', 5.0, 'Tops') => [0.0, 5.0, 4.0]\n",
      "----------\n",
      "Text: a new staple ! . tried these on out of sheer curiosity -- i've got a long torso & was pleasantly surprised how flattering they are ! they manage to look flowing & sleek without shortening the legs . took a size 6 with my 27 \" waist , 37 \" hips . it's a bit of a generous fit , especially around the waist , but they're extremely comfortable & have room to tuck tops into . i have the cowled sweater tank in gray & it looks fantastic over these ! couldn't resist getting both the rust and black . perfect for a dressy casual look\n",
      "Label: ('General Petite', 5.0, 'Bottoms') => [1.0, 5.0, 0.0]\n",
      "----------\n",
      "Text: maybe swing is for me ! . i love swing dresses but they never seem to work out for me . however , lately i have been trying on swing tops like this one and they are super scores ! i love this top ! in my store , they had a rack of test materials where they don't have the full line but they have a look at some online features or clothes that are very new releases . this was on the rack . i knew it wasn't my size but i tried it on anyway and i am absolutely in love . i am waiting for a sale ( as always ) but i am going to get this i\n",
      "Label: ('General', 5.0, 'Tops') => [0.0, 5.0, 4.0]\n",
      "----------\n",
      "Text: too flare . too small ... too flare ... nice thick fabric . not my favorite pant .\n",
      "Label: ('General', 2.0, 'Bottoms') => [0.0, 2.0, 0.0]\n",
      "----------\n",
      "Text: love . i love this top it is easy to wear fun and very comfortable . i was thinking about it for weeks and kept coming back to it after i read a review about going up a size i decided to go for it and i am very happy i did ! ! ! my new favorite ! ! !\n",
      "Label: ('General', 5.0, 'Tops') => [0.0, 5.0, 4.0]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Text: {tdc.main_ddict['validation']['Review Text'][i]}\")\n",
    "    print(f\"Label: {tdc.main_ddict['validation']['Division Name'][i],tdc.main_ddict['validation']['Rating'][i],tdc.main_ddict['validation']['Department Name'][i]} => {tdc.main_ddict['validation']['label'][i]}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering + Metadatas + Content Transformation + Content Augmentation (for Multi Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug2 = naw.ContextualWordEmbsAug(model_path='roberta-base', \n",
    "                                device='cuda:0', # if you don't have gpu, change to 'cpu'\n",
    "                                action=\"substitute\",\n",
    "                                top_k=10,\n",
    "                               aug_p=0.07)\n",
    "\n",
    "contextual_aug_func = partial(nlp_aug_stochastic,aug=aug2,p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\n",
    "df['Fake Label'] = [np.random.choice(df['Department Name'].unique()[:-1],size=np.random.randint(2,6),replace=False) for _ in range(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict_with_val = dset.train_test_split(test_size=0.1)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name', 'Fake Label'],\n",
       "        num_rows: 2349\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict_with_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Data Filtering --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9af8b40196345c2a8a7237865cace46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/2349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0772e14967c9487ba2256c1ef04421db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2276 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b13e5ae18b417ba2360ece04766025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2276 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "----- Performing content transformation and tokenization on validation set -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab04fdc201b9455685d708bed419c6b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2276 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc8e09269c74327afe30620b100245c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2276 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cafad72963cc4f5fb1ed693bfc46c4d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2276 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Creating a generator for content transformation, augmentation and tokenization on train set -----\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataControllerStreaming(ddict_with_val,\n",
    "                                  main_text='Review Text',\n",
    "                                  label_names='Fake Label',\n",
    "                                  sup_types='classification',\n",
    "                                  class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n",
    "                                  filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                  metadatas=['Title','Division Name'],\n",
    "                                  content_transformations=[text_normalize,str.lower],\n",
    "                                  content_augmentations= contextual_aug_func, \n",
    "                                  process_metas=True,\n",
    "                                  batch_size=1000,\n",
    "                                  num_proc=4,\n",
    "                                  seed=42\n",
    "                                 )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: initmates . so cute , but not for me . this suit is so adorable ! i love the retro style and the two patterns and i really wish it worked for me . the suit is comfy , seems well made and fit nicely . but , although it fit technically , i don't like the way it cuts me off at the thigh . it's not flattering for me at all . the suit is kind of a neoprene material , so it's thicker than a normal bathing suit , so i think even though it isn't lined , it might not be see-through , but i didn't get it wet to test it out . in terms of fit , i think it\n",
      "Label: ['Jackets', 'Tops', 'Intimate'] => [0 0 1 1 1 0]\n",
      "----------\n",
      "Text: general. beautiful, stunning, cozy top!. i read the first article on this and wanted both a small and a medium as i thought this top run small! i have to strongly disagree with the reviewer! i find that this top runs true to size or even generous! the sky color is so pretty and this top can be dressed up with some white heels and a necklace or it can be comfy casual! i usually wear a small or hh brand and this one was true good fit ( 5 \" 2 \", broad shoulders, 120 lb )\n",
      "Label: ['Bottoms', 'Dresses'] => [1 1 0 0 0 0]\n",
      "----------\n",
      "Text: general . love ! . love love love this dress ! but , if you are not wearing a slip ... you should be ! please wear a slip , you can see right through this dress .\n",
      "Label: ['Bottoms', 'Tops', 'Trend', 'Dresses'] => [1 1 0 0 1 1]\n",
      "----------\n",
      "Text: general. runs big. i loved the idea of these pants as i've been looking for an updated pair of tuxedo pants. i wear them in most but my jeans. i'm not super fat & consider my legs medium ( not too skinny & not too athletic ). i tried some on in xs ($ 36 european as marked on them ) & they were big between the waist & hip area but there was so much gap in the leg which made them look frumpy.. i do however liked the length. the fabric is nice & heavy which i also loved. sadly, these didn't work for me though. really wish t\n",
      "Label: ['Dresses', 'Bottoms', 'Trend', 'Tops', 'Intimate'] => [1 1 1 0 1 1]\n",
      "----------\n",
      "Text: general petite. looks pretty but fit is off. this dress is everything i hoped it to be--bright, colorful, quality dress, classic silhouette. unfortunately it runs small up top on the bust and i can even zip it up all the way. returning : (\n",
      "Label: ['Bottoms', 'Tops'] => [1 0 0 0 1 0]\n",
      "----------\n",
      "Text: general petite. very nice. i'm 5'5 and curvy body shape, bought size 6 because they want it to be a bit loose, turns out i could go with size 4 too, beautiful detail.. pretty amazing fabric. this come with a crop tank top to go to the lace jacket.\n",
      "Label: ['Bottoms', 'Intimate', 'Dresses', 'Tops'] => [1 1 1 0 1 0]\n",
      "----------\n",
      "Text: general.. very pretty dress! my blue is a little lighter than navy though is still really warm. the embroidered flowers are lovely as well. the a-line cut is very flattering on. i can't wait to wear this dress this spring!\n",
      "Label: ['Intimate', 'Bottoms', 'Jackets', 'Trend', 'Tops'] => [1 0 1 1 1 1]\n",
      "----------\n",
      "Text: general petite. so flattering, no need for petite : i just tried this top on in xs regular even though i generally wear xspetite in dresses and it fit great ( 34 aa + 26 - 34 ). i think it's flattering on this fairly straight body with the halter neck, fitted waist and peplum. i would guess it would be flattering on many body types ; it highlights shoulders beautifully. it seemed very hard to pull this head through those small - not-too-stretchy sleeves so you may want to try it on without makeup. i knocked off one star because the neck band wasn't symmetrical,\n",
      "Label: ['Jackets', 'Bottoms'] => [1 0 0 1 0 0]\n",
      "----------\n",
      "Text: general . a new wardrobe staple ! . love this sweater ! i purchased both the green and gray versions and will wear them constantly this winter . although some reviews were critical of the feel of the yarn , i do not find this to be a problem ... so happy with these products ! for reference to size , i bought the m and am 5 ' 11 ... the sweater hits me exactly where it does on the model .\n",
      "Label: ['Trend', 'Dresses', 'Intimate'] => [0 1 1 0 0 1]\n",
      "----------\n",
      "Text: general . unique and adorable . the photos don't do the top justice . the split back is very unique and adorable . i typically take a size 8 in tops , but ordered a 12 since a reviewer suggested it was narrow in the shoulders . the 12 shoulder fit perfectly , but the body is way too big -- i was swimming it in . i like it enough that i'm going to visit the tailor to take in the body . with the sale price , it is worth tailoring .\n",
      "Label: ['Trend', 'Jackets', 'Tops', 'Bottoms'] => [1 0 0 1 1 1]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    if i==10:break\n",
    "    print(f\"Text: {v['Review Text']}\\nLabel: {v['Fake Label']} => {v['label']}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: general petite . . this top has great detailing and color . does run a little big , but adds to the style and movement of the tank . the stitching around the bottom makes it cute for layering .\n",
      "Label: ['Dresses', 'Intimate', 'Trend', 'Tops', 'Bottoms'] => [1, 1, 1, 0, 1, 1]\n",
      "----------\n",
      "Text: general . . i love this top . i got it on sale and am so glad that i did . it is a short too but still super flattering . it isn't too boxy on me .\n",
      "Label: ['Intimate', 'Trend', 'Jackets', 'Dresses', 'Tops'] => [0, 1, 1, 1, 1, 1]\n",
      "----------\n",
      "Text: general . beautiful idea ... . i ordered my normal size in this dress . i am 6 foot tall , but the regular sizes were too large and too long ( mid-calf ) . i returned the dress for a size smaller in petite for a more flattering hemline . the dress is lovely , especially on the models in the pictures , but didn't quite work out for me . also , it feels like there are hundreds of closure hooks that make putting on / taking off the dress seem to take an unusually long time !\n",
      "Label: ['Dresses', 'Tops'] => [0, 1, 0, 0, 1, 0]\n",
      "----------\n",
      "Text: general petite . comfy , but not made to last . this sweater is fine for the casual days . i bought this in cream and i have to say after one wash it looks old . i'm a huge retailer lover and buy a lot of clothes from them . this is just not the best quality and looks tired after a few wears . very soft , but poor material . not my favorite purchase .\n",
      "Label: ['Trend', 'Jackets'] => [0, 0, 0, 1, 0, 1]\n",
      "----------\n",
      "Text: general . great cool looking jeans . i just bought these jeans today & they are really cute & comfortable on . i love pilcro jeans as they fit really well , they are made well & they are always on style . i did have to go down a size as well but they fit beautifully ! comfy & stylish !\n",
      "Label: ['Tops', 'Intimate', 'Jackets', 'Bottoms'] => [1, 0, 1, 1, 1, 0]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Text: {tdc.main_ddict['validation']['Review Text'][i]}\")\n",
    "    print(f\"Label: {tdc.main_ddict['validation']['Fake Label'][i]} => {tdc.main_ddict['validation']['label'][i]}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.label_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load TextDataControllerStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### TextDataControllerStreaming.save_as_pickles\n",
       "\n",
       ">      TextDataControllerStreaming.save_as_pickles (fname,\n",
       ">                                                   parent='pickle_files',\n",
       ">                                                   drop_attributes=False)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |\n",
       "| drop_attributes | bool | False | Whether to drop large-size attributes |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### TextDataControllerStreaming.save_as_pickles\n",
       "\n",
       ">      TextDataControllerStreaming.save_as_pickles (fname,\n",
       ">                                                   parent='pickle_files',\n",
       ">                                                   drop_attributes=False)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |\n",
       "| drop_attributes | bool | False | Whether to drop large-size attributes |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataControllerStreaming.save_as_pickles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### TextDataControllerStreaming.from_pickle\n",
       "\n",
       ">      TextDataControllerStreaming.from_pickle (fname, parent='pickle_files')\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### TextDataControllerStreaming.from_pickle\n",
       "\n",
       ">      TextDataControllerStreaming.from_pickle (fname, parent='pickle_files')\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataControllerStreaming.from_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextDataControllerStreaming object can be saved and loaded with ease. This is especially useful after text processing and/or tokenization have been done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import disable_caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_caching() # disable huggingface caching to see data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import text_normalize\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_aug_stochastic(x,aug=None,p=0.5):\n",
    "    if not isinstance(x,list): \n",
    "        if random.random()<p: return aug.augment(x)[0]\n",
    "        return x\n",
    "    news=[]\n",
    "    originals=[]\n",
    "    for _x in x:\n",
    "        if random.random()<p: news.append(_x)\n",
    "        else: originals.append(_x)\n",
    "    # only perform augmentation when needed\n",
    "    if len(news): news = aug.augment(news)\n",
    "    return news+originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug2 = naw.ContextualWordEmbsAug(model_path='roberta-base', \n",
    "                                device='cuda:0', # if you don't have gpu, change to 'cpu'\n",
    "                                action=\"substitute\",\n",
    "                                top_k=10,\n",
    "                               aug_p=0.07)\n",
    "\n",
    "contextual_aug_func = partial(nlp_aug_stochastic,aug=aug2,p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Data Filtering --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62873ee3d25e473b8db54cdc43e5af65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/4698 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ac806f2adf4195a1de21df75aae84b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/4530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c43816b82948f68947b4e478113659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7aca230b31444cbdcd7f3b83c2e562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "----- Performing content transformation and tokenization on validation set -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f152052d0a684580843a96e5e08aa287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce0d0c043964157b9ef55093c8b9166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d513623072ab4ac1bf374adf5456fdbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Creating a generator for content transformation, augmentation and tokenization on train set -----\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.2)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']\n",
    "\n",
    "tdc = TextDataControllerStreaming(ddict_with_val,\n",
    "                                  main_text='Review Text',\n",
    "                                  label_names='Department Name',\n",
    "                                  sup_types='classification',\n",
    "                                  class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n",
    "                                  filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                               'Department Name': lambda x: x is not None,\n",
    "                                              },\n",
    "                                  metadatas=['Title','Division Name'],\n",
    "                                  content_transformations=[text_normalize,str.lower],\n",
    "                                  content_augmentations= contextual_aug_func,\n",
    "                                  process_metas=True,\n",
    "                                  batch_size=100,\n",
    "                                  num_proc=4,\n",
    "                                  seed=42\n",
    "                                 )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Title', 'Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4529\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.save_as_pickles('my_tdc_stream')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Size in MegaBytes is 479.023\n"
     ]
    }
   ],
   "source": [
    "file_stats = os.stat(Path('pickle_files/my_tdc_stream.pkl'))\n",
    "print(f'File Size in MegaBytes is {round(file_stats.st_size / (1024 * 1024), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load back our object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc2 = TextDataControllerStreaming.from_pickle('my_tdc_stream')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can still access all its attributes, data, preprocessings, transformation/augmentation ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Title', 'Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4529\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc2.main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: general . eye spy a great vest . i purchased this in my usual small ( size 4-6 ) and it fits just the way it shows in the photo . it is very flowy which is th point . i wore it over a black romper and it looked great . i can also wear with jeans and a simple black tank . keep the vest open or use the hook and eye and close it up . seeing the black through the sheer white is simply dreamy ! glad i purchased it during the sale on sale promotion . it is a classic piece for sure .\n",
      "Label: Jackets => 3\n",
      "----------\n",
      "Text: general petite.. i love this soft, colorful, flowy beauty! it's the new color palette! i'm 5'5 \", 34 d, size 6 and a small fit me with room to spare. don't wait!\n",
      "Label: Dresses => 1\n",
      "----------\n",
      "Text: general petite . cool top . impecable workmanship ( overseas ) . i usually wear a petite 2 but ordered this in a regular size 0 and glad that i did . since it curves up on this side , it barely overs the waistband on my jeans . ordered this in pink but it's more of pale coral pink . the style is more of a semi-halter sleeve but it have snaps on the inside shoulders to keep the bra straps from showing or you can wear a halter bra . it's a very light weight cotton and flowing and it will be great for the hot weather .\n",
      "Label: Tops => 4\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(tdc2.main_ddict['train']):\n",
    "    if i==3:break\n",
    "    print(f\"Text: {v['Review Text']}\\nLabel: {v['Department Name']} => {v['label']}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc2.label_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Review Text': <function __main__.<lambda>(x)>,\n",
       "  'Department Name': <function __main__.<lambda>(x)>},\n",
       " [<function underthesea.pipeline.text_normalize.text_normalize(text, tokenizer='underthesea')>,\n",
       "  <method 'lower' of 'str' objects>],\n",
       " [functools.partial(<function nlp_aug_stochastic>, aug=<nlpaug.augmenter.word.context_word_embs.ContextualWordEmbsAug object>, p=0.1)])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc2.filter_dict,tdc2.content_tfms,tdc2.aug_tfms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't want to store the HuggingFace DatasetDict in your `TextDataControllerStreaming`, or the augmentation functions (typically when you already have a trained model, and you only use `TextDataControllerStreaming` to preprocess the test set), you can remove it in the `save_as_pickles` step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.save_as_pickles('my_lightweight_tdc_stream',drop_attributes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Size in MegaBytes is 1.911\n"
     ]
    }
   ],
   "source": [
    "file_stats = os.stat(Path('pickle_files/my_lightweight_tdc_stream.pkl'))\n",
    "print(f'File Size in MegaBytes is {round(file_stats.st_size / (1024 * 1024), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc3 = TextDataControllerStreaming.from_pickle('my_lightweight_tdc_stream')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this object to demonstrate the Test Set Construction in the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### TextDataControllerStreaming.prepare_test_dataset\n",
       "\n",
       ">      TextDataControllerStreaming.prepare_test_dataset (test_dset,\n",
       ">                                                        do_filtering=False)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| test_dset |  |  | The HuggingFace Dataset as Test set |\n",
       "| do_filtering | bool | False | whether to perform data filtering on this test set |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### TextDataControllerStreaming.prepare_test_dataset\n",
       "\n",
       ">      TextDataControllerStreaming.prepare_test_dataset (test_dset,\n",
       ">                                                        do_filtering=False)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| test_dset |  |  | The HuggingFace Dataset as Test set |\n",
       "| do_filtering | bool | False | whether to perform data filtering on this test set |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataControllerStreaming.prepare_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### TextDataControllerStreaming.prepare_test_dataset_from_csv\n",
       "\n",
       ">      TextDataControllerStreaming.prepare_test_dataset_from_csv (file_path,\n",
       ">                                                                 do_filtering=F\n",
       ">                                                                 alse)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| file_path |  |  | path to csv file |\n",
       "| do_filtering | bool | False | whether to perform data filtering on this test set |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### TextDataControllerStreaming.prepare_test_dataset_from_csv\n",
       "\n",
       ">      TextDataControllerStreaming.prepare_test_dataset_from_csv (file_path,\n",
       ">                                                                 do_filtering=F\n",
       ">                                                                 alse)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| file_path |  |  | path to csv file |\n",
       "| do_filtering | bool | False | whether to perform data filtering on this test set |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataControllerStreaming.prepare_test_dataset_from_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### TextDataControllerStreaming.prepare_test_dataset_from_df\n",
       "\n",
       ">      TextDataControllerStreaming.prepare_test_dataset_from_df (df,\n",
       ">                                                                validate=True, \n",
       ">                                                                do_filtering=Fa\n",
       ">                                                                lse)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df |  |  | Pandas Dataframe |\n",
       "| validate | bool | True | whether to perform input data validation |\n",
       "| do_filtering | bool | False | whether to perform data filtering on this test set |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### TextDataControllerStreaming.prepare_test_dataset_from_df\n",
       "\n",
       ">      TextDataControllerStreaming.prepare_test_dataset_from_df (df,\n",
       ">                                                                validate=True, \n",
       ">                                                                do_filtering=Fa\n",
       ">                                                                lse)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df |  |  | Pandas Dataframe |\n",
       "| validate | bool | True | whether to perform input data validation |\n",
       "| do_filtering | bool | False | whether to perform data filtering on this test set |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataControllerStreaming.prepare_test_dataset_from_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### TextDataControllerStreaming.prepare_test_dataset_from_raws\n",
       "\n",
       ">      TextDataControllerStreaming.prepare_test_dataset_from_raws (content)\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| content | Either a single sentence, list of sentence or a dictionary with keys are metadata columns and values are list |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### TextDataControllerStreaming.prepare_test_dataset_from_raws\n",
       "\n",
       ">      TextDataControllerStreaming.prepare_test_dataset_from_raws (content)\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| content | Either a single sentence, list of sentence or a dictionary with keys are metadata columns and values are list |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataControllerStreaming.prepare_test_dataset_from_raws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you have done your preprocessing and tokenization in your training set, and have a nicely trained model, ready to do inference on new data. Here is how you can use `TextDataControllerStreaming` to apply all the necessary preprocessings to your new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reuse the lightweight tdc object we created in the previous section (since we don't really need all the training data just to construct new data). Also, we will take a small sample of our training data and pretend it is our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc = TextDataControllerStreaming.from_pickle('my_lightweight_tdc_stream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4692, 10)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig').sample(frac=0.2,random_state=1)\n",
    "# drop NaN values in the label column\n",
    "df_test = df_test[~df_test['Department Name'].isna()].reset_index(drop=True)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>872</td>\n",
       "      <td>42</td>\n",
       "      <td>Perfect for work and play</td>\n",
       "      <td>This shirt works for both going out and going ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1033</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I don't know why i had the opposite problem mo...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Jeans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1037</td>\n",
       "      <td>45</td>\n",
       "      <td>Great pants</td>\n",
       "      <td>These cords are great--lightweight for fl wint...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Jeans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>829</td>\n",
       "      <td>35</td>\n",
       "      <td>Surprisingly comfy for a button down</td>\n",
       "      <td>I am a 10 m and got the 10. it fits perfectly ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>872</td>\n",
       "      <td>29</td>\n",
       "      <td>Short and small</td>\n",
       "      <td>The shirt is mostly a thick sweatshirt materia...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clothing ID  Age                                 Title   \n",
       "0          872   42             Perfect for work and play  \\\n",
       "1         1033   40                                   NaN   \n",
       "2         1037   45                           Great pants   \n",
       "3          829   35  Surprisingly comfy for a button down   \n",
       "4          872   29                       Short and small   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND   \n",
       "0  This shirt works for both going out and going ...       5                1  \\\n",
       "1  I don't know why i had the opposite problem mo...       4                1   \n",
       "2  These cords are great--lightweight for fl wint...       5                1   \n",
       "3  I am a 10 m and got the 10. it fits perfectly ...       5                1   \n",
       "4  The shirt is mostly a thick sweatshirt materia...       3                0   \n",
       "\n",
       "   Positive Feedback Count   Division Name Department Name Class Name  \n",
       "0                        0         General            Tops      Knits  \n",
       "1                        0  General Petite         Bottoms      Jeans  \n",
       "2                        1  General Petite         Bottoms      Jeans  \n",
       "3                        1  General Petite            Tops    Blouses  \n",
       "4                       15  General Petite            Tops      Knits  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Input Validation Precheck -\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title          758\n",
      "Review Text    164\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 2 rows\n",
      "-------------------- Start Test Set Transformation --------------------\n",
      "-------------------- Data Filtering --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bdbebd077a74ad7808eb094b769427f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/4692 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e66765dd3d3a4ac6a3750b524bd196e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286387e00081429d9757f289dbc3fd7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "----- Performing content transformation and tokenization on test set -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c329cd29b84ad4a11beb5f17d6df89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c44ef50dba4e7d9a4b4a6d3febd932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8d202b0c8b4243a62600acf563f736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "test_dset = tdc.prepare_test_dataset_from_df(df_test,validate=True,do_filtering=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: general . perfect for work and play . this shirt works for both going out and going to work , and i can wear it with everything . fits perfect , tucked and untucked , tied and untied . i love it .\n",
      "Input_ids: [0, 15841, 479, 1969, 13, 173, 8, 310, 479, 42, 6399, 1364, 13, 258, 164, 66, 8, 164, 7, 173, 2156, 8, 939, 64, 3568, 24, 19, 960, 479, 10698, 1969, 2156, 21222, 8, 7587, 23289, 2156, 3016, 8, 7587, 2550, 479, 939, 657, 24, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "----------\n",
      "Text: general petite . . i don't know why i had the opposite problem most reviewers had with these ..... i tried on the regular length in the store and found that they were just a bit too short with heels . ( i'm 5 ' 5 ) . i had them ordered in a petite and when they came , they were too short with flats ! maybe it's the way i like to wear them , i like my flare jeans to barely skim the ground . i just exchanged them for regular length and will wear them with a small wedge shoe . aside from the length issues , these are super cute\n",
      "Input_ids: [0, 15841, 4716, 1459, 479, 479, 939, 218, 75, 216, 596, 939, 56, 5, 5483, 936, 144, 34910, 56, 19, 209, 29942, 734, 939, 1381, 15, 5, 1675, 5933, 11, 5, 1400, 8, 303, 14, 51, 58, 95, 10, 828, 350, 765, 19, 8872, 479, 36, 939, 437, 195, 128, 195, 4839, 479, 939, 56, 106, 2740, 11, 10, 4716, 1459, 8, 77, 51, 376, 2156, 51, 58, 350, 765, 19, 20250, 27785, 2085, 24, 18, 5, 169, 939, 101, 7, 3568, 106, 2156, 939, 101, 127, 24186, 10844, 7, 6254, 28772, 5, 1255, 479, 939, 95, 11024, 106, 13, 1675, 5933, 8, 40, 3568, 106, 19, 10, 650, 27288, 12604, 479, 4364, 31, 5, 5933, 743, 2156, 209, 32, 2422, 11962, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "----------\n",
      "Text: general petite . great pants . thes e cords are great--lightweight for fl winters , and the bootcut flare bottom is super cute with ballet flats or booties . i am 5 ' 10 \" and typically a size 8 ; the size 29 fit perfectly . they have a little stretch to them , which is great . very flattering--wish i could order in more colors ! !\n",
      "Input_ids: [0, 15841, 4716, 1459, 479, 372, 9304, 479, 5, 29, 364, 37687, 32, 372, 5579, 6991, 4301, 13, 2342, 31000, 2156, 8, 5, 9759, 8267, 24186, 2576, 16, 2422, 11962, 19, 22573, 20250, 50, 9759, 918, 479, 939, 524, 195, 128, 158, 22, 8, 3700, 10, 1836, 290, 25606, 5, 1836, 1132, 2564, 6683, 479, 51, 33, 10, 410, 4140, 7, 106, 2156, 61, 16, 372, 479, 182, 34203, 5579, 605, 1173, 939, 115, 645, 11, 55, 8089, 27785, 27785, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Text: {test_dset['Review Text'][i]}\")\n",
    "    print(f\"Input_ids: {test_dset['input_ids'][i]}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make our test data streamed as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dset_raw = Dataset.from_pandas(df_test).to_iterable_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test dataset might have some NaN values in the text field (`Review Text`), thus we will turn on the filtering option to get rid of these NaNs, as this is what we did in the training set. If your test dataset don't need any filtering, turn off this option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Test Set Transformation --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "----- Performing content transformation and tokenization on test set -----\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "test_dset = tdc.prepare_test_dataset(test_dset_raw,do_filtering=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: general . perfect for work and play . this shirt works for both going out and going to work , and i can wear it with everything . fits perfect , tucked and untucked , tied and untied . i love it .\\Input_ids: [0, 15841, 479, 1969, 13, 173, 8, 310, 479, 42, 6399, 1364, 13, 258, 164, 66, 8, 164, 7, 173, 2156, 8, 939, 64, 3568, 24, 19, 960, 479, 10698, 1969, 2156, 21222, 8, 7587, 23289, 2156, 3016, 8, 7587, 2550, 479, 939, 657, 24, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----------\n",
      "Text: general petite . . i don't know why i had the opposite problem most reviewers had with these ..... i tried on the regular length in the store and found that they were just a bit too short with heels . ( i'm 5 ' 5 ) . i had them ordered in a petite and when they came , they were too short with flats ! maybe it's the way i like to wear them , i like my flare jeans to barely skim the ground . i just exchanged them for regular length and will wear them with a small wedge shoe . aside from the length issues , these are super cute\\Input_ids: [0, 15841, 4716, 1459, 479, 479, 939, 218, 75, 216, 596, 939, 56, 5, 5483, 936, 144, 34910, 56, 19, 209, 29942, 734, 939, 1381, 15, 5, 1675, 5933, 11, 5, 1400, 8, 303, 14, 51, 58, 95, 10, 828, 350, 765, 19, 8872, 479, 36, 939, 437, 195, 128, 195, 4839, 479, 939, 56, 106, 2740, 11, 10, 4716, 1459, 8, 77, 51, 376, 2156, 51, 58, 350, 765, 19, 20250, 27785, 2085, 24, 18, 5, 169, 939, 101, 7, 3568, 106, 2156, 939, 101, 127, 24186, 10844, 7, 6254, 28772, 5, 1255, 479, 939, 95, 11024, 106, 13, 1675, 5933, 8, 40, 3568, 106, 19, 10, 650, 27288, 12604, 479, 4364, 31, 5, 5933, 743, 2156, 209, 32, 2422, 11962, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----------\n",
      "Text: general petite . great pants . thes e cords are great--lightweight for fl winters , and the bootcut flare bottom is super cute with ballet flats or booties . i am 5 ' 10 \" and typically a size 8 ; the size 29 fit perfectly . they have a little stretch to them , which is great . very flattering--wish i could order in more colors ! !\\Input_ids: [0, 15841, 4716, 1459, 479, 372, 9304, 479, 5, 29, 364, 37687, 32, 372, 5579, 6991, 4301, 13, 2342, 31000, 2156, 8, 5, 9759, 8267, 24186, 2576, 16, 2422, 11962, 19, 22573, 20250, 50, 9759, 918, 479, 939, 524, 195, 128, 158, 22, 8, 3700, 10, 1836, 290, 25606, 5, 1836, 1132, 2564, 6683, 479, 51, 33, 10, 410, 4140, 7, 106, 2156, 61, 16, 372, 479, 182, 34203, 5579, 605, 1173, 939, 115, 645, 11, 55, 8089, 27785, 27785, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(test_dset):\n",
    "    if i==3:break\n",
    "    print(f\"Text: {v['Review Text']}\\Input_ids: {v['input_ids']}\\nAttention mask: {v['attention_mask']}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
