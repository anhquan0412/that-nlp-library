{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Main Streaming\n",
    "\n",
    "> This module contains the main Python class for the **streaming** version of `TextDataController`\n",
    "\n",
    "- skip_showdoc: true\n",
    "- skip_exec: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp text_main_streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from datasets import DatasetDict,Dataset,IterableDataset,load_dataset,Value\n",
    "from pathlib import Path\n",
    "from that_nlp_library.utils import *\n",
    "from that_nlp_library.text_main import tokenize_function,concat_metadatas\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "from datasets.utils.logging import disable_progress_bar, enable_progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from that_nlp_library.text_transformation import *\n",
    "from that_nlp_library.text_augmentation import *\n",
    "from importlib.machinery import SourceFileLoader\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class TextDataControllerStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TextDataControllerStreaming():\n",
    "    def __init__(self,\n",
    "                 inp, # HuggingFainpce Dataset or DatasetDict\n",
    "                 main_text:str, # Name of the main text column\n",
    "                 label_names=[], # Names of the label (dependent variable) columns\n",
    "                 sup_types=[], # Type of supervised learning for each label name ('classification' or 'regression')\n",
    "                 class_names_predefined=[], # List of names associated with the labels (same index order)\n",
    "                 filter_dict={}, # A dictionary: {feature: filtering_function_based_on_the_feature}\n",
    "                 label_tfm_dict={}, # A dictionary: {label_name: transform_function_for_that_label}\n",
    "                 metadatas=[], # Names of the metadata columns\n",
    "                 process_metas=True, # Whether to do simple text processing on the chosen metadatas\n",
    "                 content_transformations=[], # A list of text transformations\n",
    "                 content_augmentations=[], # A list of text augmentations\n",
    "                 seed=None, # Random seed\n",
    "                 batch_size=1024, # CPU batch size\n",
    "                 num_proc=1, # Number of process for multiprocessing. This will be applied on non-streamed validation set\n",
    "                 cols_to_keep=None, # Columns to keep after all processings\n",
    "                 verbose=True, # Whether to print processing information\n",
    "                ):\n",
    "            \n",
    "        self.main_text = main_text\n",
    "        \n",
    "        self.label_names = val2iterable(label_names)\n",
    "        self.sup_types = val2iterable(sup_types)\n",
    "        self._check_sup_types()\n",
    "        self.label_lists = class_names_predefined\n",
    "        \n",
    "        self.filter_dict = filter_dict\n",
    "        self.label_tfm_dict = label_tfm_dict\n",
    "        self.metadatas = val2iterable(metadatas)\n",
    "        self.process_metas = process_metas\n",
    "\n",
    "        self.content_tfms = val2iterable(content_transformations)\n",
    "        self.aug_tfms = val2iterable(content_augmentations)\n",
    "        \n",
    "        self.seed = seed\n",
    "        self.is_batched = batch_size>1\n",
    "        self.batch_size = batch_size\n",
    "        self.num_proc = num_proc\n",
    "        self.cols_to_keep = cols_to_keep\n",
    "\n",
    "        self.main_ddict=DatasetDict()\n",
    "        self.verbose = verbose\n",
    "        self.verboseprint = print if verbose else lambda *a, **k: None\n",
    "        if not self.verbose:\n",
    "            disable_progress_bar() # turn off huggingface `map` progress bar\n",
    "        else:\n",
    "            enable_progress_bar()\n",
    "            \n",
    "        if hasattr(inp,'keys'): # is datasetdict\n",
    "            if 'train' not in inp.keys(): \n",
    "                raise ValueError('The given DatasetDict has no \"train\" split')\n",
    "            else:\n",
    "                self.main_ddict['train'] = inp['train']\n",
    "            val_key = list(set(inp.keys()) & set(['val','validation','valid']))\n",
    "            if len(val_key)>1: raise ValueError('Your DatasetDict has more than 1 validation split')\n",
    "            if len(val_key)==1:\n",
    "                self.main_ddict['validation'] = inp[val_key[0]]\n",
    "        else: # is dataset\n",
    "            self.main_ddict['train'] = inp\n",
    "          \n",
    "        is_streamed=isinstance(self.main_ddict['train'],IterableDataset)\n",
    "        if not is_streamed: raise Exception('This Text Data Controller only handles streamed dataset')\n",
    "        \n",
    "        self.all_cols = get_dset_col_names(self.main_ddict['train'])\n",
    "        \n",
    "        if is_streamed and ('classification' in self.sup_types) and len(self.label_lists)==0:\n",
    "            raise ValueError('All classification labels must be provided when streaming')\n",
    "            \n",
    "        self._determine_multihead_multilabel()\n",
    "        self._convert_regression_to_float()\n",
    "        self._processed_call=False\n",
    "        \n",
    "            \n",
    "    @classmethod\n",
    "    def from_pickle(cls,\n",
    "                    fname, # Name of the pickle file\n",
    "                    parent='pickle_files' # Parent folder\n",
    "                   ):\n",
    "        return load_pickle(fname,parent=parent)\n",
    "    \n",
    "    def set_verbose(self,verbose):\n",
    "        self.verbose = verbose\n",
    "        self.verboseprint = print if verbose else lambda *a, **k: None\n",
    "    \n",
    "    def _convert_regression_to_float(self):\n",
    "        if len(self.sup_types)==0: return\n",
    "        # convert regression labels to float64\n",
    "        reg_idxs = [i for i,v in enumerate(self.sup_types) if v=='regression']\n",
    "        for i in reg_idxs:\n",
    "            self.main_ddict['train'] = self.main_ddict['train'].cast_column(self.label_names[i],Value(\"float64\"))\n",
    "            if 'validation' in self.main_ddict.keys():\n",
    "                self.main_ddict['validation'] = self.main_ddict['validation'].cast_column(self.label_names[i],Value(\"float64\"))\n",
    "        \n",
    "    def _check_sup_types(self):\n",
    "        assert len(self.label_names)==len(self.sup_types), \"The number of supervised learning declaration must equal to the number of label\"\n",
    "        assert len(set(self.sup_types) - set(['classification','regression']))==0, 'Accepted inputs for `sup_types` are `classification` and `regression`'\n",
    "        \n",
    "    def _determine_multihead_multilabel(self):\n",
    "        self.is_multilabel=False\n",
    "        self.is_multihead=False\n",
    "        if len(self.label_names)==0: return\n",
    "        \n",
    "        if len(self.label_names)>1:\n",
    "            self.is_multihead=True\n",
    "        # get label of first row\n",
    "        first_label = next(iter(self.main_ddict['train']))[self.label_names[0]]\n",
    "        if isinstance(first_label,(list,set,tuple)):\n",
    "            # This is multi-label. Ignore self.label_names[1:]\n",
    "            self.label_names = [self.label_names[0]]\n",
    "            self.is_multihead=False\n",
    "            self.is_multilabel=True\n",
    "                     \n",
    "    \n",
    "    def save_as_pickles(self,\n",
    "                        fname, # Name of the pickle file\n",
    "                        parent='pickle_files', # Parent folder\n",
    "                        drop_attributes=False # Whether to drop large-size attributes\n",
    "                       ):\n",
    "        if drop_attributes:\n",
    "            if hasattr(self, 'main_ddict'):\n",
    "                del self.main_ddict\n",
    "            if hasattr(self, 'ddict_rest'):\n",
    "                del self.ddict_rest\n",
    "            if hasattr(self, 'aug_tfms'):\n",
    "                del self.aug_tfms\n",
    "        save_to_pickle(self,fname,parent=parent)\n",
    "    \n",
    "    def _process_metadatas(self,dtrain):\n",
    "        if len(self.metadatas):\n",
    "            map_func = partial(concat_metadatas,\n",
    "                               main_text=self.main_text,\n",
    "                               metadatas=self.metadatas,\n",
    "                               process_metas=self.process_metas,\n",
    "                               is_batched=self.is_batched)\n",
    "            dtrain = hf_map_dset(dtrain,map_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "        return dtrain\n",
    "    \n",
    "    def _do_label_transformation(self):\n",
    "        if len(self.label_names)==0 or len(self.label_tfm_dict)==0: return\n",
    "        print_msg('Label Transformation',20,verbose=self.verbose)\n",
    "        for f,tfm in self.label_tfm_dict.items():\n",
    "            if f in self.label_names:\n",
    "                _func = partial(lambda_map_batch,\n",
    "                                feature=f,\n",
    "                                func=tfm,\n",
    "                                is_batched=self.is_batched\n",
    "                               )                \n",
    "                self.main_ddict['train'] = hf_map_dset(self.main_ddict['train'],_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "                if 'validation' in self.main_ddict.keys():\n",
    "                    self.main_ddict['validation'] = hf_map_dset(self.main_ddict['validation'],\n",
    "                                                                _func,\n",
    "                                                                self.is_batched,\n",
    "                                                                self.batch_size,\n",
    "                                                                self.num_proc)\n",
    "        self.verboseprint('Done')\n",
    "                    \n",
    "                      \n",
    "                \n",
    "    def _create_label_mapping_func(self,encoder_classes):\n",
    "        if self.is_multihead:\n",
    "            label2idxs = [{v:i for i,v in enumerate(l_classes)} for l_classes in encoder_classes]\n",
    "            _func = lambda inp: {'label': [[label2idxs[i][v] if len(label2idxs[i]) else v for i,v in enumerate(vs)] \\\n",
    "                                           for vs in zip(*[inp[l] for l in self.label_names])] if self.is_batched \\\n",
    "                                 else [label2idxs[i][v] if len(label2idxs[i]) else v for i,v in enumerate([inp[l] for l in self.label_names])]\n",
    "                                }\n",
    "            \n",
    "        else: # single-head\n",
    "            if self.sup_types[0]=='regression':\n",
    "                _func1 = lambda x: x\n",
    "            else:\n",
    "                label2idx = {v:i for i,v in enumerate(encoder_classes[0])}\n",
    "                _func1 = lambda x: label2idx[x]\n",
    "                \n",
    "            _func = partial(lambda_map_batch,\n",
    "                           feature=self.label_names[0],\n",
    "                           func=_func1,\n",
    "                           output_feature='label',\n",
    "                           is_batched=self.is_batched)\n",
    "        return _func\n",
    "        \n",
    "    def _encode_labels(self):\n",
    "        if len(self.label_names)==0: return\n",
    "        print_msg('Label Encoding',verbose=self.verbose)\n",
    "        \n",
    "        if len(self.label_lists) and not isinstance(self.label_lists[0],list):\n",
    "            self.label_lists = [self.label_lists]\n",
    "                    \n",
    "        encoder_classes=[]\n",
    "        if not self.is_multilabel:\n",
    "            for idx,l in enumerate(self.label_names):\n",
    "                if self.sup_types[idx]=='regression':\n",
    "                    l_classes=[]\n",
    "                else: # classification\n",
    "                    l_classes = sorted(list(self.label_lists[idx]))\n",
    "                encoder_classes.append(l_classes)\n",
    "                \n",
    "            _func = self._create_label_mapping_func(encoder_classes)\n",
    "            \n",
    "            self.main_ddict['train'] = hf_map_dset(self.main_ddict['train'],_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "            if 'validation' in self.main_ddict.keys():\n",
    "                self.main_ddict['validation'] = hf_map_dset(self.main_ddict['validation'],_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "                    \n",
    "        else:\n",
    "            # For MultiLabel, we transform the label itself to one-hot (or actually, few-hot)\n",
    "            l_classes = sorted(list(self.label_lists[0]))   \n",
    "            encoder_classes.append(l_classes)\n",
    "            \n",
    "            l_encoder = MultiLabelBinarizer(classes=encoder_classes[0])\n",
    "            _ = l_encoder.fit(None)\n",
    "            _func = partial(lambda_map_batch,\n",
    "                            feature=self.label_names[0],\n",
    "                            func=lambda x: l_encoder.transform(x),\n",
    "                            output_feature='label',\n",
    "                            is_batched=self.is_batched,\n",
    "                            is_func_batched=True)\n",
    "            self.main_ddict['train'] = hf_map_dset(self.main_ddict['train'],_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "            if 'validation' in self.main_ddict.keys():\n",
    "                self.main_ddict['validation'] = hf_map_dset(self.main_ddict['validation'],_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "            \n",
    "        self.label_lists = encoder_classes\n",
    "        self.verboseprint('Done')\n",
    "        \n",
    "            \n",
    "            \n",
    "    def _simplify_ddict(self):\n",
    "        print_msg('Dropping unused features',20,verbose=self.verbose)\n",
    "        if self.cols_to_keep is None:\n",
    "            self.cols_to_keep= [self.main_text] + self.metadatas + self.label_names\n",
    "        cols_to_remove = set(self.all_cols) - set(self.cols_to_keep)\n",
    "        self.main_ddict['train']=self.main_ddict['train'].remove_columns(list(cols_to_remove))\n",
    "        if 'validation' in self.main_ddict.keys():\n",
    "            self.main_ddict['validation']=self.main_ddict['validation'].remove_columns(list(cols_to_remove))\n",
    "        self.verboseprint('Done')\n",
    "\n",
    "    def _do_filtering(self,dtrain):\n",
    "        if len(self.filter_dict):\n",
    "            col_names = get_dset_col_names(dtrain)\n",
    "            for f,tfm in self.filter_dict.items():\n",
    "                if f in col_names:\n",
    "                    _func = partial(lambda_batch,\n",
    "                                    feature=f,\n",
    "                                    func=tfm,\n",
    "                                    is_batched=self.is_batched)\n",
    "                    dtrain = hf_filter_dset(dtrain,_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "        return dtrain\n",
    "        \n",
    "\n",
    "    def _do_transformation_tokenization(self,dtrain):\n",
    "        tok_func = partial(tokenize_function,tok=self.tokenizer,max_length=self.max_length)\n",
    "        if len(self.content_tfms):            \n",
    "            for tfm in self.content_tfms:\n",
    "                _func = partial(lambda_map_batch,\n",
    "                                feature=self.main_text,\n",
    "                                func=tfm,\n",
    "                                is_batched=self.is_batched)\n",
    "                dtrain = hf_map_dset(dtrain,_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "        \n",
    "        _func = partial(lambda_map_batch,\n",
    "                        feature=self.main_text,\n",
    "                        func=tok_func,\n",
    "                        output_feature=None,\n",
    "                        is_batched=self.is_batched)\n",
    "        dtrain = hf_map_dset(dtrain,_func,self.is_batched,self.batch_size,self.tok_num_proc)\n",
    "            \n",
    "        return dtrain \n",
    "\n",
    "    def _do_transformation_augmentation_tokenization(self,dtrain,tok_func,all_tfms):\n",
    "\n",
    "        num_proc=1 # high num_proc is not beneficial with each batch size (which is only around 1k)\n",
    "        # Content transformation + augmentation\n",
    "        for tfm in all_tfms:\n",
    "            bs = self.batch_size\n",
    "            is_func_batched=False\n",
    "            is_batched = self.is_batched\n",
    "            if hasattr(tfm, \"run_on_gpu\") and getattr(tfm,'run_on_gpu')==True:\n",
    "                bs = min(32,self.batch_size) if not hasattr(tfm, \"batch_size\") else getattr(tfm,'batch_size')\n",
    "                is_func_batched=True\n",
    "                is_batched=True\n",
    "\n",
    "            _func = partial(lambda_map_batch,\n",
    "                            feature=self.main_text,\n",
    "                            func=tfm,\n",
    "                            is_batched=is_batched,\n",
    "                            is_func_batched=is_func_batched\n",
    "                            )\n",
    "            dtrain = hf_map_dset(dtrain,_func,\n",
    "                                 is_batched=is_batched,\n",
    "                                 batch_size=bs,\n",
    "                                 num_proc=num_proc\n",
    "                                )\n",
    "        # Tokenization\n",
    "        _func = partial(lambda_map_batch,\n",
    "                        feature=self.main_text,\n",
    "                        func=tok_func,\n",
    "                        output_feature=None,\n",
    "                        is_batched=self.is_batched)\n",
    "        dtrain = hf_map_dset(dtrain,_func,self.is_batched,self.batch_size,num_proc)\n",
    "            \n",
    "        return dtrain\n",
    "    \n",
    "    def _construct_generator_with_batch(self,dset,tok_func,all_tfms):        \n",
    "        def _get_generator(dset):\n",
    "            for v in dset: yield v\n",
    "            \n",
    "        final_dict = defaultdict(list)\n",
    "        for inp in dset: # dset is generator\n",
    "            # inp[text_name] will be a single item\n",
    "            for k,v in inp.items():\n",
    "                final_dict[k].append(v)\n",
    "            \n",
    "            if len(final_dict[self.main_text])==self.batch_size:\n",
    "                # a full batch (self.batch_size) is created\n",
    "                dtrain = Dataset.from_dict(final_dict)\n",
    "                dtrain = self._do_transformation_augmentation_tokenization(dtrain,tok_func,all_tfms)\n",
    "                yield from _get_generator(dtrain)\n",
    "                final_dict=defaultdict(list)            \n",
    "            \n",
    "        if len(final_dict[self.main_text]):\n",
    "            # hasn't reached batch_size (of last batch)\n",
    "            dtrain = Dataset.from_dict(final_dict)\n",
    "            dtrain = self._do_transformation_augmentation_tokenization(dtrain,tok_func,all_tfms)\n",
    "            yield from _get_generator(dtrain)\n",
    "            \n",
    "        \n",
    "            \n",
    "    def _do_transformation_augmentation_tokenization_generator(self):\n",
    "        tok_func = partial(tokenize_function,tok=self.tokenizer,max_length=self.max_length)\n",
    "        all_tfms = self.content_tfms + self.aug_tfms\n",
    "        \n",
    "        self.main_ddict['train'] = IterableDataset.from_generator(self._construct_generator_with_batch,\n",
    "                                                   gen_kwargs={'dset': self.main_ddict['train'],\n",
    "                                                               'tok_func':tok_func,\n",
    "                                                               'all_tfms': all_tfms\n",
    "                                                              }\n",
    "                                                                 )\n",
    "    \n",
    "    def _do_transformation_augmentation_tokenization_generator_linebyline(self):\n",
    "        def _get_generator(dset,tok_func,all_tfms):\n",
    "            for inp in dset:\n",
    "                # inp[text_name] will be a single item\n",
    "                inp[self.main_text]=all_tfms(inp[self.main_text])\n",
    "                result_dict = tok_func(inp[self.main_text])\n",
    "                for k,v in result_dict.items():\n",
    "                    inp[k]=v\n",
    "                yield inp\n",
    "        \n",
    "        # no padding for tokenization\n",
    "        tok_func = partial(tokenize_function,tok=self.tokenizer,max_length=-1) \n",
    "        all_tfms = self.content_tfms + self.aug_tfms\n",
    "        all_tfms = partial(func_all,functions=all_tfms) if len(all_tfms) else lambda x: x\n",
    "           \n",
    "        self.main_ddict['train'] = IterableDataset.from_generator(_get_generator,\n",
    "                                                   gen_kwargs={'dset': self.main_ddict['train'],\n",
    "                                                               'tok_func':tok_func,\n",
    "                                                               'all_tfms': all_tfms\n",
    "                                                              }\n",
    "                                                                 )\n",
    "\n",
    "        \n",
    "    def process_and_tokenize(self,\n",
    "                             tokenizer, # Tokenizer (preferably from HuggingFace)\n",
    "                             max_length=None, # pad to model's allowed max length (default is max_sequence_length)\n",
    "                             tok_num_proc=None, # Number of processes for tokenization\n",
    "                             line_by_line=False, # To whether process + tokenize each sentence separately. Faster, but no padding applied\n",
    "                            ):\n",
    "        if self._processed_call:\n",
    "            warnings.warn('Your dataset has already been processed. Returning the previous processed DatasetDict...')\n",
    "            return self.main_ddict\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.tok_num_proc = tok_num_proc if tok_num_proc else self.num_proc\n",
    "        \n",
    "        # Filtering\n",
    "        print_msg('Data Filtering',20,verbose=self.verbose)\n",
    "        for k in self.main_ddict.keys():   \n",
    "            self.main_ddict[k] = self._do_filtering(self.main_ddict[k])\n",
    "        self.verboseprint('Done')\n",
    "\n",
    "        \n",
    "        # Process metadatas\n",
    "        print_msg('Metadata Simple Processing & Concatenating to Main Content',verbose=self.verbose)\n",
    "        for k in self.main_ddict.keys():   \n",
    "            self.main_ddict[k] = self._process_metadatas(self.main_ddict[k])\n",
    "        self.verboseprint('Done')\n",
    "        \n",
    "        # Label transformation\n",
    "        self._do_label_transformation()\n",
    "        \n",
    "        # Process labels\n",
    "        self._encode_labels()\n",
    "\n",
    "        # Dropping unused columns\n",
    "        self._simplify_ddict()\n",
    "\n",
    "        if self.seed:\n",
    "            seed_everything(self.seed)\n",
    "            \n",
    "        # Content transformation + tokenization for validation\n",
    "        if 'validation' in self.main_ddict.keys():\n",
    "            print_msg('Performing Content Transformation and Tokenization on validation set',verbose=self.verbose)\n",
    "            self.main_ddict['validation'] = self._do_transformation_tokenization(self.main_ddict['validation'])\n",
    "            self.verboseprint('Done')\n",
    " \n",
    "        # Content transformation + augmentation + tokenization for train\n",
    "        print_msg('Creating a generator for content transformation, augmentation and tokenization on train set',verbose=self.verbose)\n",
    "        if line_by_line:\n",
    "            self._do_transformation_augmentation_tokenization_generator_linebyline()\n",
    "        else:\n",
    "            self._do_transformation_augmentation_tokenization_generator()\n",
    "        self.verboseprint('Done')\n",
    "        \n",
    "        self._processed_call=True\n",
    "    \n",
    "        \n",
    "    \n",
    "    def set_data_collator(self,data_collator):\n",
    "        self.data_collator = data_collator\n",
    "        \n",
    "    \n",
    "    def prepare_test_dataset_from_csv(self,\n",
    "                                      file_path, # path to csv file\n",
    "                                      do_filtering=False # whether to perform data filtering on this test set\n",
    "                                     ):\n",
    "        file_path = Path(file_path)\n",
    "        ds = load_dataset(str(file_path.parent),\n",
    "                          data_files=file_path.name,\n",
    "                          split='train')\n",
    "        return self.prepare_test_dataset(ds,do_filtering)\n",
    "    \n",
    "    def prepare_test_dataset_from_df(self,\n",
    "                                     df, # Pandas Dataframe\n",
    "                                     validate=True, # whether to perform input data validation\n",
    "                                     do_filtering=False # whether to perform data filtering on this test set \n",
    "                                    ):\n",
    "        if validate:\n",
    "            check_input_validation(df)\n",
    "        ds = Dataset.from_pandas(df)\n",
    "        return self.prepare_test_dataset(ds,do_filtering)\n",
    "    \n",
    "    def prepare_test_dataset_from_raws(self,\n",
    "                                       content, # Either a single sentence, list of sentence or a dictionary with keys are metadata columns and values are list\n",
    "                                      ):\n",
    "        if len(self.metadatas) and not isinstance(content,dict):\n",
    "            raise ValueError(f'There is/are metadatas in the preprocessing step. Please include a dictionary including these keys for metadatas: {self.metadatas}, and texture content: {self.main_text}')\n",
    "            \n",
    "        _dic = {self.main_text:[content]} if isinstance(content,str) else content\n",
    "        for k in _dic.keys():\n",
    "            _dic[k] = val2iterable(_dic[k])\n",
    "        \n",
    "        test_dict = Dataset.from_dict(_dic)\n",
    "        \n",
    "        # set num_proc to 1 for small data processing\n",
    "        _tmp1 = self.num_proc\n",
    "        _tmp2 = self.tok_num_proc\n",
    "        self.num_proc=1\n",
    "        self_tok_num_proc=1\n",
    "        results = self.prepare_test_dataset(test_dict,do_filtering=False)\n",
    "        self.num_proc = _tmp1\n",
    "        self.tok_num_proc=_tmp2\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def prepare_test_dataset(self,\n",
    "                             test_dset, # The HuggingFace Dataset as Test set\n",
    "                             do_filtering=False, # whether to perform data filtering on this test set\n",
    "                            ):\n",
    "        test_cols = set(get_dset_col_names(test_dset))\n",
    "        label_names_set = set(self.label_names)\n",
    "        test_cols = test_cols - label_names_set\n",
    "        missing_cols = set(self.cols_to_keep) - label_names_set - test_cols\n",
    "        if len(missing_cols):\n",
    "            raise ValueError(f'Test set does not have these columns required for preprocessings: {missing_cols}')\n",
    "            \n",
    "        print_msg('Start Test Set Transformation',20,verbose=self.verbose)\n",
    "\n",
    "        # Filtering\n",
    "        if do_filtering:\n",
    "            print_msg('Data Filtering',20,verbose=self.verbose)\n",
    "            test_dset = self._do_filtering(test_dset)\n",
    "            self.verboseprint('Done')\n",
    "        \n",
    "        # Process metadatas\n",
    "        print_msg('Metadata Simple Processing & Concatenating to Main Content',verbose=self.verbose)    \n",
    "        test_dset = self._process_metadatas(test_dset)\n",
    "        self.verboseprint('Done')\n",
    "        \n",
    "        # Drop unused columns\n",
    "        print_msg('Dropping unused features',20,verbose=self.verbose)\n",
    "        cols_to_remove = test_cols - set(self.cols_to_keep)\n",
    "        test_dset=test_dset.remove_columns(list(cols_to_remove))\n",
    "        self.verboseprint('Done')\n",
    "        \n",
    "        \n",
    "        # Content transformation and tokenization\n",
    "        print_msg('Performing Content Transformation and Tokenization on test set',verbose=self.verbose)\n",
    "        test_dset = self._do_transformation_tokenization(test_dset)\n",
    "        self.verboseprint('Done')\n",
    "        \n",
    "        return test_dset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L19){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming\n",
       "\n",
       ">      TextDataControllerStreaming (inp, main_text:str, label_names=[],\n",
       ">                                   sup_types=[], class_names_predefined=[],\n",
       ">                                   filter_dict={}, label_tfm_dict={},\n",
       ">                                   metadatas=[], process_metas=True,\n",
       ">                                   content_transformations=[],\n",
       ">                                   content_augmentations=[], seed=None,\n",
       ">                                   batch_size=1024, num_proc=1,\n",
       ">                                   cols_to_keep=None, verbose=True)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | HuggingFainpce Dataset or DatasetDict |\n",
       "| main_text | str |  | Name of the main text column |\n",
       "| label_names | list | [] | Names of the label (dependent variable) columns |\n",
       "| sup_types | list | [] | Type of supervised learning for each label name ('classification' or 'regression') |\n",
       "| class_names_predefined | list | [] | List of names associated with the labels (same index order) |\n",
       "| filter_dict | dict | {} | A dictionary: {feature: filtering_function_based_on_the_feature} |\n",
       "| label_tfm_dict | dict | {} | A dictionary: {label_name: transform_function_for_that_label} |\n",
       "| metadatas | list | [] | Names of the metadata columns |\n",
       "| process_metas | bool | True | Whether to do simple text processing on the chosen metadatas |\n",
       "| content_transformations | list | [] | A list of text transformations |\n",
       "| content_augmentations | list | [] | A list of text augmentations |\n",
       "| seed | NoneType | None | Random seed |\n",
       "| batch_size | int | 1024 | CPU batch size |\n",
       "| num_proc | int | 1 | Number of process for multiprocessing. This will be applied on non-streamed validation set |\n",
       "| cols_to_keep | NoneType | None | Columns to keep after all processings |\n",
       "| verbose | bool | True | Whether to print processing information |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L19){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming\n",
       "\n",
       ">      TextDataControllerStreaming (inp, main_text:str, label_names=[],\n",
       ">                                   sup_types=[], class_names_predefined=[],\n",
       ">                                   filter_dict={}, label_tfm_dict={},\n",
       ">                                   metadatas=[], process_metas=True,\n",
       ">                                   content_transformations=[],\n",
       ">                                   content_augmentations=[], seed=None,\n",
       ">                                   batch_size=1024, num_proc=1,\n",
       ">                                   cols_to_keep=None, verbose=True)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | HuggingFainpce Dataset or DatasetDict |\n",
       "| main_text | str |  | Name of the main text column |\n",
       "| label_names | list | [] | Names of the label (dependent variable) columns |\n",
       "| sup_types | list | [] | Type of supervised learning for each label name ('classification' or 'regression') |\n",
       "| class_names_predefined | list | [] | List of names associated with the labels (same index order) |\n",
       "| filter_dict | dict | {} | A dictionary: {feature: filtering_function_based_on_the_feature} |\n",
       "| label_tfm_dict | dict | {} | A dictionary: {label_name: transform_function_for_that_label} |\n",
       "| metadatas | list | [] | Names of the metadata columns |\n",
       "| process_metas | bool | True | Whether to do simple text processing on the chosen metadatas |\n",
       "| content_transformations | list | [] | A list of text transformations |\n",
       "| content_augmentations | list | [] | A list of text augmentations |\n",
       "| seed | NoneType | None | Random seed |\n",
       "| batch_size | int | 1024 | CPU batch size |\n",
       "| num_proc | int | 1 | Number of process for multiprocessing. This will be applied on non-streamed validation set |\n",
       "| cols_to_keep | NoneType | None | Columns to keep after all processings |\n",
       "| verbose | bool | True | Whether to print processing information |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataControllerStreaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L379){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.process_and_tokenize\n",
       "\n",
       ">      TextDataControllerStreaming.process_and_tokenize (tokenizer,\n",
       ">                                                        max_length=None,\n",
       ">                                                        tok_num_proc=None,\n",
       ">                                                        line_by_line=False)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| max_length | NoneType | None | pad to model's allowed max length (default is max_sequence_length) |\n",
       "| tok_num_proc | NoneType | None | Number of processes for tokenization |\n",
       "| line_by_line | bool | False | To whether process + tokenize each sentence separately. Faster, but no padding applied |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L379){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.process_and_tokenize\n",
       "\n",
       ">      TextDataControllerStreaming.process_and_tokenize (tokenizer,\n",
       ">                                                        max_length=None,\n",
       ">                                                        tok_num_proc=None,\n",
       ">                                                        line_by_line=False)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| max_length | NoneType | None | pad to model's allowed max length (default is max_sequence_length) |\n",
       "| tok_num_proc | NoneType | None | Number of processes for tokenization |\n",
       "| line_by_line | bool | False | To whether process + tokenize each sentence separately. Faster, but no padding applied |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataControllerStreaming.process_and_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Capability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of streaming capability of `TextDataControllerStreaming` is adapted from [HuggingFace's stream](https://huggingface.co/docs/datasets/stream)\n",
    "\n",
    "Streaming is a method to let you work with data without having it in your hard drive. This is especially helpful when the dataset size exceeds the amount of disk space you have on your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few things to be aware of when using `TextDataControllerStreaming` streaming functionality (versus `TextDataController`)\n",
    "\n",
    "- The list of label names must be available beforehand (except for regression label)\n",
    "- To avoid out-of-memory error, reduce batch_size argument.\n",
    "- There will NOT be any validation split functionality. If you want to include a validation set, provide a `validation` split in your HuggingFace DatasetDict beforehand\n",
    "- There's no upsampling, and there's no shuffling the training set\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To stream, you must provide a streamed HuggingFace dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat few examples mentioned in [this tutorial](https://anhquan0412.github.io/that-nlp-library/text_main.html), but with a streaming dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import text_normalize\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_aug_stochastic(x,aug=None,p=0.5):\n",
    "    if not isinstance(x,list): \n",
    "        if random.random()<p: return aug.augment(x)[0]\n",
    "        return x\n",
    "    news=[]\n",
    "    originals=[]\n",
    "    for _x in x:\n",
    "        if random.random()<p: news.append(_x)\n",
    "        else: originals.append(_x)\n",
    "    # only perform augmentation when needed\n",
    "    if len(news): news = aug.augment(news)\n",
    "    return news+originals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering + Metadatas + Label Transformation +  Content Transformation (for Single Head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc = TextDataControllerStreaming(ddict_with_val,\n",
    "                                  main_text='Review Text',\n",
    "                                  label_names='Department Name',\n",
    "                                  sup_types='classification',\n",
    "                                  class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trending'],\n",
    "                                  filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                              },\n",
    "                                  label_tfm_dict={'Department Name': lambda x: x if x!='Trend' else 'Trending'},\n",
    "                                  metadatas=['Title','Division Name'],\n",
    "                                  content_transformations=[text_normalize,str.lower],\n",
    "                                  process_metas=True,\n",
    "                                  batch_size=1000,\n",
    "                                  num_proc=4,\n",
    "                                  seed=42\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Data Filtering --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c713f1b0974e8d97d550f2fa64280f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/2349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e387113946054445aa343bd36cc85d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f25c94de2984d86bc79be63de409c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Label Transformation --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b564150e1487407187f3c90d18849b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4436ef14b32f42f5ad154d56d65d5d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "----- Performing Content Transformation and Tokenization on validation set -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd91c3fc8c3042a5bc0f31bd88e282f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba585cf26ee41798cd4ed6623f3aa8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2467049a324a11888b77aa740a5c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Creating a generator for content transformation, augmentation and tokenization on train set -----\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc.process_and_tokenize(tokenizer,max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4d52c8b76040ba9f7d208815b70f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9888061d7044eeaa89a2a7fcc12038d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183378ea6ba94e3184cad208d729513f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "CPU times: user 2 s, sys: 923 ms, total: 2.92 s\n",
      "Wall time: 2.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    if i%100==0:\n",
    "        print(i)\n",
    "    if i==1000-1:\n",
    "        break\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f18deb12abf401c9457984f403cc856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a75613da2a74bb98392c81b20ae8cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f50cffc993f49b2abe4c914015c274f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: general petite . beautiful top , worth the necessary tailoring . the beautiful bold print drew me to this top and it did not disappoint upon receipt . however , the bottom ruffle belled so far out on each side that it was laughable ! the actual fit is nothing like the picture ; clearly the model's arms are placed in front of all the extra fabric to hold the ruffle back . however , the fabric is beautiful , the fit was perfect ( size 2 , 5 ' 4 \" , 106 lbs . ) , the quality is great and i love the print so i decided to take it to my tailor to \" sew away \" the \" wings \" on both si\n",
      "Label: Tops => 4\n",
      "----------\n",
      "Text: general . not as short on me ( petite ) . i ordered the xxs p as this dress is not a fitted dress , and that was the right size for me . only thing is the length is a bit linger still 9 lower on calf for me ) , the straps are almost tight , so i would say the dress is a reversed taper shape . color is beautiful , i ordered green as the other color ( plum ) doesn't have petite available . green is rich , and classy , the fabric is surprisingly soft . i love the little details in the velvet . definitely need a strapless bra for this one . 115 lbsm 30 d\n",
      "Label: Dresses => 1\n",
      "----------\n",
      "Text: general . perfect .... for two wears . ok ladies .... you need to know that this type of fabric is the one that will get holes ( i bought the white one ) . it is super thin and lovely , but i was only able to get two wears out of it . i did wash it and it maintained it's size because i restretched it while wet then hung to dry . i was super disappointed about the wear but appreciated being able to return it without question at my local retailer .\n",
      "Label: Tops => 4\n",
      "----------\n",
      "Text: initmates . . i love this dress . it is so soft and comfortable , perfect for summer ! ! i wish it came in more colors because i would buy everyone ! !\n",
      "Label: Intimate => 2\n",
      "----------\n",
      "Text: general petite . adorable and excellent quality . this is such a clean and cute printed dress and i knew that i had to try the dress when i first saw it online . after reading other reviews , i sized up . i am normally a 0 or 2 in retailer . i ordered the 2 and it fits nicely and looks great . however , i feel like the 2 buttons at the lowered rib cage area gape slightly . the tie covers it and holds it in place , unless i sit , then it gapes freely . i am a 32 a so not big chested at all , and yet this fit snug in the chest area . i would worry about\n",
      "Label: Dresses => 1\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    if i==5:break\n",
    "    print(f\"Text: {v['Review Text']}\\nLabel: {v['Department Name']} => {v['label']}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: general . soft , feminine and fun pockets ! . i love this tunic . purchased the dark orange in medium ( i am 5 ' 9 and 140 lbs ) . tried the small and almost kept it but i felt seams around my arm pits a tad , so went with the medium and glad i did - this top should be comfortable . feels very fall and perfect for casual get-togethers and running around town . only comment is that it is rayon ... and for me anyway rayon doesn't wash too well - so we shall see how this one fairs .\n",
      "Label: Tops => 4\n",
      "----------\n",
      "Text: general petite . a new staple ! . tried these on out of sheer curiosity -- i've got a long torso & was pleasantly surprised how flattering they are ! they manage to look flowing & sleek without shortening the legs . took a size 6 with my 27 \" waist , 37 \" hips . it's a bit of a generous fit , especially around the waist , but they're extremely comfortable & have room to tuck tops into . i have the cowled sweater tank in gray & it looks fantastic over these ! couldn't resist getting both the rust and black . perfect for a dressy casual look\n",
      "Label: Bottoms => 0\n",
      "----------\n",
      "Text: general . maybe swing is for me ! . i love swing dresses but they never seem to work out for me . however , lately i have been trying on swing tops like this one and they are super scores ! i love this top ! in my store , they had a rack of test materials where they don't have the full line but they have a look at some online features or clothes that are very new releases . this was on the rack . i knew it wasn't my size but i tried it on anyway and i am absolutely in love . i am waiting for a sale ( as always ) but i am going to get this i\n",
      "Label: Tops => 4\n",
      "----------\n",
      "Text: general . too flare . too small ... too flare ... nice thick fabric . not my favorite pant .\n",
      "Label: Bottoms => 0\n",
      "----------\n",
      "Text: general . love . i love this top it is easy to wear fun and very comfortable . i was thinking about it for weeks and kept coming back to it after i read a review about going up a size i decided to go for it and i am very happy i did ! ! ! my new favorite ! ! !\n",
      "Label: Tops => 4\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Text: {tdc.main_ddict['validation']['Review Text'][i]}\")\n",
    "    print(f\"Label: {tdc.main_ddict['validation']['Department Name'][i]} => {tdc.main_ddict['validation']['label'][i]}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to non-streamed version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine streaming data controller with verbose=False\n",
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']\n",
    "\n",
    "tdc = TextDataControllerStreaming(ddict_with_val,\n",
    "                                  main_text='Review Text',\n",
    "                                  label_names='Department Name',\n",
    "                                  sup_types='classification',\n",
    "                                  class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trending'],\n",
    "                                  filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                              },\n",
    "                                  label_tfm_dict={'Department Name': lambda x: x if x!='Trend' else 'Trending'},\n",
    "                                  metadatas=['Title','Division Name'],\n",
    "                                  content_transformations=[text_normalize,str.lower],\n",
    "                                  process_metas=True,\n",
    "                                  batch_size=1000,\n",
    "                                  num_proc=4,\n",
    "                                  seed=42,\n",
    "                                  verbose=False\n",
    "                                 )\n",
    "\n",
    "tdc.process_and_tokenize(tokenizer,max_length=256,tok_num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from that_nlp_library.text_main import TextDataController"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset2 = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val2 = dset2.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val2['validation'] = ddict_with_val2['test']\n",
    "del ddict_with_val2['test']\n",
    "\n",
    "\n",
    "tdc2 = TextDataController(ddict_with_val2,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trending'],\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         label_tfm_dict={'Department Name': lambda x: x if x!='Trend' else 'Trending'},\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         process_metas=True,\n",
    "                         batch_size=1000,\n",
    "                         num_proc=4,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "tdc2.process_and_tokenize(tokenizer,max_length=256,shuffle_trn=False,tok_num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether train sets are the same\n",
    "assert len(list(tdc.main_ddict['train']))==len(tdc2.main_ddict['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter1 = iter(tdc.main_ddict['train'])\n",
    "iter2 = iter(tdc2.main_ddict['train'])\n",
    "for a,b in zip(iter1,iter2):\n",
    "    assert a==b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether validation set is the same\n",
    "assert len(list(tdc.main_ddict['validation']))==len(tdc2.main_ddict['validation'])\n",
    "\n",
    "iter1 = iter(tdc.main_ddict['validation'])\n",
    "iter2 = iter(tdc2.main_ddict['validation'])\n",
    "for a,b in zip(iter1,iter2):\n",
    "    assert a==b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering + Metadatas + Label Transformation +  Content Transformation + Content Augmentation (for Multi Head: Classification + Regression + Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug2 = naw.ContextualWordEmbsAug(model_path='roberta-base', \n",
    "                                device='cuda:0', # if you don't have gpu, change to 'cpu'\n",
    "                                action=\"substitute\",\n",
    "                                top_k=10,\n",
    "                               aug_p=0.07)\n",
    "\n",
    "contextual_aug_func = partial(nlp_aug_stochastic,aug=aug2,p=0.5)\n",
    "contextual_aug_func.run_on_gpu=True\n",
    "contextual_aug_func.batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1954b915a3434e84ba904d2b613c411e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Data Filtering --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21bf858178241b5a43592de84981c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d591a0e000f34bdd9c74d7969aef7929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1bc5d0057ee4d95b0d12f0d2e5fb6bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Label Transformation --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8275239245747ca92d890b7157d1dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0947a3a90354721af5e7a84b7df2874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "----- Performing Content Transformation and Tokenization on validation set -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d186b1da81d946f1a3267251c7bb07c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65620d5563f14b5eb5f2b5f434cdbde3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d333eac6592d4a288262e072c7e23a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Creating a generator for content transformation, augmentation and tokenization on train set -----\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataControllerStreaming(ddict_with_val,\n",
    "                                  main_text='Review Text',\n",
    "                                  label_names=['Division Name','Rating','Department Name'],\n",
    "                                  sup_types=['classification','regression','classification'],\n",
    "                                  class_names_predefined=[['General', 'General Petite', 'Initmates'],\n",
    "                                                          [], # empty list for regression\n",
    "                                                          ['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trending']],\n",
    "                                  filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                               'Department Name': lambda x: x is not None,\n",
    "                                              },\n",
    "                                  metadatas=['Title'],\n",
    "                                  label_tfm_dict={'Department Name': lambda x: x if x!='Trend' else 'Trending'},\n",
    "                                  content_transformations=[text_normalize,str.lower],\n",
    "                                  content_augmentations=contextual_aug_func,\n",
    "                                  process_metas=True,\n",
    "                                  batch_size=1000,\n",
    "                                  num_proc=1,\n",
    "                                  seed=42\n",
    "                                 )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ec3b770819456da4e8c4e44a450542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57fd5250b75048bda277750e762129af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a046b04cd0af4c7e8b3efe28d6f76d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87da1716933e41fba02980efe2103a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: not as short on me ( petite ). i ordered black xxs p as this model is not a fitted dress, and that was the right size for me. only thing is the length is 1 bit linger still 9 lower on calf for me ). the straps are almost tight, so i would say the dress is a reversed taper shape. color = beautiful! i ordered green as the other color ( ) ( doesn't have petite available. green is rich, and classy, the fabric is surprisingly soft. i love the little details in the front. definitely need a strapless bra for this one. 115 < 30 d\n",
      "Label: ('General Petite', 4.0, 'Tops') => [1.0, 4.0, 4.0]\n",
      "----------\n",
      "Text: perfect.... for two wears. ( ladies.... we need to know that this type of fabric is the one that will get holes when i bought the white one ). it is super thin and lovely, but i was only able to get 2 wears out of it. i did wash it and it maintained it's size because i restretched it while wet then hung to dry. i been super disappointed about the wear but appreciated being able to find it without question at my favourite retailer.\n",
      "Label: ('General', 5.0, 'Dresses') => [0.0, 5.0, 1.0]\n",
      "----------\n",
      "Text: . i love this dress. it is in soft is comfortable, perfect for summer!! i wish it came in more colors ; i would buy everyone!!\n",
      "Label: ('General', 1.0, 'Tops') => [0.0, 1.0, 4.0]\n",
      "----------\n",
      "Text: great fit and what. love this flowing a cute top! casual, but can be easily dressed up. great fit.\n",
      "Label: ('Initmates', 5.0, 'Intimate') => [2.0, 5.0, 2.0]\n",
      "----------\n",
      "Text: please bring back more!!!. love this tank! prettier color of yellow in person. love the wide width and length. wish they'd bring back more in lots of colors. only sized up one from my usual size and will have too shorten and length of the strap myself, but it's an amazing alteration and is totally worth it! especially good at sale price. i really hope they make more in the style or lots of colors. great find!\n",
      "Label: ('General Petite', 4.0, 'Dresses') => [1.0, 4.0, 1.0]\n",
      "----------\n",
      "Text: roomy and flows. comfortable and flows well, does run large one order one size smaller fuck\n",
      "Label: ('General', 5.0, 'Bottoms') => [0.0, 5.0, 0.0]\n",
      "----------\n",
      "Text: not as pictured. i just received this dress wadded around in a bag. it is nothing like the colors shown off line. in the middle it appears blue and brown. in front it is teal and a funky color of beigey pink. the top is see though but it does see have a slip under the skirt. will be sending it back and will remind myself why i shouldn't try to buy any dress on line.\n",
      "Label: ('General Petite', 2.0, 'Tops') => [1.0, 2.0, 4.0]\n",
      "----------\n",
      "Text: comfy and pretty. i bought this in red and wear it a lot. it is so soft and comfortable!! i love all the fabric and the fit. the longer sleeve length seems amazing! it is a perfect tee! you can tell from the front there's a lot a fabric and it's supposed to be smooth and drape like that. it fits like in the pics this is true to size. wear a tank underneath just case there's a strong wind.\n",
      "Label: ('General Petite', 5.0, 'Tops') => [1.0, 5.0, 4.0]\n",
      "----------\n",
      "Text: so so pretty. sunday by brooklyn has bee killing it lately with their tops! i could not decide between this and another of theirs, but i am positive i chose this one out first, the design is subtle but very cunning style : the sleeve length matches the rest of the top tier, which gives this top a very finished look. the color is a beautiful jewel-like raspberry red with a bluish undertone. and, the fabric is a nice washable texture. i wore my usual size xs at retailer.\n",
      "Label: ('Initmates', 5.0, 'Intimate') => [2.0, 5.0, 2.0]\n",
      "----------\n",
      "Text: great flares. wearing jeans, fit in to size and look as pictured. i recommended those to meet friend and she bought and liked them as well.\n",
      "Label: ('General Petite', 5.0, 'Tops') => [1.0, 5.0, 4.0]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    if i==10:break\n",
    "    print(f\"Text: {v['Review Text']}\\nLabel: {v['Division Name'],v['Rating'],v['Department Name']} => {v['label']}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: soft , feminine and fun pockets ! . i love this tunic . purchased the dark orange in medium ( i am 5 ' 9 and 140 lbs ) . tried the small and almost kept it but i felt seams around my arm pits a tad , so went with the medium and glad i did - this top should be comfortable . feels very fall and perfect for casual get-togethers and running around town . only comment is that it is rayon ... and for me anyway rayon doesn't wash too well - so we shall see how this one fairs .\n",
      "Label: ('General', 5.0, 'Tops') => [0.0, 5.0, 4.0]\n",
      "----------\n",
      "Text: a new staple ! . tried these on out of sheer curiosity -- i've got a long torso & was pleasantly surprised how flattering they are ! they manage to look flowing & sleek without shortening the legs . took a size 6 with my 27 \" waist , 37 \" hips . it's a bit of a generous fit , especially around the waist , but they're extremely comfortable & have room to tuck tops into . i have the cowled sweater tank in gray & it looks fantastic over these ! couldn't resist getting both the rust and black . perfect for a dressy casual look\n",
      "Label: ('General Petite', 5.0, 'Bottoms') => [1.0, 5.0, 0.0]\n",
      "----------\n",
      "Text: maybe swing is for me ! . i love swing dresses but they never seem to work out for me . however , lately i have been trying on swing tops like this one and they are super scores ! i love this top ! in my store , they had a rack of test materials where they don't have the full line but they have a look at some online features or clothes that are very new releases . this was on the rack . i knew it wasn't my size but i tried it on anyway and i am absolutely in love . i am waiting for a sale ( as always ) but i am going to get this i\n",
      "Label: ('General', 5.0, 'Tops') => [0.0, 5.0, 4.0]\n",
      "----------\n",
      "Text: too flare . too small ... too flare ... nice thick fabric . not my favorite pant .\n",
      "Label: ('General', 2.0, 'Bottoms') => [0.0, 2.0, 0.0]\n",
      "----------\n",
      "Text: love . i love this top it is easy to wear fun and very comfortable . i was thinking about it for weeks and kept coming back to it after i read a review about going up a size i decided to go for it and i am very happy i did ! ! ! my new favorite ! ! !\n",
      "Label: ('General', 5.0, 'Tops') => [0.0, 5.0, 4.0]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Text: {tdc.main_ddict['validation']['Review Text'][i]}\")\n",
    "    print(f\"Label: {tdc.main_ddict['validation']['Division Name'][i],tdc.main_ddict['validation']['Rating'][i],tdc.main_ddict['validation']['Department Name'][i]} => {tdc.main_ddict['validation']['label'][i]}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering + Metadatas + Content Transformation + Content Augmentation (for Multi Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug2 = naw.ContextualWordEmbsAug(model_path='roberta-base', \n",
    "                                device='cuda:0', # if you don't have gpu, change to 'cpu'\n",
    "                                action=\"substitute\",\n",
    "                                top_k=10,\n",
    "                               aug_p=0.07)\n",
    "\n",
    "contextual_aug_func = partial(nlp_aug_stochastic,aug=aug2,p=0.5)\n",
    "contextual_aug_func.run_on_gpu=True\n",
    "contextual_aug_func.batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\n",
    "df['Fake Label'] = [np.random.choice(df['Department Name'].unique()[:-1],size=np.random.randint(2,6),replace=False) for _ in range(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict_with_val = dset.train_test_split(test_size=0.1)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: IterableDataset({\n",
       "        features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name', 'Fake Label'],\n",
       "        n_shards: 1\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name', 'Fake Label'],\n",
       "        num_rows: 2349\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict_with_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Data Filtering --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d238144048e49e494f08fc1b7315402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/2349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b4712b5f7b4fb3b33754d9d85e3bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2276 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af0e5c1c7854bf2bad73823023ec8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2276 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "----- Performing Content Transformation and Tokenization on validation set -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e1424d17774ee29003e7c43f182aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2276 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc17916bf3f4d2c82d3b8ad5e04c284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2276 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc2a9f79b52456ba7ae5b9bc2acbae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2276 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Creating a generator for content transformation, augmentation and tokenization on train set -----\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataControllerStreaming(ddict_with_val,\n",
    "                                  main_text='Review Text',\n",
    "                                  label_names='Fake Label',\n",
    "                                  sup_types='classification',\n",
    "                                  class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n",
    "                                  filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                  metadatas=['Title','Division Name'],\n",
    "                                  content_transformations=[text_normalize,str.lower],\n",
    "                                  content_augmentations= contextual_aug_func, \n",
    "                                  process_metas=True,\n",
    "                                  batch_size=1000,\n",
    "                                  num_proc=4,\n",
    "                                  seed=42\n",
    "                                 )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6495d92400ae4e6fa6d759b17e69ac3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e836fcff5b4209b7defb51ef3d0441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ec329393b345888abefb6894f627fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d65b50da63b44cf82e9ccbad28de2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: general. beautiful, stunning, cozy top!. i checked the first review on here and ordered both a small and a medium as i thought this would run small. i have to totally disagree of the reviewer! i find that this top runs true on size or even generous! the sky color is so pretty and this top can be dressed up with many nice jewels and a necklace or it can be comfy casual, i usually wear a small in hh brand and this one was true to fit ( 5 \" 2 \", broad shoulders, 120 ml )\n",
      "Label: ['Jackets', 'Tops', 'Intimate'] => [0, 0, 1, 1, 1, 0]\n",
      "----------\n",
      "Text: general. love!. love love love this dress! but, and you are not wearing a slip... you should be like please wear a slip, you can see right through this dress.\"\n",
      "Label: ['Bottoms', 'Dresses'] => [1, 1, 0, 0, 0, 0]\n",
      "----------\n",
      "Text: general. runs big. i liked the idea on these pair as i've been looking for an updated pair of tuxedo pants. i wear 26 in most of their jeans. i'm not super skinny & find my legs medium ( not too skinny & not too athletic ). i tried these on in xs ( 36 european as marked on them ) & they were big around the waist & hip area. there was so much gap in the back it made them look frumpy! u did however liked the length. the material is nice & heavy which i also loved. sadly enough i didn't work for me though. really wish t\n",
      "Label: ['Bottoms', 'Tops', 'Trend', 'Dresses'] => [1, 1, 0, 0, 1, 1]\n",
      "----------\n",
      "Text: general petite. so flattering, no need for petite. i just try this top on in xs regular even : i generally wear xspetite in retailer and it fit great ( 34 aa - 35 - 34 ). i think it's flattering on any short narrow arms with the halter neck, fitted waist and peplum? i would guess it would be flattering on many body types ; it highlights shoulders beautifully. it was very hard to put my head through the small, not-too-stretchy opening so you may want to try it on without makeup. i knocked off one star, the neck band wasn't symmetrical,\n",
      "Label: ['Dresses', 'Bottoms', 'Trend', 'Tops', 'Intimate'] => [1, 1, 1, 0, 1, 1]\n",
      "----------\n",
      "Text: general. a new wardrobe staple!. love this jacket! i purchased both the green and gray versions and will have them constantly this winter! although some reviews were critical of the length of the yarn, i do not find this to be a problem... so happy with these products! for reference to size, i bought the m and am 5'11... the sweater hits me exactly where it should on the model.\n",
      "Label: ['Bottoms', 'Tops'] => [1, 0, 0, 0, 1, 0]\n",
      "----------\n",
      "Text: general. unique and adorable. the photos don't do the top justice. the split back is very unique and beautiful. i typically take a size 8 in tops, however ordered a 15 since a reviewer suggested it was narrow in the shoulders. the 12 will fit perfectly, but the body is way too big that i was swimming it in. i like it enough that i'm going to visit the tailor to take in the fit. with the sale price, i is worth tailoring.\n",
      "Label: ['Bottoms', 'Intimate', 'Dresses', 'Tops'] => [1, 1, 1, 0, 1, 0]\n",
      "----------\n",
      "Text: general. great slouchy sweater, perfect color. this sweater has the perfect slouchy coat for fall. i wish I were a little bit softer and heavier - the fabric is pretty lightweight - yet it layers beautifully and will be a highlight for me this season.\n",
      "Label: ['Intimate', 'Bottoms', 'Jackets', 'Trend', 'Tops'] => [1, 0, 1, 1, 1, 1]\n",
      "----------\n",
      "Text: general. feminine plus makeup. this top is gorgeous and versatile. i wear me with jeans and dress it up with a skirt. so happy to have this in my wardrobe.\n",
      "Label: ['Jackets', 'Bottoms'] => [1, 0, 0, 1, 0, 0]\n",
      "----------\n",
      "Text: general. great pants that don't get baggy.. e they are shit - please make them in other colors besides black and navy!\n",
      "Label: ['Trend', 'Dresses', 'Intimate'] => [0, 1, 1, 0, 0, 1]\n",
      "----------\n",
      "Text: general petite. a very blouse-like dress - very simple. this dress is very cute on. very flouncy. i know it looks like a gingham collar, but it's a more like a silk dress with a gingham collar. the one issue i had was that it stood out at the back where you tied it on any size ( large or small ), and i am larger in top, so i don't know what would happen if it with a small chest. i wouldn't be minded if it did the same thing on the front, but it was just the back.\n",
      "Label: ['Trend', 'Jackets', 'Tops', 'Bottoms'] => [1, 0, 0, 1, 1, 1]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    if i==10:break\n",
    "    print(f\"Text: {v['Review Text']}\\nLabel: {v['Fake Label']} => {v['label']}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: general petite . . this top has great detailing and color . does run a little big , but adds to the style and movement of the tank . the stitching around the bottom makes it cute for layering .\n",
      "Label: ['Dresses', 'Intimate', 'Trend', 'Tops', 'Bottoms'] => [1, 1, 1, 0, 1, 1]\n",
      "----------\n",
      "Text: general . . i love this top . i got it on sale and am so glad that i did . it is a short too but still super flattering . it isn't too boxy on me .\n",
      "Label: ['Intimate', 'Trend', 'Jackets', 'Dresses', 'Tops'] => [0, 1, 1, 1, 1, 1]\n",
      "----------\n",
      "Text: general . beautiful idea ... . i ordered my normal size in this dress . i am 6 foot tall , but the regular sizes were too large and too long ( mid-calf ) . i returned the dress for a size smaller in petite for a more flattering hemline . the dress is lovely , especially on the models in the pictures , but didn't quite work out for me . also , it feels like there are hundreds of closure hooks that make putting on / taking off the dress seem to take an unusually long time !\n",
      "Label: ['Dresses', 'Tops'] => [0, 1, 0, 0, 1, 0]\n",
      "----------\n",
      "Text: general petite . comfy , but not made to last . this sweater is fine for the casual days . i bought this in cream and i have to say after one wash it looks old . i'm a huge retailer lover and buy a lot of clothes from them . this is just not the best quality and looks tired after a few wears . very soft , but poor material . not my favorite purchase .\n",
      "Label: ['Trend', 'Jackets'] => [0, 0, 0, 1, 0, 1]\n",
      "----------\n",
      "Text: general . great cool looking jeans . i just bought these jeans today & they are really cute & comfortable on . i love pilcro jeans as they fit really well , they are made well & they are always on style . i did have to go down a size as well but they fit beautifully ! comfy & stylish !\n",
      "Label: ['Tops', 'Intimate', 'Jackets', 'Bottoms'] => [1, 0, 1, 1, 1, 0]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Text: {tdc.main_ddict['validation']['Review Text'][i]}\")\n",
    "    print(f\"Label: {tdc.main_ddict['validation']['Fake Label'][i]} => {tdc.main_ddict['validation']['label'][i]}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.label_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch process vs Line-by-line process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we are applying processing functions and tokenization on each batch (of size 1000). There's a faster way to process: we can apply these functions immediately to each row (instead of waiting for a batch to be formed). Let's discuss when to use each of these approaches\n",
    "\n",
    "1. When to use batch process over line-by-line process\n",
    "    - You have some processing functions that perform more efficient when apply to the whole batch (instead of applying line-by-line), e.g. the `contextual_aug_func` (contextual augmentation) function that utilizes a language model on GPU\n",
    "    - For tokenization, you need to apply padding (to the whole batch)\n",
    "\n",
    "\n",
    "2. When to use line-by-line process over batch process\n",
    "    - All your processing functions can perform efficient line-by-line\n",
    "    - For tokenization, you don't need padding (maybe you will do so via `DataCollatorWithPadding` during training)\n",
    "    - If you choose `DataCollatorWithPadding` to pad, be aware of there's no option to `truncate`, therefore you have to make sure the maximum length of your `token_ids` does not exceed the maximum model's sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line-by-line over batch process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']\n",
    "\n",
    "tdc = TextDataControllerStreaming(ddict_with_val,\n",
    "                                  main_text='Review Text',\n",
    "                                  label_names='Department Name',\n",
    "                                  sup_types='classification',\n",
    "                                  class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trending'],\n",
    "                                  filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                              },\n",
    "                                  label_tfm_dict={'Department Name': lambda x: x if x!='Trend' else 'Trending'},\n",
    "                                  metadatas=['Title','Division Name'],\n",
    "                                  content_transformations=[text_normalize,str.lower],\n",
    "                                  process_metas=True,\n",
    "                                  batch_size=1000,\n",
    "                                  num_proc=4,\n",
    "                                  seed=42\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, for the above data controller, two of the processing functions (`text_normalize` and `str.lower`) can perform line-by-line just fine. Therefore, we can utilize line-by-line processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Data Filtering --------------------\n",
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "Done\n",
      "-------------------- Label Transformation --------------------\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "----- Performing Content Transformation and Tokenization on validation set -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7fc636a3f4a41c19844038367c82938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Creating a generator for content transformation, augmentation and tokenization on train set -----\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc.process_and_tokenize(tokenizer,line_by_line=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "CPU times: user 2.81 s, sys: 2.67 s, total: 5.48 s\n",
      "Wall time: 5.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# time it takes to go throuch 3 batches (1000 x3)\n",
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    if i%500==0:\n",
    "        print(i)\n",
    "    if i==1000*3-1:\n",
    "        break\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 15841, 4716, 1459, 479, 2721, 299, 2156, 966, 5, 2139, 7886, 5137, 479, 5, 2721, 7457, 5780, 4855, 162, 7, 42, 299, 8, 24, 222, 45, 17534, 2115, 18245, 479, 959, 2156, 5, 2576, 910, 15315, 28, 9970, 98, 444, 66, 15, 349, 526, 14, 24, 21, 38677, 27785, 5, 3031, 2564, 16, 1085, 101, 5, 2170, 25606, 2563, 5, 1421, 18, 3701, 32, 2325, 11, 760, 9, 70, 5, 1823, 10199, 7, 946, 5, 910, 15315, 124, 479, 959, 2156, 5, 10199, 16, 2721, 2156, 5, 2564, 21, 1969, 36, 1836, 132, 2156, 195, 128, 204, 22, 2156, 13442, 23246, 479, 4839, 2156, 5, 1318, 16, 372, 8, 939, 657, 5, 5780, 98, 939, 1276, 7, 185, 24, 7, 127, 26090, 7, 22, 35043, 409, 22, 5, 22, 11954, 22, 15, 258, 3391, 2]\n",
      "Length of input_ids: 136\n",
      "----------\n",
      "[0, 15841, 479, 45, 25, 765, 15, 162, 36, 4716, 1459, 4839, 479, 939, 2740, 5, 37863, 29, 181, 25, 42, 3588, 16, 45, 10, 15898, 3588, 2156, 8, 14, 21, 5, 235, 1836, 13, 162, 479, 129, 631, 16, 5, 5933, 16, 10, 828, 18277, 202, 361, 795, 15, 16701, 13, 162, 4839, 2156, 5, 31622, 32, 818, 3229, 2156, 98, 939, 74, 224, 5, 3588, 16, 10, 13173, 326, 15888, 3989, 479, 3195, 16, 2721, 2156, 939, 2740, 2272, 25, 5, 97, 3195, 36, 36838, 4839, 630, 75, 33, 4716, 1459, 577, 479, 2272, 16, 4066, 2156, 8, 30228, 2156, 5, 10199, 16, 10262, 3793, 479, 939, 657, 5, 410, 1254, 11, 5, 29986, 479, 2299, 240, 10, 18052, 16979, 11689, 13, 42, 65, 479, 12312, 23246, 119, 389, 385, 2]\n",
      "Length of input_ids: 133\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    print(v['input_ids'])\n",
    "    print(f\"Length of input_ids: {len(v['input_ids'])}\")\n",
    "    if i==1:\n",
    "        break\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no padding at all, just by looking at the first 2 `input_ids`. Let's use `DataCollatorWithPadding` to add padding to these tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no truncation strategy! We will pad to multiple of 8, to utilize NVIDIA hardware\n",
    "data_collator = DataCollatorWithPadding(tokenizer,padding=True,pad_to_multiple_of=8)\n",
    "tdc.set_data_collator(data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ddict = tdc.main_ddict['train'].remove_columns(tdc.cols_to_keep)\n",
    "\n",
    "iter1 = iter(train_ddict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.05 s, sys: 719 ms, total: 1.77 s\n",
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "out = tdc.data_collator([next(iter1) for i in range(1000)]) # apply data collator with batch size 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 152])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 15841,  4716,  1459,   479,  2721,   299,  2156,   966,     5,\n",
       "          2139,  7886,  5137,   479,     5,  2721,  7457,  5780,  4855,   162,\n",
       "             7,    42,   299,     8,    24,   222,    45, 17534,  2115, 18245,\n",
       "           479,   959,  2156,     5,  2576,   910, 15315,    28,  9970,    98,\n",
       "           444,    66,    15,   349,   526,    14,    24,    21, 38677, 27785,\n",
       "             5,  3031,  2564,    16,  1085,   101,     5,  2170, 25606,  2563,\n",
       "             5,  1421,    18,  3701,    32,  2325,    11,   760,     9,    70,\n",
       "             5,  1823, 10199,     7,   946,     5,   910, 15315,   124,   479,\n",
       "           959,  2156,     5, 10199,    16,  2721,  2156,     5,  2564,    21,\n",
       "          1969,    36,  1836,   132,  2156,   195,   128,   204,    22,  2156,\n",
       "         13442, 23246,   479,  4839,  2156,     5,  1318,    16,   372,     8,\n",
       "           939,   657,     5,  5780,    98,   939,  1276,     7,   185,    24,\n",
       "             7,   127, 26090,     7,    22, 35043,   409,    22,     5,    22,\n",
       "         11954,    22,    15,   258,  3391,     2,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1],\n",
       "        [    0, 15841,   479,    45,    25,   765,    15,   162,    36,  4716,\n",
       "          1459,  4839,   479,   939,  2740,     5, 37863,    29,   181,    25,\n",
       "            42,  3588,    16,    45,    10, 15898,  3588,  2156,     8,    14,\n",
       "            21,     5,   235,  1836,    13,   162,   479,   129,   631,    16,\n",
       "             5,  5933,    16,    10,   828, 18277,   202,   361,   795,    15,\n",
       "         16701,    13,   162,  4839,  2156,     5, 31622,    32,   818,  3229,\n",
       "          2156,    98,   939,    74,   224,     5,  3588,    16,    10, 13173,\n",
       "           326, 15888,  3989,   479,  3195,    16,  2721,  2156,   939,  2740,\n",
       "          2272,    25,     5,    97,  3195,    36, 36838,  4839,   630,    75,\n",
       "            33,  4716,  1459,   577,   479,  2272,    16,  4066,  2156,     8,\n",
       "         30228,  2156,     5, 10199,    16, 10262,  3793,   479,   939,   657,\n",
       "             5,   410,  1254,    11,     5, 29986,   479,  2299,   240,    10,\n",
       "         18052, 16979, 11689,    13,    42,    65,   479, 12312, 23246,   119,\n",
       "           389,   385,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['input_ids'][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tokens have been padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the runtime if we use batch-processing instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Data Filtering --------------------\n",
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "Done\n",
      "-------------------- Label Transformation --------------------\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "----- Performing Content Transformation and Tokenization on validation set -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b4da9d6d5f43169b899dd1623f0f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Creating a generator for content transformation, augmentation and tokenization on train set -----\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']\n",
    "\n",
    "tdc = TextDataControllerStreaming(ddict_with_val,\n",
    "                                  main_text='Review Text',\n",
    "                                  label_names='Department Name',\n",
    "                                  sup_types='classification',\n",
    "                                  class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trending'],\n",
    "                                  filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                              },\n",
    "                                  label_tfm_dict={'Department Name': lambda x: x if x!='Trend' else 'Trending'},\n",
    "                                  metadatas=['Title','Division Name'],\n",
    "                                  content_transformations=[text_normalize,str.lower],\n",
    "                                  process_metas=True,\n",
    "                                  batch_size=1000,\n",
    "                                  num_proc=4,\n",
    "                                  seed=42\n",
    "                                 )\n",
    "\n",
    "tdc.process_and_tokenize(tokenizer,line_by_line=False,max_length=152)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a5a7e307594b92aa56b483df23751c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4fccaea0259493b9c2d162782a543d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4110eff952416c8e2227281dec635a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa9cc75b72848fc915641d58c2ceeb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21866a7948d24eaea7184a8bbd1027a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d1e803f3ab427d8be01abf3876bc07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963c845f7ce542638eac4f6d8c643458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b7a00962d741d6afadfd61b9bf095e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d71aab883c8b4caeb9c44493c7e88e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2500\n",
      "CPU times: user 5.63 s, sys: 2.67 s, total: 8.3 s\n",
      "Wall time: 8.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# time it takes to go throuch 3 batches (1000 x3)\n",
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    if i%500==0:\n",
    "        print(i)\n",
    "    if i==1000*3-1:\n",
    "        break\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took 8.28s, comparing to 5.47s from line-by-line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d3e1b8b24249e8867b090e97222ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3381d9a9be49c5905a72fa450aa38d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4649c61739f4a72a55c318bd109455a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 15841, 4716, 1459, 479, 2721, 299, 2156, 966, 5, 2139, 7886, 5137, 479, 5, 2721, 7457, 5780, 4855, 162, 7, 42, 299, 8, 24, 222, 45, 17534, 2115, 18245, 479, 959, 2156, 5, 2576, 910, 15315, 28, 9970, 98, 444, 66, 15, 349, 526, 14, 24, 21, 38677, 27785, 5, 3031, 2564, 16, 1085, 101, 5, 2170, 25606, 2563, 5, 1421, 18, 3701, 32, 2325, 11, 760, 9, 70, 5, 1823, 10199, 7, 946, 5, 910, 15315, 124, 479, 959, 2156, 5, 10199, 16, 2721, 2156, 5, 2564, 21, 1969, 36, 1836, 132, 2156, 195, 128, 204, 22, 2156, 13442, 23246, 479, 4839, 2156, 5, 1318, 16, 372, 8, 939, 657, 5, 5780, 98, 939, 1276, 7, 185, 24, 7, 127, 26090, 7, 22, 35043, 409, 22, 5, 22, 11954, 22, 15, 258, 3391, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Length of input_ids: 149\n",
      "----------\n",
      "[0, 15841, 479, 45, 25, 765, 15, 162, 36, 4716, 1459, 4839, 479, 939, 2740, 5, 37863, 29, 181, 25, 42, 3588, 16, 45, 10, 15898, 3588, 2156, 8, 14, 21, 5, 235, 1836, 13, 162, 479, 129, 631, 16, 5, 5933, 16, 10, 828, 18277, 202, 361, 795, 15, 16701, 13, 162, 4839, 2156, 5, 31622, 32, 818, 3229, 2156, 98, 939, 74, 224, 5, 3588, 16, 10, 13173, 326, 15888, 3989, 479, 3195, 16, 2721, 2156, 939, 2740, 2272, 25, 5, 97, 3195, 36, 36838, 4839, 630, 75, 33, 4716, 1459, 577, 479, 2272, 16, 4066, 2156, 8, 30228, 2156, 5, 10199, 16, 10262, 3793, 479, 939, 657, 5, 410, 1254, 11, 5, 29986, 479, 2299, 240, 10, 18052, 16979, 11689, 13, 42, 65, 479, 12312, 23246, 119, 389, 385, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Length of input_ids: 149\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    print(v['input_ids'])\n",
    "    print(f\"Length of input_ids: {len(v['input_ids'])}\")\n",
    "    if i==1:\n",
    "        break\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But at least our tokens has been padded appropriately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch-process over line-by-line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug2 = naw.ContextualWordEmbsAug(model_path='roberta-base', \n",
    "                                device='cuda:0', # if you don't have gpu, change to 'cpu'\n",
    "                                action=\"substitute\",\n",
    "                                top_k=10,\n",
    "                               aug_p=0.07)\n",
    "\n",
    "contextual_aug_func = partial(nlp_aug_stochastic,aug=aug2,p=0.5)\n",
    "contextual_aug_func.run_on_gpu=True\n",
    "contextual_aug_func.batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']\n",
    "\n",
    "tdc = TextDataControllerStreaming(ddict_with_val,\n",
    "                                  main_text='Review Text',\n",
    "                                  label_names='Department Name',\n",
    "                                  sup_types='classification',\n",
    "                                  class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trending'],\n",
    "                                  filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                              },\n",
    "                                  label_tfm_dict={'Department Name': lambda x: x if x!='Trend' else 'Trending'},\n",
    "                                  metadatas=['Title','Division Name'],\n",
    "#                                   content_transformations=[text_normalize,str.lower],\n",
    "                                  content_augmentations=[contextual_aug_func],\n",
    "                                  process_metas=True,\n",
    "                                  batch_size=1000,\n",
    "                                  num_proc=4,\n",
    "                                  seed=42\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the above data controller, there's an augmentation function `contextual_aug_func` that can utilize batch process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Data Filtering --------------------\n",
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "Done\n",
      "-------------------- Label Transformation --------------------\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "----- Performing Content Transformation and Tokenization on validation set -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54812cd01cc4117b8eea76fe4699b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Creating a generator for content transformation, augmentation and tokenization on train set -----\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc.process_and_tokenize(tokenizer,line_by_line=False,max_length=152)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7c3e208f474474854c3e72b8543166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bd7d49d2534e7298eb4512b0118337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f2c8673c6e4b6a962030f8d40c8d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5984c511c4b643bc9a55102620f27d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f054bf15ee994c3d831f9084da5ae610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52b2b04173746409174f4ce3f77cfea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2500\n",
      "CPU times: user 46.7 s, sys: 3.41 s, total: 50.1 s\n",
      "Wall time: 50.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# time it takes to go throuch 3 batches (1000 x3)\n",
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    if i%500==0:\n",
    "        print(i)\n",
    "    if i==1000*3-1:\n",
    "        break\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def493eed8dc49b0a2c70c85e76bf18e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7f58fbd666489da6b30c9563df32aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 15841, 4716, 1459, 4, 2721, 299, 6, 966, 5, 2139, 7886, 5137, 4, 20, 2721, 7457, 5780, 4855, 162, 7, 42, 1836, 8, 24, 222, 45, 17534, 2115, 18245, 4, 2223, 6, 5, 2576, 526, 28, 9970, 21, 444, 66, 11, 349, 526, 6, 24, 21, 38677, 328, 5, 3031, 2564, 21, 1085, 101, 5, 2170, 131, 2563, 5, 1421, 18, 3701, 32, 2325, 11, 760, 9, 70, 5, 1823, 10199, 7, 946, 5, 910, 15315, 124, 4, 50121, 50118, 9178, 6294, 6, 5, 10199, 16, 2721, 6, 5, 2408, 21, 1969, 36, 10799, 132, 6, 195, 108, 306, 1297, 13442, 23246, 12345, 5, 1318, 16, 372, 8, 939, 657, 5, 5780, 98, 939, 1276, 7, 492, 24, 7, 127, 26090, 7, 22, 1090, 605, 409, 113, 5, 22, 42932, 113, 15, 2185, 3391, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Length of input_ids: 152\n",
      "----------\n",
      "[0, 15841, 4, 1969, 17220, 1990, 80, 15033, 4, 5148, 10717, 17220, 6968, 33, 7, 216, 14, 42, 1907, 9, 10199, 16, 5, 65, 14, 40, 120, 6538, 36, 118, 33, 5, 1104, 65, 322, 24, 16, 2422, 11962, 8, 9869, 6, 53, 939, 21, 129, 441, 7, 120, 80, 15033, 66, 9, 24, 4, 939, 393, 10397, 24, 8, 24, 4925, 24, 18, 1836, 142, 939, 1079, 47904, 24, 683, 7727, 172, 10601, 7, 3841, 4, 939, 21, 2422, 5779, 15, 5, 3568, 53, 10874, 145, 441, 7, 671, 24, 396, 864, 23, 103, 400, 6215, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Length of input_ids: 152\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    print(v['input_ids'])\n",
    "    print(f\"Length of input_ids: {len(v['input_ids'])}\")\n",
    "    if i==1:\n",
    "        break\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this runtime to line-by-line process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Data Filtering --------------------\n",
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "Done\n",
      "-------------------- Label Transformation --------------------\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "----- Performing Content Transformation and Tokenization on validation set -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6e6e4349704b57aa431425d1dabce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Creating a generator for content transformation, augmentation and tokenization on train set -----\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']\n",
    "\n",
    "tdc = TextDataControllerStreaming(ddict_with_val,\n",
    "                                  main_text='Review Text',\n",
    "                                  label_names='Department Name',\n",
    "                                  sup_types='classification',\n",
    "                                  class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trending'],\n",
    "                                  filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                              },\n",
    "                                  label_tfm_dict={'Department Name': lambda x: x if x!='Trend' else 'Trending'},\n",
    "                                  metadatas=['Title','Division Name'],\n",
    "#                                   content_transformations=[text_normalize,str.lower],\n",
    "                                  content_augmentations=[contextual_aug_func],\n",
    "                                  process_metas=True,\n",
    "                                  batch_size=1000,\n",
    "                                  num_proc=4,\n",
    "                                  seed=42\n",
    "                                 )\n",
    "\n",
    "tdc.process_and_tokenize(tokenizer,line_by_line=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "CPU times: user 1min 4s, sys: 2.63 s, total: 1min 6s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# time it takes to go throuch 3 batches (1000 x3)\n",
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    if i%500==0:\n",
    "        print(i)\n",
    "    if i==1000*3-1:\n",
    "        break\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took 1min7s, comparing to 51s from batch-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 15841, 4716, 1459, 479, 2721, 299, 6, 966, 5, 2139, 7886, 5137, 479, 20, 2721, 7457, 5780, 4855, 162, 7, 42, 299, 8, 24, 222, 45, 17534, 2115, 18245, 4, 959, 6, 5, 2576, 910, 15315, 28, 9970, 98, 444, 66, 15, 349, 526, 14, 24, 21, 38677, 328, 5, 3031, 2564, 16, 1085, 101, 5, 2170, 131, 2563, 5, 1421, 18, 3701, 32, 2325, 11, 760, 9, 70, 5, 1823, 10199, 7, 946, 5, 910, 15315, 124, 4, 50121, 50118, 9178, 6294, 6, 5, 10199, 16, 2721, 6, 5, 2564, 21, 1969, 36, 10799, 132, 6, 195, 108, 306, 1297, 13442, 23246, 12345, 5, 1318, 16, 372, 8, 939, 657, 5, 5780, 98, 939, 1276, 7, 185, 24, 7, 127, 26090, 7, 22, 1090, 605, 409, 113, 5, 22, 42932, 113, 15, 258, 3391, 2]\n",
      "Length of input_ids: 137\n",
      "----------\n",
      "[0, 15841, 479, 45, 25, 765, 15, 162, 36, 13713, 1459, 43, 479, 38, 2740, 5, 37863, 29, 181, 25, 42, 3588, 16, 45, 10, 15898, 3588, 6, 8, 14, 21, 5, 235, 1836, 13, 162, 4, 129, 631, 16, 5, 5933, 16, 10, 828, 18277, 202, 361, 29668, 15, 16701, 13, 162, 238, 5, 31622, 32, 818, 3229, 6, 98, 939, 74, 224, 5, 3588, 16, 10, 13173, 326, 15888, 3989, 4, 3195, 16, 2721, 6, 939, 2740, 2272, 25, 5, 97, 3195, 36, 2911, 783, 43, 630, 75, 33, 4716, 1459, 577, 4, 2272, 16, 4066, 6, 8, 30228, 6, 5, 10199, 16, 10262, 3793, 4, 939, 657, 5, 410, 1254, 11, 5, 29986, 4, 2299, 240, 10, 18052, 16979, 11689, 13, 42, 65, 4, 50121, 50118, 50121, 50118, 15314, 23246, 119, 389, 417, 2]\n",
      "Length of input_ids: 137\n",
      "----------\n",
      "[0, 15841, 479, 1969, 17220, 1990, 80, 15033, 479, 5148, 10717, 17220, 6968, 240, 7, 216, 14, 42, 1907, 9, 10199, 16, 5, 65, 14, 40, 120, 6538, 36, 118, 2162, 5, 1104, 65, 322, 24, 16, 2422, 7174, 8, 9869, 6, 53, 939, 21, 129, 441, 7, 120, 80, 15033, 66, 9, 24, 4, 939, 222, 10397, 24, 8, 24, 4925, 24, 18, 1836, 142, 939, 1079, 47904, 24, 150, 7727, 172, 10601, 7, 3841, 4, 939, 21, 2422, 5779, 59, 5, 3568, 53, 10874, 145, 441, 7, 671, 24, 396, 864, 23, 127, 400, 6215, 4, 2]\n",
      "Length of input_ids: 99\n",
      "----------\n",
      "[0, 25153, 11139, 328, 479, 38, 657, 42, 3588, 4, 24, 1299, 98, 3793, 8, 3473, 6, 1969, 13, 1035, 12846, 939, 2813, 24, 376, 11, 80, 8089, 142, 939, 74, 907, 961, 12846, 2]\n",
      "Length of input_ids: 35\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    print(v['input_ids'])\n",
    "    print(f\"Length of input_ids: {len(v['input_ids'])}\")\n",
    "    if i==3:\n",
    "        break\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load TextDataControllerStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L133){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.save_as_pickles\n",
       "\n",
       ">      TextDataControllerStreaming.save_as_pickles (fname,\n",
       ">                                                   parent='pickle_files',\n",
       ">                                                   drop_attributes=False)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |\n",
       "| drop_attributes | bool | False | Whether to drop large-size attributes |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L133){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.save_as_pickles\n",
       "\n",
       ">      TextDataControllerStreaming.save_as_pickles (fname,\n",
       ">                                                   parent='pickle_files',\n",
       ">                                                   drop_attributes=False)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |\n",
       "| drop_attributes | bool | False | Whether to drop large-size attributes |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataControllerStreaming.save_as_pickles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L94){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.from_pickle\n",
       "\n",
       ">      TextDataControllerStreaming.from_pickle (fname, parent='pickle_files')\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L94){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.from_pickle\n",
       "\n",
       ">      TextDataControllerStreaming.from_pickle (fname, parent='pickle_files')\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataControllerStreaming.from_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextDataControllerStreaming object can be saved and loaded with ease. This is especially useful after text processing and/or tokenization have been done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import disable_caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_caching() # disable huggingface caching to see data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import text_normalize\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_aug_stochastic(x,aug=None,p=0.5):\n",
    "    if not isinstance(x,list): \n",
    "        if random.random()<p: return aug.augment(x)[0]\n",
    "        return x\n",
    "    news=[]\n",
    "    originals=[]\n",
    "    for _x in x:\n",
    "        if random.random()<p: news.append(_x)\n",
    "        else: originals.append(_x)\n",
    "    # only perform augmentation when needed\n",
    "    if len(news): news = aug.augment(news)\n",
    "    return news+originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug2 = naw.ContextualWordEmbsAug(model_path='roberta-base', \n",
    "                                device='cuda:0', # if you don't have gpu, change to 'cpu'\n",
    "                                action=\"substitute\",\n",
    "                                top_k=10,\n",
    "                               aug_p=0.07)\n",
    "\n",
    "contextual_aug_func = partial(nlp_aug_stochastic,aug=aug2,p=0.1)\n",
    "contextual_aug_func.run_on_gpu=True\n",
    "contextual_aug_func.batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Data Filtering --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667447adc309421daafe0db3e2fda567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/4698 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a90442a04c5408eba455a269329474b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/4530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f468b8e5b04aa28bc3d0cdc610095e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfadcf47bb2b402f8c7d11e5a1b40831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "----- Performing Content Transformation and Tokenization on validation set -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478f792953c84770b94b5c3f9b45c756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1143c05c344b87bee30688d90108e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091ebfe6f21c44b7b97aee4aa5a9c006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Creating a generator for content transformation, augmentation and tokenization on train set -----\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.2)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']\n",
    "\n",
    "tdc = TextDataControllerStreaming(ddict_with_val,\n",
    "                                  main_text='Review Text',\n",
    "                                  label_names='Department Name',\n",
    "                                  sup_types='classification',\n",
    "                                  class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n",
    "                                  filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                               'Department Name': lambda x: x is not None,\n",
    "                                              },\n",
    "                                  metadatas=['Title','Division Name'],\n",
    "                                  content_transformations=[text_normalize,str.lower],\n",
    "                                  content_augmentations= contextual_aug_func,\n",
    "                                  process_metas=True,\n",
    "                                  batch_size=100,\n",
    "                                  num_proc=4,\n",
    "                                  seed=42\n",
    "                                 )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: IterableDataset({\n",
       "        features: Unknown,\n",
       "        n_shards: 3\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Title', 'Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4529\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.save_as_pickles('my_tdc_stream')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Size in MegaBytes is 479.022\n"
     ]
    }
   ],
   "source": [
    "file_stats = os.stat(Path('pickle_files/my_tdc_stream.pkl'))\n",
    "print(f'File Size in MegaBytes is {round(file_stats.st_size / (1024 * 1024), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load back our object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc2 = TextDataControllerStreaming.from_pickle('my_tdc_stream')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can still access all its attributes, data, preprocessings, transformation/augmentation ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: IterableDataset({\n",
       "        features: Unknown,\n",
       "        n_shards: 3\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Title', 'Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4529\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc2.main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8483b19cf164adc851a7c2476a9805f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702028b1c40349898dec9dfe70de5132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146c336eca17469dba97d6a9091d7bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f3fe2d1c5e4d858f47d9da904379e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: general. very cute top but runs large. i ordered this top in all 3 colors, love the peplum.look.. returned the green shade which is actually my fav color but it wasn't a pretty shade or green and i don't think that it was the flattering contrast with the cream,, especially going into spring / summer.. kept the yellow and the navy and love.. my navy is light and the yellow is soft.. classic. it's a very cute top, runs a size large, but is very pretty.\n",
      "Label: Jackets => 3\n",
      "----------\n",
      "Text: general petite. regular dress : i just love this dress. took me ordering in number of sizes - petite and regular to find the best fit. i might still have the top altered. i received a lot of compliements, work it with some black leather straps for daytime / work.\n",
      "Label: Dresses => 1\n",
      "----------\n",
      "Text: general. not for the busty on the fabric, very versatile but the knit style and style accentuates my bust. probably not an issue for most but if your a d or up it's more attention than you may want.\n",
      "Label: Tops => 4\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(tdc2.main_ddict['train']):\n",
    "    if i==3:break\n",
    "    print(f\"Text: {v['Review Text']}\\nLabel: {v['Department Name']} => {v['label']}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc2.label_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Review Text': <function __main__.<lambda>(x)>,\n",
       "  'Department Name': <function __main__.<lambda>(x)>},\n",
       " [<function underthesea.pipeline.text_normalize.text_normalize(text, tokenizer='underthesea')>,\n",
       "  <method 'lower' of 'str' objects>],\n",
       " [functools.partial(<function nlp_aug_stochastic>, aug=<nlpaug.augmenter.word.context_word_embs.ContextualWordEmbsAug object>, p=0.1)])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc2.filter_dict,tdc2.content_tfms,tdc2.aug_tfms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't want to store the HuggingFace DatasetDict in your `TextDataControllerStreaming`, or the augmentation functions (typically when you already have a trained model, and you only use `TextDataControllerStreaming` to preprocess the test set), you can remove it in the `save_as_pickles` step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.save_as_pickles('my_lightweight_tdc_stream',drop_attributes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Size in MegaBytes is 1.908\n"
     ]
    }
   ],
   "source": [
    "file_stats = os.stat(Path('pickle_files/my_lightweight_tdc_stream.pkl'))\n",
    "print(f'File Size in MegaBytes is {round(file_stats.st_size / (1024 * 1024), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc3 = TextDataControllerStreaming.from_pickle('my_lightweight_tdc_stream')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this object to demonstrate the Test Set Construction in the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L483){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.prepare_test_dataset\n",
       "\n",
       ">      TextDataControllerStreaming.prepare_test_dataset (test_dset,\n",
       ">                                                        do_filtering=False)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| test_dset |  |  | The HuggingFace Dataset as Test set |\n",
       "| do_filtering | bool | False | whether to perform data filtering on this test set |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L483){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.prepare_test_dataset\n",
       "\n",
       ">      TextDataControllerStreaming.prepare_test_dataset (test_dset,\n",
       ">                                                        do_filtering=False)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| test_dset |  |  | The HuggingFace Dataset as Test set |\n",
       "| do_filtering | bool | False | whether to perform data filtering on this test set |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataControllerStreaming.prepare_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L440){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.prepare_test_dataset_from_csv\n",
       "\n",
       ">      TextDataControllerStreaming.prepare_test_dataset_from_csv (file_path,\n",
       ">                                                                 do_filtering=F\n",
       ">                                                                 alse)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| file_path |  |  | path to csv file |\n",
       "| do_filtering | bool | False | whether to perform data filtering on this test set |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L440){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.prepare_test_dataset_from_csv\n",
       "\n",
       ">      TextDataControllerStreaming.prepare_test_dataset_from_csv (file_path,\n",
       ">                                                                 do_filtering=F\n",
       ">                                                                 alse)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| file_path |  |  | path to csv file |\n",
       "| do_filtering | bool | False | whether to perform data filtering on this test set |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataControllerStreaming.prepare_test_dataset_from_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L450){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.prepare_test_dataset_from_df\n",
       "\n",
       ">      TextDataControllerStreaming.prepare_test_dataset_from_df (df,\n",
       ">                                                                validate=True, \n",
       ">                                                                do_filtering=Fa\n",
       ">                                                                lse)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df |  |  | Pandas Dataframe |\n",
       "| validate | bool | True | whether to perform input data validation |\n",
       "| do_filtering | bool | False | whether to perform data filtering on this test set |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L450){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.prepare_test_dataset_from_df\n",
       "\n",
       ">      TextDataControllerStreaming.prepare_test_dataset_from_df (df,\n",
       ">                                                                validate=True, \n",
       ">                                                                do_filtering=Fa\n",
       ">                                                                lse)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df |  |  | Pandas Dataframe |\n",
       "| validate | bool | True | whether to perform input data validation |\n",
       "| do_filtering | bool | False | whether to perform data filtering on this test set |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataControllerStreaming.prepare_test_dataset_from_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L460){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.prepare_test_dataset_from_raws\n",
       "\n",
       ">      TextDataControllerStreaming.prepare_test_dataset_from_raws (content)\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| content | Either a single sentence, list of sentence or a dictionary with keys are metadata columns and values are list |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L460){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.prepare_test_dataset_from_raws\n",
       "\n",
       ">      TextDataControllerStreaming.prepare_test_dataset_from_raws (content)\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| content | Either a single sentence, list of sentence or a dictionary with keys are metadata columns and values are list |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataControllerStreaming.prepare_test_dataset_from_raws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you have done your preprocessing and tokenization in your training set, and have a nicely trained model, ready to do inference on new data. Here is how you can use `TextDataControllerStreaming` to apply all the necessary preprocessings to your new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reuse the lightweight tdc object we created in the previous section (since we don't really need all the training data just to construct new data). Also, we will take a small sample of our training data and pretend it is our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc = TextDataControllerStreaming.from_pickle('my_lightweight_tdc_stream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4692, 10)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig').sample(frac=0.2,random_state=1)\n",
    "# drop NaN values in the label column\n",
    "df_test = df_test[~df_test['Department Name'].isna()].reset_index(drop=True)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>872</td>\n",
       "      <td>42</td>\n",
       "      <td>Perfect for work and play</td>\n",
       "      <td>This shirt works for both going out and going ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1033</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I don't know why i had the opposite problem mo...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Jeans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1037</td>\n",
       "      <td>45</td>\n",
       "      <td>Great pants</td>\n",
       "      <td>These cords are great--lightweight for fl wint...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Jeans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>829</td>\n",
       "      <td>35</td>\n",
       "      <td>Surprisingly comfy for a button down</td>\n",
       "      <td>I am a 10 m and got the 10. it fits perfectly ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>872</td>\n",
       "      <td>29</td>\n",
       "      <td>Short and small</td>\n",
       "      <td>The shirt is mostly a thick sweatshirt materia...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clothing ID  Age                                 Title  \\\n",
       "0          872   42             Perfect for work and play   \n",
       "1         1033   40                                   NaN   \n",
       "2         1037   45                           Great pants   \n",
       "3          829   35  Surprisingly comfy for a button down   \n",
       "4          872   29                       Short and small   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND  \\\n",
       "0  This shirt works for both going out and going ...       5                1   \n",
       "1  I don't know why i had the opposite problem mo...       4                1   \n",
       "2  These cords are great--lightweight for fl wint...       5                1   \n",
       "3  I am a 10 m and got the 10. it fits perfectly ...       5                1   \n",
       "4  The shirt is mostly a thick sweatshirt materia...       3                0   \n",
       "\n",
       "   Positive Feedback Count   Division Name Department Name Class Name  \n",
       "0                        0         General            Tops      Knits  \n",
       "1                        0  General Petite         Bottoms      Jeans  \n",
       "2                        1  General Petite         Bottoms      Jeans  \n",
       "3                        1  General Petite            Tops    Blouses  \n",
       "4                       15  General Petite            Tops      Knits  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Input Validation Precheck -\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title          758\n",
      "Review Text    164\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 2 rows\n",
      "-------------------- Start Test Set Transformation --------------------\n",
      "-------------------- Data Filtering --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6d5960a90341a7954443b9c8dc8da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/4692 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e66d774c51f647df8b64fc730a342fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "209e640d35ae4c30abd717448966917f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "----- Performing Content Transformation and Tokenization on test set -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51801aeade64dc78653272117ed4586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f88f0646c544469eb6a1b9e8ec8e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3db231a0ba420ea41c4d196f531482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "test_dset = tdc.prepare_test_dataset_from_df(df_test,validate=True,do_filtering=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: general . perfect for work and play . this shirt works for both going out and going to work , and i can wear it with everything . fits perfect , tucked and untucked , tied and untied . i love it .\n",
      "Input_ids: [0, 15841, 479, 1969, 13, 173, 8, 310, 479, 42, 6399, 1364, 13, 258, 164, 66, 8, 164, 7, 173, 2156, 8, 939, 64, 3568, 24, 19, 960, 479, 10698, 1969, 2156, 21222, 8, 7587, 23289, 2156, 3016, 8, 7587, 2550, 479, 939, 657, 24, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "----------\n",
      "Text: general petite . . i don't know why i had the opposite problem most reviewers had with these ..... i tried on the regular length in the store and found that they were just a bit too short with heels . ( i'm 5 ' 5 ) . i had them ordered in a petite and when they came , they were too short with flats ! maybe it's the way i like to wear them , i like my flare jeans to barely skim the ground . i just exchanged them for regular length and will wear them with a small wedge shoe . aside from the length issues , these are super cute\n",
      "Input_ids: [0, 15841, 4716, 1459, 479, 479, 939, 218, 75, 216, 596, 939, 56, 5, 5483, 936, 144, 34910, 56, 19, 209, 29942, 734, 939, 1381, 15, 5, 1675, 5933, 11, 5, 1400, 8, 303, 14, 51, 58, 95, 10, 828, 350, 765, 19, 8872, 479, 36, 939, 437, 195, 128, 195, 4839, 479, 939, 56, 106, 2740, 11, 10, 4716, 1459, 8, 77, 51, 376, 2156, 51, 58, 350, 765, 19, 20250, 27785, 2085, 24, 18, 5, 169, 939, 101, 7, 3568, 106, 2156, 939, 101, 127, 24186, 10844, 7, 6254, 28772, 5, 1255, 479, 939, 95, 11024, 106, 13, 1675, 5933, 8, 40, 3568, 106, 19, 10, 650, 27288, 12604, 479, 4364, 31, 5, 5933, 743, 2156, 209, 32, 2422, 11962, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "----------\n",
      "Text: general petite . great pants . thes e cords are great--lightweight for fl winters , and the bootcut flare bottom is super cute with ballet flats or booties . i am 5 ' 10 \" and typically a size 8 ; the size 29 fit perfectly . they have a little stretch to them , which is great . very flattering--wish i could order in more colors ! !\n",
      "Input_ids: [0, 15841, 4716, 1459, 479, 372, 9304, 479, 5, 29, 364, 37687, 32, 372, 5579, 6991, 4301, 13, 2342, 31000, 2156, 8, 5, 9759, 8267, 24186, 2576, 16, 2422, 11962, 19, 22573, 20250, 50, 9759, 918, 479, 939, 524, 195, 128, 158, 22, 8, 3700, 10, 1836, 290, 25606, 5, 1836, 1132, 2564, 6683, 479, 51, 33, 10, 410, 4140, 7, 106, 2156, 61, 16, 372, 479, 182, 34203, 5579, 605, 1173, 939, 115, 645, 11, 55, 8089, 27785, 27785, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Text: {test_dset['Review Text'][i]}\")\n",
    "    print(f\"Input_ids: {test_dset['input_ids'][i]}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make our test data streamed as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dset_raw = Dataset.from_pandas(df_test).to_iterable_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test dataset might have some NaN values in the text field (`Review Text`), thus we will turn on the filtering option to get rid of these NaNs, as this is what we did in the training set. If your test dataset don't need any filtering, turn off this option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Test Set Transformation --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "----- Performing Content Transformation and Tokenization on test set -----\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "test_dset = tdc.prepare_test_dataset(test_dset_raw,do_filtering=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: general . perfect for work and play . this shirt works for both going out and going to work , and i can wear it with everything . fits perfect , tucked and untucked , tied and untied . i love it .\\Input_ids: [0, 15841, 479, 1969, 13, 173, 8, 310, 479, 42, 6399, 1364, 13, 258, 164, 66, 8, 164, 7, 173, 2156, 8, 939, 64, 3568, 24, 19, 960, 479, 10698, 1969, 2156, 21222, 8, 7587, 23289, 2156, 3016, 8, 7587, 2550, 479, 939, 657, 24, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----------\n",
      "Text: general petite . . i don't know why i had the opposite problem most reviewers had with these ..... i tried on the regular length in the store and found that they were just a bit too short with heels . ( i'm 5 ' 5 ) . i had them ordered in a petite and when they came , they were too short with flats ! maybe it's the way i like to wear them , i like my flare jeans to barely skim the ground . i just exchanged them for regular length and will wear them with a small wedge shoe . aside from the length issues , these are super cute\\Input_ids: [0, 15841, 4716, 1459, 479, 479, 939, 218, 75, 216, 596, 939, 56, 5, 5483, 936, 144, 34910, 56, 19, 209, 29942, 734, 939, 1381, 15, 5, 1675, 5933, 11, 5, 1400, 8, 303, 14, 51, 58, 95, 10, 828, 350, 765, 19, 8872, 479, 36, 939, 437, 195, 128, 195, 4839, 479, 939, 56, 106, 2740, 11, 10, 4716, 1459, 8, 77, 51, 376, 2156, 51, 58, 350, 765, 19, 20250, 27785, 2085, 24, 18, 5, 169, 939, 101, 7, 3568, 106, 2156, 939, 101, 127, 24186, 10844, 7, 6254, 28772, 5, 1255, 479, 939, 95, 11024, 106, 13, 1675, 5933, 8, 40, 3568, 106, 19, 10, 650, 27288, 12604, 479, 4364, 31, 5, 5933, 743, 2156, 209, 32, 2422, 11962, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----------\n",
      "Text: general petite . great pants . thes e cords are great--lightweight for fl winters , and the bootcut flare bottom is super cute with ballet flats or booties . i am 5 ' 10 \" and typically a size 8 ; the size 29 fit perfectly . they have a little stretch to them , which is great . very flattering--wish i could order in more colors ! !\\Input_ids: [0, 15841, 4716, 1459, 479, 372, 9304, 479, 5, 29, 364, 37687, 32, 372, 5579, 6991, 4301, 13, 2342, 31000, 2156, 8, 5, 9759, 8267, 24186, 2576, 16, 2422, 11962, 19, 22573, 20250, 50, 9759, 918, 479, 939, 524, 195, 128, 158, 22, 8, 3700, 10, 1836, 290, 25606, 5, 1836, 1132, 2564, 6683, 479, 51, 33, 10, 410, 4140, 7, 106, 2156, 61, 16, 372, 479, 182, 34203, 5579, 605, 1173, 939, 115, 645, 11, 55, 8089, 27785, 27785, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(test_dset):\n",
    "    if i==3:break\n",
    "    print(f\"Text: {v['Review Text']}\\Input_ids: {v['input_ids']}\\nAttention mask: {v['attention_mask']}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
