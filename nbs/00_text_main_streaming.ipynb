{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Main Streaming\n",
    "\n",
    "> This module contains the main Python class for the **streaming** version of `TextDataController`\n",
    "\n",
    "- skip_showdoc: true\n",
    "- skip_exec: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp text_main_streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from datasets import DatasetDict,Dataset,IterableDataset,load_dataset,Value\n",
    "from pathlib import Path\n",
    "from that_nlp_library.utils import *\n",
    "from that_nlp_library.text_main import tokenize_function,concat_metadatas\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "from datasets.utils.logging import disable_progress_bar, enable_progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from that_nlp_library.text_transformation import *\n",
    "from that_nlp_library.text_augmentation import *\n",
    "from importlib.machinery import SourceFileLoader\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class TextDataControllerStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TextDataControllerStreaming():\n",
    "    def __init__(self,\n",
    "                 inp, # HuggingFainpce Dataset or DatasetDict\n",
    "                 main_text:str, # Name of the main text column\n",
    "                 label_names=[], # Names of the label (dependent variable) columns\n",
    "                 sup_types=[], # Type of supervised learning for each label name ('classification' or 'regression')\n",
    "                 class_names_predefined=[], # List of names associated with the labels (same index order)\n",
    "                 filter_dict={}, # A dictionary: {feature: filtering_function_based_on_the_feature}\n",
    "                 label_tfm_dict={}, # A dictionary: {label_name: transform_function_for_that_label}\n",
    "                 metadatas=[], # Names of the metadata columns\n",
    "                 process_metas=True, # Whether to do simple text processing on the chosen metadatas\n",
    "                 content_transformations=[], # A list of text transformations\n",
    "                 content_augmentations=[], # A list of text augmentations\n",
    "                 seed=None, # Random seed\n",
    "                 batch_size=1024, # CPU batch size\n",
    "                 num_proc=1, # Number of process for multiprocessing. This will be applied on non-streamed validation set\n",
    "                 cols_to_keep=None, # Columns to keep after all processings\n",
    "                 verbose=True, # Whether to print processing information\n",
    "                ):\n",
    "            \n",
    "        self.main_text = main_text\n",
    "        \n",
    "        self.label_names = val2iterable(label_names)\n",
    "        self.sup_types = val2iterable(sup_types)\n",
    "        self._check_sup_types()\n",
    "        self.label_lists = class_names_predefined\n",
    "        \n",
    "        self.filter_dict = filter_dict\n",
    "        self.label_tfm_dict = label_tfm_dict\n",
    "        self.metadatas = val2iterable(metadatas)\n",
    "        self.process_metas = process_metas\n",
    "\n",
    "        self.content_tfms = val2iterable(content_transformations)\n",
    "        self.aug_tfms = val2iterable(content_augmentations)\n",
    "        \n",
    "        self.seed = seed\n",
    "        self.is_batched = batch_size>1\n",
    "        self.batch_size = batch_size\n",
    "        self.num_proc = num_proc\n",
    "        self.cols_to_keep = cols_to_keep\n",
    "\n",
    "        self.main_ddict=DatasetDict()\n",
    "        self.verbose = verbose\n",
    "        self.verboseprint = print if verbose else lambda *a, **k: None\n",
    "        if not self.verbose:\n",
    "            disable_progress_bar() # turn off huggingface `map` progress bar\n",
    "        else:\n",
    "            enable_progress_bar()\n",
    "            \n",
    "        if hasattr(inp,'keys'): # is datasetdict\n",
    "            if 'train' not in inp.keys(): \n",
    "                raise ValueError('The given DatasetDict has no \"train\" split')\n",
    "            else:\n",
    "                self.main_ddict['train'] = inp['train']\n",
    "            val_key = list(set(inp.keys()) & set(['val','validation','valid']))\n",
    "            if len(val_key)>1: raise ValueError('Your DatasetDict has more than 1 validation split')\n",
    "            if len(val_key)==1:\n",
    "                self.main_ddict['validation'] = inp[val_key[0]]\n",
    "        else: # is dataset\n",
    "            self.main_ddict['train'] = inp\n",
    "          \n",
    "        is_streamed=isinstance(self.main_ddict['train'],IterableDataset)\n",
    "        if not is_streamed: raise Exception('This Text Data Controller only handles streamed dataset')\n",
    "        \n",
    "        self.all_cols = get_dset_col_names(self.main_ddict['train'])\n",
    "        \n",
    "        if is_streamed and ('classification' in self.sup_types) and len(self.label_lists)==0:\n",
    "            raise ValueError('All classification labels must be provided when streaming')\n",
    "            \n",
    "        self._determine_multihead_multilabel()\n",
    "        self._convert_regression_to_float()\n",
    "        self._processed_call=False\n",
    "        \n",
    "            \n",
    "    @classmethod\n",
    "    def from_pickle(cls,\n",
    "                    fname, # Name of the pickle file\n",
    "                    parent='pickle_files' # Parent folder\n",
    "                   ):\n",
    "        return load_pickle(fname,parent=parent)\n",
    "    \n",
    "    def set_verbose(self,verbose):\n",
    "        self.verbose = verbose\n",
    "        self.verboseprint = print if verbose else lambda *a, **k: None\n",
    "    \n",
    "    def _convert_regression_to_float(self):\n",
    "        if len(self.sup_types)==0: return\n",
    "        # convert regression labels to float64\n",
    "        reg_idxs = [i for i,v in enumerate(self.sup_types) if v=='regression']\n",
    "        for i in reg_idxs:\n",
    "            self.main_ddict['train'] = self.main_ddict['train'].cast_column(self.label_names[i],Value(\"float64\"))\n",
    "            if 'validation' in self.main_ddict.keys():\n",
    "                self.main_ddict['validation'] = self.main_ddict['validation'].cast_column(self.label_names[i],Value(\"float64\"))\n",
    "        \n",
    "    def _check_sup_types(self):\n",
    "        assert len(self.label_names)==len(self.sup_types), \"The number of supervised learning declaration must equal to the number of label\"\n",
    "        assert len(set(self.sup_types) - set(['classification','regression']))==0, 'Accepted inputs for `sup_types` are `classification` and `regression`'\n",
    "        \n",
    "    def _determine_multihead_multilabel(self):\n",
    "        self.is_multilabel=False\n",
    "        self.is_multihead=False\n",
    "        if len(self.label_names)==0: return\n",
    "        \n",
    "        if len(self.label_names)>1:\n",
    "            self.is_multihead=True\n",
    "        # get label of first row\n",
    "        first_label = next(iter(self.main_ddict['train']))[self.label_names[0]]\n",
    "        if isinstance(first_label,(list,set,tuple)):\n",
    "            # This is multi-label. Ignore self.label_names[1:]\n",
    "            self.label_names = [self.label_names[0]]\n",
    "            self.is_multihead=False\n",
    "            self.is_multilabel=True\n",
    "                     \n",
    "    \n",
    "    def save_as_pickles(self,\n",
    "                        fname, # Name of the pickle file\n",
    "                        parent='pickle_files', # Parent folder\n",
    "                        drop_attributes=False # Whether to drop large-size attributes\n",
    "                       ):\n",
    "        if drop_attributes:\n",
    "            if hasattr(self, 'main_ddict'):\n",
    "                del self.main_ddict\n",
    "            if hasattr(self, 'ddict_rest'):\n",
    "                del self.ddict_rest\n",
    "            if hasattr(self, 'aug_tfms'):\n",
    "                del self.aug_tfms\n",
    "        save_to_pickle(self,fname,parent=parent)\n",
    "    \n",
    "    def _process_metadatas(self,dtrain):\n",
    "        if len(self.metadatas):\n",
    "            map_func = partial(concat_metadatas,\n",
    "                               main_text=self.main_text,\n",
    "                               metadatas=self.metadatas,\n",
    "                               process_metas=self.process_metas,\n",
    "                               is_batched=self.is_batched)\n",
    "            dtrain = hf_map_dset(dtrain,map_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "        return dtrain\n",
    "    \n",
    "    def _do_label_transformation(self):\n",
    "        if len(self.label_names)==0 or len(self.label_tfm_dict)==0: return\n",
    "        print_msg('Label Transformation',20,verbose=self.verbose)\n",
    "        for f,tfm in self.label_tfm_dict.items():\n",
    "            if f in self.label_names:\n",
    "                _func = partial(lambda_map_batch,\n",
    "                                feature=f,\n",
    "                                func=tfm,\n",
    "                                is_batched=self.is_batched\n",
    "                               )                \n",
    "                self.main_ddict['train'] = hf_map_dset(self.main_ddict['train'],_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "                if 'validation' in self.main_ddict.keys():\n",
    "                    self.main_ddict['validation'] = hf_map_dset(self.main_ddict['validation'],\n",
    "                                                                _func,\n",
    "                                                                self.is_batched,\n",
    "                                                                self.batch_size,\n",
    "                                                                self.num_proc)\n",
    "        self.verboseprint('Done')\n",
    "                    \n",
    "                      \n",
    "                \n",
    "    def _create_label_mapping_func(self,encoder_classes):\n",
    "        if self.is_multihead:\n",
    "            label2idxs = [{v:i for i,v in enumerate(l_classes)} for l_classes in encoder_classes]\n",
    "            _func = lambda inp: {'label': [[label2idxs[i][v] if len(label2idxs[i]) else v for i,v in enumerate(vs)] \\\n",
    "                                           for vs in zip(*[inp[l] for l in self.label_names])] if self.is_batched \\\n",
    "                                 else [label2idxs[i][v] if len(label2idxs[i]) else v for i,v in enumerate([inp[l] for l in self.label_names])]\n",
    "                                }\n",
    "            \n",
    "        else: # single-head\n",
    "            if self.sup_types[0]=='regression':\n",
    "                _func1 = lambda x: x\n",
    "            else:\n",
    "                label2idx = {v:i for i,v in enumerate(encoder_classes[0])}\n",
    "                _func1 = lambda x: label2idx[x]\n",
    "                \n",
    "            _func = partial(lambda_map_batch,\n",
    "                           feature=self.label_names[0],\n",
    "                           func=_func1,\n",
    "                           output_feature='label',\n",
    "                           is_batched=self.is_batched)\n",
    "        return _func\n",
    "        \n",
    "    def _encode_labels(self):\n",
    "        if len(self.label_names)==0: return\n",
    "        print_msg('Label Encoding',verbose=self.verbose)\n",
    "        \n",
    "        if len(self.label_lists) and not isinstance(self.label_lists[0],list):\n",
    "            self.label_lists = [self.label_lists]\n",
    "                    \n",
    "        encoder_classes=[]\n",
    "        if not self.is_multilabel:\n",
    "            for idx,l in enumerate(self.label_names):\n",
    "                if self.sup_types[idx]=='regression':\n",
    "                    l_classes=[]\n",
    "                else: # classification\n",
    "                    l_classes = sorted(list(self.label_lists[idx]))\n",
    "                encoder_classes.append(l_classes)\n",
    "                \n",
    "            _func = self._create_label_mapping_func(encoder_classes)\n",
    "            \n",
    "            self.main_ddict['train'] = hf_map_dset(self.main_ddict['train'],_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "            if 'validation' in self.main_ddict.keys():\n",
    "                self.main_ddict['validation'] = hf_map_dset(self.main_ddict['validation'],_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "                    \n",
    "        else:\n",
    "            # For MultiLabel, we transform the label itself to one-hot (or actually, few-hot)\n",
    "            l_classes = sorted(list(self.label_lists[0]))   \n",
    "            encoder_classes.append(l_classes)\n",
    "            \n",
    "            l_encoder = MultiLabelBinarizer(classes=encoder_classes[0])\n",
    "            _ = l_encoder.fit(None)\n",
    "            _func = partial(lambda_map_batch,\n",
    "                            feature=self.label_names[0],\n",
    "                            func=lambda x: l_encoder.transform(x),\n",
    "                            output_feature='label',\n",
    "                            is_batched=self.is_batched,\n",
    "                            is_func_batched=True)\n",
    "            self.main_ddict['train'] = hf_map_dset(self.main_ddict['train'],_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "            if 'validation' in self.main_ddict.keys():\n",
    "                self.main_ddict['validation'] = hf_map_dset(self.main_ddict['validation'],_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "            \n",
    "        self.label_lists = encoder_classes\n",
    "        self.verboseprint('Done')\n",
    "        \n",
    "            \n",
    "            \n",
    "    def _simplify_ddict(self):\n",
    "        print_msg('Dropping unused features',20,verbose=self.verbose)\n",
    "        if self.cols_to_keep is None:\n",
    "            self.cols_to_keep= [self.main_text] + self.metadatas + self.label_names\n",
    "        cols_to_remove = set(self.all_cols) - set(self.cols_to_keep)\n",
    "        self.main_ddict['train']=self.main_ddict['train'].remove_columns(list(cols_to_remove))\n",
    "        if 'validation' in self.main_ddict.keys():\n",
    "            self.main_ddict['validation']=self.main_ddict['validation'].remove_columns(list(cols_to_remove))\n",
    "        self.verboseprint('Done')\n",
    "\n",
    "    def _do_filtering(self,dtrain):\n",
    "        if len(self.filter_dict):\n",
    "            col_names = get_dset_col_names(dtrain)\n",
    "            for f,tfm in self.filter_dict.items():\n",
    "                if f in col_names:\n",
    "                    _func = partial(lambda_batch,\n",
    "                                    feature=f,\n",
    "                                    func=tfm,\n",
    "                                    is_batched=self.is_batched)\n",
    "                    dtrain = hf_filter_dset(dtrain,_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "        return dtrain\n",
    "        \n",
    "\n",
    "    def _do_transformation_tokenization(self,dtrain):\n",
    "        tok_func = partial(tokenize_function,tok=self.tokenizer,max_length=self.max_length)\n",
    "        if len(self.content_tfms):            \n",
    "            for tfm in self.content_tfms:\n",
    "                _func = partial(lambda_map_batch,\n",
    "                                feature=self.main_text,\n",
    "                                func=tfm,\n",
    "                                is_batched=self.is_batched)\n",
    "                dtrain = hf_map_dset(dtrain,_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "        \n",
    "        _func = partial(lambda_map_batch,\n",
    "                        feature=self.main_text,\n",
    "                        func=tok_func,\n",
    "                        output_feature=None,\n",
    "                        is_batched=self.is_batched)\n",
    "        dtrain = hf_map_dset(dtrain,_func,self.is_batched,self.batch_size,self.tok_num_proc)\n",
    "            \n",
    "        return dtrain \n",
    "\n",
    "    def _do_transformation_augmentation_tokenization(self,dtrain,tok_func,all_tfms):\n",
    "        if self.seed:\n",
    "            seed_everything(self.seed)  \n",
    "            \n",
    "        # Content transformation + augmentation\n",
    "        for tfm in all_tfms:\n",
    "            bs = self.batch_size\n",
    "            is_func_batched=False\n",
    "            num_proc=1 # high num_proc is not beneficial with each batch size (which is only around 1k)\n",
    "            is_batched = self.is_batched\n",
    "            if hasattr(tfm, \"run_on_gpu\") and getattr(tfm,'run_on_gpu')==True:\n",
    "                bs = min(32,self.batch_size) if not hasattr(tfm, \"batch_size\") else getattr(tfm,'batch_size')\n",
    "                is_func_batched=True\n",
    "                is_batched=True\n",
    "\n",
    "            _func = partial(lambda_map_batch,\n",
    "                            feature=self.main_text,\n",
    "                            func=tfm,\n",
    "                            is_batched=is_batched,\n",
    "                            is_func_batched=is_func_batched\n",
    "                            )\n",
    "            dtrain = hf_map_dset(dtrain,_func,\n",
    "                                 is_batched=is_batched,\n",
    "                                 batch_size=bs,\n",
    "                                 num_proc=num_proc\n",
    "                                )\n",
    "        # Tokenization\n",
    "        _func = partial(lambda_map_batch,\n",
    "                        feature=self.main_text,\n",
    "                        func=tok_func,\n",
    "                        output_feature=None,\n",
    "                        is_batched=self.is_batched)\n",
    "        dtrain = hf_map_dset(dtrain,_func,self.is_batched,self.batch_size,num_proc)\n",
    "            \n",
    "        return dtrain\n",
    "    \n",
    "    def _construct_generator_with_batch(self,dset,tok_func,all_tfms):        \n",
    "        def _get_generator(dset):\n",
    "            for v in dset: yield v\n",
    "            \n",
    "        final_dict = defaultdict(list)\n",
    "        for inp in dset: # dset is generator\n",
    "            # inp[text_name] will be a single item\n",
    "            for k,v in inp.items():\n",
    "                final_dict[k].append(v)\n",
    "            \n",
    "            if len(final_dict[self.main_text])==self.batch_size:\n",
    "                # a full batch (self.batch_size) is created\n",
    "                dtrain = Dataset.from_dict(final_dict)\n",
    "                dtrain = self._do_transformation_augmentation_tokenization(dtrain,tok_func,all_tfms)\n",
    "                yield from _get_generator(dtrain)\n",
    "                final_dict=defaultdict(list)            \n",
    "            \n",
    "        if len(final_dict[self.main_text]):\n",
    "            # hasn't reached batch_size (of last batch)\n",
    "            dtrain = Dataset.from_dict(final_dict)\n",
    "            dtrain = self._do_transformation_augmentation_tokenization(dtrain,tok_func,all_tfms)\n",
    "            yield from _get_generator(dtrain)\n",
    "            \n",
    "        \n",
    "            \n",
    "    def _do_transformation_augmentation_tokenization_generator(self):\n",
    "        tok_func = partial(tokenize_function,tok=self.tokenizer,max_length=self.max_length)\n",
    "        all_tfms = self.content_tfms + self.aug_tfms\n",
    "        if self.seed:\n",
    "            seed_everything(self.seed)\n",
    "        \n",
    "        self.main_ddict['train'] = IterableDataset.from_generator(self._construct_generator_with_batch,\n",
    "                                                   gen_kwargs={'dset': self.main_ddict['train'],\n",
    "                                                               'tok_func':tok_func,\n",
    "                                                               'all_tfms': all_tfms\n",
    "                                                              }\n",
    "                                                                 )\n",
    "\n",
    "        \n",
    "    def process_and_tokenize(self,\n",
    "                             tokenizer, # Tokenizer (preferably from HuggingFace)\n",
    "                             max_length=None, # pad to model's allowed max length (default is max_sequence_length)\n",
    "                             tok_num_proc=None, # Number of processes for tokenization\n",
    "                            ):\n",
    "        if self._processed_call:\n",
    "            warnings.warn('Your dataset has already been processed. Returning the previous processed DatasetDict...')\n",
    "            return self.main_ddict\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.tok_num_proc = tok_num_proc if tok_num_proc else self.num_proc\n",
    "        \n",
    "        # Filtering\n",
    "        print_msg('Data Filtering',20,verbose=self.verbose)\n",
    "        for k in self.main_ddict.keys():   \n",
    "            self.main_ddict[k] = self._do_filtering(self.main_ddict[k])\n",
    "        self.verboseprint('Done')\n",
    "\n",
    "        \n",
    "        # Process metadatas\n",
    "        print_msg('Metadata Simple Processing & Concatenating to Main Content',verbose=self.verbose)\n",
    "        for k in self.main_ddict.keys():   \n",
    "            self.main_ddict[k] = self._process_metadatas(self.main_ddict[k])\n",
    "        self.verboseprint('Done')\n",
    "        \n",
    "        # Label transformation\n",
    "        self._do_label_transformation()\n",
    "        \n",
    "        # Process labels\n",
    "        self._encode_labels()\n",
    "\n",
    "        # Dropping unused columns\n",
    "        self._simplify_ddict()\n",
    "\n",
    "        \n",
    "        # Content transformation + tokenization for validation\n",
    "        if 'validation' in self.main_ddict.keys():\n",
    "            print_msg('Performing Content Transformation and Tokenization on validation set',verbose=self.verbose)\n",
    "            self.main_ddict['validation'] = self._do_transformation_tokenization(self.main_ddict['validation'])\n",
    "            self.verboseprint('Done')\n",
    " \n",
    "        # Content transformation + augmentation + tokenization for train\n",
    "        print_msg('Creating a generator for content transformation, augmentation and tokenization on train set',verbose=self.verbose)\n",
    "        self._do_transformation_augmentation_tokenization_generator()\n",
    "        self.verboseprint('Done')\n",
    "        \n",
    "        self._processed_call=True\n",
    "    \n",
    "        \n",
    "    \n",
    "    def set_data_collator(self,data_collator):\n",
    "        self.data_collator = data_collator\n",
    "        \n",
    "    \n",
    "    def prepare_test_dataset_from_csv(self,\n",
    "                                      file_path, # path to csv file\n",
    "                                      do_filtering=False # whether to perform data filtering on this test set\n",
    "                                     ):\n",
    "        file_path = Path(file_path)\n",
    "        ds = load_dataset(str(file_path.parent),\n",
    "                          data_files=file_path.name,\n",
    "                          split='train')\n",
    "        return self.prepare_test_dataset(ds,do_filtering)\n",
    "    \n",
    "    def prepare_test_dataset_from_df(self,\n",
    "                                     df, # Pandas Dataframe\n",
    "                                     validate=True, # whether to perform input data validation\n",
    "                                     do_filtering=False # whether to perform data filtering on this test set \n",
    "                                    ):\n",
    "        if validate:\n",
    "            check_input_validation(df)\n",
    "        ds = Dataset.from_pandas(df)\n",
    "        return self.prepare_test_dataset(ds,do_filtering)\n",
    "    \n",
    "    def prepare_test_dataset_from_raws(self,\n",
    "                                       content, # Either a single sentence, list of sentence or a dictionary with keys are metadata columns and values are list\n",
    "                                      ):\n",
    "        if len(self.metadatas)!=0 and not isinstance(content,dict):\n",
    "            raise ValueError(f'There is/are metadatas in the preprocessing step. Please include a dictionary including these keys for metadatas: {self.metadatas}, and texture content: {self.main_text}')\n",
    "            \n",
    "        _dic = {self.main_text:[content]} if isinstance(content,str) else content\n",
    "        for k in _dic.keys():\n",
    "            _dic[k] = val2iterable(_dic[k])\n",
    "        \n",
    "        test_dict = Dataset.from_dict(_dic)\n",
    "        \n",
    "        # set num_proc to 1 for small data processing\n",
    "        _tmp1 = self.num_proc\n",
    "        _tmp2 = self.tok_num_proc\n",
    "        self.num_proc=1\n",
    "        self_tok_num_proc=1\n",
    "        results = self.prepare_test_dataset(test_dict,do_filtering=False)\n",
    "        self.num_proc = _tmp1\n",
    "        self.tok_num_proc=_tmp2\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def prepare_test_dataset(self,\n",
    "                             test_dset, # The HuggingFace Dataset as Test set\n",
    "                             do_filtering=False, # whether to perform data filtering on this test set\n",
    "                            ):\n",
    "        test_cols = set(get_dset_col_names(test_dset))\n",
    "        label_names_set = set(self.label_names)\n",
    "        test_cols = test_cols - label_names_set\n",
    "        missing_cols = set(self.cols_to_keep) - label_names_set - test_cols\n",
    "        if len(missing_cols):\n",
    "            raise ValueError(f'Test set does not have these columns required for preprocessings: {missing_cols}')\n",
    "            \n",
    "        print_msg('Start Test Set Transformation',20,verbose=self.verbose)\n",
    "\n",
    "        # Filtering\n",
    "        if do_filtering:\n",
    "            print_msg('Data Filtering',20,verbose=self.verbose)\n",
    "            test_dset = self._do_filtering(test_dset)\n",
    "            self.verboseprint('Done')\n",
    "        \n",
    "        # Process metadatas\n",
    "        print_msg('Metadata Simple Processing & Concatenating to Main Content',verbose=self.verbose)    \n",
    "        test_dset = self._process_metadatas(test_dset)\n",
    "        self.verboseprint('Done')\n",
    "        \n",
    "        # Drop unused columns\n",
    "        print_msg('Dropping unused features',20,verbose=self.verbose)\n",
    "        cols_to_remove = test_cols - set(self.cols_to_keep)\n",
    "        test_dset=test_dset.remove_columns(list(cols_to_remove))\n",
    "        self.verboseprint('Done')\n",
    "        \n",
    "        \n",
    "        # Content transformation and tokenization\n",
    "        print_msg('Performing Content Transformation and Tokenization on test set',verbose=self.verbose)\n",
    "        test_dset = self._do_transformation_tokenization(test_dset)\n",
    "        self.verboseprint('Done')\n",
    "        \n",
    "        return test_dset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L18){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming\n",
       "\n",
       ">      TextDataControllerStreaming (inp, main_text:str, label_names=[],\n",
       ">                                   sup_types=[], class_names_predefined=[],\n",
       ">                                   filter_dict={}, label_tfm_dict={},\n",
       ">                                   metadatas=[], process_metas=True,\n",
       ">                                   content_transformations=[],\n",
       ">                                   content_augmentations=[], seed=None,\n",
       ">                                   batch_size=1024, num_proc=1,\n",
       ">                                   cols_to_keep=None, verbose=True)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | HuggingFainpce Dataset or DatasetDict |\n",
       "| main_text | str |  | Name of the main text column |\n",
       "| label_names | list | [] | Names of the label (dependent variable) columns |\n",
       "| sup_types | list | [] | Type of supervised learning for each label name ('classification' or 'regression') |\n",
       "| class_names_predefined | list | [] | List of names associated with the labels (same index order) |\n",
       "| filter_dict | dict | {} | A dictionary: {feature: filtering_function_based_on_the_feature} |\n",
       "| label_tfm_dict | dict | {} | A dictionary: {label_name: transform_function_for_that_label} |\n",
       "| metadatas | list | [] | Names of the metadata columns |\n",
       "| process_metas | bool | True | Whether to do simple text processing on the chosen metadatas |\n",
       "| content_transformations | list | [] | A list of text transformations |\n",
       "| content_augmentations | list | [] | A list of text augmentations |\n",
       "| seed | NoneType | None | Random seed |\n",
       "| batch_size | int | 1024 | CPU batch size |\n",
       "| num_proc | int | 1 | Number of process for multiprocessing. This will be applied on non-streamed validation set |\n",
       "| cols_to_keep | NoneType | None | Columns to keep after all processings |\n",
       "| verbose | bool | True | Whether to print processing information |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L18){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming\n",
       "\n",
       ">      TextDataControllerStreaming (inp, main_text:str, label_names=[],\n",
       ">                                   sup_types=[], class_names_predefined=[],\n",
       ">                                   filter_dict={}, label_tfm_dict={},\n",
       ">                                   metadatas=[], process_metas=True,\n",
       ">                                   content_transformations=[],\n",
       ">                                   content_augmentations=[], seed=None,\n",
       ">                                   batch_size=1024, num_proc=1,\n",
       ">                                   cols_to_keep=None, verbose=True)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | HuggingFainpce Dataset or DatasetDict |\n",
       "| main_text | str |  | Name of the main text column |\n",
       "| label_names | list | [] | Names of the label (dependent variable) columns |\n",
       "| sup_types | list | [] | Type of supervised learning for each label name ('classification' or 'regression') |\n",
       "| class_names_predefined | list | [] | List of names associated with the labels (same index order) |\n",
       "| filter_dict | dict | {} | A dictionary: {feature: filtering_function_based_on_the_feature} |\n",
       "| label_tfm_dict | dict | {} | A dictionary: {label_name: transform_function_for_that_label} |\n",
       "| metadatas | list | [] | Names of the metadata columns |\n",
       "| process_metas | bool | True | Whether to do simple text processing on the chosen metadatas |\n",
       "| content_transformations | list | [] | A list of text transformations |\n",
       "| content_augmentations | list | [] | A list of text augmentations |\n",
       "| seed | NoneType | None | Random seed |\n",
       "| batch_size | int | 1024 | CPU batch size |\n",
       "| num_proc | int | 1 | Number of process for multiprocessing. This will be applied on non-streamed validation set |\n",
       "| cols_to_keep | NoneType | None | Columns to keep after all processings |\n",
       "| verbose | bool | True | Whether to print processing information |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataControllerStreaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L357){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.process_and_tokenize\n",
       "\n",
       ">      TextDataControllerStreaming.process_and_tokenize (tokenizer,\n",
       ">                                                        max_length=None,\n",
       ">                                                        tok_num_proc=None)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| max_length | NoneType | None | pad to model's allowed max length (default is max_sequence_length) |\n",
       "| tok_num_proc | NoneType | None | Number of processes for tokenization |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L357){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.process_and_tokenize\n",
       "\n",
       ">      TextDataControllerStreaming.process_and_tokenize (tokenizer,\n",
       ">                                                        max_length=None,\n",
       ">                                                        tok_num_proc=None)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| max_length | NoneType | None | pad to model's allowed max length (default is max_sequence_length) |\n",
       "| tok_num_proc | NoneType | None | Number of processes for tokenization |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataControllerStreaming.process_and_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Capability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of streaming capability of `TextDataControllerStreaming` is adapted from [HuggingFace's stream](https://huggingface.co/docs/datasets/stream)\n",
    "\n",
    "Streaming is a method to let you work with data without having it in your hard drive. This is especially helpful when the dataset size exceeds the amount of disk space you have on your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few things to be aware of when using `TextDataControllerStreaming` streaming functionality (versus `TextDataController`)\n",
    "\n",
    "- The list of label names must be available beforehand (except for regression label)\n",
    "- To avoid out-of-memory error, reduce batch_size argument.\n",
    "- There will NOT be any validation split functionality. If you want to include a validation set, provide a `validation` split in your HuggingFace DatasetDict beforehand\n",
    "- There's no upsampling, and there's no shuffling the training set\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To stream, you must provide a streamed HuggingFace dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat few examples mentioned in [this tutorial](https://anhquan0412.github.io/that-nlp-library/text_main.html), but with a streaming dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import text_normalize\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_aug_stochastic(x,aug=None,p=0.5):\n",
    "    if not isinstance(x,list): \n",
    "        if random.random()<p: return aug.augment(x)[0]\n",
    "        return x\n",
    "    news=[]\n",
    "    originals=[]\n",
    "    for _x in x:\n",
    "        if random.random()<p: news.append(_x)\n",
    "        else: originals.append(_x)\n",
    "    # only perform augmentation when needed\n",
    "    if len(news): news = aug.augment(news)\n",
    "    return news+originals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering + Metadatas + Label Transformation +  Content Transformation (for Single Head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc = TextDataControllerStreaming(ddict_with_val,\n",
    "                                  main_text='Review Text',\n",
    "                                  label_names='Department Name',\n",
    "                                  sup_types='classification',\n",
    "                                  class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trending'],\n",
    "                                  filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                              },\n",
    "                                  label_tfm_dict={'Department Name': lambda x: x if x!='Trend' else 'Trending'},\n",
    "                                  metadatas=['Title','Division Name'],\n",
    "                                  content_transformations=[text_normalize,str.lower],\n",
    "                                  process_metas=True,\n",
    "                                  batch_size=1000,\n",
    "                                  num_proc=4,\n",
    "                                  seed=42\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Data Filtering --------------------\n",
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "Done\n",
      "-------------------- Label Transformation --------------------\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "----- Performing Content Transformation and Tokenization on validation set -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a975e858e54adcb46340caba130f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c76865d2da47febc31c97aa8d42f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60553f5a1b204dc199f366ab7b6c3f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Creating a generator for content transformation, augmentation and tokenization on train set -----\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc.process_and_tokenize(tokenizer,max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38b50a7b430413c8af9e6a4731341c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d182c3a6ea084b46801111ae4ccd8e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cccbafcecad64f1b867ab0e9002262cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "CPU times: user 1.97 s, sys: 425 ms, total: 2.4 s\n",
      "Wall time: 2.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    if i%100==0:\n",
    "        print(i)\n",
    "    if i==1000-1:\n",
    "        break\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fc67f41e6e49d3898d8592b690ce4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336c47ff22e441e995319caf9ccd2291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc4bfd09c954f299f17a43885851c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: general petite . beautiful top , worth the necessary tailoring . the beautiful bold print drew me to this top and it did not disappoint upon receipt . however , the bottom ruffle belled so far out on each side that it was laughable ! the actual fit is nothing like the picture ; clearly the model's arms are placed in front of all the extra fabric to hold the ruffle back . however , the fabric is beautiful , the fit was perfect ( size 2 , 5 ' 4 \" , 106 lbs . ) , the quality is great and i love the print so i decided to take it to my tailor to \" sew away \" the \" wings \" on both si\n",
      "Label: Tops => 4\n",
      "----------\n",
      "Text: general . not as short on me ( petite ) . i ordered the xxs p as this dress is not a fitted dress , and that was the right size for me . only thing is the length is a bit linger still 9 lower on calf for me ) , the straps are almost tight , so i would say the dress is a reversed taper shape . color is beautiful , i ordered green as the other color ( plum ) doesn't have petite available . green is rich , and classy , the fabric is surprisingly soft . i love the little details in the velvet . definitely need a strapless bra for this one . 115 lbsm 30 d\n",
      "Label: Dresses => 1\n",
      "----------\n",
      "Text: general . perfect .... for two wears . ok ladies .... you need to know that this type of fabric is the one that will get holes ( i bought the white one ) . it is super thin and lovely , but i was only able to get two wears out of it . i did wash it and it maintained it's size because i restretched it while wet then hung to dry . i was super disappointed about the wear but appreciated being able to return it without question at my local retailer .\n",
      "Label: Tops => 4\n",
      "----------\n",
      "Text: initmates . . i love this dress . it is so soft and comfortable , perfect for summer ! ! i wish it came in more colors because i would buy everyone ! !\n",
      "Label: Intimate => 2\n",
      "----------\n",
      "Text: general petite . adorable and excellent quality . this is such a clean and cute printed dress and i knew that i had to try the dress when i first saw it online . after reading other reviews , i sized up . i am normally a 0 or 2 in retailer . i ordered the 2 and it fits nicely and looks great . however , i feel like the 2 buttons at the lowered rib cage area gape slightly . the tie covers it and holds it in place , unless i sit , then it gapes freely . i am a 32 a so not big chested at all , and yet this fit snug in the chest area . i would worry about\n",
      "Label: Dresses => 1\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    if i==5:break\n",
    "    print(f\"Text: {v['Review Text']}\\nLabel: {v['Department Name']} => {v['label']}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: general . soft , feminine and fun pockets ! . i love this tunic . purchased the dark orange in medium ( i am 5 ' 9 and 140 lbs ) . tried the small and almost kept it but i felt seams around my arm pits a tad , so went with the medium and glad i did - this top should be comfortable . feels very fall and perfect for casual get-togethers and running around town . only comment is that it is rayon ... and for me anyway rayon doesn't wash too well - so we shall see how this one fairs .\n",
      "Label: Tops => 4\n",
      "----------\n",
      "Text: general petite . a new staple ! . tried these on out of sheer curiosity -- i've got a long torso & was pleasantly surprised how flattering they are ! they manage to look flowing & sleek without shortening the legs . took a size 6 with my 27 \" waist , 37 \" hips . it's a bit of a generous fit , especially around the waist , but they're extremely comfortable & have room to tuck tops into . i have the cowled sweater tank in gray & it looks fantastic over these ! couldn't resist getting both the rust and black . perfect for a dressy casual look\n",
      "Label: Bottoms => 0\n",
      "----------\n",
      "Text: general . maybe swing is for me ! . i love swing dresses but they never seem to work out for me . however , lately i have been trying on swing tops like this one and they are super scores ! i love this top ! in my store , they had a rack of test materials where they don't have the full line but they have a look at some online features or clothes that are very new releases . this was on the rack . i knew it wasn't my size but i tried it on anyway and i am absolutely in love . i am waiting for a sale ( as always ) but i am going to get this i\n",
      "Label: Tops => 4\n",
      "----------\n",
      "Text: general . too flare . too small ... too flare ... nice thick fabric . not my favorite pant .\n",
      "Label: Bottoms => 0\n",
      "----------\n",
      "Text: general . love . i love this top it is easy to wear fun and very comfortable . i was thinking about it for weeks and kept coming back to it after i read a review about going up a size i decided to go for it and i am very happy i did ! ! ! my new favorite ! ! !\n",
      "Label: Tops => 4\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Text: {tdc.main_ddict['validation']['Review Text'][i]}\")\n",
    "    print(f\"Label: {tdc.main_ddict['validation']['Department Name'][i]} => {tdc.main_ddict['validation']['label'][i]}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to non-streamed version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine streaming data controller with verbose=False\n",
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']\n",
    "\n",
    "tdc = TextDataControllerStreaming(ddict_with_val,\n",
    "                                  main_text='Review Text',\n",
    "                                  label_names='Department Name',\n",
    "                                  sup_types='classification',\n",
    "                                  class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trending'],\n",
    "                                  filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                              },\n",
    "                                  label_tfm_dict={'Department Name': lambda x: x if x!='Trend' else 'Trending'},\n",
    "                                  metadatas=['Title','Division Name'],\n",
    "                                  content_transformations=[text_normalize,str.lower],\n",
    "                                  process_metas=True,\n",
    "                                  batch_size=1000,\n",
    "                                  num_proc=4,\n",
    "                                  seed=42,\n",
    "                                  verbose=False\n",
    "                                 )\n",
    "\n",
    "tdc.process_and_tokenize(tokenizer,max_length=256,tok_num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from that_nlp_library.text_main import TextDataController"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset2 = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val2 = dset2.train_test_split(test_size=0.1,seed=42)\n",
    "ddict_with_val2['validation'] = ddict_with_val2['test']\n",
    "del ddict_with_val2['test']\n",
    "\n",
    "\n",
    "tdc2 = TextDataController(ddict_with_val2,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trending'],\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         label_tfm_dict={'Department Name': lambda x: x if x!='Trend' else 'Trending'},\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         process_metas=True,\n",
    "                         batch_size=1000,\n",
    "                         num_proc=4,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "tdc2.process_and_tokenize(tokenizer,max_length=256,shuffle_trn=False,tok_num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether train sets are the same\n",
    "assert len(list(tdc.main_ddict['train']))==len(tdc2.main_ddict['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter1 = iter(tdc.main_ddict['train'])\n",
    "iter2 = iter(tdc2.main_ddict['train'])\n",
    "for a,b in zip(iter1,iter2):\n",
    "    assert a==b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether validation set is the same\n",
    "assert len(list(tdc.main_ddict['validation']))==len(tdc2.main_ddict['validation'])\n",
    "\n",
    "iter1 = iter(tdc.main_ddict['validation'])\n",
    "iter2 = iter(tdc2.main_ddict['validation'])\n",
    "for a,b in zip(iter1,iter2):\n",
    "    assert a==b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering + Metadatas + Label Transformation +  Content Transformation + Content Augmentation (for Multi Head: Classification + Regression + Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug2 = naw.ContextualWordEmbsAug(model_path='roberta-base', \n",
    "                                device='cuda:0', # if you don't have gpu, change to 'cpu'\n",
    "                                action=\"substitute\",\n",
    "                                top_k=10,\n",
    "                               aug_p=0.07)\n",
    "\n",
    "contextual_aug_func = partial(nlp_aug_stochastic,aug=aug2,p=0.5)\n",
    "contextual_aug_func.run_on_gpu=True\n",
    "contextual_aug_func.batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Data Filtering --------------------\n",
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "Done\n",
      "-------------------- Label Transformation --------------------\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "----- Performing Content Transformation and Tokenization on validation set -----\n",
      "Done\n",
      "----- Creating a generator for content transformation, augmentation and tokenization on train set -----\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataControllerStreaming(ddict_with_val,\n",
    "                                  main_text='Review Text',\n",
    "                                  label_names=['Division Name','Rating','Department Name'],\n",
    "                                  sup_types=['classification','regression','classification'],\n",
    "                                  class_names_predefined=[['General', 'General Petite', 'Initmates'],\n",
    "                                                          [], # empty list for regression\n",
    "                                                          ['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trending']],\n",
    "                                  filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                               'Department Name': lambda x: x is not None,\n",
    "                                              },\n",
    "                                  metadatas=['Title'],\n",
    "                                  label_tfm_dict={'Department Name': lambda x: x if x!='Trend' else 'Trending'},\n",
    "                                  content_transformations=[text_normalize,str.lower],\n",
    "                                  content_augmentations=contextual_aug_func,\n",
    "                                  process_metas=True,\n",
    "                                  batch_size=1000,\n",
    "                                  num_proc=1,\n",
    "                                  seed=42\n",
    "                                 )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: super soft but can make you. frumpy piece dress is silk? it's soft and pretty. it's blue so you can wear brown sterling blue suede boots! i caught my reflection a thousand times and thought i looked really bottom heavy. dress is awesome. worth it.\n",
      "Label: ('General Petite', 5.0, 'Bottoms') => [1.0, 5.0, 0.0]\n",
      "----------\n",
      "Text: . this is a soft, comfortable shirt. it fits like a t-shirt should. i usually buy it but based on other reviews i purchased s. the fit on me in size small and similar to the image below the photo.\n",
      "Label: ('General Petite', 5.0, 'Dresses') => [1.0, 5.0, 1.0]\n",
      "----------\n",
      "Text: this top is versatile.. its really a pretty tee, but with the cute pattern that gives a pop of color, it should have a little more room to go out look. i thought it might be a bit wide by the picture scale but has a nice flattering flow. a bit expensive at full price but an amazing score on sale.\n",
      "Label: ('General Petite', 5.0, 'Tops') => [1.0, 5.0, 4.0]\n",
      "----------\n",
      "Text: cute top, poor fit. this is a cute top, the back is problematic. it runs small on top especially around the bust and sleeves but then kinda flares below the waist in some billowy yet unflattering way with way too much fabric. i got between an xsp and an xxsp. the xxsp fit well on bottom but i could hardly pull my arms through the sleeves. both are going back unfortunately. for reference i'm 5'103 lbs 32 b.\n",
      "Label: ('General', 5.0, 'Tops') => [0.0, 5.0, 4.0]\n",
      "----------\n",
      "Text: . love this dress!! will go from fall to winter all so comfy. got it in medium.. it fits great.\n",
      "Label: ('General', 3.0, 'Tops') => [0.0, 3.0, 4.0]\n",
      "----------\n",
      "Text: ugly!. too much material everywhere. i am 122 s 5'n \" and i ordered an xs as usual, he was overwhelmed in a blanket of knitted material of muted colors. nice green slip, though. e will return it asap. this is super-ugly.\n",
      "Label: ('General', 3.0, 'Tops') => [0.0, 3.0, 4.0]\n",
      "----------\n",
      "Text: underwhelming. fabric felt and looked cheap. hem. completely frayed, all the way around, perhaps purposely, but as ass, all the same.\n",
      "Label: ('Initmates', 3.0, 'Intimate') => [2.0, 3.0, 2.0]\n",
      "----------\n",
      "Text: my favorite buy!. \" love, love, love the jumpsuit. it's fun n flirty, and fabulous! every time i wear it, i get nothing but great compliments!\n",
      "Label: ('General', 3.0, 'Tops') => [0.0, 3.0, 4.0]\n",
      "----------\n",
      "Text: too short!. this is such a cute design, but it's so short! it barely hits at my ass. you'd have to wear a layer underneath it to keep your tummy without being exposed. i'm also not thrilled with the buttons - i am in 34 c bra size and the second button was gaping out on the size 4 top. i wish they'd been sewn in - the neckline is so big, it's not obvious the buttons need to function, also got the burgundy with pink flowers and the colors are great. too bad it's so short and gapes open at the bust. going back.\n",
      "Label: ('General', 5.0, 'Dresses') => [0.0, 5.0, 1.0]\n",
      "----------\n",
      "Text: had potential. has a mannequin this looked adorable. it is made well but doesnt you have any bustline it it billows out below like a tent and is not flattering. if it had less fullness it would have been really cute.\n",
      "Label: ('General Petite', 1.0, 'Dresses') => [1.0, 1.0, 1.0]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    if i==10:break\n",
    "    print(f\"Text: {v['Review Text']}\\nLabel: {v['Division Name'],v['Rating'],v['Department Name']} => {v['label']}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: yummysweater . sooooo soft ! not a ton of shape but very versitile and warm !\n",
      "Label: ('General Petite', 3.0, 'Tops') => [1.0, 3.0, 4.0]\n",
      "----------\n",
      "Text: great coverage . i ordered the correct size due to reading other reviews who gave their height , weight , and chest size . thank you all for that ! i am 5 feet 5 3/4 inches tall and fluctuate between 135 lbs and 139 lbs . i wear 36 b in bras . i ordered an 8 . fits great ! the coverage is adequate on my backside . the rouching is very nice in the mid section for self conscious women like me . the color is fun and beautiful . the quality is excellent ! i love the bra support ! there are real bra cups sewn in ! it was pricey bu\n",
      "Label: ('Initmates', 5.0, 'Intimate') => [2.0, 5.0, 2.0]\n",
      "----------\n",
      "Text: tts or runs big . very beautiful sheath dress that will be perfect for summer weddings and spring events because of the delicious sorbet pallette . it's a substantial weight fabric that will nip you in and give the illusion of curves where none exist ( in a good way . ) . pattern is actually knit into the fabric not printed . size 8 p is the smaller of my size range so for others who are slender on top may want to size down . if you have prominent hips , stay tts or size up . if you are very busty , it's tts or size up\n",
      "Label: ('General Petite', 5.0, 'Dresses') => [1.0, 5.0, 1.0]\n",
      "----------\n",
      "Text: love this dress ! . the material feels nice and i love the flared arms .\n",
      "Label: ('General', 4.0, 'Dresses') => [0.0, 4.0, 1.0]\n",
      "----------\n",
      "Text: this top surprised me ! . this too is easy to wear , skins the body nicely & the color is very pretty . some of the retailer tops have issues for me because i find the necklines too low . not this one . the placket lays flat & doesn't expose excessive cleavage . the details on the blouse are pretty & subtle . the top washed well in the delicate cute . just hang it to dry and it looks great !\n",
      "Label: ('General', 5.0, 'Tops') => [0.0, 5.0, 4.0]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Text: {tdc.main_ddict['validation']['Review Text'][i]}\")\n",
    "    print(f\"Label: {tdc.main_ddict['validation']['Division Name'][i],tdc.main_ddict['validation']['Rating'][i],tdc.main_ddict['validation']['Department Name'][i]} => {tdc.main_ddict['validation']['label'][i]}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering + Metadatas + Content Transformation + Content Augmentation (for Multi Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug2 = naw.ContextualWordEmbsAug(model_path='roberta-base', \n",
    "                                device='cuda:0', # if you don't have gpu, change to 'cpu'\n",
    "                                action=\"substitute\",\n",
    "                                top_k=10,\n",
    "                               aug_p=0.07)\n",
    "\n",
    "contextual_aug_func = partial(nlp_aug_stochastic,aug=aug2,p=0.5)\n",
    "contextual_aug_func.run_on_gpu=True\n",
    "contextual_aug_func.batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\n",
    "df['Fake Label'] = [np.random.choice(df['Department Name'].unique()[:-1],size=np.random.randint(2,6),replace=False) for _ in range(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict_with_val = dset.train_test_split(test_size=0.1)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name', 'Fake Label'],\n",
       "        num_rows: 2349\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict_with_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Data Filtering --------------------\n",
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "----- Performing Content Transformation and Tokenization on validation set -----\n",
      "Done\n",
      "----- Creating a generator for content transformation, augmentation and tokenization on train set -----\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataControllerStreaming(ddict_with_val,\n",
    "                                  main_text='Review Text',\n",
    "                                  label_names='Fake Label',\n",
    "                                  sup_types='classification',\n",
    "                                  class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n",
    "                                  filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                  metadatas=['Title','Division Name'],\n",
    "                                  content_transformations=[text_normalize,str.lower],\n",
    "                                  content_augmentations= contextual_aug_func, \n",
    "                                  process_metas=True,\n",
    "                                  batch_size=1000,\n",
    "                                  num_proc=4,\n",
    "                                  seed=42\n",
    "                                 )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: general. beautiful, stunning, cozy top!. i checked the first review on here and ordered both a small and a medium as i thought this would run small. i have to totally disagree of the reviewer! i find that this top runs true on size or even generous! the sky color is so pretty and this top can be dressed up with many nice jewels and a necklace or it can be comfy casual, i usually wear a small in hh brand and this one was true to fit ( 5 \" 2 \", broad shoulders, 120 ml )\n",
      "Label: ['Jackets', 'Tops', 'Intimate'] => [0, 0, 1, 1, 1, 0]\n",
      "----------\n",
      "Text: general. love!. love love love this dress! but, and you are not wearing a slip... you should be like please wear a slip, you can see right through this dress.\"\n",
      "Label: ['Bottoms', 'Dresses'] => [1, 1, 0, 0, 0, 0]\n",
      "----------\n",
      "Text: general. runs big. i liked the idea on these pair as i've been looking for an updated pair of tuxedo pants. i wear 26 in most of their jeans. i'm not super skinny & find my legs medium ( not too skinny & not too athletic ). i tried these on in xs ( 36 european as marked on them ) & they were big around the waist & hip area. there was so much gap in the back it made them look frumpy! u did however liked the length. the material is nice & heavy which i also loved. sadly enough i didn't work for me though. really wish t\n",
      "Label: ['Bottoms', 'Tops', 'Trend', 'Dresses'] => [1, 1, 0, 0, 1, 1]\n",
      "----------\n",
      "Text: general petite. so flattering, no need for petite. i just try this top on in xs regular even : i generally wear xspetite in retailer and it fit great ( 34 aa - 35 - 34 ). i think it's flattering on any short narrow arms with the halter neck, fitted waist and peplum? i would guess it would be flattering on many body types ; it highlights shoulders beautifully. it was very hard to put my head through the small, not-too-stretchy opening so you may want to try it on without makeup. i knocked off one star, the neck band wasn't symmetrical,\n",
      "Label: ['Dresses', 'Bottoms', 'Trend', 'Tops', 'Intimate'] => [1, 1, 1, 0, 1, 1]\n",
      "----------\n",
      "Text: general. a new wardrobe staple!. love this jacket! i purchased both the green and gray versions and will have them constantly this winter! although some reviews were critical of the length of the yarn, i do not find this to be a problem... so happy with these products! for reference to size, i bought the m and am 5'11... the sweater hits me exactly where it should on the model.\n",
      "Label: ['Bottoms', 'Tops'] => [1, 0, 0, 0, 1, 0]\n",
      "----------\n",
      "Text: general. unique and adorable. the photos don't do the top justice. the split back is very unique and beautiful. i typically take a size 8 in tops, however ordered a 15 since a reviewer suggested it was narrow in the shoulders. the 12 will fit perfectly, but the body is way too big that i was swimming it in. i like it enough that i'm going to visit the tailor to take in the fit. with the sale price, i is worth tailoring.\n",
      "Label: ['Bottoms', 'Intimate', 'Dresses', 'Tops'] => [1, 1, 1, 0, 1, 0]\n",
      "----------\n",
      "Text: general. great slouchy sweater, perfect color. this sweater has the perfect slouchy coat for fall. i wish I were a little bit softer and heavier - the fabric is pretty lightweight - yet it layers beautifully and will be a highlight for me this season.\n",
      "Label: ['Intimate', 'Bottoms', 'Jackets', 'Trend', 'Tops'] => [1, 0, 1, 1, 1, 1]\n",
      "----------\n",
      "Text: general. feminine plus makeup. this top is gorgeous and versatile. i wear me with jeans and dress it up with a skirt. so happy to have this in my wardrobe.\n",
      "Label: ['Jackets', 'Bottoms'] => [1, 0, 0, 1, 0, 0]\n",
      "----------\n",
      "Text: general. great pants that don't get baggy.. e they are shit - please make them in other colors besides black and navy!\n",
      "Label: ['Trend', 'Dresses', 'Intimate'] => [0, 1, 1, 0, 0, 1]\n",
      "----------\n",
      "Text: general petite. a very blouse-like dress - very simple. this dress is very cute on. very flouncy. i know it looks like a gingham collar, but it's a more like a silk dress with a gingham collar. the one issue i had was that it stood out at the back where you tied it on any size ( large or small ), and i am larger in top, so i don't know what would happen if it with a small chest. i wouldn't be minded if it did the same thing on the front, but it was just the back.\n",
      "Label: ['Trend', 'Jackets', 'Tops', 'Bottoms'] => [1, 0, 0, 1, 1, 1]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    if i==10:break\n",
    "    print(f\"Text: {v['Review Text']}\\nLabel: {v['Fake Label']} => {v['label']}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: general petite . . this top has great detailing and color . does run a little big , but adds to the style and movement of the tank . the stitching around the bottom makes it cute for layering .\n",
      "Label: ['Dresses', 'Intimate', 'Trend', 'Tops', 'Bottoms'] => [1, 1, 1, 0, 1, 1]\n",
      "----------\n",
      "Text: general . . i love this top . i got it on sale and am so glad that i did . it is a short too but still super flattering . it isn't too boxy on me .\n",
      "Label: ['Intimate', 'Trend', 'Jackets', 'Dresses', 'Tops'] => [0, 1, 1, 1, 1, 1]\n",
      "----------\n",
      "Text: general . beautiful idea ... . i ordered my normal size in this dress . i am 6 foot tall , but the regular sizes were too large and too long ( mid-calf ) . i returned the dress for a size smaller in petite for a more flattering hemline . the dress is lovely , especially on the models in the pictures , but didn't quite work out for me . also , it feels like there are hundreds of closure hooks that make putting on / taking off the dress seem to take an unusually long time !\n",
      "Label: ['Dresses', 'Tops'] => [0, 1, 0, 0, 1, 0]\n",
      "----------\n",
      "Text: general petite . comfy , but not made to last . this sweater is fine for the casual days . i bought this in cream and i have to say after one wash it looks old . i'm a huge retailer lover and buy a lot of clothes from them . this is just not the best quality and looks tired after a few wears . very soft , but poor material . not my favorite purchase .\n",
      "Label: ['Trend', 'Jackets'] => [0, 0, 0, 1, 0, 1]\n",
      "----------\n",
      "Text: general . great cool looking jeans . i just bought these jeans today & they are really cute & comfortable on . i love pilcro jeans as they fit really well , they are made well & they are always on style . i did have to go down a size as well but they fit beautifully ! comfy & stylish !\n",
      "Label: ['Tops', 'Intimate', 'Jackets', 'Bottoms'] => [1, 0, 1, 1, 1, 0]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Text: {tdc.main_ddict['validation']['Review Text'][i]}\")\n",
    "    print(f\"Label: {tdc.main_ddict['validation']['Fake Label'][i]} => {tdc.main_ddict['validation']['label'][i]}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.label_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load TextDataControllerStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L128){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.save_as_pickles\n",
       "\n",
       ">      TextDataControllerStreaming.save_as_pickles (fname,\n",
       ">                                                   parent='pickle_files',\n",
       ">                                                   drop_attributes=False)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |\n",
       "| drop_attributes | bool | False | Whether to drop large-size attributes |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L128){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.save_as_pickles\n",
       "\n",
       ">      TextDataControllerStreaming.save_as_pickles (fname,\n",
       ">                                                   parent='pickle_files',\n",
       ">                                                   drop_attributes=False)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |\n",
       "| drop_attributes | bool | False | Whether to drop large-size attributes |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataControllerStreaming.save_as_pickles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L89){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.from_pickle\n",
       "\n",
       ">      TextDataControllerStreaming.from_pickle (fname, parent='pickle_files')\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L89){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.from_pickle\n",
       "\n",
       ">      TextDataControllerStreaming.from_pickle (fname, parent='pickle_files')\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataControllerStreaming.from_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextDataControllerStreaming object can be saved and loaded with ease. This is especially useful after text processing and/or tokenization have been done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import disable_caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_caching() # disable huggingface caching to see data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import text_normalize\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_aug_stochastic(x,aug=None,p=0.5):\n",
    "    if not isinstance(x,list): \n",
    "        if random.random()<p: return aug.augment(x)[0]\n",
    "        return x\n",
    "    news=[]\n",
    "    originals=[]\n",
    "    for _x in x:\n",
    "        if random.random()<p: news.append(_x)\n",
    "        else: originals.append(_x)\n",
    "    # only perform augmentation when needed\n",
    "    if len(news): news = aug.augment(news)\n",
    "    return news+originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug2 = naw.ContextualWordEmbsAug(model_path='roberta-base', \n",
    "                                device='cuda:0', # if you don't have gpu, change to 'cpu'\n",
    "                                action=\"substitute\",\n",
    "                                top_k=10,\n",
    "                               aug_p=0.07)\n",
    "\n",
    "contextual_aug_func = partial(nlp_aug_stochastic,aug=aug2,p=0.1)\n",
    "contextual_aug_func.run_on_gpu=True\n",
    "contextual_aug_func.batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Data Filtering --------------------\n",
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "----- Performing Content Transformation and Tokenization on validation set -----\n",
      "Done\n",
      "----- Creating a generator for content transformation, augmentation and tokenization on train set -----\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.2)\n",
    "ddict_with_val['validation'] = ddict_with_val['test']\n",
    "ddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\n",
    "del ddict_with_val['test']\n",
    "\n",
    "tdc = TextDataControllerStreaming(ddict_with_val,\n",
    "                                  main_text='Review Text',\n",
    "                                  label_names='Department Name',\n",
    "                                  sup_types='classification',\n",
    "                                  class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n",
    "                                  filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                               'Department Name': lambda x: x is not None,\n",
    "                                              },\n",
    "                                  metadatas=['Title','Division Name'],\n",
    "                                  content_transformations=[text_normalize,str.lower],\n",
    "                                  content_augmentations= contextual_aug_func,\n",
    "                                  process_metas=True,\n",
    "                                  batch_size=100,\n",
    "                                  num_proc=4,\n",
    "                                  seed=42\n",
    "                                 )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Title', 'Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4529\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.save_as_pickles('my_tdc_stream')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Size in MegaBytes is 479.372\n"
     ]
    }
   ],
   "source": [
    "file_stats = os.stat(Path('pickle_files/my_tdc_stream.pkl'))\n",
    "print(f'File Size in MegaBytes is {round(file_stats.st_size / (1024 * 1024), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load back our object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc2 = TextDataControllerStreaming.from_pickle('my_tdc_stream')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can still access all its attributes, data, preprocessings, transformation/augmentation ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Title', 'Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4529\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc2.main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: general petite.. i love it soft brown glistening, flowy beauty! it's my favorite color too! i'm 5'5 \". 34 d, size 6 and a small fit and with room to spare. don't wait!\n",
      "Label: Jackets => 3\n",
      "----------\n",
      "Text: general. not the same... as i agree a other reviewer, the material of these jeans is not the same! thin, short, and you end up pulling them up all the time. me am a short, curvy girl and would prefer to have the old jean fabric back! this seems to be the trend in jeans? nydj also uses this fabric? probably too much.\n",
      "Label: Dresses => 1\n",
      "----------\n",
      "Text: general. not for the busty, simple fabric, very versatile but the knit length and style accentuates the bust. probably not an issue for most but if your a d or up it's more attention than you may want.\n",
      "Label: Tops => 4\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(tdc2.main_ddict['train']):\n",
    "    if i==3:break\n",
    "    print(f\"Text: {v['Review Text']}\\nLabel: {v['Department Name']} => {v['label']}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc2.label_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Review Text': <function __main__.<lambda>(x)>,\n",
       "  'Department Name': <function __main__.<lambda>(x)>},\n",
       " [<function underthesea.pipeline.text_normalize.text_normalize(text, tokenizer='underthesea')>,\n",
       "  <method 'lower' of 'str' objects>],\n",
       " [functools.partial(<function nlp_aug_stochastic>, aug=<nlpaug.augmenter.word.context_word_embs.ContextualWordEmbsAug object>, p=0.1)])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc2.filter_dict,tdc2.content_tfms,tdc2.aug_tfms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't want to store the HuggingFace DatasetDict in your `TextDataControllerStreaming`, or the augmentation functions (typically when you already have a trained model, and you only use `TextDataControllerStreaming` to preprocess the test set), you can remove it in the `save_as_pickles` step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.save_as_pickles('my_lightweight_tdc_stream',drop_attributes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Size in MegaBytes is 2.261\n"
     ]
    }
   ],
   "source": [
    "file_stats = os.stat(Path('pickle_files/my_lightweight_tdc_stream.pkl'))\n",
    "print(f'File Size in MegaBytes is {round(file_stats.st_size / (1024 * 1024), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc3 = TextDataControllerStreaming.from_pickle('my_lightweight_tdc_stream')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this object to demonstrate the Test Set Construction in the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L455){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.prepare_test_dataset\n",
       "\n",
       ">      TextDataControllerStreaming.prepare_test_dataset (test_dset,\n",
       ">                                                        do_filtering=False)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| test_dset |  |  | The HuggingFace Dataset as Test set |\n",
       "| do_filtering | bool | False | whether to perform data filtering on this test set |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L455){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.prepare_test_dataset\n",
       "\n",
       ">      TextDataControllerStreaming.prepare_test_dataset (test_dset,\n",
       ">                                                        do_filtering=False)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| test_dset |  |  | The HuggingFace Dataset as Test set |\n",
       "| do_filtering | bool | False | whether to perform data filtering on this test set |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataControllerStreaming.prepare_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L412){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.prepare_test_dataset_from_csv\n",
       "\n",
       ">      TextDataControllerStreaming.prepare_test_dataset_from_csv (file_path,\n",
       ">                                                                 do_filtering=F\n",
       ">                                                                 alse)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| file_path |  |  | path to csv file |\n",
       "| do_filtering | bool | False | whether to perform data filtering on this test set |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L412){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.prepare_test_dataset_from_csv\n",
       "\n",
       ">      TextDataControllerStreaming.prepare_test_dataset_from_csv (file_path,\n",
       ">                                                                 do_filtering=F\n",
       ">                                                                 alse)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| file_path |  |  | path to csv file |\n",
       "| do_filtering | bool | False | whether to perform data filtering on this test set |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataControllerStreaming.prepare_test_dataset_from_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L422){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.prepare_test_dataset_from_df\n",
       "\n",
       ">      TextDataControllerStreaming.prepare_test_dataset_from_df (df,\n",
       ">                                                                validate=True, \n",
       ">                                                                do_filtering=Fa\n",
       ">                                                                lse)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df |  |  | Pandas Dataframe |\n",
       "| validate | bool | True | whether to perform input data validation |\n",
       "| do_filtering | bool | False | whether to perform data filtering on this test set |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L422){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.prepare_test_dataset_from_df\n",
       "\n",
       ">      TextDataControllerStreaming.prepare_test_dataset_from_df (df,\n",
       ">                                                                validate=True, \n",
       ">                                                                do_filtering=Fa\n",
       ">                                                                lse)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df |  |  | Pandas Dataframe |\n",
       "| validate | bool | True | whether to perform input data validation |\n",
       "| do_filtering | bool | False | whether to perform data filtering on this test set |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataControllerStreaming.prepare_test_dataset_from_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L432){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.prepare_test_dataset_from_raws\n",
       "\n",
       ">      TextDataControllerStreaming.prepare_test_dataset_from_raws (content)\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| content | Either a single sentence, list of sentence or a dictionary with keys are metadata columns and values are list |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_streaming.py#L432){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataControllerStreaming.prepare_test_dataset_from_raws\n",
       "\n",
       ">      TextDataControllerStreaming.prepare_test_dataset_from_raws (content)\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| content | Either a single sentence, list of sentence or a dictionary with keys are metadata columns and values are list |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataControllerStreaming.prepare_test_dataset_from_raws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you have done your preprocessing and tokenization in your training set, and have a nicely trained model, ready to do inference on new data. Here is how you can use `TextDataControllerStreaming` to apply all the necessary preprocessings to your new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reuse the lightweight tdc object we created in the previous section (since we don't really need all the training data just to construct new data). Also, we will take a small sample of our training data and pretend it is our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc = TextDataControllerStreaming.from_pickle('my_lightweight_tdc_stream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4692, 10)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig').sample(frac=0.2,random_state=1)\n",
    "# drop NaN values in the label column\n",
    "df_test = df_test[~df_test['Department Name'].isna()].reset_index(drop=True)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>872</td>\n",
       "      <td>42</td>\n",
       "      <td>Perfect for work and play</td>\n",
       "      <td>This shirt works for both going out and going ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1033</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I don't know why i had the opposite problem mo...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Jeans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1037</td>\n",
       "      <td>45</td>\n",
       "      <td>Great pants</td>\n",
       "      <td>These cords are great--lightweight for fl wint...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Jeans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>829</td>\n",
       "      <td>35</td>\n",
       "      <td>Surprisingly comfy for a button down</td>\n",
       "      <td>I am a 10 m and got the 10. it fits perfectly ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>872</td>\n",
       "      <td>29</td>\n",
       "      <td>Short and small</td>\n",
       "      <td>The shirt is mostly a thick sweatshirt materia...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clothing ID  Age                                 Title   \n",
       "0          872   42             Perfect for work and play  \\\n",
       "1         1033   40                                   NaN   \n",
       "2         1037   45                           Great pants   \n",
       "3          829   35  Surprisingly comfy for a button down   \n",
       "4          872   29                       Short and small   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND   \n",
       "0  This shirt works for both going out and going ...       5                1  \\\n",
       "1  I don't know why i had the opposite problem mo...       4                1   \n",
       "2  These cords are great--lightweight for fl wint...       5                1   \n",
       "3  I am a 10 m and got the 10. it fits perfectly ...       5                1   \n",
       "4  The shirt is mostly a thick sweatshirt materia...       3                0   \n",
       "\n",
       "   Positive Feedback Count   Division Name Department Name Class Name  \n",
       "0                        0         General            Tops      Knits  \n",
       "1                        0  General Petite         Bottoms      Jeans  \n",
       "2                        1  General Petite         Bottoms      Jeans  \n",
       "3                        1  General Petite            Tops    Blouses  \n",
       "4                       15  General Petite            Tops      Knits  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Input Validation Precheck -\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title          758\n",
      "Review Text    164\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 2 rows\n",
      "-------------------- Start Test Set Transformation --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "----- Performing Content Transformation and Tokenization on test set -----\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "test_dset = tdc.prepare_test_dataset_from_df(df_test,validate=True,do_filtering=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: general . perfect for work and play . this shirt works for both going out and going to work , and i can wear it with everything . fits perfect , tucked and untucked , tied and untied . i love it .\n",
      "Input_ids: [0, 15841, 479, 1969, 13, 173, 8, 310, 479, 42, 6399, 1364, 13, 258, 164, 66, 8, 164, 7, 173, 2156, 8, 939, 64, 3568, 24, 19, 960, 479, 10698, 1969, 2156, 21222, 8, 7587, 23289, 2156, 3016, 8, 7587, 2550, 479, 939, 657, 24, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "----------\n",
      "Text: general petite . . i don't know why i had the opposite problem most reviewers had with these ..... i tried on the regular length in the store and found that they were just a bit too short with heels . ( i'm 5 ' 5 ) . i had them ordered in a petite and when they came , they were too short with flats ! maybe it's the way i like to wear them , i like my flare jeans to barely skim the ground . i just exchanged them for regular length and will wear them with a small wedge shoe . aside from the length issues , these are super cute\n",
      "Input_ids: [0, 15841, 4716, 1459, 479, 479, 939, 218, 75, 216, 596, 939, 56, 5, 5483, 936, 144, 34910, 56, 19, 209, 29942, 734, 939, 1381, 15, 5, 1675, 5933, 11, 5, 1400, 8, 303, 14, 51, 58, 95, 10, 828, 350, 765, 19, 8872, 479, 36, 939, 437, 195, 128, 195, 4839, 479, 939, 56, 106, 2740, 11, 10, 4716, 1459, 8, 77, 51, 376, 2156, 51, 58, 350, 765, 19, 20250, 27785, 2085, 24, 18, 5, 169, 939, 101, 7, 3568, 106, 2156, 939, 101, 127, 24186, 10844, 7, 6254, 28772, 5, 1255, 479, 939, 95, 11024, 106, 13, 1675, 5933, 8, 40, 3568, 106, 19, 10, 650, 27288, 12604, 479, 4364, 31, 5, 5933, 743, 2156, 209, 32, 2422, 11962, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "----------\n",
      "Text: general petite . great pants . thes e cords are great--lightweight for fl winters , and the bootcut flare bottom is super cute with ballet flats or booties . i am 5 ' 10 \" and typically a size 8 ; the size 29 fit perfectly . they have a little stretch to them , which is great . very flattering--wish i could order in more colors ! !\n",
      "Input_ids: [0, 15841, 4716, 1459, 479, 372, 9304, 479, 5, 29, 364, 37687, 32, 372, 5579, 6991, 4301, 13, 2342, 31000, 2156, 8, 5, 9759, 8267, 24186, 2576, 16, 2422, 11962, 19, 22573, 20250, 50, 9759, 918, 479, 939, 524, 195, 128, 158, 22, 8, 3700, 10, 1836, 290, 25606, 5, 1836, 1132, 2564, 6683, 479, 51, 33, 10, 410, 4140, 7, 106, 2156, 61, 16, 372, 479, 182, 34203, 5579, 605, 1173, 939, 115, 645, 11, 55, 8089, 27785, 27785, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Text: {test_dset['Review Text'][i]}\")\n",
    "    print(f\"Input_ids: {test_dset['input_ids'][i]}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make our test data streamed as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dset_raw = Dataset.from_pandas(df_test).to_iterable_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test dataset might have some NaN values in the text field (`Review Text`), thus we will turn on the filtering option to get rid of these NaNs, as this is what we did in the training set. If your test dataset don't need any filtering, turn off this option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Test Set Transformation --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "----- Performing Content Transformation and Tokenization on test set -----\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "test_dset = tdc.prepare_test_dataset(test_dset_raw,do_filtering=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: general . perfect for work and play . this shirt works for both going out and going to work , and i can wear it with everything . fits perfect , tucked and untucked , tied and untied . i love it .\\Input_ids: [0, 15841, 479, 1969, 13, 173, 8, 310, 479, 42, 6399, 1364, 13, 258, 164, 66, 8, 164, 7, 173, 2156, 8, 939, 64, 3568, 24, 19, 960, 479, 10698, 1969, 2156, 21222, 8, 7587, 23289, 2156, 3016, 8, 7587, 2550, 479, 939, 657, 24, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----------\n",
      "Text: general petite . . i don't know why i had the opposite problem most reviewers had with these ..... i tried on the regular length in the store and found that they were just a bit too short with heels . ( i'm 5 ' 5 ) . i had them ordered in a petite and when they came , they were too short with flats ! maybe it's the way i like to wear them , i like my flare jeans to barely skim the ground . i just exchanged them for regular length and will wear them with a small wedge shoe . aside from the length issues , these are super cute\\Input_ids: [0, 15841, 4716, 1459, 479, 479, 939, 218, 75, 216, 596, 939, 56, 5, 5483, 936, 144, 34910, 56, 19, 209, 29942, 734, 939, 1381, 15, 5, 1675, 5933, 11, 5, 1400, 8, 303, 14, 51, 58, 95, 10, 828, 350, 765, 19, 8872, 479, 36, 939, 437, 195, 128, 195, 4839, 479, 939, 56, 106, 2740, 11, 10, 4716, 1459, 8, 77, 51, 376, 2156, 51, 58, 350, 765, 19, 20250, 27785, 2085, 24, 18, 5, 169, 939, 101, 7, 3568, 106, 2156, 939, 101, 127, 24186, 10844, 7, 6254, 28772, 5, 1255, 479, 939, 95, 11024, 106, 13, 1675, 5933, 8, 40, 3568, 106, 19, 10, 650, 27288, 12604, 479, 4364, 31, 5, 5933, 743, 2156, 209, 32, 2422, 11962, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----------\n",
      "Text: general petite . great pants . thes e cords are great--lightweight for fl winters , and the bootcut flare bottom is super cute with ballet flats or booties . i am 5 ' 10 \" and typically a size 8 ; the size 29 fit perfectly . they have a little stretch to them , which is great . very flattering--wish i could order in more colors ! !\\Input_ids: [0, 15841, 4716, 1459, 479, 372, 9304, 479, 5, 29, 364, 37687, 32, 372, 5579, 6991, 4301, 13, 2342, 31000, 2156, 8, 5, 9759, 8267, 24186, 2576, 16, 2422, 11962, 19, 22573, 20250, 50, 9759, 918, 479, 939, 524, 195, 128, 158, 22, 8, 3700, 10, 1836, 290, 25606, 5, 1836, 1132, 2564, 6683, 479, 51, 33, 10, 410, 4140, 7, 106, 2156, 61, 16, 372, 479, 182, 34203, 5579, 605, 1173, 939, 115, 645, 11, 55, 8089, 27785, 27785, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(test_dset):\n",
    "    if i==3:break\n",
    "    print(f\"Text: {v['Review Text']}\\Input_ids: {v['input_ids']}\\nAttention mask: {v['attention_mask']}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
