{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Main\n",
    "\n",
    "> This module contains the main Python class for data control: `TextDataController`\n",
    "\n",
    "- skip_showdoc: true\n",
    "- skip_exec: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp text_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from sklearn.preprocessing import LabelEncoder,MultiLabelBinarizer\n",
    "from datasets import DatasetDict,Dataset,IterableDataset,load_dataset,concatenate_datasets,Value\n",
    "from pathlib import Path\n",
    "from that_nlp_library.utils import *\n",
    "from functools import partial\n",
    "import warnings\n",
    "from datasets.utils.logging import disable_progress_bar,enable_progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from that_nlp_library.text_transformation import *\n",
    "from that_nlp_library.text_augmentation import *\n",
    "from importlib.machinery import SourceFileLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Transformation, Augmentations, and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tokenizer_explain(inp, # Input sentence\n",
    "                      tokenizer, # Tokenizer (preferably from HuggingFace)\n",
    "                      split_word=False # Is input `inp` split into list or not\n",
    "                     ):\n",
    "    \"Display results from tokenizer\"\n",
    "    print('\\t\\t------- Tokenizer Explained -------')\n",
    "    print('----- Input -----')\n",
    "    print(inp)\n",
    "    print()\n",
    "    print('----- Tokenized results ----- ')\n",
    "    print(tokenizer(inp,is_split_into_words=split_word))\n",
    "    print()\n",
    "    tok = tokenizer.encode(inp,is_split_into_words=split_word)\n",
    "    print('----- Results from tokenizer.convert_ids_to_tokens -----')\n",
    "    print(tokenizer.convert_ids_to_tokens(tok))\n",
    "    print()\n",
    "    print('----- Results from tokenizer.decode ----- ')\n",
    "    print(tokenizer.decode(tok))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L17){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### tokenizer_explain\n",
       "\n",
       ">      tokenizer_explain (inp, tokenizer, split_word=False)\n",
       "\n",
       "Display results from tokenizer\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | Input sentence |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| split_word | bool | False | Is input `inp` split into list or not |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L17){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### tokenizer_explain\n",
       "\n",
       ">      tokenizer_explain (inp, tokenizer, split_word=False)\n",
       "\n",
       "Display results from tokenizer\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | Input sentence |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| split_word | bool | False | Is input `inp` split into list or not |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(tokenizer_explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a tokenizer from EnviBert model. Uncomment the command line below to download necessary files to build this tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gdown 14X9fGijA7kdNfe4dM_8gqfxIWtj1Q-hb -O ./envibert_cache --folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir=Path('./envibert_tokenizer')\n",
    "_tokenizer = SourceFileLoader(\"envibert.tokenizer\", \n",
    "                             str(cache_dir/'envibert_tokenizer.py')).load_module().RobertaTokenizer(cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Envibert tokenizer does not required the input to be tokenized using word_tokenize from UnderTheSea library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t------- Tokenizer Explained -------\n",
      "----- Input -----\n",
      "Hội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức. Thủ Đức là một huyện trực thuộc thành phố Hồ Chí Minh\n",
      "\n",
      "----- Tokenized results ----- \n",
      "{'input_ids': [0, 857, 1033, 191, 664, 1033, 7366, 2615, 142, 664, 1033, 671, 1355, 2294, 993, 413, 2900, 244, 1019, 827, 24, 40, 647, 773, 549, 119, 511, 1134, 1690, 758, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "----- Results from tokenizer.convert_ids_to_tokens -----\n",
      "['<s>', '▁Hội', '▁cư', '▁dân', '▁chung', '▁cư', '▁sen', '▁hồng', '-', '▁chung', '▁cư', '▁lot', 'us', '▁sóng', '▁thần', '▁thủ', '▁đức', '.', '▁Thủ', '▁Đức', '▁là', '▁một', '▁huyện', '▁trực', '▁thuộc', '▁thành', '▁phố', '▁Hồ', '▁Chí', '▁Minh', '</s>']\n",
      "\n",
      "----- Results from tokenizer.decode ----- \n",
      "<s> ▁Hội ▁cư ▁dân ▁chung ▁cư ▁sen ▁hồng - ▁chung ▁cư ▁lot us ▁sóng ▁thần ▁thủ ▁đức. ▁Thủ ▁Đức ▁là ▁một ▁huyện ▁trực ▁thuộc ▁thành ▁phố ▁Hồ ▁Chí ▁Minh </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inp = 'Hội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức. Thủ Đức là một huyện trực thuộc thành phố Hồ Chí Minh'\n",
    "tokenizer_explain(inp,_tokenizer,split_word=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t------- Tokenizer Explained -------\n",
      "----- Input -----\n",
      "['hội', 'cư', 'dân', 'chung', 'cư', 'sen', 'hồng', '-', 'chung', 'cư', 'lotus', 'sóng', 'thần', 'thủ', 'đức']\n",
      "\n",
      "----- Tokenized results ----- \n",
      "{'input_ids': [0, 227, 1033, 191, 664, 1033, 7366, 2615, 13, 664, 1033, 671, 1355, 2294, 993, 413, 2900, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "----- Results from tokenizer.convert_ids_to_tokens -----\n",
      "['<s>', '▁hội', '▁cư', '▁dân', '▁chung', '▁cư', '▁sen', '▁hồng', '▁-', '▁chung', '▁cư', '▁lot', 'us', '▁sóng', '▁thần', '▁thủ', '▁đức', '</s>']\n",
      "\n",
      "----- Results from tokenizer.decode ----- \n",
      "<s> ▁hội ▁cư ▁dân ▁chung ▁cư ▁sen ▁hồng ▁- ▁chung ▁cư ▁lot us ▁sóng ▁thần ▁thủ ▁đức </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inp = ['hội', 'cư', 'dân', 'chung', 'cư', 'sen', 'hồng', '-', 'chung', 'cư', 'lotus', 'sóng', 'thần', 'thủ', 'đức']\n",
    "tokenizer_explain(inp,_tokenizer,split_word=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try PhoBert tokenizer. PhoBert tokenizer, unlike Envibert tokenizer, requires input to be word tokenized (using UnderTheSea library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức\n"
     ]
    }
   ],
   "source": [
    "inp = apply_vnmese_word_tokenize('hội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức')\n",
    "print(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t------- Tokenizer Explained -------\n",
      "----- Input -----\n",
      "hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức\n",
      "\n",
      "----- Tokenized results ----- \n",
      "{'input_ids': [0, 1093, 1838, 1574, 3330, 2025, 31, 1574, 2029, 4885, 8554, 25625, 7344, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "----- Results from tokenizer.convert_ids_to_tokens -----\n",
      "['<s>', 'hội', 'cư_dân', 'chung_cư', 'sen', 'hồng', '-', 'chung_cư', 'lo@@', 'tus', 'sóng_thần', 'thủ_@@', 'đức', '</s>']\n",
      "\n",
      "----- Results from tokenizer.decode ----- \n",
      "<s> hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer_explain(inp,_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def two_steps_tokenization_explain(inp, # Input sentence\n",
    "                                   tokenizer, # Tokenizer (preferably from HuggingFace)\n",
    "                                   content_tfms=[], # A list of text transformations\n",
    "                                   aug_tfms=[], # A list of text augmentation \n",
    "                                  ):\n",
    "    \"Display results form each content transformation, then display results from tokenizer\"\n",
    "    print('\\t\\t------- Text Transformation Explained -------')\n",
    "    print('----- Raw sentence -----')\n",
    "    print(inp)\n",
    "    print()\n",
    "    print('----- Content Transformations (on both train and test) -----')\n",
    "    content_tfms = val2iterable(content_tfms)\n",
    "    for tfm in content_tfms:\n",
    "        print_msg(callable_name(tfm),3)\n",
    "        inp = tfm(inp)\n",
    "        print(inp)\n",
    "        print()\n",
    "    print()\n",
    "    print('----- Augmentations (on train only) -----')\n",
    "    aug_tfms = val2iterable(aug_tfms)\n",
    "    for tfm in aug_tfms:\n",
    "        print_msg(callable_name(tfm),3)\n",
    "        inp = tfm(inp)\n",
    "        print(inp)\n",
    "        print()\n",
    "    print()\n",
    "    tokenizer_explain(inp,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L38){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### two_steps_tokenization_explain\n",
       "\n",
       ">      two_steps_tokenization_explain (inp, tokenizer, content_tfms=[],\n",
       ">                                      aug_tfms=[])\n",
       "\n",
       "Display results form each content transformation, then display results from tokenizer\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | Input sentence |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| content_tfms | list | [] | A list of text transformations |\n",
       "| aug_tfms | list | [] | A list of text augmentation |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L38){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### two_steps_tokenization_explain\n",
       "\n",
       ">      two_steps_tokenization_explain (inp, tokenizer, content_tfms=[],\n",
       ">                                      aug_tfms=[])\n",
       "\n",
       "Display results form each content transformation, then display results from tokenizer\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | Input sentence |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| content_tfms | list | [] | A list of text transformations |\n",
       "| aug_tfms | list | [] | A list of text augmentation |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(two_steps_tokenization_explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load Phobert tokenizer one more time to test out this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import text_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`apply_vnmese_word_tokenize` also have an option to normalize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t------- Text Transformation Explained -------\n",
      "----- Raw sentence -----\n",
      "Hội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức. Thủ Đức là một huyện trực thuộc thành phố Hồ Chí Minh\n",
      "\n",
      "----- Content Transformations (on both train and test) -----\n",
      "--- apply_vnmese_word_tokenize ---\n",
      "Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức . Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh\n",
      "\n",
      "\n",
      "----- Augmentations (on train only) -----\n",
      "\n",
      "\t\t------- Tokenizer Explained -------\n",
      "----- Input -----\n",
      "Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức . Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh\n",
      "\n",
      "----- Tokenized results ----- \n",
      "{'input_ids': [0, 792, 1838, 1574, 3330, 2025, 31, 1574, 2029, 4885, 8554, 25625, 7344, 5, 5043, 8, 16, 149, 2850, 214, 784, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "----- Results from tokenizer.convert_ids_to_tokens -----\n",
      "['<s>', 'Hội', 'cư_dân', 'chung_cư', 'sen', 'hồng', '-', 'chung_cư', 'lo@@', 'tus', 'sóng_thần', 'thủ_@@', 'đức', '.', 'Thủ_Đức', 'là', 'một', 'huyện', 'trực_thuộc', 'thành_phố', 'Hồ_Chí_Minh', '</s>']\n",
      "\n",
      "----- Results from tokenizer.decode ----- \n",
      "<s> Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức. Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inp = 'Hội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức. Thủ Đức là một huyện trực thuộc thành phố Hồ Chí Minh'\n",
    "two_steps_tokenization_explain(inp,_tokenizer,content_tfms=[partial(apply_vnmese_word_tokenize,normalize_text=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some text augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove vietnamese accent\n",
    "remove_accent = lambda x: unidecode.unidecode(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want your function to be printed in with a different name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_accent.__name__ = 'Remove Vietnamese Accent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t------- Text Transformation Explained -------\n",
      "----- Raw sentence -----\n",
      "Hội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức. Thủ Đức là một huyện trực thuộc thành phố Hồ Chí Minh\n",
      "\n",
      "----- Content Transformations (on both train and test) -----\n",
      "--- apply_vnmese_word_tokenize ---\n",
      "Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức . Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh\n",
      "\n",
      "\n",
      "----- Augmentations (on train only) -----\n",
      "--- Remove Vietnamese Accent ---\n",
      "Hoi cu_dan chung_cu sen hong - chung_cu lotus song_than thu_duc . Thu_Duc la mot huyen truc_thuoc thanh_pho Ho_Chi_Minh\n",
      "\n",
      "\n",
      "\t\t------- Tokenizer Explained -------\n",
      "----- Input -----\n",
      "Hoi cu_dan chung_cu sen hong - chung_cu lotus song_than thu_duc . Thu_Duc la mot huyen truc_thuoc thanh_pho Ho_Chi_Minh\n",
      "\n",
      "----- Tokenized results ----- \n",
      "{'input_ids': [0, 3021, 1111, 56549, 17386, 22975, 13689, 3330, 27037, 31, 22975, 13689, 2029, 4885, 3227, 9380, 1510, 21605, 6190, 1894, 5, 5770, 4098, 1894, 2644, 3773, 1204, 18951, 2052, 10242, 9835, 1881, 22899, 17366, 10384, 30234, 8470, 1612, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "----- Results from tokenizer.convert_ids_to_tokens -----\n",
      "['<s>', 'Ho@@', 'i', 'cu_@@', 'dan', 'chung_@@', 'cu', 'sen', 'hong', '-', 'chung_@@', 'cu', 'lo@@', 'tus', 'so@@', 'ng_th@@', 'an', 'thu_@@', 'du@@', 'c', '.', 'Thu_@@', 'Du@@', 'c', 'la', 'mo@@', 't', 'huy@@', 'en', 'tru@@', 'c_th@@', 'u@@', 'oc', 'thanh_@@', 'pho', 'Ho_@@', 'Chi_@@', 'Minh', '</s>']\n",
      "\n",
      "----- Results from tokenizer.decode ----- \n",
      "<s> Hoi cu_dan chung_cu sen hong - chung_cu lotus song_than thu_duc. Thu_Duc la mot huyen truc_thuoc thanh_pho Ho_Chi_Minh </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "two_steps_tokenization_explain(inp,_tokenizer,\n",
    "                               content_tfms=[partial(apply_vnmese_word_tokenize,normalize_text=True)],\n",
    "                               aug_tfms=[remove_accent]\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even be creative with your augmentation functions; let's say you only want your augmentation to be applied 50% of the time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_accent = lambda x: unidecode.unidecode(x) if random.random()<0.5 else x\n",
    "remove_accent.__name__ = 'Remove Vietnamese Accent with 0.5 prob'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t------- Text Transformation Explained -------\n",
      "----- Raw sentence -----\n",
      "Hội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức. Thủ Đức là một huyện trực thuộc thành phố Hồ Chí Minh\n",
      "\n",
      "----- Content Transformations (on both train and test) -----\n",
      "--- apply_vnmese_word_tokenize ---\n",
      "Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức . Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh\n",
      "\n",
      "\n",
      "----- Augmentations (on train only) -----\n",
      "--- Remove Vietnamese Accent with 0.5 prob ---\n",
      "Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức . Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh\n",
      "\n",
      "\n",
      "\t\t------- Tokenizer Explained -------\n",
      "----- Input -----\n",
      "Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức . Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh\n",
      "\n",
      "----- Tokenized results ----- \n",
      "{'input_ids': [0, 792, 1838, 1574, 3330, 2025, 31, 1574, 2029, 4885, 8554, 25625, 7344, 5, 5043, 8, 16, 149, 2850, 214, 784, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "----- Results from tokenizer.convert_ids_to_tokens -----\n",
      "['<s>', 'Hội', 'cư_dân', 'chung_cư', 'sen', 'hồng', '-', 'chung_cư', 'lo@@', 'tus', 'sóng_thần', 'thủ_@@', 'đức', '.', 'Thủ_Đức', 'là', 'một', 'huyện', 'trực_thuộc', 'thành_phố', 'Hồ_Chí_Minh', '</s>']\n",
      "\n",
      "----- Results from tokenizer.decode ----- \n",
      "<s> Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức. Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "two_steps_tokenization_explain(inp,_tokenizer,\n",
    "                               content_tfms=[partial(apply_vnmese_word_tokenize,normalize_text=True)],\n",
    "                               aug_tfms=[remove_accent]\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more examples of interesting augmentation [here](https://anhquan0412.github.io/that-nlp-library/text_augmentation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tokenize_function(text,\n",
    "                      tok,\n",
    "                      max_length=None,\n",
    "                      is_split_into_words=False,\n",
    "                      return_tensors=None,\n",
    "                      return_special_tokens_mask=False\n",
    "                     ):\n",
    "    if max_length is None:\n",
    "        # pad to model's default max sequence length\n",
    "        return tok(text, padding=\"max_length\", \n",
    "                   truncation=True,\n",
    "                   is_split_into_words=is_split_into_words,\n",
    "                   return_tensors=return_tensors,\n",
    "                   return_special_tokens_mask=return_special_tokens_mask\n",
    "                  )\n",
    "    if isinstance(max_length,int) and max_length>0:\n",
    "        # pad to the largest length of the current batch, and start truncating at max_length\n",
    "        return tok(text, padding=True, \n",
    "                   max_length=max_length,\n",
    "                   truncation=True,\n",
    "                   is_split_into_words=is_split_into_words,\n",
    "                   return_tensors=return_tensors,\n",
    "                   return_special_tokens_mask=return_special_tokens_mask\n",
    "                  )\n",
    "    \n",
    "    # no padding (still truncate at model's default max sequence length)\n",
    "    return tok(text, padding=False,\n",
    "               truncation=True,\n",
    "               is_split_into_words=is_split_into_words,\n",
    "               return_tensors=return_tensors,\n",
    "               return_special_tokens_mask=return_special_tokens_mask\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L67){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### tokenize_function\n",
       "\n",
       ">      tokenize_function (text, tok, max_length=None, is_split_into_words=False,\n",
       ">                         return_tensors=None, return_special_tokens_mask=False)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L67){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### tokenize_function\n",
       "\n",
       ">      tokenize_function (text, tok, max_length=None, is_split_into_words=False,\n",
       ">                         return_tensors=None, return_special_tokens_mask=False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(tokenize_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I am processing Vietnamese text, I will use EnViBert's tokenizer. Envibert is a RoBERTa model for Vietnamese and English. This RoBERTa version is trained by using 100GB of text (50GB of Vietnamese and 50GB of English). For more information: [https://huggingface.co/nguyenvulebinh/envibert](https://huggingface.co/nguyenvulebinh/envibert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/nguyenvulebinh/envibert\n",
    "cache_dir=Path('./envibert_tokenizer')\n",
    "_tokenizer = SourceFileLoader(\"envibert.tokenizer\", \n",
    "                             str(cache_dir/'envibert_tokenizer.py')).load_module().RobertaTokenizer(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 227, 1033, 191, 664, 1033, 7366, 2615, 13, 664, 1033, 671, 1355, 2294, 993, 413, 2900, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_function('hội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức',_tokenizer,max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 227, 256, 778, 2600, 1074, 144, 76, 5489, 613, 57339, 4820, 27666, 57339, 21422, 244, 872, 635, 841, 2], [0, 880, 592, 427, 162, 171, 906, 13, 122, 6553, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_function(['hội cần mở thẻ tín dụng tại hà nội, đà nẵng, tp. hồ chí minh',\n",
    "                   \"biti's cao lãnh - đồng tháp\"],\n",
    "                  _tokenizer,max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the tokenizer outputs' type, such as pytorch's tensor, tensorflow objects, or numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,   227,   256,   778,  2600,  1074,   144,    76,  5489,   613,\n",
       "         57339,  4820, 27666, 57339, 21422,   244,   872,   635,   841,     2],\n",
       "        [    0,   880,   592,   427,   162,   171,   906,    13,   122,  6553,\n",
       "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_function(['hội cần mở thẻ tín dụng tại hà nội, đà nẵng, tp. hồ chí minh',\n",
    "                   \"biti's cao lãnh - đồng tháp\"],\n",
    "                  _tokenizer,max_length=512,return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = tokenize_function(['hội cần mở thẻ tín dụng tại hà nội, đà nẵng, tp. hồ chí minh',\n",
    "                             \"biti's cao lãnh - đồng tháp\"],\n",
    "                            _tokenizer,max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁hội', '▁cần', '▁mở', '▁thẻ', '▁tín', '▁dụng', '▁tại', '▁hà', '▁nội', ',', '▁đà', '▁nẵng', ',', '▁tp', '.', '▁hồ', '▁chí', '▁minh', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(_tokenizer.convert_ids_to_tokens(results['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change max_length (which allow truncation when sentence length is higher than max_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = tokenize_function(['hội cần mở thẻ tín dụng tại hà nội, đà nẵng, tp. hồ chí minh',\n",
    "                            \"biti's cao lãnh - đồng tháp\"],_tokenizer,\n",
    "                            max_length=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 227, 256, 778, 2], [0, 880, 592, 427, 2]], 'token_type_ids': [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadatas Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def concat_metadatas(dset:dict, # HuggingFace Dataset\n",
    "                     main_text, # Text feature name\n",
    "                     metadatas, # Metadata (or a list of metadatas)\n",
    "                     process_metas=True, # Whether apply simple metadata processing, i.e. space strip and lowercase\n",
    "                     sep='.', # Separator, for multiple metadatas concatenation\n",
    "                     is_batched=True, # whether batching is applied\n",
    "                    ):\n",
    "    \"\"\"\n",
    "    Extract, process (optional) and concatenate metadatas to the front of text\n",
    "    \"\"\"\n",
    "    results={main_text:dset[main_text]}\n",
    "    for m in metadatas:\n",
    "        m_data = dset[m]\n",
    "        if process_metas:\n",
    "            # just strip and lowercase\n",
    "            m_data = [none2emptystr(v).strip().lower() for v in m_data] if is_batched else none2emptystr(m_data).strip().lower()\n",
    "        results[m]=m_data\n",
    "        if is_batched:\n",
    "            results[main_text] = [f'{m_data[i]} {sep} {results[main_text][i]}' for i in range(len(m_data))]\n",
    "        else:\n",
    "            results[main_text] = f'{m_data} {sep} {results[main_text]}'\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L101){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### concat_metadatas\n",
       "\n",
       ">      concat_metadatas (dset:dict, main_text, metadatas, process_metas=True,\n",
       ">                        sep='.', is_batched=True)\n",
       "\n",
       "Extract, process (optional) and concatenate metadatas to the front of text\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dset | dict |  | HuggingFace Dataset |\n",
       "| main_text |  |  | Text feature name |\n",
       "| metadatas |  |  | Metadata (or a list of metadatas) |\n",
       "| process_metas | bool | True | Whether apply simple metadata processing, i.e. space strip and lowercase |\n",
       "| sep | str | . | Separator, for multiple metadatas concatenation |\n",
       "| is_batched | bool | True | whether batching is applied |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L101){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### concat_metadatas\n",
       "\n",
       ">      concat_metadatas (dset:dict, main_text, metadatas, process_metas=True,\n",
       ">                        sep='.', is_batched=True)\n",
       "\n",
       "Extract, process (optional) and concatenate metadatas to the front of text\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dset | dict |  | HuggingFace Dataset |\n",
       "| main_text |  |  | Text feature name |\n",
       "| metadatas |  |  | Metadata (or a list of metadatas) |\n",
       "| process_metas | bool | True | Whether apply simple metadata processing, i.e. space strip and lowercase |\n",
       "| sep | str | . | Separator, for multiple metadatas concatenation |\n",
       "| is_batched | bool | True | whether batching is applied |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(concat_metadatas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class TextDataController"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TextDataController():\n",
    "    def __init__(self,\n",
    "                 inp, # HuggingFainpce Dataset or DatasetDict\n",
    "                 main_text:str, # Name of the main text column\n",
    "                 label_names=[], # Names of the label (dependent variable) columns\n",
    "                 sup_types=[], # Type of supervised learning for each label name ('classification' or 'regression')\n",
    "                 class_names_predefined=[], # List of names associated with the labels (same index order). Use empty list for regression\n",
    "                 filter_dict={}, # A dictionary: {feature: filtering_function_for_that_feature}\n",
    "                 label_tfm_dict={}, # A dictionary: {label_name: transform_function_for_that_label}\n",
    "                 metadatas=[], # Names of the metadata columns\n",
    "                 process_metas=True, # Whether to do simple text processing on the chosen metadatas\n",
    "                 metas_sep='.', # Separator, for multiple metadatas concatenation\n",
    "                 content_transformations=[], # A list of text transformations\n",
    "                 val_ratio:int|float|None=0.2, # Ratio of data for validation set\n",
    "                 stratify_cols=[], # Column(s) needed to do stratified shuffle split\n",
    "                 upsampling_list={}, # A list of tuple. Each tuple: (feature,upsampling_function_based_on_the_feature)\n",
    "                 content_augmentations=[], # A list of text augmentations\n",
    "                 seed=None, # Random seed\n",
    "                 batch_size=1024, # CPU batch size\n",
    "                 num_proc=4, # Number of processes for multiprocessing\n",
    "                 cols_to_keep=None, # Columns to keep after all processings\n",
    "                 verbose=True, # Whether to print processing information\n",
    "                ):\n",
    "            \n",
    "        self.main_text = main_text\n",
    "        \n",
    "        self.label_names = val2iterable(label_names)\n",
    "        self.sup_types = val2iterable(sup_types)\n",
    "        self._check_sup_types()\n",
    "        self.label_lists = class_names_predefined\n",
    "        \n",
    "        self.filter_dict = filter_dict\n",
    "        self.label_tfm_dict = label_tfm_dict\n",
    "        self.metadatas = val2iterable(metadatas)\n",
    "        self.process_metas = process_metas\n",
    "        self.metas_sep = metas_sep\n",
    "        \n",
    "        self.content_tfms = val2iterable(content_transformations)\n",
    "        \n",
    "        self.val_ratio = val_ratio\n",
    "        self.stratify_cols = val2iterable(stratify_cols)\n",
    "        \n",
    "        self.upsampling_list = upsampling_list\n",
    "        self.aug_tfms = val2iterable(content_augmentations)\n",
    "        \n",
    "        self.seed = seed\n",
    "        self.is_batched = batch_size>1\n",
    "        self.batch_size = batch_size\n",
    "        self.num_proc = num_proc\n",
    "        self.cols_to_keep = cols_to_keep\n",
    "        \n",
    "        self.ddict_rest = DatasetDict()\n",
    "        self.set_verbose(verbose)\n",
    "        \n",
    "        if hasattr(inp,'keys'): # is datasetdict\n",
    "            if 'train' in inp.keys(): \n",
    "                self.ddict_rest = inp\n",
    "                self.dset = self.ddict_rest.pop('train')\n",
    "            else:\n",
    "                raise ValueError('The given DatasetDict has no \"train\" split')\n",
    "        else: # is dataset\n",
    "            self.dset = inp\n",
    "        \n",
    "        if isinstance(self.dset,IterableDataset):\n",
    "            raise ValueError('TextDataController does not handle Iterable Dataset as input. Consider `TextDataControllerStreaming`')\n",
    "        \n",
    "        self._determine_val_key()\n",
    "        self.all_cols = get_dset_col_names(self.dset)\n",
    "        self._determine_multihead_multilabel()\n",
    "        self._convert_regression_to_float()\n",
    "        \n",
    "        self._processed_call=False\n",
    "            \n",
    "    @classmethod\n",
    "    def from_csv(cls,file_path,**kwargs):\n",
    "        file_path = Path(file_path)\n",
    "        ds = load_dataset(str(file_path.parent),\n",
    "                                  data_files=file_path.name,\n",
    "                                  split='train')\n",
    "        return TextDataController(ds,**kwargs)\n",
    "        \n",
    "    \n",
    "    @classmethod\n",
    "    def from_df(cls,df,validate=True,**kwargs):\n",
    "        if validate:\n",
    "            check_input_validation(df)\n",
    "        ds = Dataset.from_pandas(df)\n",
    "        return TextDataController(ds,**kwargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pickle(cls,\n",
    "                    fname, # Name of the pickle file\n",
    "                    parent='pickle_files' # Parent folder\n",
    "                   ):\n",
    "        return load_pickle(fname,parent=parent)\n",
    "\n",
    "    def set_verbose(self,verbose):\n",
    "        self.verbose = verbose\n",
    "        self.verboseprint = print if verbose else lambda *a, **k: None\n",
    "        if not self.verbose:\n",
    "            disable_progress_bar() # turn off huggingface `map` progress bar\n",
    "        else:\n",
    "            enable_progress_bar()\n",
    "    \n",
    "    def _convert_regression_to_float(self):\n",
    "        if len(self.sup_types)==0: return\n",
    "        # convert regression labels to float64\n",
    "        reg_idxs = [i for i,v in enumerate(self.sup_types) if v=='regression']\n",
    "        for i in reg_idxs:\n",
    "            self.dset = self.dset.cast_column(self.label_names[i],Value(\"float64\"))\n",
    "            if self.val_key is not None:\n",
    "                self.ddict_rest[self.val_key] = self.ddict_rest[self.val_key].cast_column(self.label_names[i],Value(\"float64\"))\n",
    "        \n",
    "    def _check_sup_types(self):\n",
    "        assert len(self.label_names)==len(self.sup_types), \"The number of supervised learning declaration must equal to the number of label\"\n",
    "        assert len(set(self.sup_types) - set(['classification','regression']))==0, 'Accepted inputs for `sup_types` are `classification` and `regression`'\n",
    "    \n",
    "    def _determine_val_key(self):\n",
    "        # determine validation split key\n",
    "        self.val_key=None\n",
    "        val_key = list(set(self.ddict_rest.keys()) & set(['val','validation','valid']))\n",
    "        if len(val_key)>1: raise ValueError('Your DatasetDict has more than 1 validation split')\n",
    "        if len(val_key)==1:\n",
    "            val_key=val_key[0]\n",
    "            self.val_key=val_key\n",
    "            \n",
    "    def _determine_multihead_multilabel(self):\n",
    "        self.is_multilabel=False\n",
    "        self.is_multihead=False\n",
    "        if len(self.label_names)==0: return\n",
    "        \n",
    "        if len(self.label_names)>1:\n",
    "            self.is_multihead=True\n",
    "            \n",
    "        # get label of first row\n",
    "        first_label = self.dset[self.label_names[0]][0]\n",
    "        if isinstance(first_label,(list,set,tuple)):\n",
    "            # This is multi-label. Ignore self.label_names[1:]\n",
    "            self.label_names = [self.label_names[0]]\n",
    "            self.is_multihead=False\n",
    "            self.is_multilabel=True\n",
    "                     \n",
    "    def validate_input(self):\n",
    "        _df = self.dset.to_pandas()\n",
    "        check_input_validation(_df)\n",
    "    \n",
    "    def save_as_pickles(self,\n",
    "                        fname, # Name of the pickle file\n",
    "                        parent='pickle_files', # Parent folder\n",
    "                        drop_attributes=False # Whether to drop large-size attributes\n",
    "                       ):\n",
    "        if drop_attributes:\n",
    "            if hasattr(self, 'main_ddict'):\n",
    "                del self.main_ddict\n",
    "            if hasattr(self, 'ddict_rest'):\n",
    "                del self.ddict_rest\n",
    "            if hasattr(self, 'aug_tfms'):\n",
    "                del self.aug_tfms\n",
    "        save_to_pickle(self,fname,parent=parent)\n",
    "    \n",
    "        \n",
    "    def _check_validation_leaking(self):\n",
    "        if self.val_ratio is None:\n",
    "            return\n",
    "        \n",
    "        trn_txt = self.main_ddict['train'][self.main_text]\n",
    "        val_txt = self.main_ddict['validation'][self.main_text]        \n",
    "        val_txt_leaked = check_text_leaking(trn_txt,val_txt,verbose=self.verbose)\n",
    "        \n",
    "        if len(val_txt_leaked)==0: return\n",
    "        \n",
    "        # filter train dataset to get rid of leaks\n",
    "        self.verboseprint('Filtering leaked data out of training set...')\n",
    "        _func = partial(lambda_batch,\n",
    "                        feature=self.main_text,\n",
    "                        func=lambda x: x.strip() not in val_txt_leaked,\n",
    "                        is_batched=self.is_batched)\n",
    "        self.main_ddict['train'] = hf_filter_dset(self.main_ddict['train'],_func,self.is_batched,self.batch_size,self.num_proc)   \n",
    "        self.verboseprint('Done')\n",
    "    \n",
    "    def _process_metadatas(self,dset,ddict_rest=None):\n",
    "        if len(self.metadatas):\n",
    "            print_msg('Metadata Simple Processing & Concatenating to Main Content',verbose=self.verbose)\n",
    "            map_func = partial(concat_metadatas,\n",
    "                               main_text=self.main_text,\n",
    "                               metadatas=self.metadatas,\n",
    "                               process_metas=self.process_metas,\n",
    "                               sep=self.metas_sep,\n",
    "                               is_batched=self.is_batched)\n",
    "            dset = hf_map_dset(dset,map_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "            if ddict_rest is not None:\n",
    "                ddict_rest = hf_map_dset(ddict_rest,map_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "            self.verboseprint('Done')\n",
    "        return dset if ddict_rest is None else (dset,ddict_rest)\n",
    "    \n",
    "    def _do_label_transformation(self):\n",
    "        if len(self.label_names)==0 or len(self.label_tfm_dict)==0: return\n",
    "        print_msg('Label Transformation',20,verbose=self.verbose)\n",
    "        for f,tfm in self.label_tfm_dict.items():\n",
    "            if f in self.label_names:\n",
    "                _func = partial(lambda_map_batch,\n",
    "                                feature=f,\n",
    "                                func=tfm,\n",
    "                                is_batched=self.is_batched\n",
    "                               )                \n",
    "                self.dset = hf_map_dset(self.dset,_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "                if self.val_key is not None:\n",
    "                    self.ddict_rest[self.val_key] = hf_map_dset(self.ddict_rest[self.val_key],\n",
    "                                                                _func,\n",
    "                                                                self.is_batched,\n",
    "                                                                self.batch_size,\n",
    "                                                                self.num_proc)\n",
    "        self.verboseprint('Done')\n",
    "        \n",
    "    def _create_label_mapping_func(self,encoder_classes):\n",
    "        if self.is_multihead:\n",
    "            label2idxs = [{v:i for i,v in enumerate(l_classes)} for l_classes in encoder_classes]\n",
    "            _func = lambda inp: {'label': [[label2idxs[i][v] if len(label2idxs[i]) else v for i,v in enumerate(vs)] \\\n",
    "                                           for vs in zip(*[inp[l] for l in self.label_names])] if self.is_batched \\\n",
    "                                 else [label2idxs[i][v] if len(label2idxs[i]) else v for i,v in enumerate([inp[l] for l in self.label_names])]\n",
    "                                }\n",
    "            \n",
    "        else: # single-head\n",
    "            if self.sup_types[0]=='regression':\n",
    "                _func1 = lambda x: x\n",
    "            else:\n",
    "                label2idx = {v:i for i,v in enumerate(encoder_classes[0])}\n",
    "                _func1 = lambda x: label2idx[x]\n",
    "                \n",
    "            _func = partial(lambda_map_batch,\n",
    "                           feature=self.label_names[0],\n",
    "                           func=_func1,\n",
    "                           output_feature='label',\n",
    "                           is_batched=self.is_batched)\n",
    "        return _func\n",
    "        \n",
    "    def _encode_labels(self):\n",
    "        if len(self.label_names)==0: return\n",
    "        print_msg('Label Encoding',verbose=self.verbose)\n",
    "        \n",
    "        if len(self.label_lists) and not isinstance(self.label_lists[0],list):\n",
    "            self.label_lists = [self.label_lists]\n",
    "                    \n",
    "        encoder_classes=[]\n",
    "        if not self.is_multilabel:\n",
    "            for idx,l in enumerate(self.label_names):\n",
    "                if self.sup_types[idx]=='regression':\n",
    "                    l_classes=[]\n",
    "                else:\n",
    "                    # classification\n",
    "                    if len(self.label_lists)==0:\n",
    "                        l_encoder = LabelEncoder()\n",
    "                        _ = l_encoder.fit(self.dset[l])\n",
    "                        l_classes = list(l_encoder.classes_)\n",
    "                    else:\n",
    "                        l_classes = sorted(list(self.label_lists[idx]))\n",
    "                encoder_classes.append(l_classes)\n",
    "            \n",
    "            _func = self._create_label_mapping_func(encoder_classes)\n",
    "                \n",
    "            self.dset = hf_map_dset(self.dset,_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "            if self.val_key is not None:\n",
    "                self.ddict_rest[self.val_key] = hf_map_dset(self.ddict_rest[self.val_key],_func,\n",
    "                                                       self.is_batched,self.batch_size,self.num_proc)\n",
    "                    \n",
    "        else:\n",
    "            # For MultiLabel, we transform the label itself to one-hot (or actually, few-hot)\n",
    "            if len(self.label_lists)==0:\n",
    "                l_encoder = MultiLabelBinarizer()\n",
    "                _ = l_encoder.fit(self.dset[self.label_names[0]])\n",
    "                l_classes = list(l_encoder.classes_)\n",
    "            else:\n",
    "                l_classes = sorted(list(self.label_lists[0]))\n",
    "            \n",
    "            encoder_classes.append(l_classes)\n",
    "            \n",
    "            l_encoder = MultiLabelBinarizer(classes=encoder_classes[0])\n",
    "            _ = l_encoder.fit(None)\n",
    "            _func = partial(lambda_map_batch,\n",
    "                            feature=self.label_names[0],\n",
    "                            func=lambda x: l_encoder.transform(x),\n",
    "                            output_feature='label',\n",
    "                            is_batched=self.is_batched,\n",
    "                            is_func_batched=True)\n",
    "            self.dset = hf_map_dset(self.dset,_func,self.is_batched,self.batch_size,self.num_proc)                                                 \n",
    "            \n",
    "            if self.val_key is not None:\n",
    "                self.ddict_rest[self.val_key] = hf_map_dset(self.ddict_rest[self.val_key],_func,\n",
    "                                                       self.is_batched,self.batch_size,self.num_proc)\n",
    "            \n",
    "        self.label_lists = encoder_classes\n",
    "        self.verboseprint('Done')\n",
    "        \n",
    "    \n",
    "    def _train_test_split(self):\n",
    "        print_msg('Train Test Split',20,verbose=self.verbose)\n",
    "        if self.val_key is not None: # val split exists\n",
    "            self.verboseprint('Validation split already exists')\n",
    "            self.main_ddict=DatasetDict({'train':self.dset,\n",
    "                                         'validation':self.ddict_rest.pop(self.val_key)})\n",
    "            \n",
    "    \n",
    "        elif self.val_ratio is None: # use all data\n",
    "            self.verboseprint('No validation split defined')\n",
    "            self.main_ddict=DatasetDict({'train':self.dset})\n",
    "            \n",
    "        elif (isinstance(self.val_ratio,float) or isinstance(self.val_ratio,int)) and len(self.stratify_cols)==0:\n",
    "            self.verboseprint('Validation split based on val_ratio')\n",
    "            # train val split\n",
    "            self.main_ddict = self.dset.train_test_split(test_size=self.val_ratio,shuffle=True,seed=self.seed)\n",
    "            self.main_ddict['validation']=self.main_ddict['test']\n",
    "            del self.main_ddict['test']\n",
    "        \n",
    "        else: # val_ratio split with stratifying             \n",
    "            if self.is_multilabel and self.label_names[0] in self.stratify_cols:\n",
    "                raise ValueError('For MultiLabel classification, you cannot choose the label as your stratified column')\n",
    "            self.verboseprint('Validation split based on val_ratio, with stratifying')\n",
    "            # Create a new feature 'stratified', which is a concatenation of values in stratify_cols\n",
    "            if self.is_batched:\n",
    "                stratified_creation = lambda x: {'stratified':\n",
    "                                     ['_'.join(list(map(str,[x[v][i] for v in self.stratify_cols]))) \n",
    "                                      for i in range(len(x[self.stratify_cols[0]]))]}\n",
    "            else:\n",
    "                stratified_creation = lambda x: {'stratified':\n",
    "                                     '_'.join(list(map(str,[x[v] for v in self.stratify_cols]))) \n",
    "                                      }\n",
    "            self.dset = self.dset.map(stratified_creation,\n",
    "                                      batched=self.is_batched,\n",
    "                                      batch_size=self.batch_size,\n",
    "                                      num_proc=self.num_proc)\n",
    "            \n",
    "            self.dset=self.dset.class_encode_column(\"stratified\")\n",
    "            # train val split\n",
    "            self.main_ddict = self.dset.train_test_split(test_size=self.val_ratio,\n",
    "                                                         shuffle=True,seed=self.seed,\n",
    "                                                        stratify_by_column='stratified')\n",
    "            self.main_ddict['validation']=self.main_ddict['test']\n",
    "            del self.main_ddict['test']\n",
    "            self.main_ddict=self.main_ddict.remove_columns(['stratified'])\n",
    "            \n",
    "        \n",
    "        del self.dset\n",
    "        self.verboseprint('Done')\n",
    "    \n",
    "    def _simplify_ddict(self):\n",
    "        print_msg('Dropping unused features',20,verbose=self.verbose)\n",
    "        if self.cols_to_keep is None:\n",
    "            self.cols_to_keep= [self.main_text] + self.metadatas + self.label_names\n",
    "            \n",
    "        cols_to_remove = set(self.all_cols) - set(self.cols_to_keep)\n",
    "        self.main_ddict['train']=self.main_ddict['train'].remove_columns(list(cols_to_remove))\n",
    "        if 'validation' in self.main_ddict.keys():\n",
    "            self.main_ddict['validation']=self.main_ddict['validation'].remove_columns(list(cols_to_remove))\n",
    "        self.verboseprint('Done')\n",
    "    \n",
    "    def _do_transformation(self,dset,ddict_rest=None):\n",
    "        if len(self.content_tfms):\n",
    "            print_msg('Text Transformation',20,verbose=self.verbose)\n",
    "            for tfm in self.content_tfms:\n",
    "                print_msg(callable_name(tfm),verbose=self.verbose)\n",
    "                _func = partial(lambda_map_batch,\n",
    "                               feature=self.main_text,\n",
    "                               func=tfm,\n",
    "                               is_batched=self.is_batched)\n",
    "                dset = hf_map_dset(dset,_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "                if ddict_rest is not None:\n",
    "                    ddict_rest = hf_map_dset(ddict_rest,_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "            self.verboseprint('Done')\n",
    "        return dset if ddict_rest is None else (dset,ddict_rest)\n",
    " \n",
    "    def _do_filtering(self,dset,ddict_rest=None):\n",
    "        if len(self.filter_dict):\n",
    "            print_msg('Data Filtering',20,verbose=self.verbose)\n",
    "            col_names = get_dset_col_names(dset)\n",
    "            for f,tfm in self.filter_dict.items():\n",
    "                if f in col_names:\n",
    "                    print_msg(f'Do {callable_name(tfm)} on {f}',verbose=self.verbose)\n",
    "                    _func = partial(lambda_batch,\n",
    "                                    feature=f,\n",
    "                                    func=tfm,\n",
    "                                    is_batched=self.is_batched)\n",
    "                    dset = hf_filter_dset(dset,_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "                if ddict_rest is not None: # assuming ddict_rest has the column to filter, always\n",
    "                    ddict_rest = hf_filter_dset(ddict_rest,_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "            self.verboseprint('Done')\n",
    "        return dset if ddict_rest is None else (dset,ddict_rest)\n",
    "    \n",
    "    def _upsampling(self):\n",
    "        if len(self.upsampling_list):\n",
    "            print_msg('Upsampling data',20,verbose=self.verbose)\n",
    "            results=[]\n",
    "            for f,tfm in self.upsampling_list:\n",
    "                print_msg(f'Do {callable_name(tfm)} on {f}',verbose=self.verbose)\n",
    "                _func = partial(lambda_batch,\n",
    "                                feature=f,\n",
    "                                func=tfm,\n",
    "                                is_batched=self.is_batched)\n",
    "                new_dset = hf_filter_dset(self.main_ddict['train'],_func,self.is_batched,self.batch_size,self.num_proc)\n",
    "                results.append(new_dset)\n",
    "            # slow concatenation for iterable dataset    \n",
    "            self.main_ddict['train'] = concatenate_datasets(results+[self.main_ddict['train']])\n",
    "            self.verboseprint('Done')\n",
    "      \n",
    "    def _do_augmentation(self):\n",
    "        if len(self.aug_tfms):\n",
    "            print_msg('Text Augmentation',20,verbose=self.verbose)\n",
    "            for tfm in self.aug_tfms:\n",
    "                print_msg(callable_name(tfm),verbose=self.verbose)\n",
    "                bs = self.batch_size\n",
    "                is_func_batched=False\n",
    "                num_proc = self.num_proc\n",
    "                is_batched = self.is_batched\n",
    "                if hasattr(tfm, \"run_on_gpu\") and getattr(tfm,'run_on_gpu')==True:\n",
    "                    bs = min(32,self.batch_size) if not hasattr(tfm, \"batch_size\") else getattr(tfm,'batch_size')\n",
    "                    is_func_batched=True\n",
    "                    is_batched=True\n",
    "                    num_proc=1\n",
    "\n",
    "                _func = partial(lambda_map_batch,\n",
    "                               feature=self.main_text,\n",
    "                               func=tfm,\n",
    "                               is_batched=is_batched,\n",
    "                               is_func_batched=is_func_batched\n",
    "                               )\n",
    "                self.main_ddict['train'] = hf_map_dset(self.main_ddict['train'],_func,\n",
    "                                                          is_batched=is_batched,\n",
    "                                                          batch_size=bs,\n",
    "                                                          num_proc=num_proc\n",
    "                                                         )\n",
    "\n",
    "\n",
    "            self.verboseprint('Done')\n",
    "        \n",
    "\n",
    "            \n",
    "    def _do_train_shuffling(self):\n",
    "        print_msg('Shuffling and flattening train set',20,verbose=self.verbose)\n",
    "        self.main_ddict['train'] = self.main_ddict['train'].shuffle(seed=self.seed).flatten_indices(num_proc = self.num_proc)\n",
    "        self.verboseprint('Done')\n",
    "        \n",
    "    def do_all_preprocessing(self,\n",
    "                             shuffle_trn=True # To shuffle the train set before tokenization\n",
    "                            ):\n",
    "        if self._processed_call:\n",
    "            warnings.warn('Your dataset has already been processed. Returning the previous processed DatasetDict...')\n",
    "            return self.main_ddict\n",
    "            \n",
    "        print_msg('Start Main Text Processing',20,verbose=self.verbose)\n",
    "        \n",
    "        # Filtering\n",
    "        self.dset,self.ddict_rest = self._do_filtering(self.dset,self.ddict_rest)\n",
    "        \n",
    "        # Process metadatas\n",
    "        self.dset,self.ddict_rest = self._process_metadatas(self.dset,self.ddict_rest)\n",
    "        \n",
    "        # Label transformation\n",
    "        self._do_label_transformation()\n",
    "        \n",
    "        # Process labels\n",
    "        self._encode_labels()\n",
    "        \n",
    "        # Content transformation\n",
    "        self.dset,self.ddict_rest = self._do_transformation(self.dset,self.ddict_rest)\n",
    "         \n",
    "        # Train Test Split.\n",
    "        ### self.main_ddict is created here\n",
    "        self._train_test_split()\n",
    "        \n",
    "        # Dropping unused columns\n",
    "        self._simplify_ddict()\n",
    "        \n",
    "        # Check validation leaking\n",
    "        self._check_validation_leaking()\n",
    "        \n",
    "        ### The rest of these functions applies only to the train dataset\n",
    "        # Upsampling\n",
    "        self._upsampling()\n",
    "        \n",
    "        # Augmentation\n",
    "        self._do_augmentation()\n",
    "        \n",
    "        # Shuffle train\n",
    "        if shuffle_trn:\n",
    "            self._do_train_shuffling()\n",
    "        \n",
    "        self._processed_call=True\n",
    "        \n",
    "        return self.main_ddict\n",
    "    \n",
    "        \n",
    "    def do_tokenization(self,\n",
    "                        tokenizer, # Tokenizer (preferably from HuggingFace)\n",
    "                        max_length=None, # pad to model's allowed max length (default is max_sequence_length). Use -1 for no padding at all\n",
    "                        trn_size=None, # The number of training data to be tokenized\n",
    "                        tok_num_proc=None, # Number of processes for tokenization\n",
    "                       ):\n",
    "        print_msg('Tokenization',20,verbose=self.verbose)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.tok_num_proc = tok_num_proc if tok_num_proc else self.num_proc\n",
    "        tok_func = partial(tokenize_function,tok=tokenizer,max_length=max_length)\n",
    "        _func = partial(lambda_map_batch,\n",
    "                        feature=self.main_text,\n",
    "                        func=tok_func,\n",
    "                        output_feature=None,\n",
    "                        is_batched=self.is_batched)\n",
    "        \n",
    "        if trn_size is not None:\n",
    "            if isinstance(trn_size,float):\n",
    "                num_shard = int(1/trn_size)\n",
    "            else: # int\n",
    "                trn_len=len(self.main_ddict['train'])\n",
    "                num_shard = trn_len//trn_size\n",
    "            self.main_ddict['train'] = self.main_ddict['train'].shard(num_shard,0)\n",
    "        \n",
    "        for k in self.main_ddict.keys():\n",
    "            self.main_ddict[k] = hf_map_dset(self.main_ddict[k],_func,self.is_batched,self.batch_size,self.tok_num_proc)\n",
    "\n",
    "        self.verboseprint('Done')\n",
    "        return self.main_ddict\n",
    "        \n",
    "    def process_and_tokenize(self,\n",
    "                             tokenizer, # Tokenizer (preferably from HuggingFace)\n",
    "                             max_length=None, # pad to model's allowed max length (default is max_sequence_length)\n",
    "                             trn_size=None, # The number of training data to be tokenized\n",
    "                             tok_num_proc=None, # Number of processes for tokenization\n",
    "                             shuffle_trn=True, # To shuffle the train set before tokenization\n",
    "                            ):\n",
    "        \"\"\"\n",
    "        This will perform `do_all_processing` then `do_tokenization`\n",
    "        \"\"\"\n",
    "        if self.seed:\n",
    "            seed_everything(self.seed) \n",
    "        _ = self.do_all_preprocessing(shuffle_trn)\n",
    "        _ = self.do_tokenization(tokenizer,max_length,trn_size,tok_num_proc)\n",
    "        \n",
    "    \n",
    "    def set_data_collator(self,data_collator):\n",
    "        self.data_collator = data_collator\n",
    "        \n",
    "    \n",
    "    def prepare_test_dataset_from_csv(self,\n",
    "                                      file_path, # path to csv file\n",
    "                                      do_filtering=False # whether to perform data filtering on this test set\n",
    "                                     ):\n",
    "        file_path = Path(file_path)\n",
    "        ds = load_dataset(str(file_path.parent),\n",
    "                          data_files=file_path.name,\n",
    "                          split='train')\n",
    "        return self.prepare_test_dataset(ds,do_filtering)\n",
    "    \n",
    "    def prepare_test_dataset_from_df(self,\n",
    "                                     df, # Pandas Dataframe\n",
    "                                     validate=True, # whether to perform input data validation\n",
    "                                     do_filtering=False # whether to perform data filtering on this test set \n",
    "                                    ):\n",
    "        if validate:\n",
    "            check_input_validation(df)\n",
    "        ds = Dataset.from_pandas(df)\n",
    "        return self.prepare_test_dataset(ds,do_filtering)\n",
    "    \n",
    "    def prepare_test_dataset_from_raws(self,\n",
    "                                       content, # Either a single sentence, list of sentence or a dictionary with keys are metadata columns and values are list\n",
    "                                      ):\n",
    "        if len(self.metadatas) and not isinstance(content,dict):\n",
    "            raise ValueError(f'There is/are metadatas in the preprocessing step. Please include a dictionary including these keys for metadatas: {self.metadatas}, and texture content: {self.main_text}')\n",
    "            \n",
    "        _dic = {self.main_text:[content]} if isinstance(content,str) else content\n",
    "        for k in _dic.keys():\n",
    "            _dic[k] = val2iterable(_dic[k])\n",
    "        \n",
    "        test_dict = Dataset.from_dict(_dic)\n",
    "        \n",
    "        # set num_proc to 1 for small data processing\n",
    "        _tmp1 = self.num_proc\n",
    "        _tmp2 = self.tok_num_proc\n",
    "        self.num_proc=1\n",
    "        self.tok_num_proc=1\n",
    "        results = self.prepare_test_dataset(test_dict,do_filtering=False)\n",
    "        self.num_proc = _tmp1\n",
    "        self.tok_num_proc=_tmp2\n",
    "\n",
    "        return results\n",
    "    \n",
    "    def prepare_test_dataset(self,\n",
    "                             test_dset, # The HuggingFace Dataset as Test set\n",
    "                             do_filtering=False, # whether to perform data filtering on this test set\n",
    "                            ):\n",
    "        test_cols = set(get_dset_col_names(test_dset))\n",
    "        label_names_set = set(self.label_names)\n",
    "        test_cols = test_cols - label_names_set\n",
    "        missing_cols = set(self.cols_to_keep) - label_names_set - test_cols\n",
    "        if len(missing_cols):\n",
    "            raise ValueError(f'Test set does not have these columns required for preprocessings: {missing_cols}')\n",
    "            \n",
    "        print_msg('Start Test Set Transformation',20,verbose=self.verbose)\n",
    "\n",
    "        # Filtering\n",
    "        if do_filtering:\n",
    "            test_dset = self._do_filtering(test_dset)\n",
    "        \n",
    "        # Process metadatas\n",
    "        test_dset = self._process_metadatas(test_dset)\n",
    "        \n",
    "        # Content transformation\n",
    "        test_dset = self._do_transformation(test_dset)\n",
    "        \n",
    "        # Drop unused columns\n",
    "        cols_to_remove = test_cols - set(self.cols_to_keep)\n",
    "        test_dset=test_dset.remove_columns(list(cols_to_remove))\n",
    "        \n",
    "        # Tokenization\n",
    "        print_msg('Tokenization',20,verbose=self.verbose)\n",
    "        _func = partial(lambda_map_batch,\n",
    "                        feature=self.main_text,\n",
    "                        func=partial(tokenize_function,tok=self.tokenizer,max_length=self.max_length),\n",
    "                        output_feature=None,\n",
    "                        is_batched=self.is_batched)\n",
    "        test_dset = hf_map_dset(test_dset,_func,self.is_batched,self.batch_size,self.tok_num_proc)\n",
    "        \n",
    "        self.verboseprint('Done')\n",
    "        return test_dset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L125){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController\n",
       "\n",
       ">      TextDataController (inp, main_text:str, label_names=[], sup_types=[],\n",
       ">                          class_names_predefined=[], filter_dict={},\n",
       ">                          label_tfm_dict={}, metadatas=[], process_metas=True,\n",
       ">                          metas_sep='.', content_transformations=[],\n",
       ">                          val_ratio:int|float|None=0.2, stratify_cols=[],\n",
       ">                          upsampling_list={}, content_augmentations=[],\n",
       ">                          seed=None, batch_size=1024, num_proc=4,\n",
       ">                          cols_to_keep=None, verbose=True)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | HuggingFainpce Dataset or DatasetDict |\n",
       "| main_text | str |  | Name of the main text column |\n",
       "| label_names | list | [] | Names of the label (dependent variable) columns |\n",
       "| sup_types | list | [] | Type of supervised learning for each label name ('classification' or 'regression') |\n",
       "| class_names_predefined | list | [] | List of names associated with the labels (same index order). Use empty list for regression |\n",
       "| filter_dict | dict | {} | A dictionary: {feature: filtering_function_for_that_feature} |\n",
       "| label_tfm_dict | dict | {} | A dictionary: {label_name: transform_function_for_that_label} |\n",
       "| metadatas | list | [] | Names of the metadata columns |\n",
       "| process_metas | bool | True | Whether to do simple text processing on the chosen metadatas |\n",
       "| metas_sep | str | . | Separator, for multiple metadatas concatenation |\n",
       "| content_transformations | list | [] | A list of text transformations |\n",
       "| val_ratio | int \\| float \\| None | 0.2 | Ratio of data for validation set |\n",
       "| stratify_cols | list | [] | Column(s) needed to do stratified shuffle split |\n",
       "| upsampling_list | dict | {} | A list of tuple. Each tuple: (feature,upsampling_function_based_on_the_feature) |\n",
       "| content_augmentations | list | [] | A list of text augmentations |\n",
       "| seed | NoneType | None | Random seed |\n",
       "| batch_size | int | 1024 | CPU batch size |\n",
       "| num_proc | int | 4 | Number of processes for multiprocessing |\n",
       "| cols_to_keep | NoneType | None | Columns to keep after all processings |\n",
       "| verbose | bool | True | Whether to prdint processing information |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L125){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController\n",
       "\n",
       ">      TextDataController (inp, main_text:str, label_names=[], sup_types=[],\n",
       ">                          class_names_predefined=[], filter_dict={},\n",
       ">                          label_tfm_dict={}, metadatas=[], process_metas=True,\n",
       ">                          metas_sep='.', content_transformations=[],\n",
       ">                          val_ratio:int|float|None=0.2, stratify_cols=[],\n",
       ">                          upsampling_list={}, content_augmentations=[],\n",
       ">                          seed=None, batch_size=1024, num_proc=4,\n",
       ">                          cols_to_keep=None, verbose=True)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | HuggingFainpce Dataset or DatasetDict |\n",
       "| main_text | str |  | Name of the main text column |\n",
       "| label_names | list | [] | Names of the label (dependent variable) columns |\n",
       "| sup_types | list | [] | Type of supervised learning for each label name ('classification' or 'regression') |\n",
       "| class_names_predefined | list | [] | List of names associated with the labels (same index order). Use empty list for regression |\n",
       "| filter_dict | dict | {} | A dictionary: {feature: filtering_function_for_that_feature} |\n",
       "| label_tfm_dict | dict | {} | A dictionary: {label_name: transform_function_for_that_label} |\n",
       "| metadatas | list | [] | Names of the metadata columns |\n",
       "| process_metas | bool | True | Whether to do simple text processing on the chosen metadatas |\n",
       "| metas_sep | str | . | Separator, for multiple metadatas concatenation |\n",
       "| content_transformations | list | [] | A list of text transformations |\n",
       "| val_ratio | int \\| float \\| None | 0.2 | Ratio of data for validation set |\n",
       "| stratify_cols | list | [] | Column(s) needed to do stratified shuffle split |\n",
       "| upsampling_list | dict | {} | A list of tuple. Each tuple: (feature,upsampling_function_based_on_the_feature) |\n",
       "| content_augmentations | list | [] | A list of text augmentations |\n",
       "| seed | NoneType | None | Random seed |\n",
       "| batch_size | int | 1024 | CPU batch size |\n",
       "| num_proc | int | 4 | Number of processes for multiprocessing |\n",
       "| cols_to_keep | NoneType | None | Columns to keep after all processings |\n",
       "| verbose | bool | True | Whether to prdint processing information |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataController)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L565){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.do_all_preprocessing\n",
       "\n",
       ">      TextDataController.do_all_preprocessing (shuffle_trn=True)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| shuffle_trn | bool | True | To shuffle the train set before tokenization |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L565){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.do_all_preprocessing\n",
       "\n",
       ">      TextDataController.do_all_preprocessing (shuffle_trn=True)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| shuffle_trn | bool | True | To shuffle the train set before tokenization |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataController.do_all_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L615){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.do_tokenization\n",
       "\n",
       ">      TextDataController.do_tokenization (tokenizer, max_length=None,\n",
       ">                                          trn_size=None, tok_num_proc=None)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| max_length | NoneType | None | pad to model's allowed max length (default is max_sequence_length). Use -1 for no padding at all |\n",
       "| trn_size | NoneType | None | The number of training data to be tokenized |\n",
       "| tok_num_proc | NoneType | None | Number of processes for tokenization |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L615){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.do_tokenization\n",
       "\n",
       ">      TextDataController.do_tokenization (tokenizer, max_length=None,\n",
       ">                                          trn_size=None, tok_num_proc=None)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| max_length | NoneType | None | pad to model's allowed max length (default is max_sequence_length). Use -1 for no padding at all |\n",
       "| trn_size | NoneType | None | The number of training data to be tokenized |\n",
       "| tok_num_proc | NoneType | None | Number of processes for tokenization |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataController.do_tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L646){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.process_and_tokenize\n",
       "\n",
       ">      TextDataController.process_and_tokenize (tokenizer, max_length=None,\n",
       ">                                               trn_size=None,\n",
       ">                                               tok_num_proc=None,\n",
       ">                                               shuffle_trn=True)\n",
       "\n",
       "This will perform `do_all_processing` then `do_tokenization`\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| max_length | NoneType | None | pad to model's allowed max length (default is max_sequence_length) |\n",
       "| trn_size | NoneType | None | The number of training data to be tokenized |\n",
       "| tok_num_proc | NoneType | None | Number of processes for tokenization |\n",
       "| shuffle_trn | bool | True | To shuffle the train set before tokenization |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L646){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.process_and_tokenize\n",
       "\n",
       ">      TextDataController.process_and_tokenize (tokenizer, max_length=None,\n",
       ">                                               trn_size=None,\n",
       ">                                               tok_num_proc=None,\n",
       ">                                               shuffle_trn=True)\n",
       "\n",
       "This will perform `do_all_processing` then `do_tokenization`\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| max_length | NoneType | None | pad to model's allowed max length (default is max_sequence_length) |\n",
       "| trn_size | NoneType | None | The number of training data to be tokenized |\n",
       "| tok_num_proc | NoneType | None | Number of processes for tokenization |\n",
       "| shuffle_trn | bool | True | To shuffle the train set before tokenization |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataController.process_and_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data + Basic use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L199){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.from_csv\n",
       "\n",
       ">      TextDataController.from_csv (file_path, **kwargs)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L199){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.from_csv\n",
       "\n",
       ">      TextDataController.from_csv (file_path, **kwargs)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataController.from_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L208){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.from_df\n",
       "\n",
       ">      TextDataController.from_df (df, validate=True, **kwargs)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L208){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.from_df\n",
       "\n",
       ">      TextDataController.from_df (df, validate=True, **kwargs)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataController.from_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a `TextDataController` from a csv, pandas DataFrame, or directly from a HuggingFace dataset object. Currently, `TextDataController` is designed for text classification and text regression, as we will explore in this documentation\n",
    "\n",
    "We will load a sample data to prepare for a classification task: which `Department Name` a comment (`Review Text`) belongs to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset source: https://www.kaggle.com/datasets/kavita5/review_ecommerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23486, 10)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14541</th>\n",
       "      <td>876</td>\n",
       "      <td>43</td>\n",
       "      <td>Beautiful shirt, delicate care required</td>\n",
       "      <td>I love this shirt and have received compliment...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9008</th>\n",
       "      <td>872</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The shirt looks so much more flattering on the...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8386</th>\n",
       "      <td>1092</td>\n",
       "      <td>57</td>\n",
       "      <td>Sweet little dress</td>\n",
       "      <td>I was not so sure i would like this dress as i...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11518</th>\n",
       "      <td>1066</td>\n",
       "      <td>21</td>\n",
       "      <td>Beautiful pant</td>\n",
       "      <td>Great structure</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10251</th>\n",
       "      <td>592</td>\n",
       "      <td>34</td>\n",
       "      <td>Love and hate</td>\n",
       "      <td>I was excited when i saw these shorts in my lo...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>General</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Shorts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Clothing ID  Age                                    Title   \n",
       "14541          876   43  Beautiful shirt, delicate care required  \\\n",
       "9008           872   26                                      NaN   \n",
       "8386          1092   57                       Sweet little dress   \n",
       "11518         1066   21                           Beautiful pant   \n",
       "10251          592   34                            Love and hate   \n",
       "\n",
       "                                             Review Text  Rating   \n",
       "14541  I love this shirt and have received compliment...       4  \\\n",
       "9008   The shirt looks so much more flattering on the...       5   \n",
       "8386   I was not so sure i would like this dress as i...       5   \n",
       "11518                                    Great structure       5   \n",
       "10251  I was excited when i saw these shorts in my lo...       4   \n",
       "\n",
       "       Recommended IND  Positive Feedback Count   Division Name   \n",
       "14541                1                        2  General Petite  \\\n",
       "9008                 1                        0         General   \n",
       "8386                 1                        1         General   \n",
       "11518                1                        0  General Petite   \n",
       "10251                1                        2         General   \n",
       "\n",
       "      Department Name Class Name  \n",
       "14541            Tops      Knits  \n",
       "9008             Tops      Knits  \n",
       "8386          Dresses    Dresses  \n",
       "11518         Bottoms      Pants  \n",
       "10251         Bottoms     Shorts  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a `TextDataController` from a dataframe. This also provides a quick input validation check (NaN check and Duplication check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Input Validation Precheck -\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 sup_types='classification',\n",
    "                                 label_names='Department Name',\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create a `TextDataController` directly from the csv file. The good thing about using HuggingFace Dataset as the main backend of the TextDataController is that you can utilize lots of its useful functionality, such as caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc = TextDataController.from_csv('sample_data/Womens_Clothing_Reviews.csv',\n",
    "                                  main_text='Review Text',\n",
    "                                  sup_types='classification',\n",
    "                                  label_names='Department Name',\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create a `TextDataController` from a HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'],\n",
       "    num_rows: 23486\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         sup_types='classification',\n",
    "                         label_names='Department Name',\n",
    "                         seed=42\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the \"Input Validation Precheck\" above, we notice that our dataset has missing values in the text field and the label field. For now, let's load the data as a Pandas' DataFrame, perform some cleaning, and create our `TextDataController`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(~df['Review Text'].isna()) & (~df['Department Name'].isna())].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Input Validation Precheck -\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title    2966\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 1 rows\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 sup_types='classification',\n",
    "                                 label_names='Department Name',\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you can start perform 2 important steps on your data\n",
    "\n",
    "1. Text preprocessings, Label Encoding, Train/Validation Split\n",
    "2. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't provided any preprocessings to the `TextDataController`; we will see more on how to use preprocessings (step by step) as we progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c576078a5141f595f8061cc5237b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 1, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c348dbce5e6a41b998c2207afdf084d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7baad305f6aa4304be874a71efe59355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "ddict = tdc.do_all_preprocessing(shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Review Text', 'Department Name', 'label'],\n",
       "        num_rows: 18100\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text', 'Department Name', 'label'],\n",
       "        num_rows: 4526\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our DatasetDict now has two split: train and validation. Note that train split is now IterableDataset, for processing efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Review Text': [\"I think this top would have been amazing if it was in a jersey or cotton club but the gauzey fabric is so wrinkly and so sheer i don't know how i would use this top. i don't mind most of the slightly sheer retailer tops (if people see my bra slightly so be it) but this is really sheer, unless you are madonna you are going to want to wear something under this, and in my mind, that defeats the purpose of a lightweight, sleeveless top. i returned it and was sorry to have to do that.\",\n",
       "  'This dress is gorgeous. it is better in person than online. i love the detail of the pleats and the waist. it is light weight and perfectly lines and a beautiful dress for twirling with a martini in hand. in fact, my sister tried to steal it from during a recent trip to europe. it is that adorable and flattering - highly recommend!',\n",
       "  'I was able to try this one on at the store, and it was so loose on me, i assumed it was a large, but no, it was a small! (i usually wear size 6-8 in dress size, s/m in tops) admittedly, it does pull over the head, with no zippers or elastic to give shape. but it feels like water on the skin and has just the right amount of \"show it,\" plus coverage for wearing around the house, i\\'m keeping it.'],\n",
       " 'Department Name': ['Tops', 'Dresses', 'Intimate'],\n",
       " 'label': [4, 1, 2]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict['train'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Review Text': ['Really comfortable, great for casual event or everyday wearing. best style with jeans.',\n",
       "  'Originally ordered a large but it was huge. the medium fit perfectly. i\\'m 165lbs, 5\\'7\". lovely material and interesting side details. the sides do come up quite high but a camo or tank underneath does the trick. lots of compliments!!',\n",
       "  'I love these jeans! you can dress them up or down.'],\n",
       " 'Department Name': ['Tops', 'Tops', 'Bottoms'],\n",
       " 'label': [4, 4, 0]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict['validation'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start with the tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d11b724b85747d089d61a284a3911ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/18100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f201d860b384af9a8c33c8e9666a5e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "ddict = tdc.do_tokenization(tokenizer,max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 18100\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4526\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 100, 206, 42, 299, 74, 33, 57, 2770, 114, 24, 21, 11, 10, 11458, 50, 13178, 950, 53, 5, 25665, 2158, 219, 10199, 16, 98, 45218, 352, 8, 98, 11708, 939, 218, 75, 216, 141, 939, 74, 304, 42, 299, 4, 939, 218, 75, 1508, 144, 9, 5, 2829, 11708, 6215, 13657, 36, 1594, 82, 192, 127, 11689, 2829, 98, 28, 24, 43, 53, 42, 16, 269, 11708, 6, 3867, 47, 32, 7758, 19864, 47, 32, 164, 7, 236, 7, 3568, 402, 223, 42, 6, 8, 11, 127, 1508, 6, 14, 16393, 5, 3508, 9, 10, 13719, 6, 30145, 5536, 3361, 299, 4, 939, 1835, 24, 8, 21, 6661, 7, 33, 7, 109, 14, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(ddict['train'][0]['input_ids'][:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 30327, 3473, 6, 372, 13, 9881, 515, 50, 7476, 2498, 4, 275, 2496, 19, 10844, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(ddict['validation'][0]['input_ids'][:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can combine Text Processing and Tokenization with 1 method call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Input Validation Precheck -\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title    2966\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 1 rows\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 sup_types='classification',\n",
    "                                 label_names='Department Name'\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff90c28a5c8d4352a8d5ef2fcd2fe980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 4, which is 0.02% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef20389b06694560a78ce180cddff97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d61b1669e9041a79c5ef6433c4e5c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18097 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "570b2939ace848e8942e56195e551f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/18097 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4011fa16f54b4d5b80749882947b2ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the DatasetDict from the instance variable `main_ddict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 18097\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4526\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DatasetDict is ready to be put into any HuggingFace text model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This preprocessing step allow you to filter out certain values of a certain column in your dataset. Let's say I want to filter out any None value in the column 'Review Text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Clothing ID                   0\n",
       "Age                           0\n",
       "Title                      2966\n",
       "Review Text                   0\n",
       "Rating                        0\n",
       "Recommended IND               0\n",
       "Positive Feedback Count       0\n",
       "Division Name                13\n",
       "Department Name              13\n",
       "Class Name                   13\n",
       "dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\n",
    "df[(~df['Review Text'].isna())].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will provide a dictionary containing the name of the column and the filtering function to apply on that column. Note that **the filtering function will receive an item from the column, and the function should return a boolean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Input Validation Precheck -\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 sup_types='classification',\n",
    "                                 label_names='Department Name',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                 seed=42\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2673fdf185fc4823828247e50730fb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/23486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e83ac385fc458e91ed44ff96885acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 1, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d374a7778df4beab8b6896b7a4614d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b1c3ca7ad649e18d8cbd525a94cfa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18111 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8054ebdfb59b49c2ad88e1917a6f8199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/18111 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22227033a90a4588aeec154b4877ce35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 18111\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4529\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we have filtered out all NaN/None value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tdc.main_ddict['train']['Review Text']:\n",
    "    assert i is not None\n",
    "for i in tdc.main_ddict['validation']['Review Text']:\n",
    "    assert i is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even add multiple filtering functions. Remember from our precheck, there are also None values in our label 'Department Name'. While we are at it, let's filter out any rating that is less than 3 (just to showcase what our filtering can do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rating\n",
       "5    13131\n",
       "4     5077\n",
       "3     2871\n",
       "2     1565\n",
       "1      842\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Rating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `TextDataController` will only keep the text, the labels and the metadatas columns; any other column will be dropped. To keep the 'Rating', we need to define the `cols_to_keep` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Input Validation Precheck -\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\n",
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 sup_types='classification',\n",
    "                                 label_names='Department Name',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                              'Rating': lambda x: x>=3\n",
    "                                             },\n",
    "                                 cols_to_keep=['Review Text','Rating','Department Name'],\n",
    "                                 seed=42\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d57ccf7e437455c9219befc3107bde8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/23486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73af6bf4f824429aa0656a2cd002487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/22641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Rating -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ecd2196c59746939c47708f5de241b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f817975a495c46c1a04caf442e5a2ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/20258 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 1, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ba42b5143546d591217a4b1a6c8eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/16206 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa0e65d15714a60960394ded396a0ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/16205 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d6a447f304422aa5a8a9bcf5592e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/16205 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b445d288b604c179f458e17aa582629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4052 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tdc.main_ddict['train']['Department Name']:\n",
    "    assert i is not None\n",
    "\n",
    "for i in tdc.main_ddict['validation']['Department Name']:\n",
    "    assert i is not None\n",
    "    \n",
    "for i in tdc.main_ddict['validation']['Rating']:\n",
    "    assert i >= 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a sample from training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only want to extract a training sample of your data, you can use the `trn_size` argument of the method `process_and_tokenize` (or `do_tokenization`). Since we use [sharding](https://huggingface.co/docs/datasets/process#shard) to extract a sample from a DatasetDict, if `trn_size` is a integer, an approximated size will be returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "----- Do <lambda> on Department Name -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 3, which is 0.02% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f67844dbcfb24557b425228e5a39e52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e57feac589c48ebb52fdfb4165f4c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2fb1206764471cba7e9df8cf6c9dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1006 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00549d09894043c181bc8904bf2ceb2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         sup_types='classification',\n",
    "                         label_names='Department Name',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True,trn_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1006\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4526\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "----- Do <lambda> on Department Name -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 3, which is 0.02% of training set\n",
      "Filtering leaked data out of training set...\n",
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec00324997c84335a083b0363c87003b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1810 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         sup_types='classification',\n",
    "                         label_names='Department Name',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True,trn_size=0.1) # return 10% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1810\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4526\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadatas concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we think metadatas can be helpful, we can concatenate them into the front of your text, so that our text classification model is aware of it.\n",
    "\n",
    "In this example, Let's add 'Title' as our metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Input Validation Precheck -\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\n",
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 sup_types='classification',\n",
    "                                 label_names='Department Name',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                             },\n",
    "                                 metadatas='Title',\n",
    "                                 process_metas=True, # to preprocess the metadata (currently it's just empty space stripping and lowercasing),\n",
    "                                 seed=42\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6429aa7a64046c587f3aea291b99161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/23486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbac5996b73b44fe8521c4fab2b538c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/22641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6936a5a0e3d04cdb9d01274911a1e18e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43e7d78391445a7867365b108ea2b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 1, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72872a43828945a79a1f96266935af79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "766cbd236b2841c8b9bc3a0dba78f59c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6dbb3f3f6d4130b97fd20dff2ab7e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/18101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e993ea8189a84f5eaf472b4c2b0393ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beautiful! . I love this top. it was everything i hoped it would be. it is lined so it is not see through in the chest/back; sleeves are sheer. soft. gorgeous color. love the layers. runs large so definitely size down. i am usually a m and ordered the s. i\\'m 5\\'8\" curvy 32dd',\n",
       " 'very flattering . This dress fits to a t! true to size. very flattering. fabric is soft and comfortable.',\n",
       " \"the worst . I don't typically write bad reviews, but this dress is so bad and i want to save someone else from buying it. i read the mostly bad reviews and still purchased anyway (my fault i know). the dress is super stiff ( i know denim can be that way and it is possible it would soften up after a few washes). i'm typically a 6/8 and the size small swallowed me, and the xs was big everywhere except through the bust (i ordered both sizes to try). i wouldn't recommend buying this if you are a size 8 or small\",\n",
       " \"love this jacket! . I was on the lookout for a denim jacket when i found this beauty on line. i fell in love immediately and didn't think twice about paying full price. i wear it with moss green chinos and it looks really good. the little dots in the jacket are actually a pale green, which gives it extra character. very well made. i was a bit skeptical about the hook and eye fastenings, but they are very secure. \\r\\n\\r\\ni ordered my usual xl and found it roomy enough in the bust and arms. i would definitely call it tru\",\n",
       " 'great spring/summer dress. . I am excited for spring so i can wear this. i purchased the orange. it is actually more of a red, but i like it. colorful and flattering fit.']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['train']['Review Text'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' . Such a fun jacket! great to wear in the spring or to the office as an alternative to a formal blazer. very comfortable!',\n",
       " 'simple and elegant . I thought this shirt was really feminine and elegant. only downsides is some of the punched out holes had fabric still attached which you have cut off with scissors- otherwise the shirt will snag. and the second issue of bigger importance are the low armholes. lots of bra showing- not really sure how to get around that so i always wear it with a cardigan. but it would be nice not to have to. \\r\\nother than that it looks nice and pairs nicely with almost anything.',\n",
       " 'retro and pretty . This top has a bit of a retro flare but so adorable on. looks really cute with a pair of faded boot cut jeans.',\n",
       " 'summer/fall wear . I first spotted this on an retailer employee, she paired it with a peasant top & wore it open w/jeans & boots- so darn cute. love how this peice transitions from summer to fall. i\\'m 5\\'4\" so i had to order the small petite which is perfect. note that this dress is very long! it\\'s just a must have garment. the colors/ print are just beautiful.',\n",
       " \"perfect except slip . This is my new favorite dress! my only complaint is the slip is too small and the dress cannot be worn without it. i can't order a size up as the dress would then be huge. not sure what the solution is but the dress itself is stunning.\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['validation']['Review Text'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add multiple metadatas. Let's say 'Division Name' is the second metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Input Validation Precheck -\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n",
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba5f590d62c41a08626774aae44c090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/23486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee9ad20b32184851b90c1bf5d1ee3e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/22641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade2089a8691417384ed1623f9c8c743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936162a32235498e96dbaba332ee5d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 0, which is 0.00% of training set\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6130978bb2b44547bb44a33dd05e663e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7188896cf5e94dc4a69321e5b53f766b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b487a39e8ef492e833334cd720a177f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\n",
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 sup_types='classification',\n",
    "                                 label_names='Department Name',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                             },\n",
    "                                 metadatas=['Title','Division Name'],\n",
    "                                 process_metas=True,\n",
    "                                 seed=42\n",
    "                                )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"general petite . meh . This tunic is way over priced for the style and quality. it fit comfortably (runs a size larger) but it's not really flattering, it jut kind of hangs there looking ok. it is a little too deep of a v cut for a work top as well. this top does not support the price at all. it felt like something i could find at department store for way less. i will be returning it.\",\n",
       " 'general . awesome buy! . I am so happy i took a chance on this jumpsuit! i am post-baby (six weeks) and although i intend on slimming down more i would say that it is flattering even at my current size. and it will only get better! \\r\\nthe quality and color are great!',\n",
       " 'general . warm grey . These are a lovely neutral to slightly warm grey pair of jeans from a great line. i wore my usual size without issues.',\n",
       " \"general . loved it, but it didn't work for me. . I wanted this top to work so bad. unfortunately the way the bust of the top is designed it isn't flattering if you aren't flat chested. it squishes on side of your chest and leaves the other side alone. i'm a b cup and had this problem so if you are a b or larger, i don't recommend. however, if you are smaller busted, this piece would be worth the purchase.\",\n",
       " \"general . varying feelings and opinions . As you can see, there is an array of differing opinions on here, and i share sentiments on both:\\r\\n_______\\r\\npros:\\r\\n- the texture and feel of this is great; it is very comfortable and is different.\\r\\n- tts for the most part; i normally can wear sizes 10 and 12 (m and l) with most retailer and got the medium and the fit was overall fine but more snug at the hips. if you're more slim/straight, it'll probably fit you like on the model. \\r\\n- good length, not too short or too long.\\r\\n- the mock collar is ni\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['train']['Review Text'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['general petite .  . Such a fun jacket! great to wear in the spring or to the office as an alternative to a formal blazer. very comfortable!',\n",
       " 'general petite . simple and elegant . I thought this shirt was really feminine and elegant. only downsides is some of the punched out holes had fabric still attached which you have cut off with scissors- otherwise the shirt will snag. and the second issue of bigger importance are the low armholes. lots of bra showing- not really sure how to get around that so i always wear it with a cardigan. but it would be nice not to have to. \\r\\nother than that it looks nice and pairs nicely with almost anything.',\n",
       " 'general . retro and pretty . This top has a bit of a retro flare but so adorable on. looks really cute with a pair of faded boot cut jeans.',\n",
       " 'general petite . summer/fall wear . I first spotted this on an retailer employee, she paired it with a peasant top & wore it open w/jeans & boots- so darn cute. love how this peice transitions from summer to fall. i\\'m 5\\'4\" so i had to order the small petite which is perfect. note that this dress is very long! it\\'s just a must have garment. the colors/ print are just beautiful.',\n",
       " \"general petite . perfect except slip . This is my new favorite dress! my only complaint is the slip is too small and the dress cannot be worn without it. i can't order a size up as the dress would then be huge. not sure what the solution is but the dress itself is stunning.\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['validation']['Review Text'][:5] # The metadata for this text is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-head prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have briefly gone through the simplest case of label encoding, where we only need to predict 1 single label. We call this **single head classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "----- Do <lambda> on Department Name -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 3, which is 0.02% of training set\n",
      "Filtering leaked data out of training set...\n",
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b8e372e6ef468fb16400c7fe13bea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',                         \n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All label names will be saved in instance variable `label_lists`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.label_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and all labels will be encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 4, 1, 1]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['validation']['label'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also keep the original labeling, for references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Intimate', 'Tops', 'Tops', 'Dresses', 'Dresses']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['validation']['Department Name'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also do **single-head regression**. Let's say we want to predict **Rating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'],\n",
       "    num_rows: 23486\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         sup_types='regression',\n",
    "                         label_names='Rating',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None},\n",
    "                         seed=42,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 1, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8985e4b5c114ac5bf890631101b39c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1faef46f4e8d45558b33d6da245deb8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18111 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f836512176420ba8621fd588aa7a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/18111 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59631dc94d93423f92ad64a40db50efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 1.0, 4.0, 3.0, 5.0]\n",
      "[3.0, 1.0, 4.0, 3.0, 5.0]\n"
     ]
    }
   ],
   "source": [
    "print(tdc.main_ddict['train']['Rating'][:5])\n",
    "print(tdc.main_ddict['train']['label'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.0, 4.0, 3.0, 5.0, 5.0]\n",
      "[5.0, 4.0, 3.0, 5.0, 5.0]\n"
     ]
    }
   ],
   "source": [
    "print(tdc.main_ddict['validation']['Rating'][:5])\n",
    "print(tdc.main_ddict['validation']['label'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we need to predict 2 different labels as once? We call this **multi-head classification/regression**. For example, let's define our dataset so that we need to predict both `Department Name` and `Division Name` (both as classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "----- Do <lambda> on Department Name -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 3, which is 0.02% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eba3d439c6d439ba0e06b28f071b59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a7674978a14f9ea4523f5cc2f508a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11bbf2c8d7743828ad846a3ec708499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a148c364ac0d4b00859d4a6a772d0c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names=['Division Name','Department Name'],\n",
    "                         sup_types=['classification','classification'],\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['General', 'General Petite', 'Initmates'],\n",
       " ['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.label_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have two lists, one for label names of `Division Name`, and one for label names of `Department Name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 18099\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4526\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['General Petite', 'General Petite', 'General', 'General Petite', 'General Petite']\n",
      "['Intimate', 'Tops', 'Tops', 'Dresses', 'Dresses']\n",
      "[[1, 2], [1, 4], [0, 4], [1, 1], [1, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(tdc.main_ddict['validation']['Division Name'][:5])\n",
    "print(tdc.main_ddict['validation']['Department Name'][:5])\n",
    "print(tdc.main_ddict['validation']['label'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if one label is classification, and another label is regression? We will predict `Department Name` (classification) and `Rating` (regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "----- Do <lambda> on Department Name -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 3, which is 0.02% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11c1b1e6cad4caaacd3ba535a9ee5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd132eaf2ea4413089828d41e0425cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca39d008b1bc4a5282057ab95a53146d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c2c72388ee49d399ffd9330ff8c0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names=['Rating','Department Name'],\n",
    "                         sup_types=['regression','classification'],\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.0, 5.0, 5.0, 5.0, 4.0]\n",
      "['Intimate', 'Tops', 'Tops', 'Dresses', 'Dresses']\n",
      "[[5.0, 2.0], [5.0, 4.0], [5.0, 4.0], [5.0, 1.0], [4.0, 1.0]]\n"
     ]
    }
   ],
   "source": [
    "print(tdc.main_ddict['validation']['Rating'][:5])\n",
    "print(tdc.main_ddict['validation']['Department Name'][:5])\n",
    "print(tdc.main_ddict['validation']['label'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's multi-head, you can define multiple classification/regression labels, as many as you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "----- Do <lambda> on Department Name -----\n",
      "----- Do <lambda> on Division Name -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 3, which is 0.02% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b9f717bf744bbc873fc57ede69b555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ffac94b066489a8047f80d420fbd26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f276b7c3fa5e447db7f8bc678bc1c38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9162b10c3ae74fc1a91a995a1db14ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names=['Division Name','Rating','Department Name'],\n",
    "                         sup_types=['classification','regression','classification'],\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                      'Division Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['General Petite', 'General Petite', 'General', 'General', 'General Petite']\n",
      "[4.0, 4.0, 5.0, 3.0, 5.0]\n",
      "['Tops', 'Tops', 'Tops', 'Tops', 'Dresses']\n",
      "[[1.0, 4.0, 4.0], [1.0, 4.0, 4.0], [0.0, 5.0, 4.0], [0.0, 3.0, 4.0], [1.0, 5.0, 1.0]]\n"
     ]
    }
   ],
   "source": [
    "print(tdc.main_ddict['train']['Division Name'][:5])\n",
    "print(tdc.main_ddict['train']['Rating'][:5])\n",
    "print(tdc.main_ddict['train']['Department Name'][:5])\n",
    "print(tdc.main_ddict['train']['label'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['General Petite', 'General Petite', 'General', 'General Petite', 'General Petite']\n",
      "[5.0, 5.0, 5.0, 5.0, 4.0]\n",
      "['Intimate', 'Tops', 'Tops', 'Dresses', 'Dresses']\n",
      "[[1.0, 5.0, 2.0], [1.0, 5.0, 4.0], [0.0, 5.0, 4.0], [1.0, 5.0, 1.0], [1.0, 4.0, 1.0]]\n"
     ]
    }
   ],
   "source": [
    "print(tdc.main_ddict['validation']['Division Name'][:5])\n",
    "print(tdc.main_ddict['validation']['Rating'][:5])\n",
    "print(tdc.main_ddict['validation']['Department Name'][:5])\n",
    "print(tdc.main_ddict['validation']['label'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-label classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's define a **multi-label classification**, where a text can have 1 or more label. Our data don't have such labeling, so we will make a new one, just for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Intimate', 'Dresses', 'Bottoms', 'Tops', 'Jackets', 'Trend', nan],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Department Name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Fake Label'] = [np.random.choice(df['Department Name'].unique()[:-1],size=np.random.randint(2,6),replace=False) for _ in range(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "      <th>Fake Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>767</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Initmates</td>\n",
       "      <td>Intimate</td>\n",
       "      <td>Intimates</td>\n",
       "      <td>[Bottoms, Intimate, Jackets]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1080</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[Dresses, Jackets, Intimate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[Dresses, Bottoms, Trend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "      <td>[Bottoms, Jackets, Intimate, Tops]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>[Intimate, Jackets, Trend]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clothing ID  Age                    Title   \n",
       "0          767   33                      NaN  \\\n",
       "1         1080   34                      NaN   \n",
       "2         1077   60  Some major design flaws   \n",
       "3         1049   50         My favorite buy!   \n",
       "4          847   47         Flattering shirt   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND   \n",
       "0  Absolutely wonderful - silky and sexy and comf...       4                1  \\\n",
       "1  Love this dress!  it's sooo pretty.  i happene...       5                1   \n",
       "2  I had such high hopes for this dress and reall...       3                0   \n",
       "3  I love, love, love this jumpsuit. it's fun, fl...       5                1   \n",
       "4  This shirt is very flattering to all due to th...       5                1   \n",
       "\n",
       "   Positive Feedback Count   Division Name Department Name Class Name   \n",
       "0                        0       Initmates        Intimate  Intimates  \\\n",
       "1                        4         General         Dresses    Dresses   \n",
       "2                        0         General         Dresses    Dresses   \n",
       "3                        0  General Petite         Bottoms      Pants   \n",
       "4                        6         General            Tops    Blouses   \n",
       "\n",
       "                           Fake Label  \n",
       "0        [Bottoms, Intimate, Jackets]  \n",
       "1        [Dresses, Jackets, Intimate]  \n",
       "2           [Dresses, Bottoms, Trend]  \n",
       "3  [Bottoms, Jackets, Intimate, Tops]  \n",
       "4          [Intimate, Jackets, Trend]  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't have to add any extra argument; the controller will determine whether this is for multilabel classification, based on the format of the label values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Input Validation Precheck -\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "befdb0de6d2e44b6abab578aa87348ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/23486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5582f1118c406a8d127cc18d24bbf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 1, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103c1e8278bc40af90699eacbe8c6499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e34a93b8a9477fbd4765f66132ac99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18111 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2e8d52c63042718d5560ca858ac5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/18111 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac762f51278845c29bb544b24f1cb9c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                 label_names='Fake Label',\n",
    "                                 sup_types='classification',\n",
    "                                 seed=42,\n",
    "                                )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Review Text', 'Fake Label', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 18111\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text', 'Fake Label', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4529\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.label_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jackets', 'Dresses']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['validation']['Fake Label'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is **multilabel classification**, the label will be one-hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1, 0, 0]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['validation']['label'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 0, 1, 0],\n",
       " [1, 1, 1, 0, 1, 0],\n",
       " [0, 1, 0, 1, 0, 0],\n",
       " [1, 1, 0, 1, 0, 1],\n",
       " [1, 1, 0, 1, 1, 1]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['validation']['label'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have a label to define, leave all label-related arguments blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 1, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee562701a8c4966ad608101c100a097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574c40def015401b864ffe451daf5877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18111 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3424e3fc78944e25ab7dbbba395ee068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/18111 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235ac29419e04eb89c47a6439c49506b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None},\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Review Text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 18111\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4529\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, you want to apply some light transformation to your label(s) before apply label encoding, e.g. there are some typos in your string label (classification), or you want to scale your regression label. `TextDataController` provides a way for you to do so, via `label_tfm_dict` argument. For the following example, I will fix the typo 'Initmates' in `Division Name` label, and log scale the `Rating`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "----- Do <lambda> on Department Name -----\n",
      "----- Do <lambda> on Division Name -----\n",
      "Done\n",
      "-------------------- Label Transformation --------------------\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 3, which is 0.02% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3db99acfdb49178e31814e598f7204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49478568bbec4d888f562ce2cfdffa8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b783325585e4e249c6c1c44b0a2c515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af0a37539ce406eb26387eb67b8337c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names=['Division Name','Rating','Department Name'],\n",
    "                         sup_types=['classification','regression','classification'],\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                      'Division Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         label_tfm_dict={'Division Name': lambda x: x if x!='Initmates' else 'Intimates',\n",
    "                                         'Rating': lambda x: math.log(x)+1},\n",
    "                         seed=42,\n",
    "                         num_proc=1\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the label_lists, the label 'Initmates' has been replaced by 'Intimates'\n",
    "\n",
    "Also, the second empty list corresponds to the label value of `Rating`, which is for regression, thus results in an empty list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['General', 'General Petite', 'Intimates'],\n",
       " [],\n",
       " ['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.label_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['General Petite', 'General Petite', 'General', 'General', 'General Petite']\n",
      "[2.386294361119891, 2.386294361119891, 2.6094379124341005, 2.09861228866811, 2.6094379124341005]\n",
      "['Tops', 'Tops', 'Tops', 'Tops', 'Dresses']\n",
      "[[1.0, 2.386294361119891, 4.0], [1.0, 2.386294361119891, 4.0], [0.0, 2.6094379124341005, 4.0], [0.0, 2.09861228866811, 4.0], [1.0, 2.6094379124341005, 1.0]]\n"
     ]
    }
   ],
   "source": [
    "print(tdc.main_ddict['train']['Division Name'][:5])\n",
    "print(tdc.main_ddict['train']['Rating'][:5])\n",
    "print(tdc.main_ddict['train']['Department Name'][:5])\n",
    "print(tdc.main_ddict['train']['label'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['General Petite', 'General Petite', 'General', 'General Petite', 'General Petite']\n",
      "[2.6094379124341005, 2.6094379124341005, 2.6094379124341005, 2.6094379124341005, 2.386294361119891]\n",
      "['Intimate', 'Tops', 'Tops', 'Dresses', 'Dresses']\n",
      "[[1.0, 2.6094379124341005, 2.0], [1.0, 2.6094379124341005, 4.0], [0.0, 2.6094379124341005, 4.0], [1.0, 2.6094379124341005, 1.0], [1.0, 2.386294361119891, 1.0]]\n"
     ]
    }
   ],
   "source": [
    "print(tdc.main_ddict['validation']['Division Name'][:5])\n",
    "print(tdc.main_ddict['validation']['Rating'][:5])\n",
    "print(tdc.main_ddict['validation']['Department Name'][:5])\n",
    "print(tdc.main_ddict['validation']['label'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This processing allows you to **alter the text content in your dataset**. You need to define a function that accepts a single string and returns a new, processed string. Note that this transformation will be applied to ALL of your dataset (both train and validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to normalize our text, because the text might contain some extra spaces between words, or not follow the \"single space after a period\" rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp = \"This is a      sentence,which doesn't follow any rule!No single space is provided after period or punctuation marks.    Maybe there are too many spaces!?!   \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import text_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is a sentence , which doesn't follow any rule ! No single space is provided after period or punctuation marks . Maybe there are too many spaces ! ? !\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_normalize(_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "----- Do <lambda> on Department Name -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Text Transformation --------------------\n",
      "----- text_normalize -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 3, which is 0.02% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f8ee3cf6f149c8bfb6700c04d35fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd5ce84e79854a40bad0d617aa88f192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896cab42382e43ee91ee9537b03d906b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71895a378a143f19c74a183055f240e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         content_transformations=text_normalize,\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This sweater is beautiful , but is definitely more for looks than warmth . it's very soft , but very thin . i prefer the way it looks open rather than buttoned . i got the moss green color on sale , and i am glad i didn't pay full price for it--it ' s lovely , but certainly not worth $ 88 .\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['train']['Review Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Such a fun jacket ! great to wear in the spring or to the office as an alternative to a formal blazer . very comfortable !'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['validation']['Review Text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can chain multiple functions. Let's say after text normalizing, I want to lowercase the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is not lowercase'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str.lower('tHis IS NoT lowerCASE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "----- Do <lambda> on Department Name -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Text Transformation --------------------\n",
      "----- text_normalize -----\n",
      "----- lower -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 3, which is 0.02% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e1858d35f04c5fbbd2f944984ff797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20fcfe22d992445ea8bade6d576abb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d089b401f4454298ade8e3c61d1f46e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a73b10aca2c44b7958ac2330d939b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this sweater is beautiful , but is definitely more for looks than warmth . it's very soft , but very thin . i prefer the way it looks open rather than buttoned . i got the moss green color on sale , and i am glad i didn't pay full price for it--it ' s lovely , but certainly not worth $ 88 .\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['train']['Review Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'such a fun jacket ! great to wear in the spring or to the office as an alternative to a formal blazer . very comfortable !'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['validation']['Review Text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even perform some complex transformations, such as removing text inside parentheses, or replacing some texts via a pattern (which is doable using regular expression). Let's make an example of such transformations, where we remove text inside parentheses, and convert any hashtag into the string 'hashtag'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(s):\n",
    "    # Remove texts inside parentheses\n",
    "    s = re.sub(r'\\(.*?\\)', '', s)\n",
    "    \n",
    "    # Convert any hashtag into the string 'hashtag'\n",
    "    s = re.sub(r'#\\w+', 'hashtag', s)\n",
    "    \n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hashtag There's no way it works , however it surprises me hashtag hashtag\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_text(\"#Promotions There's no way it works (I checked!), however it surprises me #howonearth #mindblowing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "----- Do <lambda> on Department Name -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Text Transformation --------------------\n",
      "----- process_text -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 3, which is 0.02% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9058b08ca8754895a6541ea7286da56b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65002ac24f3d46efb16be8f159095e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a5cb3ba46b4d858e73f11aed32e7ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332b97f123244c5f8d59bf0e9ee82bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         content_transformations=process_text,\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to perform a train/validation split with `TextDataController`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first way is when you already have a validation split in your HuggingFace's Dataset. Let's use the Dataset built-in function `train_test_split` to simulate this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1)\n",
    "# This will create a 'test' split instead of 'validation', so we will process a bit to have a validation split\n",
    "ddict_with_val['validation']=ddict_with_val['test']\n",
    "del ddict_with_val['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'],\n",
       "        num_rows: 21137\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'],\n",
       "        num_rows: 2349\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict_with_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b09f1f473842f0be3b21fcbd3d4c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/21137 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdab0f5e74fa4130b11dfed795ea43af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/2349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21101408a6c5421cbba9de8c706c477e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/20391 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b341f3d44b4450ca6215cca92a3d9b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/2250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a7fb543fda42b89e2ab1efbb426905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/20379 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12700037b3544db86c57ffcbce2798a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2249 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split already exists\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 4, which is 0.02% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c4c1e0acf44db9847c38e658be298f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/20379 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cfe7f28f1684d368ece80d52c90843a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/20374 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0309df12664d41a6a07503f47ad2ea36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/20374 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "251cc722bdab43a0b5df039edb384a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2249 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataController(ddict_with_val,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 20374\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2249\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second way is to split randomly based on a ratio (a float between 0 and 1), or based on the number of data in your validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0319d8cd2443fda2ae59c3609af077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/19233 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70299e3e6d3d4bccb1da889f8a173880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/19231 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35b759c063341dfbb3356b31960a6a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/19231 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77de27c20ae6497db346e4f063261db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/3395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 19231\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 3395\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         val_ratio=0.15,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n",
    "print(tdc.main_ddict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153e328ec8ac4ce58f6361ab5358711d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/17628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1d8aa1508b42a388b76acc5ccd131b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/17624 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d13b4c10b3403280c7d4cca7cbafae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/17624 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58b51d5bb104cbcaca9e89ca13f9bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 17624\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         val_ratio=5000,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n",
    "print(tdc.main_ddict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A third way is to do a random stratified split (inspired by [sklearn's](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)). Let's do a stratified split based on our label 'Department Name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Department Name\n",
       "Tops        0.445978\n",
       "Dresses     0.269214\n",
       "Bottoms     0.161852\n",
       "Intimate    0.073918\n",
       "Jackets     0.043967\n",
       "Trend       0.005070\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Department Name'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Input Validation Precheck -\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n",
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95fb1155b1ab42f78d357662bd0c2721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/23486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2da94637854c54b467c7d521bf3213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/22641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b26c059a51435ebda61d1d03a44a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio, with stratifying\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23cda70ca84a4efa9388090dd175fae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c399fa0298432b9ec794b45215d7d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 2, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9f3789886142fbbc0f1d91facf851b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c596aff3ef324567a17ec541f5143839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b53c17e31fc947b7b307b56bd1231206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/18100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767e92adf2c14e358f0edec71d33924a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                 sup_types='classification',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                             },\n",
    "                                 val_ratio=0.2,\n",
    "                                 stratify_cols='Department Name',\n",
    "                                 seed=42\n",
    "                                )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tops        0.444033\n",
       "Dresses     0.271602\n",
       "Bottoms     0.161878\n",
       "Intimate    0.072983\n",
       "Jackets     0.044309\n",
       "Trend       0.005193\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(tdc.main_ddict['train']['Department Name']).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tops        0.444101\n",
       "Dresses     0.271542\n",
       "Bottoms     0.161732\n",
       "Intimate    0.073133\n",
       "Jackets     0.044189\n",
       "Trend       0.005303\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(tdc.main_ddict['validation']['Department Name']).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use multiple columns for your stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Input Validation Precheck -\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3b6287d6b94577956ec23a0c6bec9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/23486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48478e739dda4a30bd1f6b1723817bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/22641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a012e2aca7743318bd481ed26c61e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b25e5312964c3b997a19aaae30c440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a73c7d89c74ef8a32d85e99a0f2365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb8ac3e9c184dbd9234a07a177ba030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c57a360749d54c698950a87bf95efb89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "509ca12ab54d4e329ef2c777b89ebd16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/18100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6acb521f167e4868a2b2daa0f9152457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 sup_types='classification',\n",
    "                                 label_names='Department Name',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                             },\n",
    "                                 val_ratio=0.2,\n",
    "                                 stratify_cols=['Department Name','Rating'],\n",
    "                                 seed=42,\n",
    "                                 verbose=False\n",
    "                                )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, you can omit any validation split if you specify `val_ratio` as ```None```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Input Validation Precheck -\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n",
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f88626e1ee3145ab9fbdd90efddd6b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/23486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03fe224d6a524b47864614c1fa8f8825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/22641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326e87debe4d448ab1fd6edbb3130a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "No validation split defined\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576bf656ccc74cba8b055d83691a9b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15afe6974e0848bd8b5b7ee825cd4dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 22628\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                 sup_types='classification',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                             },\n",
    "                                 val_ratio=None,\n",
    "                                 seed=42\n",
    "                                )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n",
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is useful when you have an imbalanced dataset and you want to perform some upsampling (oversampling) on the minority class. In `TextDataController`, you can perform upsampling on any column of the original dataset, and you can even do upsampling on multiple columns at once\n",
    "\n",
    "Behind the scene, upsampling contains 2 steps; first, the subset of the data is collected based on the filtering condition, and then this subset is concatenated back into the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Department Name\n",
       "Tops        8360\n",
       "Dresses     5038\n",
       "Bottoms     3047\n",
       "Intimate    1403\n",
       "Jackets      838\n",
       "Trend         92\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Department Name'].sample(frac=0.8).value_counts() \n",
    "# fraction 0.8 because we only do upsampling on train data, which is 80% of the total data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Department Name\n",
       "Tops        0.445734\n",
       "Dresses     0.270263\n",
       "Bottoms     0.159921\n",
       "Intimate    0.073970\n",
       "Jackets     0.045106\n",
       "Trend       0.005006\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Department Name'].sample(frac=0.8).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say I want to upsampling the 'Trend' by the factor of 2 (x2 the amount of 'Trend' data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Input Validation Precheck -\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n",
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038a27eac25f4d0cbc216d904a81484d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/23486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe9bbde0bff476f882cbe20789a04e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/22641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9172ad7e7d924abc982d02e323b7af0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio, with stratifying\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ff09302aa14d089a0cb774e6affa07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1376991fad846168b7041440ea0d49a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 2, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd5d28fd7c2486d9b1d3f312cb0e87f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Upsampling data --------------------\n",
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a30e0f62049464581c304fdeb353a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29acbbd5396845abbd65d7294813f860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18194 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58bc77790c544dbfa9c36a82da54f8dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/18194 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7853347c067c4d89a4b9b91eadc1d476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                 sup_types='classification',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                             },\n",
    "                                 val_ratio=0.2,\n",
    "                                 stratify_cols='Department Name',\n",
    "                                 upsampling_list=[('Department Name',lambda x: x=='Trend')],\n",
    "                                 seed=42\n",
    "                                )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tops        8037\n",
       "Dresses     4916\n",
       "Bottoms     2930\n",
       "Intimate    1321\n",
       "Jackets      802\n",
       "Trend        188\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(tdc.main_ddict['train']['Department Name']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tops        0.441739\n",
       "Dresses     0.270199\n",
       "Bottoms     0.161042\n",
       "Intimate    0.072606\n",
       "Jackets     0.044080\n",
       "Trend       0.010333\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(tdc.main_ddict['train']['Department Name']).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percenntage of 'Trend' data in the train set has approximately doubled (note that we filter some NaN text value so the result is not exactly doubled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tops        0.444101\n",
       "Dresses     0.271542\n",
       "Bottoms     0.161732\n",
       "Intimate    0.073133\n",
       "Jackets     0.044189\n",
       "Trend       0.005303\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(tdc.main_ddict['validation']['Department Name']).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since augmentation is applied only to the train set, the distribution of label in the validation set remains the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can triple the amount of 'Trend' by repeating the procedure twice. In the following examples, I will triple the 'Trend' and double the 'Jackets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Input Validation Precheck -\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n",
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee683fc7c47b4401b471c49e424d0bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/23486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d49f26dcd2473ea0396218af2f6fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/22641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73ea1f300e345d192ebdbf46d70f8b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio, with stratifying\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70869e0731d848478f198845e64b7c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5386421436684e8c85406758acb4705c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 2, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d2cb8a00ce4c78b84b72da64de0826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Upsampling data --------------------\n",
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e821b107f0f4b91a4e6e5357789e124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f618ccf4c519416c89720adbe693741e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982af4ab95654f66a1aa7a318c25fbfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385028cb0398424b95a8d742b5b72443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/19090 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50af584a184f41069ccb2b90a54e80fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/19090 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6776c6300874078b3e0a7c7205bcd57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                 sup_types='classification',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                             },\n",
    "                                 val_ratio=0.2,\n",
    "                                 stratify_cols='Department Name',\n",
    "                                 upsampling_list=[('Department Name',lambda x: x=='Trend'),\n",
    "                                                  ('Department Name',lambda x: x=='Trend'),\n",
    "                                                  ('Department Name',lambda x: x=='Jackets')\n",
    "                                                 ],\n",
    "                                 # This can be simplified as\n",
    "#                                  upsampling_list=[('Department Name',lambda x: x=='Trend' or x=='Jackets'),\n",
    "#                                                   ('Department Name',lambda x: x=='Trend')],\n",
    "                                 seed=42\n",
    "                                )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tops        8037\n",
       "Dresses     4916\n",
       "Bottoms     2930\n",
       "Jackets     1604\n",
       "Intimate    1321\n",
       "Trend        282\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(tdc.main_ddict['train']['Department Name']).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A word of warning**: **Upsampling is a slow procedure**, as it requires multiple dataset concatenation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to Content Transformation, Content Augmentation allows to alter the text content in your dataset. You also need to provide a function accepting a single string, and return a new, processed string. Unlike Content Transformation which is applied to ALL data, the Content Augmentation only applies to your TRAINING data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the popular library for data augmentation is [nlpaug](https://github.com/makcedward/nlpaug). We will demonstrate how to integrate its augmentation functions into our `TextDataController`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.char as nac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp = \"I like my clothes loose fitting but even for me this ran large, i am 5'7 134b and medium fit in the shoulders but was too big overall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_aug(x,aug=None):\n",
    "    results = aug.augment(x)\n",
    "    if not isinstance(x,list): return results[0]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmentation by replacing character with nearby one on the keyboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = nac.KeyboardAug(aug_char_max=3,aug_char_p=0.1,aug_word_p=0.07)\n",
    "nearby_aug_func = partial(nlp_aug,aug=aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I liJe my clothSs loose fitting but even for me this ran large, i am 5 ' 7 134b and medium fit in the shou>ders but was too big overall\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearby_aug_func(_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "----- Do <lambda> on Department Name -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 3, which is 0.02% of training set\n",
      "Filtering leaked data out of training set...\n",
      "Done\n",
      "-------------------- Text Augmentation --------------------\n",
      "----- nlp_aug -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05348f904c86436d9b2ff7f8ed209ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071456cec4324f5296e66c5e0560cf18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e4c90308954e409bfd994e3e7f9741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836fc83ea8de4e1fb6115d665db95d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         content_augmentations=nearby_aug_func,\n",
    "                         seed=42,\n",
    "                         verbose=True\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"This sweater is beautiful, but is definitely more for kooks than warmth. it ' s very soft, but very thin. i prefDr the way it looks open rather than buttoned. i got the moss green col(r on sale, and i am glwd i didn ' t pay full pr8ce for it - - it ' s lovely, but certainly not worth $ 88.\",\n",
       " \"I ' m a curvy person, so my rFview m9ght not be suited to everyone. my standard size in retailer tops is xl, and it is the sahe for this blouse. - overall: overall gorgeous, well made blouse but i wish there was oess fabric involved and the burnt out d$sign didn ' t make a horizontal stripe across the back and biceps. Rhis blouse just might not work out as well if you are a full figured )erson. - pros: Horgeous blouse high quality unique - cons: i wish the burnt out design didn ' t make a hor\",\n",
       " 'This blouse is wonderful. i just got and wore the wine colorRd blouse tpday. i received so many comolimeGts. i love it and with the sale price it is so worth it.',\n",
       " 'When i saw fhis, i ordered immediately thinking it was similar to the popular colorblocked stripe Dweater from last year. the knit is stretchy and textured and feels like great quality (would wash well ), but it \\' s (retty lightweight. the fit is huge. .. could eacily size down. i \\' m 5 \\' 7 \" 128 # and found the smxll to be loose everywhere, insluding the arms. the length was at my knees, and the stripr fell awkwardly across my chest. no idea whah i \\' d wear tGis with even if it fit better. sadly, it \\' s going',\n",
       " \"This dress is a zillion timeE cuter in real life. it ' s very retro - swingy and girlish - it reminds me of something mia farrow would ' ve worh in her rosemary ' s baby era. i have the black version and i ' ve paired mine wi6h tall black gladiator sandals for a more sultry nighttime look and also flip flops for beaVhy summer days. i think it ' s a total steal at the salr pr&ce.\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['train']['Review Text'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, since this is Content Augmentation, the validation set is unmodified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Such a fun jacket! great to wear in the spring or to the office as an alternative to a formal blazer. very comfortable!',\n",
       " 'I thought this shirt was really feminine and elegant. only downsides is some of the punched out holes had fabric still attached which you have cut off with scissors- otherwise the shirt will snag. and the second issue of bigger importance are the low armholes. lots of bra showing- not really sure how to get around that so i always wear it with a cardigan. but it would be nice not to have to. \\r\\nother than that it looks nice and pairs nicely with almost anything.',\n",
       " 'This top has a bit of a retro flare but so adorable on. looks really cute with a pair of faded boot cut jeans.',\n",
       " 'I first spotted this on an retailer employee, she paired it with a peasant top & wore it open w/jeans & boots- so darn cute. love how this peice transitions from summer to fall. i\\'m 5\\'4\" so i had to order the small petite which is perfect. note that this dress is very long! it\\'s just a must have garment. the colors/ print are just beautiful.',\n",
       " \"This is my new favorite dress! my only complaint is the slip is too small and the dress cannot be worn without it. i can't order a size up as the dress would then be huge. not sure what the solution is but the dress itself is stunning.\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['validation']['Review Text'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even apply Content Augmentation stochastically, by adding a random condition in your augmentation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nlp_aug_stochastic(x,aug=None,p=0.5):\n",
    "#     results = aug.augment(x)\n",
    "#     if not isinstance(x,list): return results[0] if random.random()<p else x\n",
    "#     return [a if random.random()<p else b for a,b in zip(results,x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_aug_stochastic(x,aug=None,p=0.5):\n",
    "    if not isinstance(x,list): \n",
    "        if random.random()<p: return aug.augment(x)[0]\n",
    "        return x\n",
    "    news=[]\n",
    "    originals=[]\n",
    "    for _x in x:\n",
    "        if random.random()<p: news.append(_x)\n",
    "        else: originals.append(_x)\n",
    "    # only perform augmentation when needed\n",
    "    if len(news): news = aug.augment(news)\n",
    "    return news+originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = nac.KeyboardAug(aug_char_max=3,aug_char_p=0.1,aug_word_p=0.07)\n",
    "nearby_aug_func = partial(nlp_aug_stochastic,aug=aug,p=0.3) # nearby_augmentation only applies 30% of the time, with p=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "----- Do <lambda> on Department Name -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 3, which is 0.02% of training set\n",
      "Filtering leaked data out of training set...\n",
      "Done\n",
      "-------------------- Text Augmentation --------------------\n",
      "----- nlp_aug_stochastic -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c2d546491b468d8924a1204c3cec1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "825f50983587446aa23e526334e8e041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1545444d9e44fc98315bb210ef0f0a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         content_augmentations=nearby_aug_func,\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"TNis sAeater is beautiful, but is definitely more for looks than warmth. it ' s very soft, but very thin. i prefer the way it looks open rather than buttoned. i got the moss geeen color on sale, and i am g/ad i cidn ' t pay full price for it - - it ' s lovely, but certainly not worth $ 88.\",\n",
       " \"I ' m a curvy person, so my review might not be suitFd to everyone. my standard size in retailer tops is xl, and it is the sake for this blouse. - overall: overall gorgeous, well made blouse but i wish there was less fabric involved and the nurnt out desigG didn ' t make a hoDizontal stripe across the back and biceps. this blouse just might not wo5k out as well if you are a full figured pers8n. - pros: gorgeous blouse high quality unique - cons: i wish the hurnt out design didn ' t make a hor\",\n",
       " 'This blouse is wonderful. i just got and wore the wine colored blouse today. i received so many compliments. i love it and with the sale price it is so worth it.',\n",
       " 'When i saw this, i ordered immediately thinking it was similar to the popular colorblocked stripe sweater from last year. the Mnit is streYchy and textured and feels iike great quality (would wash well ), but it \\' s pretty lightweight. the fit is huge. .. cIuld easily size down. i \\' m 5 \\' 7 \" 128 # and found the sma>l to be loose everywhere, includKng the arms. the length was at my knees, and the stripe fell awkwardly across my chest. no Ldea what i \\' d wear tBis with even if it fit better. sadly, it \\' s Roing',\n",
       " \"This dress is a zillion times cuter in real life. it ' s very retro - swingj and girlish - it Deminds me of something mia farrow would ' ve worn in her rosemary ' s bab7 era. i have the black version and i ' ve paired mine with tall blacU gladiator sandals for a mo3e sultry nighttime look and also flip flops for beachy summer days. i think it ' s a total st4al at the sale price.\",\n",
       " \"This top is so soft and with a henley neck opening and longer ribbed shirttail hems, it not only feels heavenly against the skin but it gives off a casual chic vibe. it is also great for layering under shorter sweaters and sweatshirts to give my staples a little oomph. it is a bit sheer so cami is a must. i am also not sure how well it will hold up after washings, especially since it's priced quite high. i love it so much that i will most probably end up keeping it it is true to size. i ordered\",\n",
       " \"This is my first pair of ag and i love them so far. they are not cuffed as shown in the picture. they are long so i had to get them altered (i'm 5''5). the color is a rich blue and they have a nice stretch. i haven't worn them all day yet to see if they keep their shape. usually a 28 or 29 and went with the 28 on these. got them on 20 perc off sale so very happy!\",\n",
       " 'I liked this coat but my family said it looked too much like something hilary clinton would wear. i am 54 and i think it made me look a bit dowdy since it runs a bit big.',\n",
       " 'I saw a photographer wearing this at a wedding i went to in october. i absolutely fell in love. it is beautiful. i can\\'t wait to wear it for the holidays! i got the small petite and i am 5\\'2\", 125 lbs. fit great. enjoy!',\n",
       " \"This dress was adorable & fit greQt! regrettably, i had to return it since it wasn ' t /ined.\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['train']['Review Text'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the advanced augmentation is \"Contexttual Word Embeddings Augmenter\" (code example: https://github.com/makcedward/nlpaug/blob/master/example/textual_augmenter.ipynb), where you can insert/substitute words using LLM such as BERT, RoBERTA ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as naw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.ContextualWordEmbsAug(model_path='roberta-base', \n",
    "                                device='cuda:0', # if you don't have gpu, change to 'cpu'\n",
    "                                action=\"substitute\",\n",
    "                                top_k=10,\n",
    "                               aug_p=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_aug_func = partial(nlp_aug,aug=aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp = \"I like my clothes loose fitting but even for me this ran large, i am 5'7 134b and medium fit in the shoulders but was too big overall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I kept my clothes slim fitting but even for me this ran large, i am 5'7 134b and medium fit in upper shoulders but was too big overall\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextual_aug_func(_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I like my clothes big enough but even for me this ran large, i am 5'7 134b and medium fit in the shoulders but felt too big overall\",\n",
       " \"I like my clothes loose fitting but even for me this ran large, i am 5'7 134b and I fit in the back but still too big overall\",\n",
       " \"I like my big loose fitting but even for me this ran large, i stand 5'7 134b and medium light in the shoulders but was too big overall\",\n",
       " \"I liked its own loose fitting but even for me this ran large, i am 5'7 134b and medium fit in the shoulders but was too big overall\",\n",
       " \"I like my clothes loose fitting but had given me this ran large, i am 5'7 134b is medium fit in the shoulders but was too big overall\",\n",
       " \"I made my clothes loose fitting but even for me this ran large, i am 5'7 134b and barely fit over the shoulders but was too big overall\",\n",
       " \"I like my clothes loose fitting but honestly for me this ran large, i am 5'7 134b and medium fit in all shoulders it was too big overall\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextual_aug_func([_tmp for i in range(7)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this type of augmentation, it's wise to use GPU to minimize processing speed. You also don't want all of your text to be augmented, so let's reuse the stochastic augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_aug_func = partial(nlp_aug_stochastic,aug=aug,p=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add these 2 instance variables to your gpu augmentation\n",
    "contextual_aug_func.run_on_gpu=True\n",
    "contextual_aug_func.batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "----- Do <lambda> on Department Name -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 3, which is 0.02% of training set\n",
      "Filtering leaked data out of training set...\n",
      "Done\n",
      "-------------------- Text Augmentation --------------------\n",
      "----- nlp_aug_stochastic -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2ead9e1a5c41c08344748f6b0447c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe70619676e4b1cbff64b35a3207deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a9f7bfa8784b8b864d905254bf48cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         sup_types='classification',\n",
    "                         label_names='Department Name',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         content_augmentations=contextual_aug_func,\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This dress makes me so sad...the textured stretchy fabric, color, length, and overall swingy fit are spot on. as another reviewer noted though, the armholes and neck totally ruin the dress. the neck is tiny, which i could have gotten over once it was on, but the arm holes were just awful - too big all around (too tall of a cut plus too wide of a cut). basically, you could see my bra plus from the front there was unflattering exposure near the armholes. it could have been so good, but alas, but t',\n",
       " \"This top is very flattering, the fabric flows and moves. it fits perfectly (slim cut), but hides tummy bulges and other imperfections. and it's slimming too. can be dressed up or down, goes with everything. i ended up buying all three colors, and if there were more, i would buy more!\",\n",
       " 'This blouse is wonderful. i just got and wore the wine colored blouse today. i received so many compliments. i love it and with the sale price it is so worth it.',\n",
       " 'This top is very versatile. i wore it out to dinner with skinny jeans on a friday night, but it can easily transition to a saturday afternoon stroll around town top.',\n",
       " 'This top is so soft and luxuriously comfy! i love wearing it around the house, haven\\'t really \"dressed\" it up yet with jeans or jewelry. it runs slightly big, but if you like the oversized look, this is definitely perfect.',\n",
       " \"I was in love with this shirt from the moment i put it on. it is of high fit, with layers to ensure the top isn't sheer. the embroidery is incredibly pretty and the top looks way less grandma in it. i ordered the top xxs and it fits perfectly. i really appreciate that the underarm holes are just the right size and cant show off any of my bra, which sometimes happens with small tops. i can wear it with jeans and boots or with a pencil skirt and heels but is looks great with both outfits. o\",\n",
       " \"I read the other review and from the picture it looked as though it may be a little tight, so i ordered up to a large. the medium would have fit, but since i'm in ur mid-40's i felt more comfortable with large. but if people are trim and young or young at heart your usual medium will be fine. live the material and the navy makes it classy and rich looking. could be dressed up or down. have worn it to a cocktail party fundraiser with white crop sleeves and received many reviews. i'm always challenge\",\n",
       " \"This skirt is so ladylike and light as air! the cherry red color is beautiful - just as pictured. i can imagine so many opportunities to wear this skirt. with a sweater and tights now, and maybe a striped tee and sandals in the spring.\\ni'm sure i'll have this gorgeous classic in my wardrobe for a very long time to come!\",\n",
       " 'Very pretty dress, perfect style for my build, bigger busted, muffin top. the material/pattern is really pretty.',\n",
       " \"I purchased this top in the navy. the picture gives the top looking like an interesting blue with some purple in it, but in person the top is just... navy. the lace and fabric are soft. it fits true to me; i almost always wear a small and the small fit me. the wasn't quite my style, but it's a pretty top it will be great for spring and summer.\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['train']['Review Text'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, similarly to `Content Transformation`, you can link multiple augmentation functions together by providing a list of those functions in `content_augmentations`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load TextDataController"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L265){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.save_as_pickles\n",
       "\n",
       ">      TextDataController.save_as_pickles (fname, parent='pickle_files',\n",
       ">                                          drop_attributes=False)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |\n",
       "| drop_attributes | bool | False | Whether to drop large-size attributes |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L265){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.save_as_pickles\n",
       "\n",
       ">      TextDataController.save_as_pickles (fname, parent='pickle_files',\n",
       ">                                          drop_attributes=False)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |\n",
       "| drop_attributes | bool | False | Whether to drop large-size attributes |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataController.save_as_pickles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L213){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.from_pickle\n",
       "\n",
       ">      TextDataController.from_pickle (fname, parent='pickle_files')\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L213){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.from_pickle\n",
       "\n",
       ">      TextDataController.from_pickle (fname, parent='pickle_files')\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataController.from_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextDataController object can be saved and loaded with ease. This is especially useful after text processing and/or tokenization have been done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import disable_caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_caching() # disable huggingface caching to see data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import text_normalize\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_aug_stochastic(x,aug=None,p=0.5):\n",
    "    if not isinstance(x,list): \n",
    "        if random.random()<p: return aug.augment(x)[0]\n",
    "        return x\n",
    "    news=[]\n",
    "    originals=[]\n",
    "    for _x in x:\n",
    "        if random.random()<p: news.append(_x)\n",
    "        else: originals.append(_x)\n",
    "    # only perform augmentation when needed\n",
    "    if len(news): news = aug.augment(news)\n",
    "    return news+originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug2 = naw.ContextualWordEmbsAug(model_path='roberta-base', \n",
    "                                device='cuda:0', # if you don't have gpu, change to 'cpu'\n",
    "                                action=\"substitute\",\n",
    "                                top_k=10,\n",
    "                               aug_p=0.07)\n",
    "\n",
    "contextual_aug_func = partial(nlp_aug_stochastic,aug=aug2,p=0.1)\n",
    "# add these 2 instance variables to your gpu augmentation\n",
    "contextual_aug_func.run_on_gpu=True\n",
    "contextual_aug_func.batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3344ef1d97334ad39f112d529a167e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/23486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646ee7fa56704694b8ec31f964c12484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/22641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8590d5b2c404795b9f4024f98cc75ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238e913e2d4548f983d12f42d0ff7488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Text Transformation --------------------\n",
      "----- text_normalize -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab4f9e1cc424eb4ae0e7625d859628e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- lower -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d1169af6bb43778523e7151ee9115a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 0, which is 0.00% of training set\n",
      "-------------------- Text Augmentation --------------------\n",
      "----- nlp_aug_stochastic -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65ab12d96064c2295c6cb37c3b58bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e52e3e6cfc4bc38eb83d22c96b9475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70abc2b998194208925f7515cf681f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a773924f474f6aa997aca54d32d31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         content_transformations = [text_normalize,str.lower],\n",
    "                         content_augmentations = contextual_aug_func, \n",
    "                         process_metas=True,\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Title', 'Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 18102\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Title', 'Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4526\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.save_as_pickles('my_tdc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Size in MegaBytes is 479.387\n"
     ]
    }
   ],
   "source": [
    "file_stats = os.stat(Path('pickle_files/my_tdc.pkl'))\n",
    "print(f'File Size in MegaBytes is {round(file_stats.st_size / (1024 * 1024), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load back our object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc2 = TextDataController.from_pickle('my_tdc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can still access all its attributes, data, preprocessings, transformation/augmentation ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Title', 'Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 18102\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Title', 'Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4526\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc2.main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: general petite . meh . this tunic is way over priced for the style and quality . it fit comfortably ( runs a size larger ) but it's not really flattering , it jut kind of hangs there looking ok . it is a little too deep of a v cut for a work top as well . this top does not support the price at all . it felt like something i could find at department store for way less . i will be returning it .\n",
      "Label: Tops => 4\n",
      "----------\n",
      "Text: general petite . love byron lars . this dress , like all byron lars dresses is a work of art . it has true quality of workmanship . and fits like a glove . i always get compliments when i wear any of his dresses and i have 5 ! . this one is somewhere between casual and dressy . perfect for a dinner out on a saturday night ! . order a petite if you are under 5 5 \"\n",
      "Label: Bottoms => 0\n",
      "----------\n",
      "Text: general petite . snap neck pullover . i love this top . i ordered it in a large thinking it would be a tight rib but it is not so i reordered it in a small . i am 5 ' 7 \" 145 lbs 34 g chest . the small fits perfectly and probably could have taken an xs . it is stretchy but fits wonderfully . i bought the black . i love how the neck snaps and adds a little pizzazz to a simple black turtle neck . i'm wearing it today with straight leg jeans and my leopard print ballet flats . i feel like audrey hepburn ! ! i will not be dry cleaning it . i will wash\n",
      "Label: Bottoms => 0\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(tdc2.main_ddict['train']):\n",
    "    if i==3:break\n",
    "    print(f\"Text: {v['Review Text']}\\nLabel: {v['Department Name']} => {v['label']}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc2.label_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Review Text': <function __main__.<lambda>(x)>,\n",
       "  'Department Name': <function __main__.<lambda>(x)>},\n",
       " [<function underthesea.pipeline.text_normalize.text_normalize(text, tokenizer='underthesea')>,\n",
       "  <method 'lower' of 'str' objects>],\n",
       " [functools.partial(<function nlp_aug_stochastic>, aug=<nlpaug.augmenter.word.context_word_embs.ContextualWordEmbsAug object>, p=0.1)])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc2.filter_dict,tdc2.content_tfms,tdc2.aug_tfms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't want to store the HuggingFace DatasetDict in your `TextDataController`, or the augmentation functions (typically when you already have a trained model, and you only use `TextDataController` to preprocess the test set), you can remove it in the `save_as_pickles` step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.save_as_pickles('my_lightweight_tdc',drop_attributes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Size in MegaBytes is 2.279\n"
     ]
    }
   ],
   "source": [
    "file_stats = os.stat(Path('pickle_files/my_lightweight_tdc.pkl'))\n",
    "print(f'File Size in MegaBytes is {round(file_stats.st_size / (1024 * 1024), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc3 = TextDataController.from_pickle('my_lightweight_tdc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this object to demonstrate the Test Set Construction in the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L702){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.prepare_test_dataset\n",
       "\n",
       ">      TextDataController.prepare_test_dataset (test_dset, do_filtering=False)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| test_dset |  |  | The HuggingFace Dataset as Test set |\n",
       "| do_filtering | bool | False | whether to perform data filtering on this test set |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L702){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.prepare_test_dataset\n",
       "\n",
       ">      TextDataController.prepare_test_dataset (test_dset, do_filtering=False)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| test_dset |  |  | The HuggingFace Dataset as Test set |\n",
       "| do_filtering | bool | False | whether to perform data filtering on this test set |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataController.prepare_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L659){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.prepare_test_dataset_from_csv\n",
       "\n",
       ">      TextDataController.prepare_test_dataset_from_csv (file_path,\n",
       ">                                                        do_filtering=False)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| file_path |  |  | path to csv file |\n",
       "| do_filtering | bool | False | whether to perform data filtering on this test set |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L659){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.prepare_test_dataset_from_csv\n",
       "\n",
       ">      TextDataController.prepare_test_dataset_from_csv (file_path,\n",
       ">                                                        do_filtering=False)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| file_path |  |  | path to csv file |\n",
       "| do_filtering | bool | False | whether to perform data filtering on this test set |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataController.prepare_test_dataset_from_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L669){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.prepare_test_dataset_from_df\n",
       "\n",
       ">      TextDataController.prepare_test_dataset_from_df (df, validate=True,\n",
       ">                                                       do_filtering=False)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df |  |  | Pandas Dataframe |\n",
       "| validate | bool | True | whether to perform input data validation |\n",
       "| do_filtering | bool | False | whether to perform data filtering on this test set |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L669){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.prepare_test_dataset_from_df\n",
       "\n",
       ">      TextDataController.prepare_test_dataset_from_df (df, validate=True,\n",
       ">                                                       do_filtering=False)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df |  |  | Pandas Dataframe |\n",
       "| validate | bool | True | whether to perform input data validation |\n",
       "| do_filtering | bool | False | whether to perform data filtering on this test set |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataController.prepare_test_dataset_from_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L679){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.prepare_test_dataset_from_raws\n",
       "\n",
       ">      TextDataController.prepare_test_dataset_from_raws (content)\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| content | Either a single sentence, list of sentence or a dictionary with keys are metadata columns and values are list |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L679){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.prepare_test_dataset_from_raws\n",
       "\n",
       ">      TextDataController.prepare_test_dataset_from_raws (content)\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| content | Either a single sentence, list of sentence or a dictionary with keys are metadata columns and values are list |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataController.prepare_test_dataset_from_raws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you have done your preprocessing and tokenization in your training set, and have a nicely trained model, ready to do inference on new data. Here is how you can use `TextDataController` to apply all the necessary preprocessings to your new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reuse the lightweight tdc object we created in the previous section (since we don't really need all the training data just to construct new data). Also, we will take a small sample of our training data and pretend it is our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc = TextDataController.from_pickle('my_lightweight_tdc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict a few raw texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we only provide a raw text as follows\n",
    "\n",
    "```python\n",
    "tdc.prepare_test_dataset_from_raws('This shirt is so comfortable I love it!')\n",
    "```\n",
    "You will counter this error:\n",
    "```\n",
    "ValueError: There is/are metadatas in the preprocessing step. Please include a dictionary including these keys for \n",
    "metadatas: ['Title', 'Division Name'], and texture content: Review Text\n",
    "```\n",
    "\n",
    "Since our preprocessing includes some metadatas, you have to provide a dictionary as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Test Set Transformation --------------------\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5228fcba71db491cbad3c9f40364439e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Text Transformation --------------------\n",
      "----- text_normalize -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cbd2b56ec014616bda0dc8983aa69b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- lower -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea3dbe1892b4ca9acb1cce378d5edae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34b9a54c82049b7922011c6b1b39fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "results = tdc.prepare_test_dataset_from_raws({'Review Text': 'This shirt is so comfortable I love it!',\n",
    "                                    'Title': 'Great shirt',\n",
    "                                    'Division Name': 'general'\n",
    "                                   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Review Text': 'general . great shirt . this shirt is so comfortable i love it !', 'Title': 'great shirt', 'Division Name': 'general', 'input_ids': [0, 15841, 479, 372, 6399, 479, 42, 6399, 16, 98, 3473, 939, 657, 24, 27785, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make prediction from a pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4692, 10)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig').sample(frac=0.2,random_state=1)\n",
    "# drop NaN values in the label column\n",
    "df_test = df_test[~df_test['Department Name'].isna()].reset_index(drop=True)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are few things to pay attention to when constructing your new test set using `TextDataController`:\n",
    "- Only a few processings will be applied to your test set: **Metadatas concatenation, Filtering (can be omited), Content Transformation, and Tokenization**. Therefore, all columns required to perform these processings must exist in your test dataset\n",
    "- You can exclude the label column (e.g. `Department Name` in this example), since it's a test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view all required columns, access the attribute `cols_to_keep` (you can omit the last column, which is the name of the label column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Review Text', 'Title', 'Division Name', 'Department Name']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.cols_to_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test dataset might have some NaN values in the text field (`Review Text`), thus we will turn on the filtering option to get rid of these NaNs, as this is what we did in the training set. If your test dataset don't need any filtering, turn off this option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Input Validation Precheck -\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title          758\n",
      "Review Text    164\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 2 rows\n",
      "-------------------- Start Test Set Transformation --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f096d8ae224541aa86270b832b035f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/4692 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22b571946674df6be09606d4e6c8fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6086716492e2478795e6efa0f78724e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Text Transformation --------------------\n",
      "----- text_normalize -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dfd8c36e70844adbcf07b0d6a3f667a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- lower -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48062514082d4557b6ebf33bec54b7a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697ffff341e649aab511440fd6ec4f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "test_dset = tdc.prepare_test_dataset_from_df(df_test,validate=True,do_filtering=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Title', 'Review Text', 'Division Name', 'Department Name', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 4528\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: general . perfect for work and play . this shirt works for both going out and going to work , and i can wear it with everything . fits perfect , tucked and untucked , tied and untied . i love it .\n",
      "Input_ids: [0, 15841, 479, 1969, 13, 173, 8, 310, 479, 42, 6399, 1364, 13, 258, 164, 66, 8, 164, 7, 173, 2156, 8, 939, 64, 3568, 24, 19, 960, 479, 10698, 1969, 2156, 21222, 8, 7587, 23289, 2156, 3016, 8, 7587, 2550, 479, 939, 657, 24, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "----------\n",
      "Text: general petite . . i don't know why i had the opposite problem most reviewers had with these ..... i tried on the regular length in the store and found that they were just a bit too short with heels . ( i'm 5 ' 5 ) . i had them ordered in a petite and when they came , they were too short with flats ! maybe it's the way i like to wear them , i like my flare jeans to barely skim the ground . i just exchanged them for regular length and will wear them with a small wedge shoe . aside from the length issues , these are super cute\n",
      "Input_ids: [0, 15841, 4716, 1459, 479, 479, 939, 218, 75, 216, 596, 939, 56, 5, 5483, 936, 144, 34910, 56, 19, 209, 29942, 734, 939, 1381, 15, 5, 1675, 5933, 11, 5, 1400, 8, 303, 14, 51, 58, 95, 10, 828, 350, 765, 19, 8872, 479, 36, 939, 437, 195, 128, 195, 4839, 479, 939, 56, 106, 2740, 11, 10, 4716, 1459, 8, 77, 51, 376, 2156, 51, 58, 350, 765, 19, 20250, 27785, 2085, 24, 18, 5, 169, 939, 101, 7, 3568, 106, 2156, 939, 101, 127, 24186, 10844, 7, 6254, 28772, 5, 1255, 479, 939, 95, 11024, 106, 13, 1675, 5933, 8, 40, 3568, 106, 19, 10, 650, 27288, 12604, 479, 4364, 31, 5, 5933, 743, 2156, 209, 32, 2422, 11962, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "----------\n",
      "Text: general petite . great pants . thes e cords are great--lightweight for fl winters , and the bootcut flare bottom is super cute with ballet flats or booties . i am 5 ' 10 \" and typically a size 8 ; the size 29 fit perfectly . they have a little stretch to them , which is great . very flattering--wish i could order in more colors ! !\n",
      "Input_ids: [0, 15841, 4716, 1459, 479, 372, 9304, 479, 5, 29, 364, 37687, 32, 372, 5579, 6991, 4301, 13, 2342, 31000, 2156, 8, 5, 9759, 8267, 24186, 2576, 16, 2422, 11962, 19, 22573, 20250, 50, 9759, 918, 479, 939, 524, 195, 128, 158, 22, 8, 3700, 10, 1836, 290, 25606, 5, 1836, 1132, 2564, 6683, 479, 51, 33, 10, 410, 4140, 7, 106, 2156, 61, 16, 372, 479, 182, 34203, 5579, 605, 1173, 939, 115, 645, 11, 55, 8089, 27785, 27785, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Text: {test_dset['Review Text'][i]}\")\n",
    "    print(f\"Input_ids: {test_dset['input_ids'][i]}\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
