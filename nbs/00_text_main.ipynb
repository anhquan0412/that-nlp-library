{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Main\n",
    "\n",
    "> This module contains the main Python class for data control: `TextDataMain`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```#| default_exp text_main```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder,MultiLabelBinarizer\n",
    "from datasets import DatasetDict,Dataset,IterableDataset,load_dataset,concatenate_datasets\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from that_nlp_library.utils import *\n",
    "from functools import partial\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from that_nlp_library.text_transformation import *\n",
    "from that_nlp_library.text_augmentation import *\n",
    "from importlib.machinery import SourceFileLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Transformation, Augmentations, and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tokenizer_explain(inp, # Input sentence\n",
    "                      tokenizer, # Tokenizer (preferably from HuggingFace)\n",
    "                      split_word=False # Is input `inp` split into list or not\n",
    "                     ):\n",
    "    \"Display results from tokenizer\"\n",
    "    print('----- Tokenizer Explained -----')\n",
    "    print('--- Input ---')\n",
    "    print(inp)\n",
    "    print()\n",
    "    print('--- Tokenized results --- ')\n",
    "    print(tokenizer(inp,is_split_into_words=split_word))\n",
    "    print()\n",
    "    tok = tokenizer.encode(inp,is_split_into_words=split_word)\n",
    "    print('--- Results from tokenizer.convert_ids_to_tokens ---')\n",
    "    print(tokenizer.convert_ids_to_tokens(tok))\n",
    "    print()\n",
    "    print('--- Results from tokenizer.decode --- ')\n",
    "    print(tokenizer.decode(tok))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L20){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### tokenizer_explain\n",
       "\n",
       ">      tokenizer_explain (inp, tokenizer, split_word=False)\n",
       "\n",
       "Display results from tokenizer\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | Input sentence |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| split_word | bool | False | Is input `inp` split into list or not |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L20){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### tokenizer_explain\n",
       "\n",
       ">      tokenizer_explain (inp, tokenizer, split_word=False)\n",
       "\n",
       "Display results from tokenizer\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | Input sentence |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| split_word | bool | False | Is input `inp` split into list or not |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(tokenizer_explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a tokenizer from EnviBert model. Uncomment the command line below to download necessary files to build this tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gdown 14X9fGijA7kdNfe4dM_8gqfxIWtj1Q-hb -O ./envibert_cache --folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir=Path('./envibert_tokenizer')\n",
    "tokenizer = SourceFileLoader(\"envibert.tokenizer\", \n",
    "                             str(cache_dir/'envibert_tokenizer.py')).load_module().RobertaTokenizer(cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Envibert tokenizer does not required the input to be tokenized using word_tokenize from UnderTheSea library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Tokenizer Explained -----\n",
      "--- Input ---\n",
      "Hội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức. Thủ Đức là một huyện trực thuộc thành phố Hồ Chí Minh\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 857, 1033, 191, 664, 1033, 7366, 2615, 142, 664, 1033, 671, 1355, 2294, 993, 413, 2900, 244, 1019, 827, 24, 40, 647, 773, 549, 119, 511, 1134, 1690, 758, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens ---\n",
      "['<s>', '▁Hội', '▁cư', '▁dân', '▁chung', '▁cư', '▁sen', '▁hồng', '-', '▁chung', '▁cư', '▁lot', 'us', '▁sóng', '▁thần', '▁thủ', '▁đức', '.', '▁Thủ', '▁Đức', '▁là', '▁một', '▁huyện', '▁trực', '▁thuộc', '▁thành', '▁phố', '▁Hồ', '▁Chí', '▁Minh', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> ▁Hội ▁cư ▁dân ▁chung ▁cư ▁sen ▁hồng - ▁chung ▁cư ▁lot us ▁sóng ▁thần ▁thủ ▁đức. ▁Thủ ▁Đức ▁là ▁một ▁huyện ▁trực ▁thuộc ▁thành ▁phố ▁Hồ ▁Chí ▁Minh </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inp = 'Hội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức. Thủ Đức là một huyện trực thuộc thành phố Hồ Chí Minh'\n",
    "tokenizer_explain(inp,tokenizer,split_word=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Tokenizer Explained -----\n",
      "--- Input ---\n",
      "['hội', 'cư', 'dân', 'chung', 'cư', 'sen', 'hồng', '-', 'chung', 'cư', 'lotus', 'sóng', 'thần', 'thủ', 'đức']\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 227, 1033, 191, 664, 1033, 7366, 2615, 13, 664, 1033, 671, 1355, 2294, 993, 413, 2900, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens ---\n",
      "['<s>', '▁hội', '▁cư', '▁dân', '▁chung', '▁cư', '▁sen', '▁hồng', '▁-', '▁chung', '▁cư', '▁lot', 'us', '▁sóng', '▁thần', '▁thủ', '▁đức', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> ▁hội ▁cư ▁dân ▁chung ▁cư ▁sen ▁hồng ▁- ▁chung ▁cư ▁lot us ▁sóng ▁thần ▁thủ ▁đức </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inp = ['hội', 'cư', 'dân', 'chung', 'cư', 'sen', 'hồng', '-', 'chung', 'cư', 'lotus', 'sóng', 'thần', 'thủ', 'đức']\n",
    "tokenizer_explain(inp,tokenizer,split_word=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try PhoBert tokenizer. PhoBert tokenizer, unlike Envibert tokenizer, requires input to be word tokenized (using UnderTheSea library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức\n"
     ]
    }
   ],
   "source": [
    "inp = apply_vnmese_word_tokenize('hội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức')\n",
    "print(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Tokenizer Explained -----\n",
      "--- Input ---\n",
      "hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 1093, 1838, 1574, 3330, 2025, 31, 1574, 2029, 4885, 8554, 25625, 7344, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens ---\n",
      "['<s>', 'hội', 'cư_dân', 'chung_cư', 'sen', 'hồng', '-', 'chung_cư', 'lo@@', 'tus', 'sóng_thần', 'thủ_@@', 'đức', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer_explain(inp,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def two_steps_tokenization_explain(inp, # Input sentence\n",
    "                                   tokenizer, # Tokenizer (preferably from HuggingFace)\n",
    "                                   content_tfms=[], # A list of text transformations\n",
    "                                   aug_tfms=[], # A list of text augmentation \n",
    "                                  ):\n",
    "    \"Display results form each content transformation, then display results from tokenizer\"\n",
    "    print('----- Text Transformation Explained -----')\n",
    "    print('--- Raw sentence ---')\n",
    "    print(inp)\n",
    "    print('--- Content Transformations (on both train and test) ---')\n",
    "    content_tfms = val2iterable(content_tfms)\n",
    "    for tfm in content_tfms:\n",
    "        print_msg(callable_name(tfm),3)\n",
    "        inp = tfm(inp)\n",
    "        print(inp)\n",
    "    print('--- Augmentations (on train only) ---')\n",
    "    aug_tfms = val2iterable(aug_tfms)\n",
    "    for tfm in aug_tfms:\n",
    "        print_msg(callable_name(tfm),3)\n",
    "        inp = tfm(inp)\n",
    "        print(inp)\n",
    "    print()\n",
    "    tokenizer_explain(inp,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L41){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### two_steps_tokenization_explain\n",
       "\n",
       ">      two_steps_tokenization_explain (inp, tokenizer, content_tfms=[],\n",
       ">                                      aug_tfms=[])\n",
       "\n",
       "Display results form each content transformation, then display results from tokenizer\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | Input sentence |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| content_tfms | list | [] | A list of text transformations |\n",
       "| aug_tfms | list | [] | A list of text augmentation |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L41){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### two_steps_tokenization_explain\n",
       "\n",
       ">      two_steps_tokenization_explain (inp, tokenizer, content_tfms=[],\n",
       ">                                      aug_tfms=[])\n",
       "\n",
       "Display results form each content transformation, then display results from tokenizer\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | Input sentence |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| content_tfms | list | [] | A list of text transformations |\n",
       "| aug_tfms | list | [] | A list of text augmentation |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(two_steps_tokenization_explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load Phobert tokenizer one more time to test out this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import text_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`apply_vnmese_word_tokenize` also have an option to normalize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Text Transformation Explained -----\n",
      "--- Raw sentence ---\n",
      "Hội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức. Thủ Đức là một huyện trực thuộc thành phố Hồ Chí Minh\n",
      "--- Content Transformations (on both train and test) ---\n",
      "--- apply_vnmese_word_tokenize ---\n",
      "Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức . Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh\n",
      "--- Augmentations (on train only) ---\n",
      "\n",
      "----- Tokenizer Explained -----\n",
      "--- Input ---\n",
      "Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức . Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 792, 1838, 1574, 3330, 2025, 31, 1574, 2029, 4885, 8554, 25625, 7344, 5, 5043, 8, 16, 149, 2850, 214, 784, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens ---\n",
      "['<s>', 'Hội', 'cư_dân', 'chung_cư', 'sen', 'hồng', '-', 'chung_cư', 'lo@@', 'tus', 'sóng_thần', 'thủ_@@', 'đức', '.', 'Thủ_Đức', 'là', 'một', 'huyện', 'trực_thuộc', 'thành_phố', 'Hồ_Chí_Minh', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức. Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inp = 'Hội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức. Thủ Đức là một huyện trực thuộc thành phố Hồ Chí Minh'\n",
    "two_steps_tokenization_explain(inp,tokenizer,content_tfms=[partial(apply_vnmese_word_tokenize,normalize_text=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some text augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove vietnamese accent\n",
    "remove_accent = lambda x: unidecode.unidecode(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want your function to be printed in with a different name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_accent.__name__ = 'Remove Vietnamese Accent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Text Transformation Explained -----\n",
      "--- Raw sentence ---\n",
      "Hội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức. Thủ Đức là một huyện trực thuộc thành phố Hồ Chí Minh\n",
      "--- Content Transformations (on both train and test) ---\n",
      "--- apply_vnmese_word_tokenize ---\n",
      "Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức . Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh\n",
      "--- Augmentations (on train only) ---\n",
      "--- Remove Vietnamese Accent ---\n",
      "Hoi cu_dan chung_cu sen hong - chung_cu lotus song_than thu_duc . Thu_Duc la mot huyen truc_thuoc thanh_pho Ho_Chi_Minh\n",
      "\n",
      "----- Tokenizer Explained -----\n",
      "--- Input ---\n",
      "Hoi cu_dan chung_cu sen hong - chung_cu lotus song_than thu_duc . Thu_Duc la mot huyen truc_thuoc thanh_pho Ho_Chi_Minh\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 3021, 1111, 56549, 17386, 22975, 13689, 3330, 27037, 31, 22975, 13689, 2029, 4885, 3227, 9380, 1510, 21605, 6190, 1894, 5, 5770, 4098, 1894, 2644, 3773, 1204, 18951, 2052, 10242, 9835, 1881, 22899, 17366, 10384, 30234, 8470, 1612, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens ---\n",
      "['<s>', 'Ho@@', 'i', 'cu_@@', 'dan', 'chung_@@', 'cu', 'sen', 'hong', '-', 'chung_@@', 'cu', 'lo@@', 'tus', 'so@@', 'ng_th@@', 'an', 'thu_@@', 'du@@', 'c', '.', 'Thu_@@', 'Du@@', 'c', 'la', 'mo@@', 't', 'huy@@', 'en', 'tru@@', 'c_th@@', 'u@@', 'oc', 'thanh_@@', 'pho', 'Ho_@@', 'Chi_@@', 'Minh', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Hoi cu_dan chung_cu sen hong - chung_cu lotus song_than thu_duc. Thu_Duc la mot huyen truc_thuoc thanh_pho Ho_Chi_Minh </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "two_steps_tokenization_explain(inp,tokenizer,\n",
    "                               content_tfms=[partial(apply_vnmese_word_tokenize,normalize_text=True)],\n",
    "                               aug_tfms=[remove_accent]\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even be creative with your augmentation functions; let's say you only want your augmentation to be applied 50% of the time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_accent = lambda x: unidecode.unidecode(x) if random.random()<0.5 else x\n",
    "remove_accent.__name__ = 'Remove Vietnamese Accent with 0.5 prob'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Text Transformation Explained -----\n",
      "--- Raw sentence ---\n",
      "Hội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức. Thủ Đức là một huyện trực thuộc thành phố Hồ Chí Minh\n",
      "--- Content Transformations (on both train and test) ---\n",
      "--- apply_vnmese_word_tokenize ---\n",
      "Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức . Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh\n",
      "--- Augmentations (on train only) ---\n",
      "--- Remove Vietnamese Accent with 0.5 prob ---\n",
      "Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức . Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh\n",
      "\n",
      "----- Tokenizer Explained -----\n",
      "--- Input ---\n",
      "Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức . Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 792, 1838, 1574, 3330, 2025, 31, 1574, 2029, 4885, 8554, 25625, 7344, 5, 5043, 8, 16, 149, 2850, 214, 784, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens ---\n",
      "['<s>', 'Hội', 'cư_dân', 'chung_cư', 'sen', 'hồng', '-', 'chung_cư', 'lo@@', 'tus', 'sóng_thần', 'thủ_@@', 'đức', '.', 'Thủ_Đức', 'là', 'một', 'huyện', 'trực_thuộc', 'thành_phố', 'Hồ_Chí_Minh', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức. Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "two_steps_tokenization_explain(inp,tokenizer,\n",
    "                               content_tfms=[partial(apply_vnmese_word_tokenize,normalize_text=True)],\n",
    "                               aug_tfms=[remove_accent]\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more examples of interesting augmentation [here](https://anhquan0412.github.io/that-nlp-library/text_augmentation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tokenize_function(examples:dict,\n",
    "                      tok,\n",
    "                      text_name,\n",
    "                      max_length=None,\n",
    "                      is_split_into_words=False):\n",
    "    if max_length is None:\n",
    "        # pad to model's default max sequence length\n",
    "        return tok(examples[text_name], padding=\"max_length\", truncation=True,is_split_into_words=is_split_into_words)\n",
    "    if isinstance(max_length,int) and max_length>0:\n",
    "        # pad to max length of the current batch, and start truncating at max_length\n",
    "        return tok(examples[text_name], padding=True, max_length=max_length,truncation=True,is_split_into_words=is_split_into_words)\n",
    "    \n",
    "    # no padding (still truncate at model's default max sequence length)\n",
    "    return tok(examples[text_name], truncation=True,is_split_into_words=is_split_into_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L58){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### tokenize_function\n",
       "\n",
       ">      tokenize_function (examples:dict, tok, text_name, max_length=None,\n",
       ">                         is_split_into_words=False)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L58){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### tokenize_function\n",
       "\n",
       ">      tokenize_function (examples:dict, tok, text_name, max_length=None,\n",
       ">                         is_split_into_words=False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(tokenize_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I am processing Vietnamese text, I will use EnViBert's tokenizer. Envibert is a RoBERTa model for Vietnamese and English. This RoBERTa version is trained by using 100GB of text (50GB of Vietnamese and 50GB of English). For more information: [https://huggingface.co/nguyenvulebinh/envibert](https://huggingface.co/nguyenvulebinh/envibert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/nguyenvulebinh/envibert\n",
    "cache_dir=Path('./envibert_tokenizer')\n",
    "tokenizer = SourceFileLoader(\"envibert.tokenizer\", \n",
    "                             str(cache_dir/'envibert_tokenizer.py')).load_module().RobertaTokenizer(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples={\n",
    "    'text':[\n",
    "         'hội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức',\n",
    "         'This is the recommended way to make a Python package importable from anywhere',\n",
    "         'hội cần mở thẻ tín dụng tại hà nội, đà nẵng, tp. hồ chí minh',\n",
    "         \"biti's cao lãnh - đồng tháp\",\n",
    "         'chợ phòng trọ + việc làm...khu lĩnh nam - vĩnh hưng - mai động (hoàng mai)'\n",
    "          ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = tokenize_function(examples,tokenizer,text_name='text',max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 227, 1033, 191, 664, 1033, 7366, 2615, 13, 664, 1033, 671, 1355, 2294, 993, 413, 2900, 2, 1, 1, 1, 1, 1, 1, 1], [0, 116, 14, 6, 3169, 270, 9, 364, 10, 23963, 5360, 15930, 2003, 51, 5906, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 227, 256, 778, 2600, 1074, 144, 76, 5489, 613, 57339, 4820, 27666, 57339, 21422, 244, 872, 635, 841, 2, 1, 1, 1, 1, 1], [0, 880, 592, 427, 162, 171, 906, 13, 122, 6553, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 2299, 315, 5995, 1349, 99, 83, 55025, 244, 6356, 1114, 1213, 1163, 13, 8233, 11051, 13, 3335, 109, 28, 11695, 13377, 3335, 3, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁hội', '▁cư', '▁dân', '▁chung', '▁cư', '▁sen', '▁hồng', '▁-', '▁chung', '▁cư', '▁lot', 'us', '▁sóng', '▁thần', '▁thủ', '▁đức', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(results['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change max_length (which allow truncation when sentence length is higher than max_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = tokenize_function(examples,tokenizer,text_name='text',max_length=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 227, 1033, 191, 2], [0, 116, 14, 6, 2], [0, 227, 256, 778, 2], [0, 880, 592, 427, 2], [0, 2299, 315, 5995, 2]], 'token_type_ids': [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadatas Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def concat_metadatas(dset:dict, # HuggingFace Dataset\n",
    "                     main_text, # Text feature name\n",
    "                     metadatas, # Metadata (or a list of metadatas)\n",
    "                     process_metas=True, # Whether apply simple metadata processing, i.e. space strip and lowercase\n",
    "                     sep='.', # separator for contatenating to main_text\n",
    "                     is_batched=True, # whether batching is applied\n",
    "                    ):\n",
    "    \"\"\"\n",
    "    Extract, process (optional) and concatenate metadatas to the front of text\n",
    "    \"\"\"\n",
    "    results={main_text:dset[main_text]}\n",
    "    for m in metadatas:\n",
    "        m_data = dset[m]\n",
    "        if process_metas:\n",
    "            # just strip and lowercase\n",
    "            m_data = [nan2emptystr(v).strip().lower() for v in m_data] if is_batched else nan2emptystr(m_data).strip().lower()\n",
    "        results[m]=m_data\n",
    "        if is_batched:\n",
    "            results[main_text] = [f'{m_data[i]} {sep} {results[main_text][i]}' for i in range(len(m_data))]\n",
    "        else:\n",
    "            results[main_text] = f'{m_data} {sep} {results[main_text]}'\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### concat_metadatas\n",
       "\n",
       ">      concat_metadatas (dset:dict, main_text, metadatas, process_metas=True,\n",
       ">                        sep='.', is_batched=True)\n",
       "\n",
       "Extract, process (optional) and concatenate metadatas to the front of text\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dset | dict |  | HuggingFace Dataset |\n",
       "| main_text |  |  | Text feature name |\n",
       "| metadatas |  |  | Metadata (or a list of metadatas) |\n",
       "| process_metas | bool | True | Whether apply simple metadata processing, i.e. space strip and lowercase |\n",
       "| sep | str | . | separator for contatenating to main_text |\n",
       "| is_batched | bool | True | whether batching is applied |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### concat_metadatas\n",
       "\n",
       ">      concat_metadatas (dset:dict, main_text, metadatas, process_metas=True,\n",
       ">                        sep='.', is_batched=True)\n",
       "\n",
       "Extract, process (optional) and concatenate metadatas to the front of text\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dset | dict |  | HuggingFace Dataset |\n",
       "| main_text |  |  | Text feature name |\n",
       "| metadatas |  |  | Metadata (or a list of metadatas) |\n",
       "| process_metas | bool | True | Whether apply simple metadata processing, i.e. space strip and lowercase |\n",
       "| sep | str | . | separator for contatenating to main_text |\n",
       "| is_batched | bool | True | whether batching is applied |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(concat_metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class TextDataController"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TextDataController():\n",
    "    def __init__(self,\n",
    "                 inp, # HuggingFace Dataset or DatasetDict\n",
    "                 main_text:str, # Name of the main text column\n",
    "                 label_names=None, # Names of the label (dependent variable) columns\n",
    "                 class_names_predefined=None, # List of names associated with the labels (same index order)\n",
    "                 filter_dict={}, # A dictionary: {feature: filtering_function_based_on_the_feature}\n",
    "                 metadatas=[], # Names of the metadata columns\n",
    "                 process_metas=True, # Whether to do simple text processing on the chosen metadatas\n",
    "                 content_transformations=[], # A list of text transformations\n",
    "                 val_ratio:list|float|None=0.2, # Ratio of data for validation set. If given a list, validation set will be chosen based on indices in this list\n",
    "                 stratify_cols=[], # Column(s) needed to do stratified shuffle split\n",
    "                 upsampling_dict={}, # A dictionary: {feature: upsampling_function_based_on_the_feature}\n",
    "                 content_augmentations=[], # A list of text augmentations\n",
    "                 seed=None, # Random seed\n",
    "                 is_batched=True, # Whether to perform operations in batch\n",
    "                 batch_size=1000, # Batch size, for when is_batched is True\n",
    "                 num_proc=4, # Number of process for multiprocessing\n",
    "                 cols_to_keep=None, # Columns to keep after all processings\n",
    "                 buffer_size=10000, # For shuffling data\n",
    "                 num_shards=64, # Number of shards\n",
    "                ):\n",
    "            \n",
    "        self.main_text = main_text\n",
    "        self.metadatas = val2iterable(metadatas)\n",
    "        self.process_metas = process_metas\n",
    "        self.label_names = val2iterable(label_names) if label_names is not None else None\n",
    "        self.label_lists = class_names_predefined\n",
    "        self.filter_dict = filter_dict\n",
    "        self.content_tfms = val2iterable(content_transformations)\n",
    "        self.upsampling_dict = upsampling_dict\n",
    "        self.aug_tfms = val2iterable(content_augmentations)\n",
    "        self.val_ratio = val_ratio\n",
    "        self.stratify_cols = val2iterable(stratify_cols)\n",
    "        self.seed = seed\n",
    "        self.is_batched = is_batched\n",
    "        self.batch_size = batch_size\n",
    "        self.num_proc = num_proc\n",
    "        self.is_streamed = False\n",
    "        self.cols_to_keep = cols_to_keep\n",
    "        self.buffer_size = buffer_size\n",
    "        self.num_shards = num_shards\n",
    "        self.ddict_rest = DatasetDict()\n",
    "        \n",
    "        if hasattr(inp,'keys'):\n",
    "            if 'train' in inp.keys(): # is datasetdict\n",
    "                self.ddict_rest = inp\n",
    "                self.dset = self.ddict_rest.pop('train')\n",
    "            else:\n",
    "                raise ValueError('The given DatasetDict has no \"train\" split')\n",
    "        else: # is dataset\n",
    "            self.dset = inp\n",
    "        if isinstance(self.dset,IterableDataset):\n",
    "            self.is_streamed=True\n",
    "        self.all_cols = self.dset.column_names\n",
    "        \n",
    "        if self.is_streamed and self.label_names is not None and self.label_lists is None:\n",
    "            raise ValueError('All class labels must be provided when streaming')\n",
    "        \n",
    "        if self.is_streamed and len(self.upsampling_dict):\n",
    "            warnings.warn(\"Upsampling requires dataset concatenation, which can be extremely slow (x2) for streamed dataset\")\n",
    "            \n",
    "        self._processed_call=False\n",
    "        self.is_multilabel=False\n",
    "        self.is_multihead=False\n",
    "        \n",
    "        \n",
    "            \n",
    "    @classmethod\n",
    "    def from_csv(cls,file_path,**kwargs):\n",
    "        file_path = Path(file_path)\n",
    "        ds = load_dataset(str(file_path.parent),\n",
    "                                  data_files=file_path.name,\n",
    "                                  split='train')\n",
    "        return TextDataController(ds,**kwargs)\n",
    "        \n",
    "    \n",
    "    @classmethod\n",
    "    def from_df(cls,df,validate=True,**kwargs):\n",
    "        if validate:\n",
    "            check_input_validation(df)\n",
    "        ds = Dataset.from_pandas(df)\n",
    "        return TextDataController(ds,**kwargs)\n",
    "    \n",
    "    \n",
    "    def _map_dset(self,dset,func):\n",
    "        if self.is_streamed:\n",
    "            return dset.map(func,\n",
    "                            batched=self.is_batched,\n",
    "                            batch_size=self.batch_size\n",
    "                           )\n",
    "        return dset.map(func,\n",
    "                        batched=self.is_batched,\n",
    "                        batch_size=self.batch_size,\n",
    "                        num_proc=self.num_proc\n",
    "                       )\n",
    "    \n",
    "    def _filter_dset(self,dset,func):\n",
    "        if self.is_streamed:\n",
    "            return dset.filter(func,\n",
    "                            batched=self.is_batched,\n",
    "                            batch_size=self.batch_size\n",
    "                           )\n",
    "        return dset.filter(func,\n",
    "                        batched=self.is_batched,\n",
    "                        batch_size=self.batch_size,\n",
    "                        num_proc=self.num_proc\n",
    "                       )\n",
    "                     \n",
    "    def validate_input(self):\n",
    "        if self.is_streamed:\n",
    "            print('Input validation check is disabled when data is streamed')\n",
    "            return\n",
    "        _df = self.dset.to_pandas()\n",
    "        check_input_validation(_df)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def save_as_pickles(self,\n",
    "                        fname, # Name of the pickle file\n",
    "                        parent='pickle_files', # Parent folder\n",
    "                        drop_data_attributes=False # Whether to drop all large-size data attributes\n",
    "                       ):\n",
    "        if drop_data_attributes:\n",
    "            if hasattr(self, 'main_ddict'):\n",
    "                del self.main_ddict\n",
    "        save_to_pickle(self,fname,parent=parent)\n",
    "    \n",
    "        \n",
    "    def _check_validation_leaking(self):\n",
    "        if self.val_ratio is None or self.is_streamed:\n",
    "            return\n",
    "        \n",
    "        trn_txt = self.main_ddict['train'][self.main_text]\n",
    "        val_txt = self.main_ddict['validation'][self.main_text]        \n",
    "        val_txt_leaked = check_text_leaking(trn_txt,val_txt)\n",
    "        \n",
    "        if len(val_txt_leaked)==0: return\n",
    "        \n",
    "        # filter train dataset to get rid of leaks\n",
    "        print('Filtering leaked data out of training set...')\n",
    "        _func = partial(lambda_batch,\n",
    "                        feature=self.main_text,\n",
    "                        func=lambda x: x.strip().lower() not in val_txt_leaked,\n",
    "                        is_batched=self.is_batched)\n",
    "        self.main_ddict['train'] = self._filter_dset(self.main_ddict['train'],_func)   \n",
    "        print('Done')\n",
    "           \n",
    "    def _train_test_split(self):\n",
    "        print_msg('Train Test Split',20)\n",
    "        val_key = list(set(self.ddict_rest.keys()) & set(['val','validation','valid']))\n",
    "        if len(val_key)==1: # val split exists\n",
    "            self.main_ddict=DatasetDict({'train':self.dset,\n",
    "                                         'validation':self.ddict_rest.pop(val_key[0])})\n",
    "            \n",
    "    \n",
    "        elif self.val_ratio is None: # use all data\n",
    "            self.main_ddict=DatasetDict({'train':self.dset})\n",
    "        \n",
    "        elif isinstance(self.val_ratio,list) or isinstance(self.val_ratio,np.ndarray): # filter with indices\n",
    "            if self.is_streamed: raise ValueError('Data streaming does not support validation set filtering using indices')\n",
    "            val_idxs = list(self.val_ratio)\n",
    "            trn_idxs = list(set(range(len(self.dset))) - set(val_idxs))\n",
    "            self.main_ddict=DatasetDict({'train':self.dset.select(trn_idxs),\n",
    "                                         'validation':self.dset.select(val_idxs)})\n",
    "            \n",
    "        elif (isinstance(self.val_ratio,float) or isinstance(self.val_ratio,int)) and not len(self.stratify_cols):\n",
    "            if self.is_streamed:\n",
    "                # shuffle dataset before splitting it\n",
    "                self.dset = self.dset.shuffle(seed=self.seed,buffer_size=self.buffer_size)\n",
    "                if isinstance(self.val_ratio,float):\n",
    "                    warnings.warn(\"Length of streamed dataset is unknown to use float validation ratio. Default to 5000 data points for validation\")\n",
    "                    self.val_ratio=5000\n",
    "                    \n",
    "                trn_dset = self.dset.skip(self.val_ratio)\n",
    "                val_datas = list(self.dset.take(self.val_ratio))\n",
    "                val_dict={k: [v[k] for v in val_datas] for k in val_datas[0].keys()}\n",
    "                val_dset = Dataset.from_dict(val_dict)\n",
    "                self.main_ddict=DatasetDict({'train':trn_dset,\n",
    "                                         'validation':val_dset})\n",
    "#                 self.main_ddict=DatasetDict({'train':self.dset.skip(self.val_ratio),\n",
    "#                                          'validation':self.dset.take(self.val_ratio)})\n",
    "            else:\n",
    "                # train val split\n",
    "                self.main_ddict = self.dset.train_test_split(test_size=self.val_ratio,shuffle=True,seed=self.seed)\n",
    "                self.main_ddict['validation']=self.main_ddict['test']\n",
    "                del self.main_ddict['test']\n",
    "        \n",
    "        else: # val_ratio split with stratifying\n",
    "            if self.is_streamed: raise ValueError('Stratified split is not supported for streamed data')                \n",
    "            if self.is_multilabel and self.label_names[0] in self.stratify_cols:\n",
    "                raise ValueError('For MultiLabel classification, you cannot choose the label as your stratified column')\n",
    "            \n",
    "            # Create a new feature 'stratified', which is a concatenation of values in stratify_cols\n",
    "            if self.is_batched:\n",
    "                stratified_creation = lambda x: {'stratified':\n",
    "                                     ['_'.join(list(map(str,[x[v][i] for v in self.stratify_cols]))) \n",
    "                                      for i in range(len(x[self.stratify_cols[0]]))]}\n",
    "            else:\n",
    "                stratified_creation = lambda x: {'stratified':\n",
    "                                     '_'.join(list(map(str,[x[v] for v in self.stratify_cols]))) \n",
    "                                      }\n",
    "            self.dset = self.dset.map(stratified_creation,\n",
    "                                      batched=self.is_batched,\n",
    "                                      batch_size=self.batch_size,\n",
    "                                      num_proc=self.num_proc)\n",
    "            self.dset=self.dset.class_encode_column(\"stratified\")\n",
    "            # train val split\n",
    "            self.main_ddict = self.dset.train_test_split(test_size=self.val_ratio,\n",
    "                                                         shuffle=True,seed=self.seed,\n",
    "                                                        stratify_by_column='stratified')\n",
    "            self.main_ddict['validation']=self.main_ddict['test']\n",
    "            del self.main_ddict['test']\n",
    "            self.main_ddict=self.main_ddict.remove_columns(['stratified'])\n",
    "            \n",
    "        \n",
    "        del self.dset\n",
    "        print('Done')\n",
    "\n",
    "                             \n",
    "    def _create_label_mapping_func(self,encoder_classes):\n",
    "        if self.is_multihead:\n",
    "            label2idxs = [{v:i for i,v in enumerate(l_classes)} for l_classes in encoder_classes]\n",
    "                    \n",
    "            _func = lambda inp: {'label': [[label2idxs[i][v] for i,v in enumerate(vs)] for vs in zip(*[inp[l] for l in self.label_names])] \\\n",
    "                                    if self.is_batched else [label2idxs[i][v] for i,v in enumerate([inp[l] for l in self.label_names])]\n",
    "                              }\n",
    "            \n",
    "        else:\n",
    "            label2idx = {v:i for i,v in enumerate(encoder_classes[0])}\n",
    "            _func = partial(lambda_map_batch,\n",
    "                           feature=self.label_names[0],\n",
    "                           func=lambda x: label2idx[x],\n",
    "                           output_feature='label',\n",
    "                           is_batched=self.is_batched)\n",
    "        return _func\n",
    "        \n",
    "    def _encode_labels(self):\n",
    "        print_msg('Label Encoding')\n",
    "        if len(self.label_names)>1:\n",
    "            self.is_multihead=True\n",
    "        \n",
    "        if self.label_lists is not None and not isinstance(self.label_lists[0],list):\n",
    "            self.label_lists = [self.label_lists]\n",
    "        \n",
    "        # get label of first row\n",
    "        first_label = self.dset[self.label_names[0]][0] if not self.is_streamed else next(iter(self.dset))[self.label_names[0]]\n",
    "        if isinstance(first_label,list):\n",
    "            # This is multi-label. Ignore self.label_names[1:]\n",
    "            self.label_names = [self.label_names[0]]\n",
    "            self.is_multihead=False\n",
    "            self.is_multilabel=True\n",
    "            \n",
    "        encoder_classes=[]\n",
    "        if not self.is_multilabel:\n",
    "            for idx,l in enumerate(self.label_names):\n",
    "                if self.label_lists is None:\n",
    "                    l_encoder = LabelEncoder()\n",
    "                    _ = l_encoder.fit(self.dset[l])\n",
    "                    l_classes = list(l_encoder.classes_)\n",
    "                else:\n",
    "                    l_classes = sorted(list(self.label_lists[idx]))\n",
    "                encoder_classes.append(l_classes)\n",
    "            \n",
    "            _func = self._create_label_mapping_func(encoder_classes)\n",
    "                \n",
    "            self.dset = self._map_dset(self.dset,_func)\n",
    "\n",
    "            val_key = list(set(self.ddict_rest.keys()) & set(['val','validation','valid']))\n",
    "            if len(val_key)>1: raise ValueError('Your DatasetDict has more than 1 validation split')\n",
    "            if len(val_key)==1:\n",
    "                val_key=val_key[0]\n",
    "                self.ddict_rest[val_key] = self._map_dset(self.ddict_rest[val_key],_func)\n",
    "                    \n",
    "        else:\n",
    "            # For MultiLabel, we transform the label itself to one-hot (or actually, few-hot)\n",
    "            if self.label_lists is None:\n",
    "                l_encoder = MultiLabelBinarizer()\n",
    "                _ = l_encoder.fit(self.dset[self.label_names[0]])\n",
    "                l_classes = list(l_encoder.classes_)\n",
    "            else:\n",
    "                l_classes = sorted(list(self.label_lists[0]))\n",
    "            \n",
    "            encoder_classes.append(l_classes)\n",
    "            \n",
    "            l_encoder = MultiLabelBinarizer(classes=encoder_classes[0])\n",
    "            _ = l_encoder.fit(None)\n",
    "            _func = partial(lambda_map_batch,\n",
    "                            feature=self.label_names[0],\n",
    "                            func=lambda x: l_encoder.transform(x),\n",
    "                            output_feature='label',\n",
    "                            is_batched=self.is_batched,\n",
    "                            is_func_batched=True)\n",
    "            self.dset = self._map_dset(self.dset,_func)                                                  \n",
    "            \n",
    "            val_key = list(set(self.ddict_rest.keys()) & set(['val','validation','valid']))\n",
    "            if len(val_key)>1: raise ValueError('Your DatasetDict has more than 1 validation dataset')\n",
    "            if len(val_key)==1:\n",
    "                val_key=val_key[0]\n",
    "                self.ddict_rest[val_key] = self._map_dset(self.ddict_rest[val_key],_func)\n",
    "            \n",
    "        self.label_lists = encoder_classes\n",
    "        print('Done')\n",
    "        \n",
    "    def _process_metadatas(self,dset,ddict_rest=None):\n",
    "        if len(self.metadatas)>0:\n",
    "            print_msg('Metadata Simple Processing & Concatenating to Main Content')\n",
    "            map_func = partial(concat_metadatas,\n",
    "                               main_text=self.main_text,\n",
    "                               metadatas=self.metadatas,\n",
    "                               process_metas=self.process_metas,\n",
    "                               is_batched=self.is_batched)\n",
    "            dset = self._map_dset(dset,map_func)\n",
    "            if ddict_rest is not None:\n",
    "                ddict_rest = self._map_dset(ddict_rest,map_func)\n",
    "            print('Done')\n",
    "        return dset if ddict_rest is None else (dset,ddict_rest)\n",
    "            \n",
    "            \n",
    "    \n",
    "    def _simplify_ddict(self):\n",
    "        print_msg('Dropping unused features',20)\n",
    "        if self.cols_to_keep is None:\n",
    "            self.cols_to_keep= [self.main_text] + self.metadatas\n",
    "            if self.label_names is not None: self.cols_to_keep+=self.label_names\n",
    "        cols_to_remove = set(self.all_cols) - set(self.cols_to_keep)\n",
    "        self.main_ddict['train']=self.main_ddict['train'].remove_columns(list(cols_to_remove))\n",
    "        if 'validation' in self.main_ddict.keys():\n",
    "            self.main_ddict['validation']=self.main_ddict['validation'].remove_columns(list(cols_to_remove))\n",
    "        print('Done')\n",
    "    \n",
    "    def _do_transformation(self,dset,ddict_rest=None):\n",
    "        if len(self.content_tfms):\n",
    "            print_msg('Text Transformation',20)\n",
    "            for tfm in self.content_tfms:\n",
    "                print_msg(callable_name(tfm))\n",
    "                _func = partial(lambda_map_batch,\n",
    "                               feature=self.main_text,\n",
    "                               func=tfm,\n",
    "                               is_batched=self.is_batched)\n",
    "                dset = self._map_dset(dset,_func)\n",
    "                if ddict_rest is not None:\n",
    "                    ddict_rest = self._map_dset(ddict_rest,_func)\n",
    "            print('Done')\n",
    "        return dset if ddict_rest is None else (dset,ddict_rest)\n",
    " \n",
    "    def _do_filtering(self,dset,ddict_rest=None):\n",
    "        if len(self.filter_dict):\n",
    "            print_msg('Data Filtering',20)\n",
    "            for f,tfm in self.filter_dict.items():\n",
    "                print_msg(f'Do {callable_name(tfm)} on {f}')\n",
    "                _func = partial(lambda_batch,\n",
    "                                feature=f,\n",
    "                                func=tfm,\n",
    "                                is_batched=self.is_batched)\n",
    "                dset = self._filter_dset(dset,_func)\n",
    "                if ddict_rest is not None:\n",
    "                    ddict_rest = self._filter_dset(ddict_rest,_func)\n",
    "            print('Done')\n",
    "        return dset if ddict_rest is None else (dset,ddict_rest)\n",
    "    \n",
    "    def _upsampling(self):\n",
    "        if len(self.upsampling_dict):\n",
    "            print_msg('Upsampling data',20)\n",
    "            results=[]\n",
    "            for f,tfm in self.upsampling_dict.items():\n",
    "                print_msg(f'Do {callable_name(tfm)} on {f}')\n",
    "                _func = partial(lambda_batch,\n",
    "                                feature=f,\n",
    "                                func=tfm,\n",
    "                                is_batched=self.is_batched)\n",
    "                new_dset = self._filter_dset(self.main_ddict['train'],_func)\n",
    "                results.append(new_dset)\n",
    "            # slow concatenation for iterable dataset    \n",
    "            self.main_ddict['train'] = concatenate_datasets(results+[self.main_ddict['train']])\n",
    "            print('Done')\n",
    "      \n",
    "    def _do_augmentation(self):\n",
    "        \n",
    "        if len(self.aug_tfms):\n",
    "            print_msg('Text Augmentation',20)\n",
    "\n",
    "            seed_notorch(self.seed)\n",
    "            if not self.is_streamed:  \n",
    "#                 self.main_ddict['train'] = self.main_ddict['train'].with_transform(partial(augmentation_helper,\n",
    "#                                                                        text_name=self.main_text,\n",
    "#                                                                        func=partial(func_all,functions=self.aug_tfms)))              \n",
    "                for tfm in self.aug_tfms:\n",
    "                    print_msg(callable_name(tfm))\n",
    "                    _func = partial(lambda_map_batch,\n",
    "                                   feature=self.main_text,\n",
    "                                   func=tfm,\n",
    "                                   is_batched=self.is_batched)\n",
    "                    self.main_ddict['train'] = self._map_dset(self.main_ddict['train'],_func)\n",
    "\n",
    "            else:\n",
    "                self.main_ddict['train'] = IterableDataset.from_generator(augmentation_stream_generator,\n",
    "                                               features = self.main_ddict['train'].features,\n",
    "                                               gen_kwargs={'dset': self.main_ddict['train'],\n",
    "                                                           'text_name':self.main_text,\n",
    "                                                           'func':partial(func_all,functions=self.aug_tfms)\n",
    "                                                          })\n",
    "            print('Done')\n",
    "        \n",
    "    def _convert_to_iterable(self):\n",
    "        if not self.is_streamed:\n",
    "            self.main_ddict['train'] = self.main_ddict['train'].to_iterable_dataset(num_shards=self.num_shards)\n",
    "            self.is_streamed=True\n",
    "            \n",
    "    def _do_train_shuffling(self):\n",
    "        print_msg('Shuffling train set',20)\n",
    "        self.main_ddict['train'] = self.main_ddict['train'].shuffle(seed=self.seed, buffer_size=self.buffer_size)\n",
    "        print('Done')\n",
    "        \n",
    "    def do_all_preprocessing(self,shuffle_trn=True): \n",
    "        if self._processed_call:\n",
    "            warnings.warn('Your dataset has already been processed. Returning the previous processed DatasetDict...')\n",
    "            return self.main_ddict\n",
    "            \n",
    "        print_msg('Start Main Text Processing',20)\n",
    "        \n",
    "        # Filtering\n",
    "        self.dset,self.ddict_rest = self._do_filtering(self.dset,self.ddict_rest)\n",
    "        \n",
    "        # Process metadatas\n",
    "        self.dset,self.ddict_rest = self._process_metadatas(self.dset,self.ddict_rest)\n",
    "        \n",
    "        # Process labels\n",
    "        if self.label_names is not None:\n",
    "            self._encode_labels()\n",
    "        \n",
    "        # Content transformation\n",
    "        self.dset,self.ddict_rest = self._do_transformation(self.dset,self.ddict_rest)\n",
    "         \n",
    "        # Train Test Split.\n",
    "        ### self.main_ddict is created here\n",
    "        self._train_test_split()\n",
    "        \n",
    "        # Dropping unused columns\n",
    "        self._simplify_ddict()\n",
    "        \n",
    "        # Check validation leaking\n",
    "        self._check_validation_leaking()\n",
    "        \n",
    "        ### The rest of these functions applies only to the train dataset\n",
    "        # Upsampling\n",
    "        self._upsampling()\n",
    "        \n",
    "        # Augmentation\n",
    "        self._do_augmentation()\n",
    "           \n",
    "        # Convert train set to iterable\n",
    "        self._convert_to_iterable()\n",
    "        \n",
    "        # Shuffle train\n",
    "        if shuffle_trn:\n",
    "            self._do_train_shuffling()\n",
    "        \n",
    "        self._processed_call=True\n",
    "        \n",
    "        return self.main_ddict\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    def do_tokenization(self,\n",
    "                       tokenizer, # Tokenizer (preferably from HuggingFace)\n",
    "                       is_split_into_words=False, # Is text split into words or not\n",
    "                       max_length=None, # pad to model's allowed max length (default is max_sequence_length)\n",
    "                       trn_size=None, # The number of training data to be tokenized\n",
    "                      ):\n",
    "        print_msg('Tokenization',20)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_split_into_words= is_split_into_words\n",
    "        self.max_length = max_length\n",
    "        if trn_size is not None:\n",
    "            if trn_ratio<1:\n",
    "                raise ValueError(\"Length of streamed dataset is unknown to use floating ratio\")\n",
    "            self.main_ddict['train'] = self.main_ddict['train'].take(trn_ratio)\n",
    "        \n",
    "        for k in self.main_ddict.keys():\n",
    "            self.main_ddict[k] = self.main_ddict[k].map(partial(tokenize_function,\n",
    "                                                                    text_name=self.main_text,\n",
    "                                                                    tok=tokenizer,\n",
    "                                                                    is_split_into_words=is_split_into_words,\n",
    "                                                                    max_length=max_length),\n",
    "                                                            batched=True, # always true\n",
    "                                                            batch_size=self.batch_size\n",
    "                                                           )\n",
    "        print('Done')\n",
    "        return self.main_ddict\n",
    "        \n",
    "    def process_and_tokenize(self,\n",
    "                             tokenizer, # Tokenizer (preferably from HuggingFace)\n",
    "                             is_split_into_words=False, # Is text split into list or not\n",
    "                             max_length=None, # pad to model's allowed max length (default is max_sequence_length)\n",
    "                             trn_size=None, # The number of training data to be tokenized\n",
    "                             shuffle_trn=True, # To shuffle the train set before tokenization\n",
    "                            ):\n",
    "        _ = self.do_all_preprocessing(shuffle_trn)\n",
    "        _ = self.do_tokenization(tokenizer,is_split_into_words,max_length,trn_size)\n",
    "        \n",
    "    \n",
    "    def set_data_collator(self,data_collator):\n",
    "        self.data_collator = data_collator\n",
    "        \n",
    "    \n",
    "    def prepare_test_dataset_from_csv(self,file_path):\n",
    "        file_path = Path(file_path)\n",
    "        ds = load_dataset(str(file_path.parent),\n",
    "                          data_files=file_path.name,\n",
    "                          split='train')\n",
    "        return self.prepare_test_dataset(ds)\n",
    "    \n",
    "    def prepare_test_dataset_from_df(self,df,validate=True):\n",
    "        if validate:\n",
    "            check_input_validation(df)\n",
    "        ds = Dataset.from_pandas(df)\n",
    "        return self.prepare_test_dataset(ds)\n",
    "        \n",
    "    def prepare_test_dataset(self,test_dset,do_filtering=False):\n",
    "        print_msg('Start Test Set Transformation',20)\n",
    "\n",
    "        # Filtering\n",
    "        if do_filtering:\n",
    "            test_dset = self._do_filtering(test_dset)\n",
    "        \n",
    "        # Process metadatas\n",
    "        test_dset = self._process_metadatas(test_dset)\n",
    "        \n",
    "        \n",
    "        # Content transformation\n",
    "        test_dset = self._do_transformation(test_dset)\n",
    "        \n",
    "        # Tokenization\n",
    "        print_msg('Tokenization',20)\n",
    "        test_dset = test_dset.map(partial(tokenize_function,\n",
    "                                          text_name=self.main_text,\n",
    "                                          tok=tokenizer,\n",
    "                                          is_split_into_words=is_split_into_words,\n",
    "                                          max_length=max_length),\n",
    "                                  batched=True, # always true\n",
    "                                  batch_size=self.batch_size\n",
    "                                 )\n",
    "        return test_dset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### TextDataController\n",
       "\n",
       ">      TextDataController (inp, main_text:str, label_names=None,\n",
       ">                          class_names_predefined=None, filter_dict={},\n",
       ">                          metadatas=[], process_metas=True,\n",
       ">                          content_transformations=[],\n",
       ">                          val_ratio:list|float|None=0.2, stratify_cols=[],\n",
       ">                          upsampling_dict={}, content_augmentations=[],\n",
       ">                          seed=None, is_batched=True, batch_size=1000,\n",
       ">                          num_proc=4, cols_to_keep=None, buffer_size=10000,\n",
       ">                          num_shards=64)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | HuggingFace Dataset or DatasetDict |\n",
       "| main_text | str |  | Name of the main text column |\n",
       "| label_names | NoneType | None | Names of the label (dependent variable) columns |\n",
       "| class_names_predefined | NoneType | None | List of names associated with the labels (same index order) |\n",
       "| filter_dict | dict | {} | A dictionary: {feature: filtering_function_based_on_the_feature} |\n",
       "| metadatas | list | [] | Names of the metadata columns |\n",
       "| process_metas | bool | True | Whether to do simple text processing on the chosen metadatas |\n",
       "| content_transformations | list | [] | A list of text transformations |\n",
       "| val_ratio | list \\| float \\| None | 0.2 | Ratio of data for validation set. If given a list, validation set will be chosen based on indices in this list |\n",
       "| stratify_cols | list | [] | Column(s) needed to do stratified shuffle split |\n",
       "| upsampling_dict | dict | {} | A dictionary: {feature: upsampling_function_based_on_the_feature} |\n",
       "| content_augmentations | list | [] | A list of text augmentations |\n",
       "| seed | NoneType | None | Random seed |\n",
       "| is_batched | bool | True | Whether to perform operations in batch |\n",
       "| batch_size | int | 1000 | Batch size, for when is_batched is True |\n",
       "| num_proc | int | 4 | Number of process for multiprocessing |\n",
       "| cols_to_keep | NoneType | None | Columns to keep after all processings |\n",
       "| buffer_size | int | 10000 | For shuffling data |\n",
       "| num_shards | int | 64 | Number of shards |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### TextDataController\n",
       "\n",
       ">      TextDataController (inp, main_text:str, label_names=None,\n",
       ">                          class_names_predefined=None, filter_dict={},\n",
       ">                          metadatas=[], process_metas=True,\n",
       ">                          content_transformations=[],\n",
       ">                          val_ratio:list|float|None=0.2, stratify_cols=[],\n",
       ">                          upsampling_dict={}, content_augmentations=[],\n",
       ">                          seed=None, is_batched=True, batch_size=1000,\n",
       ">                          num_proc=4, cols_to_keep=None, buffer_size=10000,\n",
       ">                          num_shards=64)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | HuggingFace Dataset or DatasetDict |\n",
       "| main_text | str |  | Name of the main text column |\n",
       "| label_names | NoneType | None | Names of the label (dependent variable) columns |\n",
       "| class_names_predefined | NoneType | None | List of names associated with the labels (same index order) |\n",
       "| filter_dict | dict | {} | A dictionary: {feature: filtering_function_based_on_the_feature} |\n",
       "| metadatas | list | [] | Names of the metadata columns |\n",
       "| process_metas | bool | True | Whether to do simple text processing on the chosen metadatas |\n",
       "| content_transformations | list | [] | A list of text transformations |\n",
       "| val_ratio | list \\| float \\| None | 0.2 | Ratio of data for validation set. If given a list, validation set will be chosen based on indices in this list |\n",
       "| stratify_cols | list | [] | Column(s) needed to do stratified shuffle split |\n",
       "| upsampling_dict | dict | {} | A dictionary: {feature: upsampling_function_based_on_the_feature} |\n",
       "| content_augmentations | list | [] | A list of text augmentations |\n",
       "| seed | NoneType | None | Random seed |\n",
       "| is_batched | bool | True | Whether to perform operations in batch |\n",
       "| batch_size | int | 1000 | Batch size, for when is_batched is True |\n",
       "| num_proc | int | 4 | Number of process for multiprocessing |\n",
       "| cols_to_keep | NoneType | None | Columns to keep after all processings |\n",
       "| buffer_size | int | 10000 | For shuffling data |\n",
       "| num_shards | int | 64 | Number of shards |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataController)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data + Basic use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a `TextDataController` from a csv, pandas DataFrame, or directly from a HuggingFace dataset object. Currently, `TextDataController` is designed for text classification, so you must provide the column name for the label (or multi-label)\n",
    "\n",
    "We will load a sample data, modified to match a task where you need to determine which category `L1` a comment (`Content`) belongs to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset source: https://www.kaggle.com/datasets/kavita5/review_ecommerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23486, 10)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1711</th>\n",
       "      <td>970</td>\n",
       "      <td>37</td>\n",
       "      <td>Soft and flattering</td>\n",
       "      <td>This is a cute work jacket, as well as paired ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General</td>\n",
       "      <td>Jackets</td>\n",
       "      <td>Jackets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1870</th>\n",
       "      <td>1080</td>\n",
       "      <td>33</td>\n",
       "      <td>Gorgeous!</td>\n",
       "      <td>I feel like an indian princess in this dress! ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>873</td>\n",
       "      <td>42</td>\n",
       "      <td>Love the color!</td>\n",
       "      <td>I love this shirt so much i am ordering the co...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17773</th>\n",
       "      <td>819</td>\n",
       "      <td>67</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Loved the fit and colors, but the fabric is ve...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10683</th>\n",
       "      <td>1083</td>\n",
       "      <td>32</td>\n",
       "      <td>Showstopper!</td>\n",
       "      <td>This dress has been on retailer's site for a w...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Clothing ID  Age                Title   \n",
       "1711           970   37  Soft and flattering  \\\n",
       "1870          1080   33            Gorgeous!   \n",
       "1058           873   42      Love the color!   \n",
       "17773          819   67                  NaN   \n",
       "10683         1083   32         Showstopper!   \n",
       "\n",
       "                                             Review Text  Rating   \n",
       "1711   This is a cute work jacket, as well as paired ...       5  \\\n",
       "1870   I feel like an indian princess in this dress! ...       5   \n",
       "1058   I love this shirt so much i am ordering the co...       5   \n",
       "17773  Loved the fit and colors, but the fabric is ve...       2   \n",
       "10683  This dress has been on retailer's site for a w...       5   \n",
       "\n",
       "       Recommended IND  Positive Feedback Count Division Name Department Name   \n",
       "1711                 1                        1       General         Jackets  \\\n",
       "1870                 1                        0       General         Dresses   \n",
       "1058                 1                        3       General            Tops   \n",
       "17773                0                       18       General            Tops   \n",
       "10683                1                        0       General         Dresses   \n",
       "\n",
       "      Class Name  \n",
       "1711     Jackets  \n",
       "1870     Dresses  \n",
       "1058       Knits  \n",
       "17773    Blouses  \n",
       "10683    Dresses  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a `TextDataController` from a dataframe. This also provides a quick input validation check (NaN check and Duplication check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Input Validation Precheck -----\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create a `TextDataController` directly from the csv file. The good thing about using HuggingFace Dataset as the main backend of the TextDataController is that you can utilize lots of its useful functionality, such as caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataController.from_csv('sample_data/Womens_Clothing_Reviews.csv',\n",
    "                                  main_text='Review Text',\n",
    "                                  label_names='Department Name',\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create a `TextDataController` from a HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'],\n",
       "    num_rows: 23486\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         seed=42\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we noticed, our dataset has missing values in the text field and the label field. For now, let's load the data as a Pandas' DataFrame, perform some cleaning, and create our `TextDataController`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(~df['Review Text'].isna()) & (~df['Department Name'].isna())].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Input Validation Precheck -----\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title    2966\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 1 rows\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you can start perform 2 important steps on your data\n",
    "\n",
    "1. Text preprocessings, Label Encoding, Train/Validation Split\n",
    "2. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't provided any preprocessings to the `TextDataController`; we will see more on how to use preprocessings (step by step) as we progress. In fact, we can even perform NaN filtering as a preprocessing step inside `TextDataController`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 2, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "ddict = tdc.do_all_preprocessing(shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text', 'Department Name', 'label'],\n",
       "        num_rows: 4526\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our DatasetDict now has two split: train and validation. Note that train split is now IterableDataset, for processing efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Review Text': [\"Goes with absolutely everything. it's very comfortable and versatile. can be dressed up or dressed down. great thing to have in the closet when you don't know what to wear!\",\n",
       "  \"This is a beautiful blouse...sheer and feminine. i am small busted and slender so i need a size smaller than usual. it is a full top...can't tell exactly how full in the photos but with a small chest there is just too much under the arms. so if your chest is more ample you could prob order your regular size. this is supposed to be a full, shorter fit...i would say the style is going to look better on someone who is a little taller with a medium sized bust rather than someone who is shorter and b\",\n",
       "  'I\\'m a size 28 jeans 5\\'9 with a bubble bottom. these were falling off of me around the hips. these are warm. i was expecting them to not be sheer since they are fleece lined but when i sat down i was shocked to notice it. it doesn\\'t matter functionality wise because i never felt a \"draft.\" they look good and are well made but i will be returning due to the lack of fit.'],\n",
       " 'Department Name': ['Tops', 'Tops', 'Intimate'],\n",
       " 'label': [4, 4, 2]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict['validation'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Review Text': 'I must agree with some of the other reviewers--this sweater is so pretty! but the quality of the sweater is not great. my sensitive skin found it to be itchy. so sadly i had to return it.', 'Department Name': 'Tops', 'label': 4}\n",
      "{'Review Text': \"This skirt is about an inch longer on me than on the model in photo (i'm 5 ft, ordered petite) but the length actually works. i have a bit of a tummy, so ordered size 4 - fits great. really love the colors - so many tops will go with this skirt, so it will be easy to vary the look.\", 'Department Name': 'Bottoms', 'label': 0}\n",
      "{'Review Text': 'I typically wear a 4/6 but am a little bigger right now, so i went with the medium. was so excited to get these but was very disappointed when i tried them on. they look beautiful, but there is no lining, leaving the fabric uncomfortable. they were also huge! definitely make sure you size down if you d', 'Department Name': 'Bottoms', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(ddict['train']):\n",
    "    print(v)\n",
    "    if i==2: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start with the tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80dead651ca0475d868c1fe6d02b53ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345e8935b96b420482806fde67ccbb2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce13093eb424e65b359d128286b55c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "ddict = tdc.do_tokenization(tokenizer,max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4526\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 534, 8013, 19, 3668, 960, 4, 24, 18, 182, 3473, 8, 16106, 4, 64, 28, 7001, 62, 50, 7001, 159, 4, 372, 631, 7, 33, 11, 5, 16198, 77, 47, 218, 75, 216, 99, 7, 3568, 328, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(ddict['validation'][0]['input_ids'][:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 100, 531, 2854, 19, 103, 9, 5, 97, 34910, 5579, 9226, 23204, 16, 98, 1256, 328, 53, 5, 1318, 9, 5, 23204, 16, 45, 372, 4, 127, 5685, 3024, 303, 24, 7, 28, 24, 17414, 4, 98, 16748, 939, 56, 7, 671, 24, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(ddict['train']))['input_ids'][:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `TextDataController`, you can also perform Text Processing and Tokenization with one method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Input Validation Precheck -----\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title    2966\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 1 rows\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 2, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the DatasetDict from the instance variable `main_ddict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4526\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DatasetDict is ready to be put into any HuggingFace text model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This preprocessing step allow you to filter out certain values of a certain column in your dataset. Let's say I want to filter out any 'HC search' value in the column 'Source'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Source\n",
       "Google Play    1434\n",
       "Non Owned       499\n",
       "Owned           139\n",
       "iOS             124\n",
       "HC search        73\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Source.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will provide a dictionary containing the name of the column and the filtering function to apply on that column. Note that the preprocessing step will auto-remove some unused columns, so we need to provide a list of columns to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['sample_large.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Content',\n",
    "                         label_names='L1',\n",
    "                         filter_dict={'Source':lambda x: x!='HC search'},\n",
    "                         cols_to_keep=['Source','Content','L1'],\n",
    "                         seed=42\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-66d19029fdb1ba64_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-28b4f526c9790ebf_*_of_00004.arrow\n",
      "Loading cached split indices for dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-0dc37ad5a804f1d2.arrow and /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-c37dbd859a7e31eb.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Source -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 8, which is 0.46% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/1756 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-0da2cd7fcb59203d.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Source', 'Content', 'L1', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 440\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check whether 'HC search' is still in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Google Play', 'Non Owned', 'Owned', 'iOS'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(tdc.main_ddict['validation']['Source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Google Play', 'Non Owned', 'Owned', 'iOS'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([v['Source'] for v in tdc.main_ddict['train']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even add multiple filtering functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "L1\n",
       "Others                     811\n",
       "Feature                    541\n",
       "Commercial                 305\n",
       "Delivery                   200\n",
       "Shopee account             186\n",
       "Buyer complained seller     64\n",
       "Return/Refund               45\n",
       "Payment                     44\n",
       "Order/Item                  41\n",
       "Services                    32\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.L1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['sample_large.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Content',\n",
    "                         label_names='L1',\n",
    "                         filter_dict={'Source':lambda x: x!='HC search',\n",
    "                                      'L1': lambda x: x not in ['Order/Item','Services']\n",
    "                                     },\n",
    "                         cols_to_keep=['Source','Content','L1'],\n",
    "                         seed=42\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-66d19029fdb1ba64_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-d9d0358cd1660021_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-024d2fb434ed2194_*_of_00004.arrow\n",
      "Loading cached split indices for dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-caa70b0e2d1ee8fa.arrow and /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-0335773383a168d1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Source -----\n",
      "----- Do <lambda> on L1 -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 4, which is 0.24% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/1700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-de7a39d90b9efb49.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 'L1' is our label, we can access the label list to check whether our L1 filtering is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Buyer complained seller',\n",
       "  'Commercial',\n",
       "  'Delivery',\n",
       "  'Feature',\n",
       "  'Others',\n",
       "  'Payment',\n",
       "  'Return/Refund',\n",
       "  'Shopee account']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.label_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Google Play', 'Non Owned', 'Owned', 'iOS'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(tdc.main_ddict['validation']['Source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadatas concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we think the metadatas can be helpful, we can concatenate them into the front of your text, so that our text classification model is aware of it.\n",
    "\n",
    "In this example, 'Source' will be our metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-1ce83717afece17c_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-4667f0b67eaedd6e_*_of_00004.arrow\n",
      "Loading cached split indices for dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-613c169dda05938f.arrow and /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-904746b36977d169.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 7, which is 0.39% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/1815 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-904b6d93e98f2824.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['sample_large.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Content',\n",
    "                         label_names='L1',\n",
    "                         metadatas='Source',\n",
    "                         process_metas=False,\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Non Owned . Huawei mở đặt trước đồng hồ Watch Fit 2, Watch GT 3 Pro và Watch Kids Pro 4'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(tdc.main_ddict['train']))['Content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add multiple metadatas. Let's say L2 is the second metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-02e842e167e8cd44_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-ac039bba33dde05e_*_of_00004.arrow\n",
      "Loading cached split indices for dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-496b3bea3d2b2904.arrow and /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-476c2414740b2f39.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 5, which is 0.28% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/1815 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-045a55c61ba6226a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['sample_large.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Content',\n",
    "                         label_names='L1',\n",
    "                         metadatas=['Source','L2'],\n",
    "                         process_metas=False,\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Process RR . Non Owned . :((( mình chưa nhận được hàng mà nó đã hiện được yêu cầu trả hàng hoàn tiền rồi là ntn ạ? Đơn mình đặt mà mình k có thông tin gì về nó luôn mng cho mình xin cách liên hệ với shopee với ạ? :((( chưa nhận được thì nó có cho mình hoàn tiền hay là nó chuyển vào mục review luôn rồi k ạ?'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(tdc.main_ddict['train']))['Content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to preprocess the metadata (currently it's just empty space stripping and lowercasing), set `process_metas` to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-65897bac5e39cbaa_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-ea0e555513f7aaf9_*_of_00004.arrow\n",
      "Loading cached split indices for dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-41c8d98a12fe2203.arrow and /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-254380030a2a01ec.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 7, which is 0.39% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/1815 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-434bc0967abeb3ec.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['sample_large.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Content',\n",
    "                         label_names='L1',\n",
    "                         metadatas='Source',\n",
    "                         process_metas=True,\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'non owned . Huawei mở đặt trước đồng hồ Watch Fit 2, Watch GT 3 Pro và Watch Kids Pro 4'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(tdc.main_ddict['train']))['Content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have briefly gone through the simplest case of label encodings, which is when we only need to predict 1 single label (L1). In this library this is called **single head classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-056a3d88e5d7e7f1_*_of_00004.arrow\n",
      "Loading cached split indices for dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-417be8257836e100.arrow and /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-1969cf23af4918bb.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-85a87410822b39a8_*_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 7, which is 0.39% of training set\n",
      "Filtering leaked data out of training set...\n",
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-f91699a587bf9b9a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['sample_large.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Content',\n",
    "                         label_names='L1',\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All label names will be saved in instance variable `label_lists`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Buyer complained seller',\n",
       "  'Commercial',\n",
       "  'Delivery',\n",
       "  'Feature',\n",
       "  'Order/Item',\n",
       "  'Others',\n",
       "  'Payment',\n",
       "  'Return/Refund',\n",
       "  'Services',\n",
       "  'Shopee account']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.label_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and all labels will be encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 2, 5, 5]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['validation']['label'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also keep the original labeling, for references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Feature', 'Others', 'Delivery', 'Others', 'Others']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['validation']['L1'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say our case is no longer predicting 1 single thing. What if we need to predict 2 different labels as once (this is called **multi-head classification**). For example, let's define our dataset so that we need to predict both L1 and L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2269 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 7, which is 0.39% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/1815 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/454 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['sample_large.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Content',\n",
    "                         label_names=['L1','L2'],\n",
    "                         seed=42,\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 56)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tdc.label_lists[0]),len(tdc.label_lists[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have two lists, one for label names of L1, and one for label names of L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Content', 'L1', 'L2', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 454\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Feature', 'Others', 'Delivery', 'Others', 'Others']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['validation']['L1'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['App performance', 'Cannot defined', 'Shipping fee', 'Cannot defined', 'Scam']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['validation']['L2'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 2], [5, 8], [2, 47], [5, 8], [5, 41]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['validation']['label'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's define a **multi-label classification**, where a text can have 1 or more label. In this example, we will combine L1 and L2 to have a label containing multiple values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_data/sample_large.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Content</th>\n",
       "      <th>L1</th>\n",
       "      <th>L2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google Play</td>\n",
       "      <td>App ncc lúc nào cx lag đơ, phần tìm kiếm thì v...</td>\n",
       "      <td>Feature</td>\n",
       "      <td>App performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Non Owned</td>\n",
       "      <td>..❗️ GÓC THANH LÝ Tính ra rẻ hơn cả mua #Shope...</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>Items/price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Google Play</td>\n",
       "      <td>Mắc gì người ta đặt hàng toàn lỗi 😃????</td>\n",
       "      <td>Feature</td>\n",
       "      <td>App performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Owned</td>\n",
       "      <td>#GhienShopeePayawardT8 Khi bạn chơi shopee quá...</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>Shopee Programs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Google Play</td>\n",
       "      <td>Rất bức xúc khi dùng . mã giảm giá người dùng ...</td>\n",
       "      <td>Feature</td>\n",
       "      <td>Apply Voucher</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Source                                            Content          L1   \n",
       "0  Google Play  App ncc lúc nào cx lag đơ, phần tìm kiếm thì v...     Feature  \\\n",
       "1    Non Owned  ..❗️ GÓC THANH LÝ Tính ra rẻ hơn cả mua #Shope...  Commercial   \n",
       "2  Google Play            Mắc gì người ta đặt hàng toàn lỗi 😃????     Feature   \n",
       "3        Owned  #GhienShopeePayawardT8 Khi bạn chơi shopee quá...  Commercial   \n",
       "4  Google Play  Rất bức xúc khi dùng . mã giảm giá người dùng ...     Feature   \n",
       "\n",
       "                L2  \n",
       "0  App performance  \n",
       "1      Items/price  \n",
       "2  App performance  \n",
       "3  Shopee Programs  \n",
       "4    Apply Voucher  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['L1L2'] = df[['L1','L2']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['L1','L2'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Content</th>\n",
       "      <th>L1L2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google Play</td>\n",
       "      <td>App ncc lúc nào cx lag đơ, phần tìm kiếm thì v...</td>\n",
       "      <td>[Feature, App performance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Non Owned</td>\n",
       "      <td>..❗️ GÓC THANH LÝ Tính ra rẻ hơn cả mua #Shope...</td>\n",
       "      <td>[Commercial, Items/price]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Google Play</td>\n",
       "      <td>Mắc gì người ta đặt hàng toàn lỗi 😃????</td>\n",
       "      <td>[Feature, App performance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Owned</td>\n",
       "      <td>#GhienShopeePayawardT8 Khi bạn chơi shopee quá...</td>\n",
       "      <td>[Commercial, Shopee Programs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Google Play</td>\n",
       "      <td>Rất bức xúc khi dùng . mã giảm giá người dùng ...</td>\n",
       "      <td>[Feature, Apply Voucher]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Source                                            Content   \n",
       "0  Google Play  App ncc lúc nào cx lag đơ, phần tìm kiếm thì v...  \\\n",
       "1    Non Owned  ..❗️ GÓC THANH LÝ Tính ra rẻ hơn cả mua #Shope...   \n",
       "2  Google Play            Mắc gì người ta đặt hàng toàn lỗi 😃????   \n",
       "3        Owned  #GhienShopeePayawardT8 Khi bạn chơi shopee quá...   \n",
       "4  Google Play  Rất bức xúc khi dùng . mã giảm giá người dùng ...   \n",
       "\n",
       "                            L1L2  \n",
       "0     [Feature, App performance]  \n",
       "1      [Commercial, Items/price]  \n",
       "2     [Feature, App performance]  \n",
       "3  [Commercial, Shopee Programs]  \n",
       "4       [Feature, Apply Voucher]  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't have to add any extra argument; the controller will determine whether this is for multilabel classification, based on the format of the label values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Input Validation Precheck -----\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 16 rows\n",
      "-------------------- Start Main Text Processing --------------------\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2269 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 7, which is 0.39% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/1815 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/454 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Content',\n",
    "                                 label_names=['L1L2'],\n",
    "                                 seed=42,\n",
    "                                )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tdc.label_lists[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(tdc.main_ddict['validation']['label'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is **multilabel classification**, the label will be one-hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Feature', 'App performance']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['validation']['L1L2'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a quick check to see whether it works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('App performance', 'Feature')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.label_lists[0][2],tdc.label_lists[0][24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Phần trả hàng hoàn tiền ko bao giờ thành công, app chậm, duyệt cẩn thận mấy bọn buôn bán trên shopee toàn hàng nhái rồi sai hàng, quá tệ',\n",
       "       'Cần nhường bán lại   Mình bán giày thể thao với giá chỉ ₫80.000 - ₫150.000. Mua ngay trên Shopee nhé!',\n",
       "       'Lỗi ko vào đc', 'Như cc v',\n",
       "       'Sao ko lm trang trên web cứ bắt phải tải app ?', 'Như quần',\n",
       "       'Ai biết chỗ nào bán serum inod trị hôi nách chính hãng ko,bữa mua trên shoppe bị trúng hàng giả',\n",
       "       'Liên kết shopeepay danh tính chỉ được một lần, khi mà đổi sdt gọi điện mãi mới chịu hỗ trợ. Đổi cho rồi thì bị khóa nick shopee mới, nói là lạm dụng mã giảm giá????? Ba lần rồi cứ mở cho sau lại khóa, khó chịu ghét vl, mất bao nhiêu thời gian, vấn đề. Không biết lần này có chịu mở cho không nữa, tức vãi',\n",
       "       'Lỗi ko à', 'Bọn chó . chưa ji khóa tài khoản'], dtype=object)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Content.sample(10).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁', '<unk>', 'D', 'ior', '▁Sau', 'v', 'age', '▁ED', 'P', '▁100', 'ml', '<unk>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(tokenizer('⭐️𝑫𝒊𝒐𝒓 𝑺𝒂𝒖𝒗𝒂𝒈𝒆 𝑬𝑫𝑷 100ml⭐️')['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: do something wicked, such as detect whether there's website in the text, if yes, concat to front"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Capability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Source', 'Content', 'L1', 'L2'],\n",
       "    num_rows: 2269\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main_ddict = load_dataset('secret_data',data_files=['buyer_listening_with_all_raw_data_w28.csv','buyer_listening_with_all_raw_data_w28.csv'],split='train')\n",
    "# main_ddict\n",
    "\n",
    "# main_ddict = load_dataset('sample_data',data_files=['sample_large.csv','sample_large.csv','sample_large.csv'])\n",
    "# main_ddict\n",
    "\n",
    "main_ddict = load_dataset('sample_data',data_files=['sample_large.csv'],split='train')\n",
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Source': ['Google Play', 'Non Owned', 'Google Play', 'Owned', 'Google Play'],\n",
       " 'Content': ['App ncc lúc nào cx lag đơ, phần tìm kiếm thì viết kiểu gì sp đó vẫn ko ra, thế phải ghi đúng tên mới chịu à? Lỡ quên tên ngta ghi mé mé như v cx phải gợi ý sp tương tự chứ??? ☻',\n",
       "  '..❗️ GÓC THANH LÝ Tính ra rẻ hơn cả mua #Shopee Mong 1 lần đc check ib mỏi tay 😆😆😆   Em chuyển cửa hàng nên dọn lại có thừa vài chục tấm nệm xuất nhật này.   1mx2m : 1m2x2 : 1m4x2m : 1m6x2m : 1m8x2m :2mx2m Đệm dày 7-8 phân Nhưng vì còn ít nên topic này em bán thanh lý giá rẻ ạ.   Ship cod nhận hàng được kiểm tra thoải mái.  Miễn ship toàn quốc. Nên đừng bom tội nghiệp em nhé dày 7-8 phân Nhắn tin em gửi mẫu nhé🥰',\n",
       "  'Mắc gì người ta đặt hàng toàn lỗi 😃????',\n",
       "  '#GhienShopeePayawardT8 Khi bạn chơi shopee quá lâu thì không thể nào không biết đến với ShopeePay . Liên kết thanh toán được cho các đơn hàng Shopee và ShopeeFood luôn nè.',\n",
       "  'Rất bức xúc khi dùng . mã giảm giá người dùng thì m02 vậy cho ưu đãi đấy làm gì ạ . Mua đc mấy đơn thì bị m04 vậy tải về để mua hàng nhưng bh lại để làm cảnh . Thật sự là ko đáng để đc đánh giá một sao'],\n",
       " 'L1': ['Feature', 'Commercial', 'Feature', 'Commercial', 'Feature'],\n",
       " 'L2': ['App performance',\n",
       "  'Items/price',\n",
       "  'App performance',\n",
       "  'Shopee Programs',\n",
       "  'Apply Voucher']}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 210 µs, sys: 118 µs, total: 328 µs\n",
      "Wall time: 227 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Source': 'Google Play',\n",
       " 'Content': 'App ncc lúc nào cx lag đơ, phần tìm kiếm thì viết kiểu gì sp đó vẫn ko ra, thế phải ghi đúng tên mới chịu à? Lỡ quên tên ngta ghi mé mé như v cx phải gợi ý sp tương tự chứ??? ☻',\n",
       " 'L1': 'Feature',\n",
       " 'L2': 'App performance'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "main_ddict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_filter_dict={'L1':lambda x: x!='Others'}\n",
    "\n",
    "_content_tfms = partial(apply_vnmese_word_tokenize,normalize_text=True)\n",
    "_content_tfms.__name__='VNM word segmentation'\n",
    "\n",
    "_upsampling_dict={\n",
    "    'Source': lambda x: x=='hc search' if random.random()<0.5 else False\n",
    "}\n",
    "\n",
    "_aug_tfms=partial(remove_vnmese_accent,prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc = TextDataController(main_ddict,main_text='Content',\n",
    "                         label_names='L1',\n",
    "                         filter_dict=_filter_dict,\n",
    "                         metadatas='Source',\n",
    "                         content_transformations=_content_tfms,\n",
    "                         val_ratio=0.25,\n",
    "                         stratify_cols='Source',\n",
    "                         upsampling_dict=_upsampling_dict,\n",
    "                         content_augmentations=_aug_tfms,\n",
    "                         seed=42,\n",
    "                         is_batched=True,\n",
    "                         is_streamed=False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-e8025282b10ede62_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-0ecebfc0c7f83a1d_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-966fc97b537cb573_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-9067e41b077d8eb9_*_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on L1 -----\n",
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Text Transformation --------------------\n",
      "----- VNM word segmentation -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1458 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/1458 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 1, which is 0.09% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/1093 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Upsampling data --------------------\n",
      "----- Do <lambda> on Source -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/1091 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Text Augmentation --------------------\n",
      "----- remove_vnmese_accent -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "CPU times: user 252 ms, sys: 154 ms, total: 406 ms\n",
      "Wall time: 736 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "my_ddict = tdc.do_all_preprocessing(shuffle_trn=True)\n",
    "# 2x big data\n",
    "# is_batched True, shuffle_trn True. Wall time: 1min 6s\n",
    "# is_batched False, shuffle_trn True, Wall time: 1min 18s\n",
    "\n",
    "# 3x sample_large.csv\n",
    "# is_batched True, shuffle_trn True. Wall time: 2.79 s\n",
    "# is_batched False, shuffle_trn True, Wall time: 2.96 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Source', 'Content', 'L1'],\n",
       "        num_rows: 365\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 478 µs, sys: 0 ns, total: 478 µs\n",
      "Wall time: 310 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Source': 'non owned',\n",
       " 'Content': 'non owned . Mã_500K toàn sàn Shopee cho ai cần nè » https://shope.ee/10QzJtpqQi',\n",
       " 'L1': 1}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "my_ddict['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp=iter(my_ddict['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.2 ms, sys: 301 µs, total: 28.5 ms\n",
      "Wall time: 26.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Source': 'google play',\n",
       " 'Content': 'google play . Chán ơi là chánnnn Toàn_bị U02_Mn có biết cách nào chữa ko thì chỉ em với ạ 🥰',\n",
       " 'L1': 3}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "next(_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 63 µs, sys: 53 µs, total: 116 µs\n",
      "Wall time: 118 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Source': 'google play',\n",
       " 'Content': 'google play . Ứng_dụng quá là đơ . Càng dùng càng đơ thì mua_bán cái mẹ j .',\n",
       " 'L1': 3}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "next(_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhobertTokenizer(name_or_path='vinai/phobert-base', vocab_size=64000, model_max_length=256, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Source', 'Content', 'L1'],\n",
       "        num_rows: 365\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tdc.main_ddict['validation'] = tdc.main_ddict['validation'].to_iterable_dataset(num_shards=tdc.num_shards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/365 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_ddict_tok = tdc.do_tokenization(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Source', 'Content', 'L1', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 365\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_ddict_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 287 µs, sys: 140 µs, total: 427 µs\n",
      "Wall time: 380 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = my_ddict_tok['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'non owned . Mã_500K toàn sàn Shopee cho ai cần nè » https://shope.ee/10QzJtpqQi'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_ddict_tok['validation'][0]['Content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'non', 'ow@@', 'ned', '.', 'Mã_@@', '500@@', 'K', 'toàn', 'sàn', 'Sho@@', 'pee', 'cho', 'ai', 'cần', 'nè', '»', 'htt@@', 'ps@@', '://@@', 'sho@@', 'pe@@', '.@@', 'ee@@', '/@@', '10@@', 'Q@@', 'z@@', 'J@@', 't@@', 'p@@', 'q@@', 'Q@@', 'i', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(my_ddict_tok['validation'][0]['input_ids'])[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_iter=iter(my_ddict_tok['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 272 ms, sys: 123 µs, total: 272 ms\n",
      "Wall time: 272 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_tmp = next(_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 5 µs, total: 5 µs\n",
      "Wall time: 6.68 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_tmp = next(_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google play . App nhu cc ko lam gi cung xoa tk\n",
      "['<s>', 'google', 'play', '.', 'App', 'nhu', 'cc', 'ko', 'lam', 'gi', 'cung', 'xoa', 't@@', 'k', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(_tmp['Content'])\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(_tmp['input_ids'])[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2v = {i:v for i,v in enumerate(tdc.label_lists[0])}\n",
    "\n",
    "vc_l1 = pd.Series(my_ddict['validation']['L1']).value_counts(normalize=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feature</td>\n",
       "      <td>0.369863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Commercial</td>\n",
       "      <td>0.213699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shopee account</td>\n",
       "      <td>0.131507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Delivery</td>\n",
       "      <td>0.128767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Buyer complained seller</td>\n",
       "      <td>0.041096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Return/Refund</td>\n",
       "      <td>0.032877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Services</td>\n",
       "      <td>0.032877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Order/Item</td>\n",
       "      <td>0.027397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Payment</td>\n",
       "      <td>0.021918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     index  proportion\n",
       "0                  Feature    0.369863\n",
       "1               Commercial    0.213699\n",
       "2           Shopee account    0.131507\n",
       "3                 Delivery    0.128767\n",
       "4  Buyer complained seller    0.041096\n",
       "5            Return/Refund    0.032877\n",
       "6                 Services    0.032877\n",
       "7               Order/Item    0.027397\n",
       "8                  Payment    0.021918"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc_l1['index'] = vc_l1['index'].map(i2v)\n",
    "vc_l1\n",
    "# Feature                    0.371056\n",
    "#  Commercial                 0.209191\n",
    "#  Delivery                   0.137174\n",
    "#  Shopee account             0.127572\n",
    "#  Buyer complained seller    0.043896\n",
    "#  Return/Refund              0.030864\n",
    "#  Payment                    0.030178\n",
    "#  Order/Item                 0.028121\n",
    "#  Services                   0.021948"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(google play    271\n",
       " non owned       34\n",
       " owned           26\n",
       " ios             22\n",
       " hc search       12\n",
       " Name: count, dtype: int64,\n",
       " google play    0.742466\n",
       " non owned      0.093151\n",
       " owned          0.071233\n",
       " ios            0.060274\n",
       " hc search      0.032877\n",
       " Name: proportion, dtype: float64)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(my_ddict['validation']['Source']).value_counts(),pd.Series(my_ddict['validation']['Source']).value_counts(normalize=True)\n",
    "#  Google Play    0.743484\n",
    "#  Non Owned      0.093278\n",
    "#  Owned          0.069959\n",
    "#  iOS            0.061043\n",
    "#  HC search      0.032236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp_trn = list(my_ddict['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_ddict['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(_tmp_trn) + len(my_ddict['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1106"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_tmp_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1110"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int((47-12)*0.5) + (1458 - 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datasets.iterable_dataset.IterableDataset>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main_ddict = load_dataset('secret_data',data_files=['buyer_listening_with_all_raw_data_w28.csv','buyer_listening_with_all_raw_data_w28.csv'],split='train')\n",
    "# main_ddict\n",
    "\n",
    "# main_ddict = load_dataset('sample_data',data_files=['sample_large.csv','sample_large.csv','sample_large.csv'],streaming=True)\n",
    "# main_ddict\n",
    "\n",
    "main_ddict = load_dataset('sample_data',data_files=['sample_large.csv'],split='train',streaming=True)\n",
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,v in enumerate(main_ddict):\n",
    "#     print(v)\n",
    "#     if i==4: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=['Feature','Commercial','Delivery',\n",
    "        'Shopee account','Buyer complained seller',\n",
    "        'Return/Refund','Payment','Order/Item',\n",
    "        'Services','Others']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_filter_dict={'L1':lambda x: x!='Others'}\n",
    "\n",
    "_content_tfms = partial(apply_vnmese_word_tokenize,normalize_text=True)\n",
    "_content_tfms.__name__='VNM word segmentation'\n",
    "\n",
    "_upsampling_dict={\n",
    "    'Source': lambda x: x=='hc search' if random.random()<0.5 else False\n",
    "}\n",
    "\n",
    "_aug_tfms=partial(remove_vnmese_accent,prob=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_ddict = load_dataset('sample_data',data_files=['sample_large.csv'],split='train',streaming=True)\n",
    "tdc = TextDataController(main_ddict,main_text='Content',\n",
    "                         label_names='L1',\n",
    "                         class_names_predefined=labels,\n",
    "                         filter_dict=_filter_dict,\n",
    "                         metadatas='Source',\n",
    "                         content_transformations=_content_tfms,\n",
    "                         val_ratio=365, \n",
    "#                          upsampling_dict=_upsampling_dict, # super slow\n",
    "                         content_augmentations=_aug_tfms,\n",
    "                         seed=42,\n",
    "                         is_batched=True,\n",
    "                         is_streamed=True,\n",
    "                         num_shards=512\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on L1 -----\n",
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Text Transformation --------------------\n",
      "----- VNM word segmentation -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "-------------------- Text Augmentation --------------------\n",
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "CPU times: user 2.62 s, sys: 2.64 s, total: 5.26 s\n",
      "Wall time: 5.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "my_ddict = tdc.do_all_preprocessing(shuffle_trn=True)\n",
    "# 2x big data\n",
    "# is_batched True, shuffle_trn True. Wall time: 1min 6s\n",
    "# is_batched False, shuffle_trn True, Wall time: 1min 18s\n",
    "\n",
    "# 3x sample_large.csv\n",
    "# is_batched True, shuffle_trn True. Wall time: 2.79 s\n",
    "# is_batched False, shuffle_trn True, Wall time: 2.96 s\n",
    "\n",
    "# TODO: redo\n",
    "# 3x sample_large.csv, streaming, with aug\n",
    "# is_batched True, shuffle_trn True. Wall time: 53.5 s\n",
    "# is_batched False, shuffle_trn True, Wall time: 17.4 s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Source', 'Content', 'L1'],\n",
       "    num_rows: 365\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_ddict['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/365 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_ddict_tok = tdc.do_tokenization(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Source', 'Content', 'L1', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 365\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_ddict_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(list(my_ddict_tok['train'])),len(list(my_ddict_tok['validation']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_iter=iter(my_ddict_tok['train'])\n",
    "_tmp = next(_iter)\n",
    "_tmp = next(_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_ddict_tok['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp=iter(my_ddict['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.29 s, sys: 2.93 s, total: 5.22 s\n",
      "Wall time: 5.22 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Source': 'google play',\n",
       " 'Content': 'google play . Bi loi khong hien_hinh_anh , xoa di tai lai app deu ko hien_thi gi het .',\n",
       " 'L1': 3}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "next(_tmp)\n",
    "# 9.44s with upsampling\n",
    "# 4.72s without upsampling\n",
    "# 5.16 without upsampling, with augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
      "Wall time: 8.58 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Source': 'hc search',\n",
       " 'Content': 'hc search . khong cap_nhat duoc sdt',\n",
       " 'L1': 9}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "next(_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp=iter(my_ddict['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 173 µs, sys: 154 µs, total: 327 µs\n",
      "Wall time: 241 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Source': 'google play',\n",
       " 'Content': 'google play . đang chơi mà quảng_cáo',\n",
       " 'L1': 1}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "next(_tmp)\n",
    "# 9.44s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 80 µs, sys: 0 ns, total: 80 µs\n",
      "Wall time: 83 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Source': 'google play',\n",
       " 'Content': 'google play . Cứ quản cáo quá nhiều , app nào củng gặp quản_cáo của shopee 😆',\n",
       " 'L1': 1}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "next(_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1093, 365)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(my_ddict['train'])),len(list(my_ddict['validation']))\n",
    "# (1124, 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1093, 365)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(my_ddict_tok['train'])),len(list(my_ddict_tok['validation']))\n",
    "# (1124, 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Source': 'ios', 'Content': 'ios . Đã chỉnh đi chỉnh lại rất nhiều nhưng nó vẫn ghi là thanh toán ko khả dụng! \\nLà sao hả SHOPEE:)))', 'L1': 6}\n",
      "{'Source': 'hc search', 'Content': 'hc search . kh đc mượt', 'L1': 3}\n",
      "{'Source': 'google play', 'Content': 'google play . Hóng đơn hàng về .khi về thì Shipper chưa giao mà báo k ai nhận. Tự ý huỷ đơn. Shipper Ngũ Hành Sơn Đà Nẵng quá kém.', 'L1': 2}\n",
      "{'Source': 'google play', 'Content': 'google play . Có cái nịt thùng mì 50k phí sip 100', 'L1': 2}\n",
      "{'Source': 'google play', 'Content': 'google play . Alo mới đặt hàng đi vắng có 1 bữa Xong shipper nt chửi ôm xồm vậy Đánh giá 1 sao cho biết', 'L1': 2}\n",
      "{'Source': 'google play', 'Content': 'google play . Đc', 'L1': 5}\n",
      "{'Source': 'google play', 'Content': 'google play . Tôi vưa bị đăng xuất khỏi shoppe một cách vô lý. Bây h k vào lại đc. Có vào đc thì tất cả nhưg đơn hàg trc đó của tui cug đã bị mất. Yêu cầu giai quyết vấn đề', 'L1': 9}\n",
      "{'Source': 'google play', 'Content': 'google play . Giao hàng gì mà tới kho gần nhà 3 km mà k chịu giao 6 ngày r đùa nhau a', 'L1': 2}\n",
      "{'Source': 'non owned', 'Content': 'non owned . # **NHANH TAY VÀO LẤY XU**  1. Thử Thách Shopee  🍒 https://shope.ee/7A1gZ0rOqG  2. Thử Thách Shopee Mall (chọn tab \"chương trình nổi bật\")  🍒 https://shope.ee/1fgk0ulXGK  3. Shopee Máy Gắp Thú - 700xu/lượt  🍒 https://shope.ee/AJyiKqnwiu', 'L1': 1}\n",
      "{'Source': 'google play', 'Content': 'google play . Đơ lag, tìm kiếm không thông minh', 'L1': 3}\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(my_ddict['train']):\n",
    "    print(v)\n",
    "    if i==9: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp=iter(my_ddict_tok['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google play . đang chơi mà quảng_cáo\n",
      "['<s>', 'google', 'play', '.', 'đang', 'chơi', 'mà', 'quảng_cáo', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(my_ddict_tok['validation']['Content'][0])\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens((my_ddict_tok['validation']['input_ids'][0])[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start a step-by-step walkthrough on how to use this class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset,load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('sample_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH/'sample_large.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_ddict = Dataset.from_csv(str(DATA_PATH/'sample_large.csv'))\n",
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_ddict = load_dataset(str(DATA_PATH),data_files={'train':'sample_large.csv'})\n",
    "main_ddict = load_dataset(str(DATA_PATH),data_files='sample_large.csv')\n",
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_ddict = load_dataset(str(DATA_PATH),data_files='sample_large.csv',split='train')\n",
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datasets.iterable_dataset.IterableDataset>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict = load_dataset(str(DATA_PATH),data_files='sample_large.csv',split='train',streaming=True)\n",
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Source': 'Google Play',\n",
       " 'Content': 'App ncc lúc nào cx lag đơ, phần tìm kiếm thì viết kiểu gì sp đó vẫn ko ra, thế phải ghi đúng tên mới chịu à? Lỡ quên tên ngta ghi mé mé như v cx phải gợi ý sp tương tự chứ??? ☻',\n",
       " 'L1': 'Feature',\n",
       " 'L2': 'App performance'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(main_ddict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(main_ddict.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "main_ddict = load_dataset(str(DATA_PATH),data_files=['sample_large.csv','sample_large.csv'])\n",
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_ddict = load_dataset('csv',data_files=str(DATA_PATH/'sample_large.csv'))\n",
    "# main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp = Path('secret_data')/'some_files.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Path('secret_data'), 'some_files.csv')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_tmp.parent,_tmp.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('.')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_tmp = Path('some_files.csv')\n",
    "_tmp.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/secret_data-042d1badc74881bf/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.06 ms, sys: 3.97 ms, total: 5.03 ms\n",
      "Wall time: 4.48 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Week', 'Group', 'Source', 'Content', 'L1', 'L2', 'L3', 'L4', 'is_valid', 'iteration'],\n",
       "    num_rows: 114605\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "main_ddict = load_dataset('secret_data',data_files='buyer_listening_with_all_raw_data_w28.csv',split='train')\n",
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/quan/.cache/huggingface/datasets/csv/default-042d1badc74881bf/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcea26c775af4774b58e4eb77438c80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1fa7b0c91d47548d9ca5d92e1b8a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/quan/.cache/huggingface/datasets/csv/default-042d1badc74881bf/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n",
      "CPU times: user 592 ms, sys: 84.5 ms, total: 677 ms\n",
      "Wall time: 1.72 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Week', 'Group', 'Source', 'Content', 'L1', 'L2', 'L3', 'L4', 'is_valid', 'iteration'],\n",
       "    num_rows: 114605\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "main_ddict = load_dataset('csv',data_files='secret_data/buyer_listening_with_all_raw_data_w28.csv',split='train')\n",
    "main_ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/secret_data-042d1badc74881bf/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e22c59234744b838afd14c7c1df86c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "dict_keys(['train'])\n"
     ]
    }
   ],
   "source": [
    "main_ddict = load_dataset('secret_data',data_files='buyer_listening_with_all_raw_data_w28.csv')\n",
    "if hasattr(main_ddict,'keys'):\n",
    "    print('yes')\n",
    "print(main_ddict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Week', 'Group', 'Source', 'Content', 'L1', 'L2', 'L3', 'L4', 'is_valid', 'iteration'],\n",
       "        num_rows: 114605\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp = main_ddict.pop('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    \n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/secret_data-042d1badc74881bf/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Week', 'Group', 'Source', 'Content', 'L1', 'L2', 'L3', 'L4', 'is_valid', 'iteration'],\n",
       "    num_rows: 114605\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict = load_dataset('secret_data',data_files='buyer_listening_with_all_raw_data_w28.csv',split='train')\n",
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(main_ddict,'keys'):\n",
    "    print('yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process metadatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Week', 'Group', 'Source', 'Content', 'L1', 'L2', 'L3', 'L4', 'is_valid', 'iteration'],\n",
       "    num_rows: 114605\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Week': [28.0, 28.0, 28.0],\n",
       " 'Group': ['Tú Bà Bà',\n",
       "  'Gia Lai-Thanh lý đồ dùng và thời trang',\n",
       "  'thuonghieucongluan.com.vn'],\n",
       " 'Source': ['Non Owned', 'Non Owned', 'Non Owned'],\n",
       " 'Content': ['Riết rồi k biết xài cái quần gì để k bị trừ tiền oan. Bữa trc thì vụ nạp thẻ qua shopee T chửi chưa đã cái miệng. Đổi qua momo cho lành. Ỷ y momo k có số dư nên chắc k sao. Chủ yếu lk tk ngân hàng thôi, cái nó tự liên kết thẻ trừ giao dịch qua apple mỗi tuần 129k/ tuần (ông cố ơi!)🙂Thêm cái chỉnh ảnh 419k/ năm (cái này cũng được đi).Dm bữa h tự trừ hết hơn 1tr trong tk ngân hàng. Ui là trời. 2 hộp sữa của con T ra đi nữa rồi đó.Hiện đại, hại điện.Khỏi cảm ơn, T xoá app rồi.Dm trả lại T 2 hộp Meiji đi rồi T sử dụng lại.!',\n",
       "  'GHN có còn ship cho Sendo không các thím? Em mua shopee, lazada, TIKI thì đơn do best, GHN vs viettel ship nhiều nhất mà mấy bên này chắc ship hầu hết các sàn lớn đúng ko nhờ?',\n",
       "  'Cục QLTT Hà Nội: Kiểm tra, xử lý nhiều vụ hàng lậu, hàng giả'],\n",
       " 'L1': ['Feature', 'Delivery', 'Buyer complained seller'],\n",
       " 'L2': ['Feature Others',\n",
       "  '3PL service (excluded SPX)',\n",
       "  'Illegal/counterfeit products'],\n",
       " 'L3': [None, None, None],\n",
       " 'L4': [None, None, None],\n",
       " 'is_valid': [0.0, 0.0, 0.0],\n",
       " 'iteration': [21, 21, 21]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadatas = ['Week','Source']\n",
    "# metadatas = val2iterable(metadatas)\n",
    "process_metas = True\n",
    "main_text='Content'\n",
    "is_batched=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/secret_data-042d1badc74881bf/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Week', 'Group', 'Source', 'Content', 'L1', 'L2', 'L3', 'L4', 'is_valid', 'iteration'],\n",
       "    num_rows: 114605\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict = load_dataset('secret_data',data_files='buyer_listening_with_all_raw_data_w28.csv',split='train')\n",
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _process_metadatas(ds:dict,\n",
    "#                        main_text,\n",
    "#                        metadatas,\n",
    "#                        process_metas=True,\n",
    "#                        sep='.',\n",
    "#                       is_batched=True):\n",
    "#     metadatas = val2iterable(metadatas)\n",
    "#     results={main_text:ds[main_text]}\n",
    "#     for m in metadatas:\n",
    "#         m_data = ds[m]\n",
    "#         if process_metas:\n",
    "#             # just strip and lowercase\n",
    "#             m_data = [str(v).strip().lower() for v in m_data] if is_batched else str(m_data).strip().lower()\n",
    "#         results[m]=m_data\n",
    "#         if is_batched:\n",
    "#             results[main_text] = [f'{m_data[i]}{sep} {results[main_text][i]}' for i in range(len(m_data))]\n",
    "#         else:\n",
    "#             results[main_text] = f'{m_data}{sep} {results[main_text]}'\n",
    "#     return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Metadatas Simple Processing & Concatenating to Main Content -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/114605 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_msg('Metadatas Simple Processing & Concatenating to Main Content')\n",
    "main_ddict_meta = main_ddict.map(partial(_process_metadatas,\n",
    "                                         main_text=main_text,\n",
    "                                         metadatas=metadatas,\n",
    "                                         process_metas=process_metas,\n",
    "                                         is_batched=is_batched),\n",
    "                                batched=is_batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Week': [1.0, 1.0, 1.0],\n",
       " 'Group': ['Google Play', 'Google Play', 'Google Play'],\n",
       " 'Source': ['google play', 'google play', 'google play'],\n",
       " 'Content': ['google play. 1.0. Tại sao cứ hiện thông báo',\n",
       "  'google play. 1.0. Mlem',\n",
       "  'google play. 1.0. 1 số sản phẩm trong giỏ hàng vừa đc cập nhật trong khi giỏ ko còn 1 hàng nào nx 😀'],\n",
       " 'L1': ['Services', 'Others', 'Feature'],\n",
       " 'L2': ['Shopee communication channels', 'Cannot defined', 'Cart & Order'],\n",
       " 'L3': ['Annoying pop-up ads', '-', 'Cart issues/suggestions'],\n",
       " 'L4': ['Non-tech', '-', 'Tech'],\n",
       " 'is_valid': [None, None, None],\n",
       " 'iteration': [1, 1, 1]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict_meta[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/secret_data-042d1badc74881bf/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-cadaba33433fe0f4.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n"
     ]
    }
   ],
   "source": [
    "print_msg('Metadata Simple Processing & Concatenating to Main Content')\n",
    "main_ddict_meta = main_ddict.map(partial(_process_metadatas,\n",
    "                                         main_text=main_text,\n",
    "                                         metadatas=metadatas,\n",
    "                                         process_metas=process_metas,\n",
    "                                         is_batched=False),\n",
    "                                batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Week': [1.0, 1.0, 1.0],\n",
       " 'Group': ['Google Play', 'Google Play', 'Google Play'],\n",
       " 'Source': ['google play', 'google play', 'google play'],\n",
       " 'Content': ['google play. 1.0. Tại sao cứ hiện thông báo',\n",
       "  'google play. 1.0. Mlem',\n",
       "  'google play. 1.0. 1 số sản phẩm trong giỏ hàng vừa đc cập nhật trong khi giỏ ko còn 1 hàng nào nx 😀'],\n",
       " 'L1': ['Services', 'Others', 'Feature'],\n",
       " 'L2': ['Shopee communication channels', 'Cannot defined', 'Cart & Order'],\n",
       " 'L3': ['Annoying pop-up ads', '-', 'Cart issues/suggestions'],\n",
       " 'L4': ['Non-tech', '-', 'Tech'],\n",
       " 'is_valid': [None, None, None],\n",
       " 'iteration': [1, 1, 1]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict_meta[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datasets.iterable_dataset.IterableDataset>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stream\n",
    "main_ddict_stream = load_dataset('secret_data',data_files='buyer_listening_with_all_raw_data_w28.csv',split='train',streaming=True)\n",
    "main_ddict_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_ddict_meta = main_ddict_stream.map(partial(_process_metadatas,\n",
    "                                         main_text=main_text,\n",
    "                                         metadatas=metadatas,\n",
    "                                         process_metas=process_metas,\n",
    "                                         is_batched=True),\n",
    "                                batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Week': '1.0',\n",
       "  'Group': 'Google Play',\n",
       "  'Source': 'google play',\n",
       "  'Content': 'google play. 1.0. Tại sao cứ hiện thông báo',\n",
       "  'L1': 'Services',\n",
       "  'L2': 'Shopee communication channels',\n",
       "  'L3': 'Annoying pop-up ads',\n",
       "  'L4': 'Non-tech',\n",
       "  'is_valid': None,\n",
       "  'iteration': 1},\n",
       " {'Week': '1.0',\n",
       "  'Group': 'Google Play',\n",
       "  'Source': 'google play',\n",
       "  'Content': 'google play. 1.0. Mlem',\n",
       "  'L1': 'Others',\n",
       "  'L2': 'Cannot defined',\n",
       "  'L3': '-',\n",
       "  'L4': '-',\n",
       "  'is_valid': None,\n",
       "  'iteration': 1},\n",
       " {'Week': '1.0',\n",
       "  'Group': 'Google Play',\n",
       "  'Source': 'google play',\n",
       "  'Content': 'google play. 1.0. 1 số sản phẩm trong giỏ hàng vừa đc cập nhật trong khi giỏ ko còn 1 hàng nào nx 😀',\n",
       "  'L1': 'Feature',\n",
       "  'L2': 'Cart & Order',\n",
       "  'L3': 'Cart issues/suggestions',\n",
       "  'L4': 'Tech',\n",
       "  'is_valid': None,\n",
       "  'iteration': 1}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(main_ddict_meta.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Week': '1.0',\n",
       "  'Group': 'Google Play',\n",
       "  'Source': 'google play',\n",
       "  'Content': 'google play. 1.0. Tại sao cứ hiện thông báo',\n",
       "  'L1': 'Services',\n",
       "  'L2': 'Shopee communication channels',\n",
       "  'L3': 'Annoying pop-up ads',\n",
       "  'L4': 'Non-tech',\n",
       "  'is_valid': None,\n",
       "  'iteration': 1},\n",
       " {'Week': '1.0',\n",
       "  'Group': 'Google Play',\n",
       "  'Source': 'google play',\n",
       "  'Content': 'google play. 1.0. Mlem',\n",
       "  'L1': 'Others',\n",
       "  'L2': 'Cannot defined',\n",
       "  'L3': '-',\n",
       "  'L4': '-',\n",
       "  'is_valid': None,\n",
       "  'iteration': 1},\n",
       " {'Week': '1.0',\n",
       "  'Group': 'Google Play',\n",
       "  'Source': 'google play',\n",
       "  'Content': 'google play. 1.0. 1 số sản phẩm trong giỏ hàng vừa đc cập nhật trong khi giỏ ko còn 1 hàng nào nx 😀',\n",
       "  'L1': 'Feature',\n",
       "  'L2': 'Cart & Order',\n",
       "  'L3': 'Cart issues/suggestions',\n",
       "  'L4': 'Tech',\n",
       "  'is_valid': None,\n",
       "  'iteration': 1}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict_meta = main_ddict_stream.map(partial(_process_metadatas,\n",
    "                                         main_text=main_text,\n",
    "                                         metadatas=metadatas,\n",
    "                                         process_metas=process_metas,\n",
    "                                         is_batched=False),\n",
    "                                batched=False)\n",
    "# batched does not matter when using streamed\n",
    "list(main_ddict_meta.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Week', 'Group', 'Source', 'Content', 'L1', 'L2', 'L3', 'L4', 'is_valid', 'iteration'],\n",
       "    num_rows: 114605\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadatas = ['Week','Source']\n",
    "process_metas = True\n",
    "main_text='Content'\n",
    "is_batched=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/secret_data-042d1badc74881bf/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-ac679a0b1f06a8e9.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Metadatas Simple Processing & Concatenating to Main Content -----\n"
     ]
    }
   ],
   "source": [
    "print_msg('Metadatas Simple Processing & Concatenating to Main Content')\n",
    "main_ddict_meta = main_ddict.map(partial(_process_metadatas,\n",
    "                                         main_text=main_text,\n",
    "                                         metadatas=metadatas,\n",
    "                                         process_metas=process_metas,\n",
    "                                         is_batched=is_batched),\n",
    "                                batched=is_batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114605"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(main_ddict_meta['L1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def _encode_labels(self):\n",
    "#         print_msg('Label Encoding')\n",
    "#         if self.label_names is None: \n",
    "#             raise ValueError('Missing label columns!')\n",
    "#         self.label_names = val2iterable(self.label_names)\n",
    "#         if len(self.label_names)>1:\n",
    "#             self.is_multihead=True\n",
    "        \n",
    "#         if self.label_lists is not None and not isinstance(self.label_lists[0],list):\n",
    "#             self.label_lists = [self.label_lists]\n",
    "        \n",
    "#         if isinstance(self.df[self.label_names[0]].iloc[0],list):\n",
    "########               (self.dset[self.label_names[0]][0],list)\n",
    "#             # This is multi-label. Ignore self.label_names[1:]\n",
    "#             self.label_names = [self.label_names[0]]\n",
    "#             self.is_multihead=False\n",
    "#             self.is_multilabel=True\n",
    "            \n",
    "#         encoder_classes=[]\n",
    "#         if not self.is_multilabel:\n",
    "#             for idx,l in enumerate(self.label_names):\n",
    "#                 if self.label_lists is None:\n",
    "#                     train_label = self.df[l].values\n",
    "#                     l_encoder = LabelEncoder()\n",
    "#                     self.df[l] = l_encoder.fit_transform(train_label)\n",
    "#                     encoder_classes.append(list(l_encoder.classes_))\n",
    "#                 else:\n",
    "#                     l_classes = sorted(list(self.label_lists[idx]))\n",
    "#                     label2idx = {v:i for i,v in enumerate(l_classes)}\n",
    "#                     self.df[l] = self.df[l].map(label2idx).values\n",
    "#                     encoder_classes.append(l_classes)\n",
    "#         else:\n",
    "#             # For MultiLabel, we only save the encoder classes without transforming the label itself to one-hot (or actually, few-hot)\n",
    "#             if self.label_lists is None:\n",
    "#                 l_encoder = MultiLabelBinarizer()\n",
    "#                 _ = l_encoder.fit(self.df[self.label_names[0]])\n",
    "#                 encoder_classes.append(list(l_encoder.classes_))\n",
    "#             else:\n",
    "#                 l_classes = sorted(list(self.label_lists[0]))\n",
    "#                 encoder_classes.append(l_classes)\n",
    "                \n",
    "#         self.label_lists = encoder_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructor/ Class Method calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just want to get the dataframe from the csv path, set ```return_df=True```. You still have the input validation precheck functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = TextDataMain.from_csv(DATA_PATH/'sample_large.csv',\n",
    "                            return_df=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```Input Validation Precheck``` will check for missing values and duplicate rows in the csv file. Since there's no such thing in our sample dataset, we won't see anything here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Source.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you are happy with this dataframe (after you did some others preprocessing), then you can start creating a `TextDataMain` object\n",
    "\n",
    "For this dataframe, I want to \n",
    "- Build a text classification model, with main text in ```Content``` column, metadatas is ```Source```, and the label is ```L1```\n",
    "- Perform `apply_word_tokenize` with text normalization (this is \"text transformation\")\n",
    "- For augmentation, I want to perform: Oversampling the ```Owned, Non Owned and HC Search``` from column ```Source```, then add some the Vietnamese no-accent text. Note that all of these are called \"text augmentation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define these transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For Text Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awt_tfm = partial(apply_word_tokenize,normalize_text=True)\n",
    "# You can also set a __name__ to your augmentation function. \n",
    "# This way you will have meaningful text messages as outputs\n",
    "awt_tfm.__name__='UTS Word Tokenization With Normalization'\n",
    "\n",
    "txt_tfms=[awt_tfm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For Text Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply_to_all means I will apply this augmentation to all the data \n",
    "# (including the original data and the augmented data/transformed data from previous augmentation/transformation)\n",
    "over_nonown_tfm = partial(sampling_with_condition,query='Source==\"non owned\"',frac=0.5,seed=42,apply_to_all=False)\n",
    "over_nonown_tfm.__name__ = 'Oversampling Non Owned'\n",
    "\n",
    "over_own_tfm = partial(sampling_with_condition,query='Source==\"owned\"',frac=2,seed=42,apply_to_all=False)\n",
    "over_own_tfm.__name__ = 'Oversampling Owned'\n",
    "\n",
    "over_hc_tfm = partial(sampling_with_condition,query='Source==\"hc search\"',frac=2.5,seed=42,apply_to_all=False)\n",
    "over_hc_tfm.__name__ = 'Oversampling HC search'\n",
    "\n",
    "remove_accent_tfm = partial(remove_vnmese_accent,frac=1,seed=42,apply_to_all=True)\n",
    "remove_accent_tfm.__name__ = 'Add No-Accent Text'\n",
    "\n",
    "aug_tfms = [over_nonown_tfm,over_own_tfm,over_hc_tfm,remove_accent_tfm]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm = TextDataMain(df,\n",
    "                    main_content='Content',\n",
    "                    metadatas='Source', # You can put a list of multiple metadatas\n",
    "                    label_names='L1', # You can put a list of multiple labels\n",
    "                    val_ratio=0.2,\n",
    "                    split_cols='L1', # You can even put a list of multiple columns to be used for validation splitting\n",
    "                    content_tfms = txt_tfms, # You can add multiple content transformation functions ...\n",
    "                    aug_tfms = aug_tfms, # ... as well as augmentation functions\n",
    "                    process_metadatas=True,\n",
    "                    seed=42,\n",
    "                    shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to directly create a ```TextDataMain``` object from our csv file, we can instead use this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm = TextDataMain.from_csv(DATA_PATH/'sample_large.csv',\n",
    "                            return_df=False,\n",
    "                            main_content='Content',\n",
    "                            metadatas='Source',\n",
    "                            label_names='L1',\n",
    "                            val_ratio=0.2,\n",
    "                            split_cols='L1',\n",
    "                            content_tfms = txt_tfms,\n",
    "                            aug_tfms = aug_tfms,\n",
    "                            process_metadatas=True,\n",
    "                            seed=42,\n",
    "                            shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TextDataMain.to_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all the previous constructor calls do not do any heavy processing yet.\n",
    "\n",
    "To actually run all the processes, one can call `TextDataMain.to_df()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = tdm.to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice this?\n",
    "```\n",
    "Previous Validation Percentage: 20.0%\n",
    "- Before leak check\n",
    "Size: 14\n",
    "- After leak check\n",
    "Size: 14\n",
    "- Number of rows leaked: 0, or 0.00% of the original validation (or test) data\n",
    "Current Validation Percentage: 20.0%\n",
    "```\n",
    "After performing train/test split, the ```TextDataMain``` object also perform a \"leak check\": After `text_transformation` is performed, it will compare the text from ```Content``` value in the validation set to the ```Content``` text in the train set. Any duplications (texts that belong to both set) will be removed from validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, since we have metadatas, the metadatas is concatenated to the front of the texture content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.Content.sample(5).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a new dataframe with only the necessary columns (the processed text column, metadatas, label, and ```is_valid``` which tells you which row belongs to the validation set). Notice that our class has also encode our label for us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our TextDataMain object also stores other useful attributes, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entire processed dataframe, similar to the df_processed above\n",
    "tdm.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class names (This will be a list of list, as this class can handle multi-label classification)\n",
    "tdm.label_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary storing unique value for each provided metadata\n",
    "tdm.metadata_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to see how a HuggingFace's tokenizer work on our processed text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will pick a random text from train set to show\n",
    "tdm.tokenizer_explain_single(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing this, we can see how the tokenizer interact with our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TextDataMain.to_datasetdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need to convert our data to HuggingFace's DatasetDict format in order to utilize HuggingFace's model well, we can directly export datasetdict using `TextDataMain.to_datasetdict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict_sample = tdm.to_datasetdict(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict_sample['train']['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ddict_sample['train']['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that PhoBert will auto-pad our sentence to its model max_sequence_length, which is 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ddict_sample['train']['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict_sample['train']['label'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TextDataMain.save_as_pickles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the transformations/augmentations can take time for large dataset, we want to save our TextDataMain object. We can use `TextDataMain.save_as_pickles` to export a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm.save_as_pickles('my_tdm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can load it with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm2 = TextDataMain.from_pickle('my_tdm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and access all the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm2.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm2.label_lists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm2.metadata_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_stats = os.stat(Path('pickle_files/my_tdm.pkl'))\n",
    "print(f'File Size in MegaBytes is {file_stats.st_size / (1024 * 1024)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it saves the entire processed dataframe (and datasetdict if you call ```to_datasetdict```), the pickle size can be large. In some scenario you don't need to store these data attributes (as inference time, or in production). Thus one can save a lighter pickle file by setting ```drop_data_attributes``` to ```True```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm.save_as_pickles('my_lightweight_tdm',drop_data_attributes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_stats = os.stat(Path('pickle_files/my_lightweight_tdm.pkl'))\n",
    "print(f'File Size in MegaBytes is {file_stats.st_size / (1024 * 1024)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see a bigger file size reduction when we work with much larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm_light = TextDataMain.from_pickle('my_lightweight_tdm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can still access some important attributes (except for any data attributes, such as ```df``` or ```main_ddict```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm_light.label_lists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm_light.metadata_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
