{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Main\n",
    "\n",
    "> This module contains the main Python class for data control: `TextDataMain`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```#| default_exp text_main```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder,MultiLabelBinarizer\n",
    "from datasets import DatasetDict,Dataset,IterableDataset,load_dataset,concatenate_datasets\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from that_nlp_library.utils import *\n",
    "from functools import partial\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from that_nlp_library.text_transformation import *\n",
    "from that_nlp_library.text_augmentation import *\n",
    "from importlib.machinery import SourceFileLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Transformation, Augmentations, and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tokenizer_explain(inp, # Input sentence\n",
    "                      tokenizer, # Tokenizer (preferably from HuggingFace)\n",
    "                      split_word=False # Is input `inp` split into list or not\n",
    "                     ):\n",
    "    \"Display results from tokenizer\"\n",
    "    print('----- Tokenizer Explained -----')\n",
    "    print('--- Input ---')\n",
    "    print(inp)\n",
    "    print()\n",
    "    print('--- Tokenized results --- ')\n",
    "    print(tokenizer(inp,is_split_into_words=split_word))\n",
    "    print()\n",
    "    tok = tokenizer.encode(inp,is_split_into_words=split_word)\n",
    "    print('--- Results from tokenizer.convert_ids_to_tokens ---')\n",
    "    print(tokenizer.convert_ids_to_tokens(tok))\n",
    "    print()\n",
    "    print('--- Results from tokenizer.decode --- ')\n",
    "    print(tokenizer.decode(tok))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L20){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### tokenizer_explain\n",
       "\n",
       ">      tokenizer_explain (inp, tokenizer, split_word=False)\n",
       "\n",
       "Display results from tokenizer\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | Input sentence |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| split_word | bool | False | Is input `inp` split into list or not |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L20){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### tokenizer_explain\n",
       "\n",
       ">      tokenizer_explain (inp, tokenizer, split_word=False)\n",
       "\n",
       "Display results from tokenizer\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | Input sentence |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| split_word | bool | False | Is input `inp` split into list or not |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(tokenizer_explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a tokenizer from EnviBert model. Uncomment the command line below to download necessary files to build this tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gdown 14X9fGijA7kdNfe4dM_8gqfxIWtj1Q-hb -O ./envibert_cache --folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir=Path('./envibert_tokenizer')\n",
    "tokenizer = SourceFileLoader(\"envibert.tokenizer\", \n",
    "                             str(cache_dir/'envibert_tokenizer.py')).load_module().RobertaTokenizer(cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Envibert tokenizer does not required the input to be tokenized using word_tokenize from UnderTheSea library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Tokenizer Explained -----\n",
      "--- Input ---\n",
      "Hội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức. Thủ Đức là một huyện trực thuộc thành phố Hồ Chí Minh\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 857, 1033, 191, 664, 1033, 7366, 2615, 142, 664, 1033, 671, 1355, 2294, 993, 413, 2900, 244, 1019, 827, 24, 40, 647, 773, 549, 119, 511, 1134, 1690, 758, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens ---\n",
      "['<s>', '▁Hội', '▁cư', '▁dân', '▁chung', '▁cư', '▁sen', '▁hồng', '-', '▁chung', '▁cư', '▁lot', 'us', '▁sóng', '▁thần', '▁thủ', '▁đức', '.', '▁Thủ', '▁Đức', '▁là', '▁một', '▁huyện', '▁trực', '▁thuộc', '▁thành', '▁phố', '▁Hồ', '▁Chí', '▁Minh', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> ▁Hội ▁cư ▁dân ▁chung ▁cư ▁sen ▁hồng - ▁chung ▁cư ▁lot us ▁sóng ▁thần ▁thủ ▁đức. ▁Thủ ▁Đức ▁là ▁một ▁huyện ▁trực ▁thuộc ▁thành ▁phố ▁Hồ ▁Chí ▁Minh </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inp = 'Hội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức. Thủ Đức là một huyện trực thuộc thành phố Hồ Chí Minh'\n",
    "tokenizer_explain(inp,tokenizer,split_word=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Tokenizer Explained -----\n",
      "--- Input ---\n",
      "['hội', 'cư', 'dân', 'chung', 'cư', 'sen', 'hồng', '-', 'chung', 'cư', 'lotus', 'sóng', 'thần', 'thủ', 'đức']\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 227, 1033, 191, 664, 1033, 7366, 2615, 13, 664, 1033, 671, 1355, 2294, 993, 413, 2900, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens ---\n",
      "['<s>', '▁hội', '▁cư', '▁dân', '▁chung', '▁cư', '▁sen', '▁hồng', '▁-', '▁chung', '▁cư', '▁lot', 'us', '▁sóng', '▁thần', '▁thủ', '▁đức', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> ▁hội ▁cư ▁dân ▁chung ▁cư ▁sen ▁hồng ▁- ▁chung ▁cư ▁lot us ▁sóng ▁thần ▁thủ ▁đức </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inp = ['hội', 'cư', 'dân', 'chung', 'cư', 'sen', 'hồng', '-', 'chung', 'cư', 'lotus', 'sóng', 'thần', 'thủ', 'đức']\n",
    "tokenizer_explain(inp,tokenizer,split_word=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try PhoBert tokenizer. PhoBert tokenizer, unlike Envibert tokenizer, requires input to be word tokenized (using UnderTheSea library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức\n"
     ]
    }
   ],
   "source": [
    "inp = apply_vnmese_word_tokenize('hội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức')\n",
    "print(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Tokenizer Explained -----\n",
      "--- Input ---\n",
      "hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 1093, 1838, 1574, 3330, 2025, 31, 1574, 2029, 4885, 8554, 25625, 7344, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens ---\n",
      "['<s>', 'hội', 'cư_dân', 'chung_cư', 'sen', 'hồng', '-', 'chung_cư', 'lo@@', 'tus', 'sóng_thần', 'thủ_@@', 'đức', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer_explain(inp,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def two_steps_tokenization_explain(inp, # Input sentence\n",
    "                                   tokenizer, # Tokenizer (preferably from HuggingFace)\n",
    "                                   content_tfms=[], # A list of text transformations\n",
    "                                   aug_tfms=[], # A list of text augmentation \n",
    "                                  ):\n",
    "    \"Display results form each content transformation, then display results from tokenizer\"\n",
    "    print('----- Text Transformation Explained -----')\n",
    "    print('--- Raw sentence ---')\n",
    "    print(inp)\n",
    "    print('--- Content Transformations (on both train and test) ---')\n",
    "    content_tfms = val2iterable(content_tfms)\n",
    "    for tfm in content_tfms:\n",
    "        print_msg(callable_name(tfm),3)\n",
    "        inp = tfm(inp)\n",
    "        print(inp)\n",
    "    print('--- Augmentations (on train only) ---')\n",
    "    aug_tfms = val2iterable(aug_tfms)\n",
    "    for tfm in aug_tfms:\n",
    "        print_msg(callable_name(tfm),3)\n",
    "        inp = tfm(inp)\n",
    "        print(inp)\n",
    "    print()\n",
    "    tokenizer_explain(inp,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L41){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### two_steps_tokenization_explain\n",
       "\n",
       ">      two_steps_tokenization_explain (inp, tokenizer, content_tfms=[],\n",
       ">                                      aug_tfms=[])\n",
       "\n",
       "Display results form each content transformation, then display results from tokenizer\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | Input sentence |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| content_tfms | list | [] | A list of text transformations |\n",
       "| aug_tfms | list | [] | A list of text augmentation |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L41){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### two_steps_tokenization_explain\n",
       "\n",
       ">      two_steps_tokenization_explain (inp, tokenizer, content_tfms=[],\n",
       ">                                      aug_tfms=[])\n",
       "\n",
       "Display results form each content transformation, then display results from tokenizer\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | Input sentence |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| content_tfms | list | [] | A list of text transformations |\n",
       "| aug_tfms | list | [] | A list of text augmentation |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(two_steps_tokenization_explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load Phobert tokenizer one more time to test out this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import text_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`apply_vnmese_word_tokenize` also have an option to normalize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Text Transformation Explained -----\n",
      "--- Raw sentence ---\n",
      "Hội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức. Thủ Đức là một huyện trực thuộc thành phố Hồ Chí Minh\n",
      "--- Content Transformations (on both train and test) ---\n",
      "--- apply_vnmese_word_tokenize ---\n",
      "Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức . Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh\n",
      "--- Augmentations (on train only) ---\n",
      "\n",
      "----- Tokenizer Explained -----\n",
      "--- Input ---\n",
      "Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức . Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 792, 1838, 1574, 3330, 2025, 31, 1574, 2029, 4885, 8554, 25625, 7344, 5, 5043, 8, 16, 149, 2850, 214, 784, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens ---\n",
      "['<s>', 'Hội', 'cư_dân', 'chung_cư', 'sen', 'hồng', '-', 'chung_cư', 'lo@@', 'tus', 'sóng_thần', 'thủ_@@', 'đức', '.', 'Thủ_Đức', 'là', 'một', 'huyện', 'trực_thuộc', 'thành_phố', 'Hồ_Chí_Minh', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức. Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inp = 'Hội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức. Thủ Đức là một huyện trực thuộc thành phố Hồ Chí Minh'\n",
    "two_steps_tokenization_explain(inp,tokenizer,content_tfms=[partial(apply_vnmese_word_tokenize,normalize_text=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some text augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove vietnamese accent\n",
    "remove_accent = lambda x: unidecode.unidecode(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want your function to be printed in with a different name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_accent.__name__ = 'Remove Vietnamese Accent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Text Transformation Explained -----\n",
      "--- Raw sentence ---\n",
      "Hội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức. Thủ Đức là một huyện trực thuộc thành phố Hồ Chí Minh\n",
      "--- Content Transformations (on both train and test) ---\n",
      "--- apply_vnmese_word_tokenize ---\n",
      "Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức . Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh\n",
      "--- Augmentations (on train only) ---\n",
      "--- Remove Vietnamese Accent ---\n",
      "Hoi cu_dan chung_cu sen hong - chung_cu lotus song_than thu_duc . Thu_Duc la mot huyen truc_thuoc thanh_pho Ho_Chi_Minh\n",
      "\n",
      "----- Tokenizer Explained -----\n",
      "--- Input ---\n",
      "Hoi cu_dan chung_cu sen hong - chung_cu lotus song_than thu_duc . Thu_Duc la mot huyen truc_thuoc thanh_pho Ho_Chi_Minh\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 3021, 1111, 56549, 17386, 22975, 13689, 3330, 27037, 31, 22975, 13689, 2029, 4885, 3227, 9380, 1510, 21605, 6190, 1894, 5, 5770, 4098, 1894, 2644, 3773, 1204, 18951, 2052, 10242, 9835, 1881, 22899, 17366, 10384, 30234, 8470, 1612, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens ---\n",
      "['<s>', 'Ho@@', 'i', 'cu_@@', 'dan', 'chung_@@', 'cu', 'sen', 'hong', '-', 'chung_@@', 'cu', 'lo@@', 'tus', 'so@@', 'ng_th@@', 'an', 'thu_@@', 'du@@', 'c', '.', 'Thu_@@', 'Du@@', 'c', 'la', 'mo@@', 't', 'huy@@', 'en', 'tru@@', 'c_th@@', 'u@@', 'oc', 'thanh_@@', 'pho', 'Ho_@@', 'Chi_@@', 'Minh', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Hoi cu_dan chung_cu sen hong - chung_cu lotus song_than thu_duc. Thu_Duc la mot huyen truc_thuoc thanh_pho Ho_Chi_Minh </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "two_steps_tokenization_explain(inp,tokenizer,\n",
    "                               content_tfms=[partial(apply_vnmese_word_tokenize,normalize_text=True)],\n",
    "                               aug_tfms=[remove_accent]\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even be creative with your augmentation functions; let's say you only want your augmentation to be applied 50% of the time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_accent = lambda x: unidecode.unidecode(x) if random.random()<0.5 else x\n",
    "remove_accent.__name__ = 'Remove Vietnamese Accent with 0.5 prob'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Text Transformation Explained -----\n",
      "--- Raw sentence ---\n",
      "Hội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức. Thủ Đức là một huyện trực thuộc thành phố Hồ Chí Minh\n",
      "--- Content Transformations (on both train and test) ---\n",
      "--- apply_vnmese_word_tokenize ---\n",
      "Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức . Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh\n",
      "--- Augmentations (on train only) ---\n",
      "--- Remove Vietnamese Accent with 0.5 prob ---\n",
      "Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức . Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh\n",
      "\n",
      "----- Tokenizer Explained -----\n",
      "--- Input ---\n",
      "Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức . Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 792, 1838, 1574, 3330, 2025, 31, 1574, 2029, 4885, 8554, 25625, 7344, 5, 5043, 8, 16, 149, 2850, 214, 784, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens ---\n",
      "['<s>', 'Hội', 'cư_dân', 'chung_cư', 'sen', 'hồng', '-', 'chung_cư', 'lo@@', 'tus', 'sóng_thần', 'thủ_@@', 'đức', '.', 'Thủ_Đức', 'là', 'một', 'huyện', 'trực_thuộc', 'thành_phố', 'Hồ_Chí_Minh', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức. Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "two_steps_tokenization_explain(inp,tokenizer,\n",
    "                               content_tfms=[partial(apply_vnmese_word_tokenize,normalize_text=True)],\n",
    "                               aug_tfms=[remove_accent]\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more examples of interesting augmentation [here](https://anhquan0412.github.io/that-nlp-library/text_augmentation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tokenize_function(examples:dict,\n",
    "                      tok,\n",
    "                      text_name,\n",
    "                      max_length=None,\n",
    "                      is_split_into_words=False):\n",
    "    if max_length is None:\n",
    "        # pad to model's default max sequence length\n",
    "        return tok(examples[text_name], padding=\"max_length\", truncation=True,is_split_into_words=is_split_into_words)\n",
    "    if isinstance(max_length,int) and max_length>0:\n",
    "        # pad to max length of the current batch, and start truncating at max_length\n",
    "        return tok(examples[text_name], padding=True, max_length=max_length,truncation=True,is_split_into_words=is_split_into_words)\n",
    "    \n",
    "    # no padding (still truncate at model's default max sequence length)\n",
    "    return tok(examples[text_name], truncation=True,is_split_into_words=is_split_into_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L58){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### tokenize_function\n",
       "\n",
       ">      tokenize_function (examples:dict, tok, text_name, max_length=None,\n",
       ">                         is_split_into_words=False)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L58){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### tokenize_function\n",
       "\n",
       ">      tokenize_function (examples:dict, tok, text_name, max_length=None,\n",
       ">                         is_split_into_words=False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(tokenize_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I am processing Vietnamese text, I will use EnViBert's tokenizer. Envibert is a RoBERTa model for Vietnamese and English. This RoBERTa version is trained by using 100GB of text (50GB of Vietnamese and 50GB of English). For more information: [https://huggingface.co/nguyenvulebinh/envibert](https://huggingface.co/nguyenvulebinh/envibert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/nguyenvulebinh/envibert\n",
    "cache_dir=Path('./envibert_tokenizer')\n",
    "tokenizer = SourceFileLoader(\"envibert.tokenizer\", \n",
    "                             str(cache_dir/'envibert_tokenizer.py')).load_module().RobertaTokenizer(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples={\n",
    "    'text':[\n",
    "         'hội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức',\n",
    "         'This is the recommended way to make a Python package importable from anywhere',\n",
    "         'hội cần mở thẻ tín dụng tại hà nội, đà nẵng, tp. hồ chí minh',\n",
    "         \"biti's cao lãnh - đồng tháp\",\n",
    "         'chợ phòng trọ + việc làm...khu lĩnh nam - vĩnh hưng - mai động (hoàng mai)'\n",
    "          ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = tokenize_function(examples,tokenizer,text_name='text',max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 227, 1033, 191, 664, 1033, 7366, 2615, 13, 664, 1033, 671, 1355, 2294, 993, 413, 2900, 2, 1, 1, 1, 1, 1, 1, 1], [0, 116, 14, 6, 3169, 270, 9, 364, 10, 23963, 5360, 15930, 2003, 51, 5906, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 227, 256, 778, 2600, 1074, 144, 76, 5489, 613, 57339, 4820, 27666, 57339, 21422, 244, 872, 635, 841, 2, 1, 1, 1, 1, 1], [0, 880, 592, 427, 162, 171, 906, 13, 122, 6553, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 2299, 315, 5995, 1349, 99, 83, 55025, 244, 6356, 1114, 1213, 1163, 13, 8233, 11051, 13, 3335, 109, 28, 11695, 13377, 3335, 3, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁hội', '▁cư', '▁dân', '▁chung', '▁cư', '▁sen', '▁hồng', '▁-', '▁chung', '▁cư', '▁lot', 'us', '▁sóng', '▁thần', '▁thủ', '▁đức', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(results['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change max_length (which allow truncation when sentence length is higher than max_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = tokenize_function(examples,tokenizer,text_name='text',max_length=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 227, 1033, 191, 2], [0, 116, 14, 6, 2], [0, 227, 256, 778, 2], [0, 880, 592, 427, 2], [0, 2299, 315, 5995, 2]], 'token_type_ids': [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadatas Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def concat_metadatas(dset:dict, # HuggingFace Dataset\n",
    "                     main_text, # Text feature name\n",
    "                     metadatas, # Metadata (or a list of metadatas)\n",
    "                     process_metas=True, # Whether apply simple metadata processing, i.e. space strip and lowercase\n",
    "                     sep='.', # separator for contatenating to main_text\n",
    "                     is_batched=True, # whether batching is applied\n",
    "                    ):\n",
    "    \"\"\"\n",
    "    Extract, process (optional) and concatenate metadatas to the front of text\n",
    "    \"\"\"\n",
    "    results={main_text:dset[main_text]}\n",
    "    for m in metadatas:\n",
    "        m_data = dset[m]\n",
    "        if process_metas:\n",
    "            # just strip and lowercase\n",
    "            m_data = [none2emptystr(v).strip().lower() for v in m_data] if is_batched else nan2emptystr(m_data).strip().lower()\n",
    "        results[m]=m_data\n",
    "        if is_batched:\n",
    "            results[main_text] = [f'{m_data[i]} {sep} {results[main_text][i]}' for i in range(len(m_data))]\n",
    "        else:\n",
    "            results[main_text] = f'{m_data} {sep} {results[main_text]}'\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### concat_metadatas\n",
       "\n",
       ">      concat_metadatas (dset:dict, main_text, metadatas, process_metas=True,\n",
       ">                        sep='.', is_batched=True)\n",
       "\n",
       "Extract, process (optional) and concatenate metadatas to the front of text\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dset | dict |  | HuggingFace Dataset |\n",
       "| main_text |  |  | Text feature name |\n",
       "| metadatas |  |  | Metadata (or a list of metadatas) |\n",
       "| process_metas | bool | True | Whether apply simple metadata processing, i.e. space strip and lowercase |\n",
       "| sep | str | . | separator for contatenating to main_text |\n",
       "| is_batched | bool | True | whether batching is applied |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### concat_metadatas\n",
       "\n",
       ">      concat_metadatas (dset:dict, main_text, metadatas, process_metas=True,\n",
       ">                        sep='.', is_batched=True)\n",
       "\n",
       "Extract, process (optional) and concatenate metadatas to the front of text\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dset | dict |  | HuggingFace Dataset |\n",
       "| main_text |  |  | Text feature name |\n",
       "| metadatas |  |  | Metadata (or a list of metadatas) |\n",
       "| process_metas | bool | True | Whether apply simple metadata processing, i.e. space strip and lowercase |\n",
       "| sep | str | . | separator for contatenating to main_text |\n",
       "| is_batched | bool | True | whether batching is applied |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(concat_metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class TextDataController"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TextDataController():\n",
    "    def __init__(self,\n",
    "                 inp, # HuggingFace Dataset or DatasetDict\n",
    "                 main_text:str, # Name of the main text column\n",
    "                 label_names=None, # Names of the label (dependent variable) columns\n",
    "                 class_names_predefined=None, # List of names associated with the labels (same index order)\n",
    "                 filter_dict={}, # A dictionary: {feature: filtering_function_based_on_the_feature}\n",
    "                 metadatas=[], # Names of the metadata columns\n",
    "                 process_metas=True, # Whether to do simple text processing on the chosen metadatas\n",
    "                 content_transformations=[], # A list of text transformations\n",
    "                 val_ratio:list|float|None=0.2, # Ratio of data for validation set. If given a list, validation set will be chosen based on indices in this list\n",
    "                 stratify_cols=[], # Column(s) needed to do stratified shuffle split\n",
    "                 upsampling_list={}, # A list of tuple. Each tuple: (feature,upsampling_function_based_on_the_feature)\n",
    "                 content_augmentations=[], # A list of text augmentations\n",
    "                 seed=None, # Random seed\n",
    "                 is_batched=True, # Whether to perform operations in batch\n",
    "                 batch_size=1000, # Batch size, for when is_batched is True\n",
    "                 num_proc=4, # Number of process for multiprocessing\n",
    "                 cols_to_keep=None, # Columns to keep after all processings\n",
    "                 buffer_size=10000, # For shuffling data\n",
    "                 num_shards=64, # Number of shards\n",
    "                ):\n",
    "            \n",
    "        self.main_text = main_text\n",
    "        self.metadatas = val2iterable(metadatas)\n",
    "        self.process_metas = process_metas\n",
    "        self.label_names = val2iterable(label_names) if label_names is not None else None\n",
    "        self.label_lists = class_names_predefined\n",
    "        self.filter_dict = filter_dict\n",
    "        self.content_tfms = val2iterable(content_transformations)\n",
    "        self.upsampling_list = upsampling_list\n",
    "        self.aug_tfms = val2iterable(content_augmentations)\n",
    "        self.val_ratio = val_ratio\n",
    "        self.stratify_cols = val2iterable(stratify_cols)\n",
    "        self.seed = seed\n",
    "        self.is_batched = is_batched\n",
    "        self.batch_size = batch_size\n",
    "        self.num_proc = num_proc\n",
    "        self.is_streamed = False\n",
    "        self.cols_to_keep = cols_to_keep\n",
    "        self.buffer_size = buffer_size\n",
    "        self.num_shards = num_shards\n",
    "        self.ddict_rest = DatasetDict()\n",
    "        \n",
    "        if hasattr(inp,'keys'):\n",
    "            if 'train' in inp.keys(): # is datasetdict\n",
    "                self.ddict_rest = inp\n",
    "                self.dset = self.ddict_rest.pop('train')\n",
    "            else:\n",
    "                raise ValueError('The given DatasetDict has no \"train\" split')\n",
    "        else: # is dataset\n",
    "            self.dset = inp\n",
    "        if isinstance(self.dset,IterableDataset):\n",
    "            self.is_streamed=True\n",
    "        self.all_cols = self.dset.column_names\n",
    "        \n",
    "        if self.is_streamed and self.label_names is not None and self.label_lists is None:\n",
    "            raise ValueError('All class labels must be provided when streaming')\n",
    "        \n",
    "        if self.is_streamed and len(self.upsampling_list):\n",
    "            warnings.warn(\"Upsampling requires dataset concatenation, which can be extremely slow (x2) for streamed dataset\")\n",
    "            \n",
    "        self._processed_call=False\n",
    "        self.is_multilabel=False\n",
    "        self.is_multihead=False\n",
    "        \n",
    "        \n",
    "            \n",
    "    @classmethod\n",
    "    def from_csv(cls,file_path,**kwargs):\n",
    "        file_path = Path(file_path)\n",
    "        ds = load_dataset(str(file_path.parent),\n",
    "                                  data_files=file_path.name,\n",
    "                                  split='train')\n",
    "        return TextDataController(ds,**kwargs)\n",
    "        \n",
    "    \n",
    "    @classmethod\n",
    "    def from_df(cls,df,validate=True,**kwargs):\n",
    "        if validate:\n",
    "            check_input_validation(df)\n",
    "        ds = Dataset.from_pandas(df)\n",
    "        return TextDataController(ds,**kwargs)\n",
    "    \n",
    "    \n",
    "    def _map_dset(self,dset,func):\n",
    "        if self.is_streamed:\n",
    "            return dset.map(func,\n",
    "                            batched=self.is_batched,\n",
    "                            batch_size=self.batch_size\n",
    "                           )\n",
    "        return dset.map(func,\n",
    "                        batched=self.is_batched,\n",
    "                        batch_size=self.batch_size,\n",
    "                        num_proc=self.num_proc\n",
    "                       )\n",
    "    \n",
    "    def _filter_dset(self,dset,func):\n",
    "        if self.is_streamed:\n",
    "            return dset.filter(func,\n",
    "                            batched=self.is_batched,\n",
    "                            batch_size=self.batch_size\n",
    "                           )\n",
    "        return dset.filter(func,\n",
    "                        batched=self.is_batched,\n",
    "                        batch_size=self.batch_size,\n",
    "                        num_proc=self.num_proc\n",
    "                       )\n",
    "                     \n",
    "    def validate_input(self):\n",
    "        if self.is_streamed:\n",
    "            print('Input validation check is disabled when data is streamed')\n",
    "            return\n",
    "        _df = self.dset.to_pandas()\n",
    "        check_input_validation(_df)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def save_as_pickles(self,\n",
    "                        fname, # Name of the pickle file\n",
    "                        parent='pickle_files', # Parent folder\n",
    "                        drop_data_attributes=False # Whether to drop all large-size data attributes\n",
    "                       ):\n",
    "        if drop_data_attributes:\n",
    "            if hasattr(self, 'main_ddict'):\n",
    "                del self.main_ddict\n",
    "        save_to_pickle(self,fname,parent=parent)\n",
    "    \n",
    "        \n",
    "    def _check_validation_leaking(self):\n",
    "        if self.val_ratio is None or self.is_streamed:\n",
    "            return\n",
    "        \n",
    "        trn_txt = self.main_ddict['train'][self.main_text]\n",
    "        val_txt = self.main_ddict['validation'][self.main_text]        \n",
    "        val_txt_leaked = check_text_leaking(trn_txt,val_txt)\n",
    "        \n",
    "        if len(val_txt_leaked)==0: return\n",
    "        \n",
    "        # filter train dataset to get rid of leaks\n",
    "        print('Filtering leaked data out of training set...')\n",
    "        _func = partial(lambda_batch,\n",
    "                        feature=self.main_text,\n",
    "                        func=lambda x: x.strip().lower() not in val_txt_leaked,\n",
    "                        is_batched=self.is_batched)\n",
    "        self.main_ddict['train'] = self._filter_dset(self.main_ddict['train'],_func)   \n",
    "        print('Done')\n",
    "           \n",
    "    def _train_test_split(self):\n",
    "        print_msg('Train Test Split',20)\n",
    "        val_key = list(set(self.ddict_rest.keys()) & set(['val','validation','valid']))\n",
    "        if len(val_key)==1: # val split exists\n",
    "            print('Validation split already exists')\n",
    "            self.main_ddict=DatasetDict({'train':self.dset,\n",
    "                                         'validation':self.ddict_rest.pop(val_key[0])})\n",
    "            \n",
    "    \n",
    "        elif self.val_ratio is None: # use all data\n",
    "            print('No validation split defined')\n",
    "            self.main_ddict=DatasetDict({'train':self.dset})\n",
    "        \n",
    "        elif isinstance(self.val_ratio,list) or isinstance(self.val_ratio,np.ndarray): # filter with indices\n",
    "            print('Validation indices are provided')\n",
    "            if self.is_streamed: raise ValueError('Data streaming does not support validation set filtering using indices')\n",
    "            val_idxs = list(self.val_ratio)\n",
    "            trn_idxs = list(set(range(len(self.dset))) - set(val_idxs))\n",
    "            self.main_ddict=DatasetDict({'train':self.dset.select(trn_idxs),\n",
    "                                         'validation':self.dset.select(val_idxs)})\n",
    "            \n",
    "        elif (isinstance(self.val_ratio,float) or isinstance(self.val_ratio,int)) and not len(self.stratify_cols):\n",
    "            print('Validation split based on val_ratio')\n",
    "            if self.is_streamed:\n",
    "                # shuffle dataset before splitting it\n",
    "                self.dset = self.dset.shuffle(seed=self.seed,buffer_size=self.buffer_size)\n",
    "                if isinstance(self.val_ratio,float):\n",
    "                    warnings.warn(\"Length of streamed dataset is unknown to use float validation ratio. Default to 5000 data points for validation\")\n",
    "                    self.val_ratio=5000\n",
    "                    \n",
    "                trn_dset = self.dset.skip(self.val_ratio)\n",
    "                val_datas = list(self.dset.take(self.val_ratio))\n",
    "                val_dict={k: [v[k] for v in val_datas] for k in val_datas[0].keys()}\n",
    "                val_dset = Dataset.from_dict(val_dict)\n",
    "                self.main_ddict=DatasetDict({'train':trn_dset,\n",
    "                                         'validation':val_dset})\n",
    "#                 self.main_ddict=DatasetDict({'train':self.dset.skip(self.val_ratio),\n",
    "#                                          'validation':self.dset.take(self.val_ratio)})\n",
    "            else:\n",
    "                # train val split\n",
    "                self.main_ddict = self.dset.train_test_split(test_size=self.val_ratio,shuffle=True,seed=self.seed)\n",
    "                self.main_ddict['validation']=self.main_ddict['test']\n",
    "                del self.main_ddict['test']\n",
    "        \n",
    "        else: # val_ratio split with stratifying\n",
    "            if self.is_streamed: raise ValueError('Stratified split is not supported for streamed data')                \n",
    "            if self.is_multilabel and self.label_names[0] in self.stratify_cols:\n",
    "                raise ValueError('For MultiLabel classification, you cannot choose the label as your stratified column')\n",
    "            print('Validation split based on val_ratio, with stratifying')\n",
    "            # Create a new feature 'stratified', which is a concatenation of values in stratify_cols\n",
    "            if self.is_batched:\n",
    "                stratified_creation = lambda x: {'stratified':\n",
    "                                     ['_'.join(list(map(str,[x[v][i] for v in self.stratify_cols]))) \n",
    "                                      for i in range(len(x[self.stratify_cols[0]]))]}\n",
    "            else:\n",
    "                stratified_creation = lambda x: {'stratified':\n",
    "                                     '_'.join(list(map(str,[x[v] for v in self.stratify_cols]))) \n",
    "                                      }\n",
    "            self.dset = self.dset.map(stratified_creation,\n",
    "                                      batched=self.is_batched,\n",
    "                                      batch_size=self.batch_size,\n",
    "                                      num_proc=self.num_proc)\n",
    "            self.dset=self.dset.class_encode_column(\"stratified\")\n",
    "            # train val split\n",
    "            self.main_ddict = self.dset.train_test_split(test_size=self.val_ratio,\n",
    "                                                         shuffle=True,seed=self.seed,\n",
    "                                                        stratify_by_column='stratified')\n",
    "            self.main_ddict['validation']=self.main_ddict['test']\n",
    "            del self.main_ddict['test']\n",
    "            self.main_ddict=self.main_ddict.remove_columns(['stratified'])\n",
    "            \n",
    "        \n",
    "        del self.dset\n",
    "        print('Done')\n",
    "\n",
    "                             \n",
    "    def _create_label_mapping_func(self,encoder_classes):\n",
    "        if self.is_multihead:\n",
    "            label2idxs = [{v:i for i,v in enumerate(l_classes)} for l_classes in encoder_classes]\n",
    "                    \n",
    "            _func = lambda inp: {'label': [[label2idxs[i][v] for i,v in enumerate(vs)] for vs in zip(*[inp[l] for l in self.label_names])] \\\n",
    "                                    if self.is_batched else [label2idxs[i][v] for i,v in enumerate([inp[l] for l in self.label_names])]\n",
    "                              }\n",
    "            \n",
    "        else:\n",
    "            label2idx = {v:i for i,v in enumerate(encoder_classes[0])}\n",
    "            _func = partial(lambda_map_batch,\n",
    "                           feature=self.label_names[0],\n",
    "                           func=lambda x: label2idx[x],\n",
    "                           output_feature='label',\n",
    "                           is_batched=self.is_batched)\n",
    "        return _func\n",
    "        \n",
    "    def _encode_labels(self):\n",
    "        print_msg('Label Encoding')\n",
    "        if len(self.label_names)>1:\n",
    "            self.is_multihead=True\n",
    "        \n",
    "        if self.label_lists is not None and not isinstance(self.label_lists[0],list):\n",
    "            self.label_lists = [self.label_lists]\n",
    "        \n",
    "        # get label of first row\n",
    "        first_label = self.dset[self.label_names[0]][0] if not self.is_streamed else next(iter(self.dset))[self.label_names[0]]\n",
    "        if isinstance(first_label,list):\n",
    "            # This is multi-label. Ignore self.label_names[1:]\n",
    "            self.label_names = [self.label_names[0]]\n",
    "            self.is_multihead=False\n",
    "            self.is_multilabel=True\n",
    "            \n",
    "        encoder_classes=[]\n",
    "        if not self.is_multilabel:\n",
    "            for idx,l in enumerate(self.label_names):\n",
    "                if self.label_lists is None:\n",
    "                    l_encoder = LabelEncoder()\n",
    "                    _ = l_encoder.fit(self.dset[l])\n",
    "                    l_classes = list(l_encoder.classes_)\n",
    "                else:\n",
    "                    l_classes = sorted(list(self.label_lists[idx]))\n",
    "                encoder_classes.append(l_classes)\n",
    "            \n",
    "            _func = self._create_label_mapping_func(encoder_classes)\n",
    "                \n",
    "            self.dset = self._map_dset(self.dset,_func)\n",
    "\n",
    "            val_key = list(set(self.ddict_rest.keys()) & set(['val','validation','valid']))\n",
    "            if len(val_key)>1: raise ValueError('Your DatasetDict has more than 1 validation split')\n",
    "            if len(val_key)==1:\n",
    "                val_key=val_key[0]\n",
    "                self.ddict_rest[val_key] = self._map_dset(self.ddict_rest[val_key],_func)\n",
    "                    \n",
    "        else:\n",
    "            # For MultiLabel, we transform the label itself to one-hot (or actually, few-hot)\n",
    "            if self.label_lists is None:\n",
    "                l_encoder = MultiLabelBinarizer()\n",
    "                _ = l_encoder.fit(self.dset[self.label_names[0]])\n",
    "                l_classes = list(l_encoder.classes_)\n",
    "            else:\n",
    "                l_classes = sorted(list(self.label_lists[0]))\n",
    "            \n",
    "            encoder_classes.append(l_classes)\n",
    "            \n",
    "            l_encoder = MultiLabelBinarizer(classes=encoder_classes[0])\n",
    "            _ = l_encoder.fit(None)\n",
    "            _func = partial(lambda_map_batch,\n",
    "                            feature=self.label_names[0],\n",
    "                            func=lambda x: l_encoder.transform(x),\n",
    "                            output_feature='label',\n",
    "                            is_batched=self.is_batched,\n",
    "                            is_func_batched=True)\n",
    "            self.dset = self._map_dset(self.dset,_func)                                                  \n",
    "            \n",
    "            val_key = list(set(self.ddict_rest.keys()) & set(['val','validation','valid']))\n",
    "            if len(val_key)>1: raise ValueError('Your DatasetDict has more than 1 validation dataset')\n",
    "            if len(val_key)==1:\n",
    "                val_key=val_key[0]\n",
    "                self.ddict_rest[val_key] = self._map_dset(self.ddict_rest[val_key],_func)\n",
    "            \n",
    "        self.label_lists = encoder_classes\n",
    "        print('Done')\n",
    "        \n",
    "    def _process_metadatas(self,dset,ddict_rest=None):\n",
    "        if len(self.metadatas)>0:\n",
    "            print_msg('Metadata Simple Processing & Concatenating to Main Content')\n",
    "            map_func = partial(concat_metadatas,\n",
    "                               main_text=self.main_text,\n",
    "                               metadatas=self.metadatas,\n",
    "                               process_metas=self.process_metas,\n",
    "                               is_batched=self.is_batched)\n",
    "            dset = self._map_dset(dset,map_func)\n",
    "            if ddict_rest is not None:\n",
    "                ddict_rest = self._map_dset(ddict_rest,map_func)\n",
    "            print('Done')\n",
    "        return dset if ddict_rest is None else (dset,ddict_rest)\n",
    "            \n",
    "            \n",
    "    \n",
    "    def _simplify_ddict(self):\n",
    "        print_msg('Dropping unused features',20)\n",
    "        if self.cols_to_keep is None:\n",
    "            self.cols_to_keep= [self.main_text] + self.metadatas\n",
    "            if self.label_names is not None: self.cols_to_keep+=self.label_names\n",
    "        cols_to_remove = set(self.all_cols) - set(self.cols_to_keep)\n",
    "        self.main_ddict['train']=self.main_ddict['train'].remove_columns(list(cols_to_remove))\n",
    "        if 'validation' in self.main_ddict.keys():\n",
    "            self.main_ddict['validation']=self.main_ddict['validation'].remove_columns(list(cols_to_remove))\n",
    "        print('Done')\n",
    "    \n",
    "    def _do_transformation(self,dset,ddict_rest=None):\n",
    "        if len(self.content_tfms):\n",
    "            print_msg('Text Transformation',20)\n",
    "            for tfm in self.content_tfms:\n",
    "                print_msg(callable_name(tfm))\n",
    "                _func = partial(lambda_map_batch,\n",
    "                               feature=self.main_text,\n",
    "                               func=tfm,\n",
    "                               is_batched=self.is_batched)\n",
    "                dset = self._map_dset(dset,_func)\n",
    "                if ddict_rest is not None:\n",
    "                    ddict_rest = self._map_dset(ddict_rest,_func)\n",
    "            print('Done')\n",
    "        return dset if ddict_rest is None else (dset,ddict_rest)\n",
    " \n",
    "    def _do_filtering(self,dset,ddict_rest=None):\n",
    "        if len(self.filter_dict):\n",
    "            print_msg('Data Filtering',20)\n",
    "            for f,tfm in self.filter_dict.items():\n",
    "                print_msg(f'Do {callable_name(tfm)} on {f}')\n",
    "                _func = partial(lambda_batch,\n",
    "                                feature=f,\n",
    "                                func=tfm,\n",
    "                                is_batched=self.is_batched)\n",
    "                dset = self._filter_dset(dset,_func)\n",
    "                if ddict_rest is not None:\n",
    "                    ddict_rest = self._filter_dset(ddict_rest,_func)\n",
    "            print('Done')\n",
    "        return dset if ddict_rest is None else (dset,ddict_rest)\n",
    "    \n",
    "    def _upsampling(self):\n",
    "        if len(self.upsampling_list):\n",
    "            print_msg('Upsampling data',20)\n",
    "            results=[]\n",
    "            for f,tfm in self.upsampling_list:\n",
    "                print_msg(f'Do {callable_name(tfm)} on {f}')\n",
    "                _func = partial(lambda_batch,\n",
    "                                feature=f,\n",
    "                                func=tfm,\n",
    "                                is_batched=self.is_batched)\n",
    "                new_dset = self._filter_dset(self.main_ddict['train'],_func)\n",
    "                results.append(new_dset)\n",
    "            # slow concatenation for iterable dataset    \n",
    "            self.main_ddict['train'] = concatenate_datasets(results+[self.main_ddict['train']])\n",
    "            print('Done')\n",
    "      \n",
    "    def _do_augmentation(self):\n",
    "        \n",
    "        if len(self.aug_tfms):\n",
    "            print_msg('Text Augmentation',20)\n",
    "\n",
    "            seed_notorch(self.seed)\n",
    "            if not self.is_streamed:  \n",
    "#                 self.main_ddict['train'] = self.main_ddict['train'].with_transform(partial(augmentation_helper,\n",
    "#                                                                        text_name=self.main_text,\n",
    "#                                                                        func=partial(func_all,functions=self.aug_tfms)))              \n",
    "                for tfm in self.aug_tfms:\n",
    "                    print_msg(callable_name(tfm))\n",
    "                    _func = partial(lambda_map_batch,\n",
    "                                   feature=self.main_text,\n",
    "                                   func=tfm,\n",
    "                                   is_batched=self.is_batched)\n",
    "                    self.main_ddict['train'] = self._map_dset(self.main_ddict['train'],_func)\n",
    "\n",
    "            else:\n",
    "                self.main_ddict['train'] = IterableDataset.from_generator(augmentation_stream_generator,\n",
    "                                               features = self.main_ddict['train'].features,\n",
    "                                               gen_kwargs={'dset': self.main_ddict['train'],\n",
    "                                                           'text_name':self.main_text,\n",
    "                                                           'func':partial(func_all,functions=self.aug_tfms)\n",
    "                                                          })\n",
    "            print('Done')\n",
    "        \n",
    "    def _convert_to_iterable(self):\n",
    "        if not self.is_streamed:\n",
    "            self.main_ddict['train'] = self.main_ddict['train'].to_iterable_dataset(num_shards=self.num_shards)\n",
    "            self.is_streamed=True\n",
    "            \n",
    "    def _do_train_shuffling(self):\n",
    "        print_msg('Shuffling train set',20)\n",
    "        self.main_ddict['train'] = self.main_ddict['train'].shuffle(seed=self.seed, buffer_size=self.buffer_size)\n",
    "        print('Done')\n",
    "        \n",
    "    def do_all_preprocessing(self,shuffle_trn=True): \n",
    "        if self._processed_call:\n",
    "            warnings.warn('Your dataset has already been processed. Returning the previous processed DatasetDict...')\n",
    "            return self.main_ddict\n",
    "            \n",
    "        print_msg('Start Main Text Processing',20)\n",
    "        \n",
    "        # Filtering\n",
    "        self.dset,self.ddict_rest = self._do_filtering(self.dset,self.ddict_rest)\n",
    "        \n",
    "        # Process metadatas\n",
    "        self.dset,self.ddict_rest = self._process_metadatas(self.dset,self.ddict_rest)\n",
    "        \n",
    "        # Process labels\n",
    "        if self.label_names is not None:\n",
    "            self._encode_labels()\n",
    "        \n",
    "        # Content transformation\n",
    "        self.dset,self.ddict_rest = self._do_transformation(self.dset,self.ddict_rest)\n",
    "         \n",
    "        # Train Test Split.\n",
    "        ### self.main_ddict is created here\n",
    "        self._train_test_split()\n",
    "        \n",
    "        # Dropping unused columns\n",
    "        self._simplify_ddict()\n",
    "        \n",
    "        # Check validation leaking\n",
    "        self._check_validation_leaking()\n",
    "        \n",
    "        ### The rest of these functions applies only to the train dataset\n",
    "        # Upsampling\n",
    "        self._upsampling()\n",
    "        \n",
    "        # Augmentation\n",
    "        self._do_augmentation()\n",
    "           \n",
    "        # Convert train set to iterable\n",
    "        self._convert_to_iterable()\n",
    "        \n",
    "        # Shuffle train\n",
    "        if shuffle_trn:\n",
    "            self._do_train_shuffling()\n",
    "        \n",
    "        self._processed_call=True\n",
    "        \n",
    "        return self.main_ddict\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    def do_tokenization(self,\n",
    "                       tokenizer, # Tokenizer (preferably from HuggingFace)\n",
    "                       is_split_into_words=False, # Is text split into words or not\n",
    "                       max_length=None, # pad to model's allowed max length (default is max_sequence_length)\n",
    "                       trn_size=None, # The number of training data to be tokenized\n",
    "                      ):\n",
    "        print_msg('Tokenization',20)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_split_into_words= is_split_into_words\n",
    "        self.max_length = max_length\n",
    "        if trn_size is not None:\n",
    "            if trn_ratio<1:\n",
    "                raise ValueError(\"Length of streamed dataset is unknown to use floating ratio\")\n",
    "            self.main_ddict['train'] = self.main_ddict['train'].take(trn_ratio)\n",
    "        \n",
    "        for k in self.main_ddict.keys():\n",
    "            self.main_ddict[k] = self.main_ddict[k].map(partial(tokenize_function,\n",
    "                                                                    text_name=self.main_text,\n",
    "                                                                    tok=tokenizer,\n",
    "                                                                    is_split_into_words=is_split_into_words,\n",
    "                                                                    max_length=max_length),\n",
    "                                                            batched=True, # always true\n",
    "                                                            batch_size=self.batch_size\n",
    "                                                           )\n",
    "        print('Done')\n",
    "        return self.main_ddict\n",
    "        \n",
    "    def process_and_tokenize(self,\n",
    "                             tokenizer, # Tokenizer (preferably from HuggingFace)\n",
    "                             is_split_into_words=False, # Is text split into list or not\n",
    "                             max_length=None, # pad to model's allowed max length (default is max_sequence_length)\n",
    "                             trn_size=None, # The number of training data to be tokenized\n",
    "                             shuffle_trn=True, # To shuffle the train set before tokenization\n",
    "                            ):\n",
    "        _ = self.do_all_preprocessing(shuffle_trn)\n",
    "        _ = self.do_tokenization(tokenizer,is_split_into_words,max_length,trn_size)\n",
    "        \n",
    "    \n",
    "    def set_data_collator(self,data_collator):\n",
    "        self.data_collator = data_collator\n",
    "        \n",
    "    \n",
    "    def prepare_test_dataset_from_csv(self,file_path):\n",
    "        file_path = Path(file_path)\n",
    "        ds = load_dataset(str(file_path.parent),\n",
    "                          data_files=file_path.name,\n",
    "                          split='train')\n",
    "        return self.prepare_test_dataset(ds)\n",
    "    \n",
    "    def prepare_test_dataset_from_df(self,df,validate=True):\n",
    "        if validate:\n",
    "            check_input_validation(df)\n",
    "        ds = Dataset.from_pandas(df)\n",
    "        return self.prepare_test_dataset(ds)\n",
    "        \n",
    "    def prepare_test_dataset(self,test_dset,do_filtering=False):\n",
    "        print_msg('Start Test Set Transformation',20)\n",
    "\n",
    "        # Filtering\n",
    "        if do_filtering:\n",
    "            test_dset = self._do_filtering(test_dset)\n",
    "        \n",
    "        # Process metadatas\n",
    "        test_dset = self._process_metadatas(test_dset)\n",
    "        \n",
    "        \n",
    "        # Content transformation\n",
    "        test_dset = self._do_transformation(test_dset)\n",
    "        \n",
    "        # Tokenization\n",
    "        print_msg('Tokenization',20)\n",
    "        test_dset = test_dset.map(partial(tokenize_function,\n",
    "                                          text_name=self.main_text,\n",
    "                                          tok=tokenizer,\n",
    "                                          is_split_into_words=is_split_into_words,\n",
    "                                          max_length=max_length),\n",
    "                                  batched=True, # always true\n",
    "                                  batch_size=self.batch_size\n",
    "                                 )\n",
    "        return test_dset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### TextDataController\n",
       "\n",
       ">      TextDataController (inp, main_text:str, label_names=None,\n",
       ">                          class_names_predefined=None, filter_dict={},\n",
       ">                          metadatas=[], process_metas=True,\n",
       ">                          content_transformations=[],\n",
       ">                          val_ratio:list|float|None=0.2, stratify_cols=[],\n",
       ">                          upsampling_list={}, content_augmentations=[],\n",
       ">                          seed=None, is_batched=True, batch_size=1000,\n",
       ">                          num_proc=4, cols_to_keep=None, buffer_size=10000,\n",
       ">                          num_shards=64)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | HuggingFace Dataset or DatasetDict |\n",
       "| main_text | str |  | Name of the main text column |\n",
       "| label_names | NoneType | None | Names of the label (dependent variable) columns |\n",
       "| class_names_predefined | NoneType | None | List of names associated with the labels (same index order) |\n",
       "| filter_dict | dict | {} | A dictionary: {feature: filtering_function_based_on_the_feature} |\n",
       "| metadatas | list | [] | Names of the metadata columns |\n",
       "| process_metas | bool | True | Whether to do simple text processing on the chosen metadatas |\n",
       "| content_transformations | list | [] | A list of text transformations |\n",
       "| val_ratio | list \\| float \\| None | 0.2 | Ratio of data for validation set. If given a list, validation set will be chosen based on indices in this list |\n",
       "| stratify_cols | list | [] | Column(s) needed to do stratified shuffle split |\n",
       "| upsampling_list | dict | {} | A list of tuple. Each tuple: (feature,upsampling_function_based_on_the_feature) |\n",
       "| content_augmentations | list | [] | A list of text augmentations |\n",
       "| seed | NoneType | None | Random seed |\n",
       "| is_batched | bool | True | Whether to perform operations in batch |\n",
       "| batch_size | int | 1000 | Batch size, for when is_batched is True |\n",
       "| num_proc | int | 4 | Number of process for multiprocessing |\n",
       "| cols_to_keep | NoneType | None | Columns to keep after all processings |\n",
       "| buffer_size | int | 10000 | For shuffling data |\n",
       "| num_shards | int | 64 | Number of shards |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### TextDataController\n",
       "\n",
       ">      TextDataController (inp, main_text:str, label_names=None,\n",
       ">                          class_names_predefined=None, filter_dict={},\n",
       ">                          metadatas=[], process_metas=True,\n",
       ">                          content_transformations=[],\n",
       ">                          val_ratio:list|float|None=0.2, stratify_cols=[],\n",
       ">                          upsampling_list={}, content_augmentations=[],\n",
       ">                          seed=None, is_batched=True, batch_size=1000,\n",
       ">                          num_proc=4, cols_to_keep=None, buffer_size=10000,\n",
       ">                          num_shards=64)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | HuggingFace Dataset or DatasetDict |\n",
       "| main_text | str |  | Name of the main text column |\n",
       "| label_names | NoneType | None | Names of the label (dependent variable) columns |\n",
       "| class_names_predefined | NoneType | None | List of names associated with the labels (same index order) |\n",
       "| filter_dict | dict | {} | A dictionary: {feature: filtering_function_based_on_the_feature} |\n",
       "| metadatas | list | [] | Names of the metadata columns |\n",
       "| process_metas | bool | True | Whether to do simple text processing on the chosen metadatas |\n",
       "| content_transformations | list | [] | A list of text transformations |\n",
       "| val_ratio | list \\| float \\| None | 0.2 | Ratio of data for validation set. If given a list, validation set will be chosen based on indices in this list |\n",
       "| stratify_cols | list | [] | Column(s) needed to do stratified shuffle split |\n",
       "| upsampling_list | dict | {} | A list of tuple. Each tuple: (feature,upsampling_function_based_on_the_feature) |\n",
       "| content_augmentations | list | [] | A list of text augmentations |\n",
       "| seed | NoneType | None | Random seed |\n",
       "| is_batched | bool | True | Whether to perform operations in batch |\n",
       "| batch_size | int | 1000 | Batch size, for when is_batched is True |\n",
       "| num_proc | int | 4 | Number of process for multiprocessing |\n",
       "| cols_to_keep | NoneType | None | Columns to keep after all processings |\n",
       "| buffer_size | int | 10000 | For shuffling data |\n",
       "| num_shards | int | 64 | Number of shards |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataController)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data + Basic use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a `TextDataController` from a csv, pandas DataFrame, or directly from a HuggingFace dataset object. Currently, `TextDataController` is designed for text classification, so you must provide the column name for the label (or multi-label)\n",
    "\n",
    "We will load a sample data, modified to match a task where you need to determine which category `L1` a comment (`Content`) belongs to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset source: https://www.kaggle.com/datasets/kavita5/review_ecommerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23486, 10)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15977</th>\n",
       "      <td>1081</td>\n",
       "      <td>39</td>\n",
       "      <td>Simple yet sexy</td>\n",
       "      <td>I tried on the xs in store, but ti was a littl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7548</th>\n",
       "      <td>1081</td>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10555</th>\n",
       "      <td>1022</td>\n",
       "      <td>36</td>\n",
       "      <td>Great fit!</td>\n",
       "      <td>Ag jeans are the only brand that fit my body t...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Jeans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21986</th>\n",
       "      <td>1011</td>\n",
       "      <td>31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Skirts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20537</th>\n",
       "      <td>877</td>\n",
       "      <td>26</td>\n",
       "      <td>Nice top</td>\n",
       "      <td>I bought this top in slate. it is very nice, l...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Clothing ID  Age            Title   \n",
       "15977         1081   39  Simple yet sexy  \\\n",
       "7548          1081   39              NaN   \n",
       "10555         1022   36       Great fit!   \n",
       "21986         1011   31              NaN   \n",
       "20537          877   26         Nice top   \n",
       "\n",
       "                                             Review Text  Rating   \n",
       "15977  I tried on the xs in store, but ti was a littl...       5  \\\n",
       "7548                                                 NaN       3   \n",
       "10555  Ag jeans are the only brand that fit my body t...       5   \n",
       "21986                                                NaN       4   \n",
       "20537  I bought this top in slate. it is very nice, l...       4   \n",
       "\n",
       "       Recommended IND  Positive Feedback Count   Division Name   \n",
       "15977                1                       11  General Petite  \\\n",
       "7548                 0                        0         General   \n",
       "10555                1                        4         General   \n",
       "21986                1                        0         General   \n",
       "20537                1                        0  General Petite   \n",
       "\n",
       "      Department Name Class Name  \n",
       "15977         Dresses    Dresses  \n",
       "7548          Dresses    Dresses  \n",
       "10555         Bottoms      Jeans  \n",
       "21986         Bottoms     Skirts  \n",
       "20537            Tops      Knits  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a `TextDataController` from a dataframe. This also provides a quick input validation check (NaN check and Duplication check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Input Validation Precheck -----\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create a `TextDataController` directly from the csv file. The good thing about using HuggingFace Dataset as the main backend of the TextDataController is that you can utilize lots of its useful functionality, such as caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataController.from_csv('sample_data/Womens_Clothing_Reviews.csv',\n",
    "                                  main_text='Review Text',\n",
    "                                  label_names='Department Name',\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create a `TextDataController` from a HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'],\n",
       "    num_rows: 23486\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         seed=42\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the \"Input Validation Precheck\" above, we notice that our dataset has missing values in the text field and the label field. For now, let's load the data as a Pandas' DataFrame, perform some cleaning, and create our `TextDataController`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(~df['Review Text'].isna()) & (~df['Department Name'].isna())].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Input Validation Precheck -----\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title    2966\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 1 rows\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you can start perform 2 important steps on your data\n",
    "\n",
    "1. Text preprocessings, Label Encoding, Train/Validation Split\n",
    "2. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't provided any preprocessings to the `TextDataController`; we will see more on how to use preprocessings (step by step) as we progress. In fact, we can even perform NaN filtering as a preprocessing step inside `TextDataController`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 2, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "ddict = tdc.do_all_preprocessing(shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text', 'Department Name', 'label'],\n",
       "        num_rows: 4526\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our DatasetDict now has two split: train and validation. Note that train split is now IterableDataset, for processing efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Review Text': [\"The fun colors drew me to this but it sure fit weird. the top was fine but it became a bit tent-like in the waist. the material doesn't feel great either.\",\n",
       "  \"I was disappointed by this dress. i listened to the reviews and ordered one size up, a size 6 instead of my usual 4, and that was the correct size. fit like a glove. unfortunately, the style left much to be desired. this dress is very costume-y. i felt like a wench from the renaissance fair. not a good vibe. it is a beautiful dress, i'm not denying that, but not as a dress you would actually wear to an event. there's just too much going on with the tapestry look and the many types of fabric. i a\",\n",
       "  \"I love this comfy top, and if it couldn't get any better, the pleated back makes it extra special! it is easy to thrown on and still feel put together. i purchased this top in the green and i'm absolutely enthralled by the rich color! i do agree that the quality of the pleats could be slightly better, however i believe it's up to par with the price point. i typically wear an xs/xsp and ultimately decided to go with a small for a more oversized tunic top look, but otherwise i'd go for your normal\"],\n",
       " 'Department Name': ['Dresses', 'Dresses', 'Tops'],\n",
       " 'label': [1, 1, 4]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict['validation'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Review Text': 'I wanted to love this as i love holding horses but it fits weird, slender in arms but baggy in the torso. i will be returning', 'Department Name': 'Tops', 'label': 4}\n",
      "{'Review Text': \"It is soooo cute!!! i'm trying to figure a few things before deciding to keep..... how one goes to the bathroom when wearing it? i can't figure out how to get it on without help tying the straps in the back?\", 'Department Name': 'Bottoms', 'label': 0}\n",
      "{'Review Text': \"I really love the look of this jumpsuit, casual but classy. shapely but comfortable. i'm 5'4 and, similar to another reviewer, proportional torso to leg ratio. the legs were longer on me than the model in the picture, but no problem there because they're cuffed.however the arm holes are too big. my bra shows, and the particular one i got came damaged with some of the embroidery coming unraveled on the back. it's also supposed to be dry cleaned and that's annoying for casual wear. returning it.\", 'Department Name': 'Bottoms', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(ddict['train']):\n",
    "    print(v)\n",
    "    if i==2: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start with the tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "ddict = tdc.do_tokenization(tokenizer,max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4526\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 133, 1531, 8089, 4855, 162, 7, 42, 53, 24, 686, 2564, 7735, 4, 5, 299, 21, 2051, 53, 24, 1059, 10, 828, 10178, 12, 3341, 11, 5, 13977, 4, 5, 1468, 630, 75, 619, 372, 1169, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(ddict['validation'][0]['input_ids'][:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 100, 770, 7, 657, 42, 25, 939, 657, 1826, 8087, 53, 24, 10698, 7735, 6, 28701, 11, 3701, 53, 3298, 4740, 11, 5, 28762, 4, 939, 40, 28, 3357, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(ddict['train']))['input_ids'][:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `TextDataController`, you can also perform Text Processing and Tokenization with one method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Input Validation Precheck -----\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title    2966\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 1 rows\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 1, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the DatasetDict from the instance variable `main_ddict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4526\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DatasetDict is ready to be put into any HuggingFace text model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This preprocessing step allow you to filter out certain values of a certain column in your dataset. Let's say I want to filter out any None value in the column 'Review Text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Clothing ID                   0\n",
       "Age                           0\n",
       "Title                      2966\n",
       "Review Text                   0\n",
       "Rating                        0\n",
       "Recommended IND               0\n",
       "Positive Feedback Count       0\n",
       "Division Name                13\n",
       "Department Name              13\n",
       "Class Name                   13\n",
       "dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\n",
    "df[(~df['Review Text'].isna())].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will provide a dictionary containing the name of the column and the filtering function to apply on that column. Note that **the filtering function will receive an item from the column, and the function should return a boolean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Input Validation Precheck -----\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                 seed=42\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/23486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 1, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4529\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tdc.main_ddict['train']:\n",
    "    assert i['Review Text'] is not None\n",
    "\n",
    "for i in tdc.main_ddict['validation']['Review Text']:\n",
    "    assert i is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even add multiple filtering functions. Remember from our precheck, there are also None values in our label 'Department Name'. Let's filter them out as well. While we at it, let's filter out any rating that is less than 3 (just to showcase what our filtering can do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rating\n",
       "5    13131\n",
       "4     5077\n",
       "3     2871\n",
       "2     1565\n",
       "1      842\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Rating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `TextDataController` will only keep the text, the labels and the metadatas columns; any other column will be dropped. To keep the 'Rating', we need to define the `cols_to_keep` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Input Validation Precheck -----\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\n",
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                              'Rating': lambda x: x>=3\n",
    "                                             },\n",
    "                                 cols_to_keep=['Review Text','Rating','Department Name'],\n",
    "                                 seed=42\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/23486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/22641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Rating -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/20258 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 1, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/16206 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4052 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tdc.main_ddict['validation']['Department Name']:\n",
    "    assert i is not None\n",
    "    \n",
    "for i in tdc.main_ddict['validation']['Rating']:\n",
    "    assert i >= 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadatas concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we think the metadatas can be helpful, we can concatenate them into the front of your text, so that our text classification model is aware of it.\n",
    "\n",
    "In this example, Let's add 'Title' as our metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Input Validation Precheck -----\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\n",
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                             },\n",
    "                                 metadatas='Title',\n",
    "                                 process_metas=True, #to preprocess the metadata (currently it's just empty space stripping and lowercasing),\n",
    "                                 seed=42\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/23486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/22641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 1, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"cute top . I wasn't excited enough about this to purchase it, but i did like it. i am wearing a medium in the photos and for reference my measurements are 38-30-40.\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(tdc.main_ddict['train']))['Review Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' . Such a fun jacket! great to wear in the spring or to the office as an alternative to a formal blazer. very comfortable!'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['validation']['Review Text'][0] # The metadata for this text is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add multiple metadatas. Let's say 'Division Name' is the second metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Input Validation Precheck -----\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n",
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/23486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/22641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 0, which is 0.00% of training set\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\n",
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                             },\n",
    "                                 metadatas=['Title','Division Name'],\n",
    "                                 process_metas=True,\n",
    "                                 seed=42\n",
    "                                )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"general . cute top . I wasn't excited enough about this to purchase it, but i did like it. i am wearing a medium in the photos and for reference my measurements are 38-30-40.\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(tdc.main_ddict['train']))['Review Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have briefly gone through the simplest case of label encodings, where we only need to predict 1 single label. We call this **single head classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-eadbd93dc0a62969_*_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/22641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 3, which is 0.02% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All label names will be saved in instance variable `label_lists`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.label_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and all labels will be encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 4, 1, 1]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['validation']['label'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also keep the original labeling, for references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Intimate', 'Tops', 'Tops', 'Dresses', 'Dresses']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['validation']['Department Name'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we need to predict 2 different labels as once (we will call this **multi-head classification**). For example, let's define our dataset so that we need to predict both 'Department Name' and 'Division Name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-eadbd93dc0a62969_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-9ff90c1ed8061e58_*_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "----- Do <lambda> on Department Name -----\n",
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 3, which is 0.02% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names=['Division Name','Department Name'],\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['General', 'General Petite', 'Initmates'],\n",
       " ['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.label_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have two lists, one for label names of 'Division Name', and one for label names of 'Department Name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4526\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Such a fun jacket! great to wear in the spring or to the office as an alternative to a formal blazer. very comfortable!',\n",
       " 'I thought this shirt was really feminine and elegant. only downsides is some of the punched out holes had fabric still attached which you have cut off with scissors- otherwise the shirt will snag. and the second issue of bigger importance are the low armholes. lots of bra showing- not really sure how to get around that so i always wear it with a cardigan. but it would be nice not to have to. \\r\\nother than that it looks nice and pairs nicely with almost anything.',\n",
       " 'This top has a bit of a retro flare but so adorable on. looks really cute with a pair of faded boot cut jeans.',\n",
       " 'I first spotted this on an retailer employee, she paired it with a peasant top & wore it open w/jeans & boots- so darn cute. love how this peice transitions from summer to fall. i\\'m 5\\'4\" so i had to order the small petite which is perfect. note that this dress is very long! it\\'s just a must have garment. the colors/ print are just beautiful.',\n",
       " \"This is my new favorite dress! my only complaint is the slip is too small and the dress cannot be worn without it. i can't order a size up as the dress would then be huge. not sure what the solution is but the dress itself is stunning.\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['validation']['Review Text'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['General Petite', 'General Petite', 'General', 'General Petite', 'General Petite']\n",
      "['Intimate', 'Tops', 'Tops', 'Dresses', 'Dresses']\n"
     ]
    }
   ],
   "source": [
    "print(tdc.main_ddict['validation']['Division Name'][:5])\n",
    "print(tdc.main_ddict['validation']['Department Name'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [1, 4], [0, 4], [1, 1], [1, 1]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['validation']['label'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's define a **multi-label classification**, where a text can have 1 or more label. Our data don't have such labeling, so we will make a new one, just for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Intimate', 'Dresses', 'Bottoms', 'Tops', 'Jackets', 'Trend'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Department Name'].unique()[:-1] # skip the nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Fake Label'] = [np.random.choice(df['Department Name'].unique()[:-1],size=np.random.randint(2,6),replace=False) for _ in range(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "      <th>Fake Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>767</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Initmates</td>\n",
       "      <td>Intimate</td>\n",
       "      <td>Intimates</td>\n",
       "      <td>[Tops, Bottoms]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1080</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[Trend, Tops, Bottoms, Intimate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[Intimate, Trend, Jackets, Bottoms, Tops]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "      <td>[Trend, Tops, Bottoms, Intimate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>[Bottoms, Jackets, Trend, Tops]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clothing ID  Age                    Title   \n",
       "0          767   33                      NaN  \\\n",
       "1         1080   34                      NaN   \n",
       "2         1077   60  Some major design flaws   \n",
       "3         1049   50         My favorite buy!   \n",
       "4          847   47         Flattering shirt   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND   \n",
       "0  Absolutely wonderful - silky and sexy and comf...       4                1  \\\n",
       "1  Love this dress!  it's sooo pretty.  i happene...       5                1   \n",
       "2  I had such high hopes for this dress and reall...       3                0   \n",
       "3  I love, love, love this jumpsuit. it's fun, fl...       5                1   \n",
       "4  This shirt is very flattering to all due to th...       5                1   \n",
       "\n",
       "   Positive Feedback Count   Division Name Department Name Class Name   \n",
       "0                        0       Initmates        Intimate  Intimates  \\\n",
       "1                        4         General         Dresses    Dresses   \n",
       "2                        0         General         Dresses    Dresses   \n",
       "3                        0  General Petite         Bottoms      Pants   \n",
       "4                        6         General            Tops    Blouses   \n",
       "\n",
       "                                  Fake Label  \n",
       "0                            [Tops, Bottoms]  \n",
       "1           [Trend, Tops, Bottoms, Intimate]  \n",
       "2  [Intimate, Trend, Jackets, Bottoms, Tops]  \n",
       "3           [Trend, Tops, Bottoms, Intimate]  \n",
       "4            [Bottoms, Jackets, Trend, Tops]  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't have to add any extra argument; the controller will determine whether this is for multilabel classification, based on the format of the label values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Input Validation Precheck -----\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/23486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 1, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                 label_names=['Fake Label'],\n",
    "                                 seed=42,\n",
    "                                )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.label_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tops', 'Intimate', 'Dresses']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['validation']['Fake Label'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is **multilabel classification**, the label will be one-hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 0, 1, 0]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['validation']['label'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use `TextDataController` for **regression**, you can twist the input argument to achieve what you want. Let's say I want to predict the 'Rating', which is a numerical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-eadbd93dc0a62969_*_of_00004.arrow\n",
      "Loading cached split indices for dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-e1ec6166c233ad6f.arrow and /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-6826d016807238a2.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 1, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None},\n",
    "                         cols_to_keep=['Review Text','Rating'], # we add 'Rating' to cols_to_keep to retain it\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text', 'Rating', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4529\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing to do is to rename the label column to `label`, for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.main_ddict['train'] = tdc.main_ddict['train'].rename_column(\"Rating\", \"label\")\n",
    "tdc.main_ddict['validation'] = tdc.main_ddict['validation'].rename_column(\"Rating\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Review Text': 'These pants are so cute and comfortable that i ordered each color.  after wearing the black ones one day, they stretched out so badly that i returned the other two colors.  i wish the fabric was thicker with more stretch because the style and cut is really cute. bummer', 'input_ids': [0, 4528, 9304, 32, 98, 11962, 8, 3473, 14, 939, 2740, 349, 3195, 4, 1437, 71, 2498, 5, 909, 1980, 65, 183, 6, 51, 13596, 66, 98, 7340, 14, 939, 1835, 5, 97, 80, 8089, 4, 1437, 939, 2813, 5, 10199, 21, 33997, 19, 55, 4140, 142, 5, 2496, 8, 847, 16, 269, 11962, 4, 741, 22539, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 3}\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(tdc.main_ddict['train'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Review Text': \"This picture doesn't do the skirt justice. i paired it with a creme colored cashmere cowlneck sweater and a silver jeweled belt. it is really pretty and flattering on.\", 'label': 5, 'input_ids': [0, 713, 2170, 630, 75, 109, 5, 16576, 2427, 4, 939, 11153, 24, 19, 10, 8633, 1794, 20585, 1055, 25416, 740, 20734, 20020, 23204, 8, 10, 4334, 24693, 196, 7494, 4, 24, 16, 269, 1256, 8, 34203, 15, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(tdc.main_ddict['validation'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This processing allows you to alter the text content in your dataset. All you need to do is having a function, accepting a single string, and return a new, processed string. Note that this transformation will be applied to ALL of your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to normalize our text, because the text might contain some extra spaces between words, or not follow the \"single space after a period\" rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp = \"This is a      sentence,which doesn't follow any rule!No single space is provided after period or punctuation marks.    Maybe there are too many spaces!?!   \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import text_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is a sentence , which doesn't follow any rule ! No single space is provided after period or punctuation marks . Maybe there are too many spaces ! ? !\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_normalize(_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-eadbd93dc0a62969_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-9ff90c1ed8061e58_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-fae4d983a7f511ce_*_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "----- Do <lambda> on Department Name -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Text Transformation --------------------\n",
      "----- text_normalize -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 3, which is 0.02% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         content_transformations=text_normalize,\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I wasn't excited enough about this to purchase it , but i did like it . i am wearing a medium in the photos and for reference my measurements are 38-30-40 .\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(tdc.main_ddict['train']))['Review Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Such a fun jacket ! great to wear in the spring or to the office as an alternative to a formal blazer . very comfortable !'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['validation']['Review Text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can chain multiple functions. Let's say after text normalizing, I want to lowercase the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is not lowercase'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str.lower('tHis IS NoT lowerCASE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-eadbd93dc0a62969_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-9ff90c1ed8061e58_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-fae4d983a7f511ce_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-88a6737dbade33ff_*_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "----- Do <lambda> on Department Name -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Text Transformation --------------------\n",
      "----- text_normalize -----\n",
      "----- lower -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 3, which is 0.02% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i wasn't excited enough about this to purchase it , but i did like it . i am wearing a medium in the photos and for reference my measurements are 38-30-40 .\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(tdc.main_ddict['train']))['Review Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'such a fun jacket ! great to wear in the spring or to the office as an alternative to a formal blazer . very comfortable !'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['validation']['Review Text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even perform some complex transformations, such as removing text inside parentheses, or replacing some texts via a pattern (which is doable using regular expression). Let's make an example of such transformations, where we remove text inside parentheses, and convert any hashtag into the string 'hashtag'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(s):\n",
    "    # Remove texts inside parentheses\n",
    "    s = re.sub(r'\\(.*?\\)', '', s)\n",
    "    \n",
    "    # Convert any hashtag into the string 'hashtag'\n",
    "    s = re.sub(r'#\\w+', 'hashtag', s)\n",
    "    \n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hashtag There's no way it works , however it surprises me hashtag hashtag\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_text(\"#Promotions There's no way it works (I checked!), however it surprises me #howonearth #mindblowing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-eadbd93dc0a62969_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-9ff90c1ed8061e58_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-fae4d983a7f511ce_*_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "----- Do <lambda> on Department Name -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Text Transformation --------------------\n",
      "----- process_text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 3, which is 0.02% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         content_transformations=process_text,\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to perform a train/validation split with `TextDataController`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first way is when you already have a validation split in your HuggingFace's Dataset. Let's use the Dataset built-in function `train_test_split` to simulate this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1)\n",
    "# This will create a 'test' split instead of 'validation', so we will process a bit to have a validation split\n",
    "ddict_with_val['validation']=ddict_with_val['test']\n",
    "del ddict_with_val['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'],\n",
       "        num_rows: 21137\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'],\n",
       "        num_rows: 2349\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict_with_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/21137 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/2349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/20376 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/2265 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/20364 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2264 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split already exists\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 2, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/20364 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2264 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataController(ddict_with_val,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2264\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second way is to split randomly based on a ratio (a float between 0 and 1), or based on the number of data in your validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-eadbd93dc0a62969_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-9ff90c1ed8061e58_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-fae4d983a7f511ce_*_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "----- Do <lambda> on Department Name -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 2, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/19233 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "DatasetDict({\n",
      "    train: <datasets.iterable_dataset.IterableDataset object>\n",
      "    validation: Dataset({\n",
      "        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 3395\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         val_ratio=0.15,\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n",
    "print(tdc.main_ddict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-eadbd93dc0a62969_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-9ff90c1ed8061e58_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-fae4d983a7f511ce_*_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "----- Do <lambda> on Department Name -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 4, which is 0.02% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/17628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "DatasetDict({\n",
      "    train: <datasets.iterable_dataset.IterableDataset object>\n",
      "    validation: Dataset({\n",
      "        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         val_ratio=5000,\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n",
    "print(tdc.main_ddict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A third way is to do a random stratified split (inspired by [sklearn's](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)). Let's do a stratified split based on our label 'Department Name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Department Name\n",
       "Tops        0.445978\n",
       "Dresses     0.269214\n",
       "Bottoms     0.161852\n",
       "Intimate    0.073918\n",
       "Jackets     0.043967\n",
       "Trend       0.005070\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Department Name'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Input Validation Precheck -----\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n",
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/23486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/22641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio, with stratifying\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 2, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                             },\n",
    "                                 val_ratio=0.2,\n",
    "                                 stratify_cols='Department Name',\n",
    "                                 seed=42\n",
    "                                )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tops        0.444033\n",
       "Dresses     0.271602\n",
       "Bottoms     0.161878\n",
       "Intimate    0.072983\n",
       "Jackets     0.044309\n",
       "Trend       0.005193\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([v['Department Name'] for v in tdc.main_ddict['train']]).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tops        0.444101\n",
       "Dresses     0.271542\n",
       "Bottoms     0.161732\n",
       "Intimate    0.073133\n",
       "Jackets     0.044189\n",
       "Trend       0.005303\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(tdc.main_ddict['validation']['Department Name']).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use multiple columns for your stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Input Validation Precheck -----\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n",
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/23486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/22641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio, with stratifying\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 2, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                             },\n",
    "                                 val_ratio=0.2,\n",
    "                                 stratify_cols=['Department Name','Rating'],\n",
    "                                 seed=42\n",
    "                                )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, you can omit any validation split if you specify `val_ratio` as ```None```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Input Validation Precheck -----\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n",
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/23486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/22641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "No validation split defined\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n",
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                             },\n",
    "                                 val_ratio=None,\n",
    "                                 seed=42\n",
    "                                )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n",
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is useful when you have an imbalanced dataset and you want to perform some upsampling (oversampling) on the minority class. In `TextDataController`, you can perform upsampling on any column of the original dataset, and you can even do upsampling on multiple columns at once\n",
    "\n",
    "Behind the scene, upsampling contains 2 steps; first, the subset of the data is collected based on the filtering condition, and then this subset is concatenated back into the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Department Name\n",
       "Tops        8356\n",
       "Dresses     5044\n",
       "Bottoms     3079\n",
       "Intimate    1377\n",
       "Jackets      820\n",
       "Trend        100\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Department Name'].sample(frac=0.8).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Department Name\n",
       "Tops        0.444368\n",
       "Dresses     0.273768\n",
       "Bottoms     0.160320\n",
       "Intimate    0.072650\n",
       "Jackets     0.043622\n",
       "Trend       0.005273\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Department Name'].sample(frac=0.8).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say I want to upsampling the 'Trend' by the factor of 2 (x2 the amount of 'Trend' data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Input Validation Precheck -----\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n",
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/23486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/22641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio, with stratifying\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 2, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Upsampling data --------------------\n",
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                             },\n",
    "                                 val_ratio=0.2,\n",
    "                                 stratify_cols='Department Name',\n",
    "                                 upsampling_list=[('Department Name',lambda x: x=='Trend')],\n",
    "                                 seed=42\n",
    "                                )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tops        8037\n",
       "Dresses     4916\n",
       "Bottoms     2930\n",
       "Intimate    1321\n",
       "Jackets      802\n",
       "Trend        188\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([v['Department Name'] for v in tdc.main_ddict['train']]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percenntage of 'Trend' data in the train set has approximately doubled (note that we filter some NaN text value so the result is not exactly doubled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tops        0.444101\n",
       "Dresses     0.271542\n",
       "Bottoms     0.161732\n",
       "Intimate    0.073133\n",
       "Jackets     0.044189\n",
       "Trend       0.005303\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(tdc.main_ddict['validation']['Department Name']).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since augmentation is applied only to the train set, the distribution of label in the validation set remains the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can triple the amount of 'Trend' by repeating the procedure twice. In the following examples, I will triple the 'Trend' and double the 'Jackets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Input Validation Precheck -----\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n",
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/23486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/22641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Label Encoding -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio, with stratifying\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 2, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Upsampling data --------------------\n",
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                             },\n",
    "                                 val_ratio=0.2,\n",
    "                                 stratify_cols='Department Name',\n",
    "                                 upsampling_list=[('Department Name',lambda x: x=='Trend'),\n",
    "                                                  ('Department Name',lambda x: x=='Trend'),\n",
    "                                                  ('Department Name',lambda x: x=='Jackets')\n",
    "                                                 ],\n",
    "                                 # This can be simplified as\n",
    "#                                  upsampling_list=[('Department Name',lambda x: x=='Trend' or x=='Jackets'),\n",
    "#                                                   ('Department Name',lambda x: x=='Trend')],\n",
    "                                 seed=42\n",
    "                                )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tops        8037\n",
       "Dresses     4916\n",
       "Bottoms     2930\n",
       "Jackets     1604\n",
       "Intimate    1321\n",
       "Trend        282\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([v['Department Name'] for v in tdc.main_ddict['train']]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A word of warning**: **Upsampling is a slow procedure**, as it requires multiple dataset concatenation. This procedure is can be extremely slow for streamed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to Content Transformation, Content Augmentation allows to alter the text content in your dataset. You also need to provide a function accepting a single string, and return a new, processed string. Unlike Content Transformation which is applied to ALL data, the Content Augmentation only applies to your TRAINING data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the popular library for data augmentation is [nlpaug](https://github.com/makcedward/nlpaug). We will demonstrate how to integrate its augmentation functions into our `TextDataController`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.char as nac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp = \"I like my clothes loose fitting but even for me this ran large, i am 5'7 134b and medium fit in the shoulders but was too big overall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation by replacing character with nearby one on the keyboard\n",
    "def nlp_aug(x,aug=None):\n",
    "    return aug.augment(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = nac.KeyboardAug(aug_char_max=3,aug_char_p=0.1,aug_word_p=0.07)\n",
    "nearby_aug_func = partial(nlp_aug,aug=aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I like my clothes loose fittibg but even for me this ran large, i am 5 ' 7 134b and Jedium fit in the shoulders but was too big overal.\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearby_aug_func(_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-eadbd93dc0a62969_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-9ff90c1ed8061e58_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-fae4d983a7f511ce_*_of_00004.arrow\n",
      "Loading cached split indices for dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-ea22fe35bd77453b.arrow and /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-b9332bee103eeff0.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "----- Do <lambda> on Department Name -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-badfb725db509bf2_*_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Number of rows leaked: 3, which is 0.02% of training set\n",
      "Filtering leaked data out of training set...\n",
      "Done\n",
      "-------------------- Text Augmentation --------------------\n",
      "----- nlp_aug -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         content_augmentations=nearby_aug_func,\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I wasn ' t excited eno6gh about this to purchase it, but i did like it. i am weariGg a medium in the (hotos and for reference my measurements are 38 - 30 - 40.\n",
      "--------------------\n",
      "I oTdered the manYo and the navy in size 28. i am 5 ' and 130 lbs. both arrives with the size 28 label on them, the mango fit perfedtly. perfect length on the legs and very flattering. the navy, on the other hand, was an inch shorter and an inch narrower! i Oept douHle - checking the packing slip and the label inside the shorts to see if they accidentally sent a 27p. but no, it was clearly a 28. i brought that back to the store and ordered another naFy 28, hoping that )air was just mislabeled 28 or\n",
      "--------------------\n",
      "TNis shirt looked plain online, but when i see it in stoTe, i thought it ' s so pretty! it is the over - sized sty<e, when i tried it on, it ' s so flattering, even though it is over - sized but it doesn ' t make you look chunky at all! i got it while it was on sale wity extra 25% off! i rang$ from 2 to 6, xs, s for retAiler sizes. i got a xs for ttis top.\n",
      "--------------------\n",
      "This runs way big. i would recomKend sizing down at least one size. possibly 2. i got the design with divers. it ' s fun and light for suHmer. soft mqterial and very comfy.\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    if i==4: break\n",
    "    print(v['Review Text'])\n",
    "    print('-'*20)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, since this is Content Augmentation, the validation set is unmodified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Such a fun jacket! great to wear in the spring or to the office as an alternative to a formal blazer. very comfortable!'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict['validation']['Review Text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even apply Content Augmentation stochastically, by adding a random condition in your augmentation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = nac.KeyboardAug(aug_char_max=3,aug_char_p=0.1,aug_word_p=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_aug_stochastic(x,aug=None,p=0.5):\n",
    "    return aug.augment(x)[0] if random.random()<p else x\n",
    "nearby_aug_func = partial(nlp_aug_stochastic,aug=aug,p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-eadbd93dc0a62969_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-9ff90c1ed8061e58_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-fae4d983a7f511ce_*_of_00004.arrow\n",
      "Loading cached split indices for dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-ea22fe35bd77453b.arrow and /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-b9332bee103eeff0.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "----- Do <lambda> on Department Name -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-f893627565d98cd2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-badfb725db509bf2_*_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Number of rows leaked: 3, which is 0.02% of training set\n",
      "Filtering leaked data out of training set...\n",
      "Done\n",
      "-------------------- Text Augmentation --------------------\n",
      "----- nlp_aug_stochastic -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         content_augmentations=nearby_aug_func,\n",
    "                         seed=42\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I wasn ' t excited enougG about this to purchase it, but i did liue it. i am wearing a medJum in the photos and for reference my measurements are 38 - 30 - 40.\n",
      "--------------------\n",
      "I ordered the mango and the navy in size 28. i am 5' and 130 lbs. both arrived with the size 28 label on them, the mango fit perfectly. perfect length on the legs and very flattering. the navy, on the other hand, was an inch shorter and an inch narrower! i kept double-checking the packing slip and the label inside the shorts to see if they accidentally sent a 27p. but no, it was clearly a 28. i brought that back to the store and ordered another navy 28, hoping that pair was just mislabeled 28 or\n",
      "--------------------\n",
      "This shirt looked plain online, but when i see it in store, i thought it ' s so pretGy! it is the over - sized style, whFn i triev it on, it ' s so flattering, evem though it is over - s8zed but it doesn ' t make you look shunky at all! i got it while it was on sale with extra 25% off! i range from 2 to 6, xs, s for retailer sizes. i got a xs for Yhis top.\n",
      "--------------------\n",
      "This ruMs way big. i Aould revommend sizing down at least one size. possibly 2. i got the design with divers. it ' s fun and light for summer. soft material and very comfy.\n",
      "--------------------\n",
      "GreAt design & execution - this is essentially a striped jersey sleeveless thrtleneck, made of a nice, high - quality jersey. with a delicate lightweight soft sweater attached over it. so you get this really p5etty look where the jersey underlay ,eans the sweatfr hangs beautifully, no clinging at all. and the sleeves are just the sheer sweater - so Lretty. i got the lav4nder - looks exactly like the pic. and it is true to size.\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(tdc.main_ddict['train']):\n",
    "    if i==5: break\n",
    "    print(v['Review Text'])\n",
    "    print('-'*20)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the advanced augmentation is \"Contexttual Word Embeddings Augmenter\" (https://github.com/makcedward/nlpaug/blob/master/example/textual_augmenter.ipynb), where you can insert/substitute words using LLM such as BERT, RoBERTA ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as naw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.ContextualWordEmbsAug(model_path='roberta-base', \n",
    "#                                 device='cuda:0', # uncomment this if you have GPU\n",
    "                                action=\"substitute\",\n",
    "                               aug_p=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp = \"I like my clothes loose fitting but even for me this ran large, i am 5'7 134b and medium fit in the shoulders but was too big overall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_aug_func = partial(nlp_aug,aug=aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I like my clothes loose fitting but even for me this sounds large, i scale 5'7 134b and medium fit in the shoulders but am too big overall\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextual_aug_func(_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically, you can put `contextual_aug_func` into the `content_augmentations` argument, as shown below, and it will run, but this takes forever since it will augment line-by-line (this is the default procedure for augmentation `TextDataController`), which means it doesn't utilize the batching options of nlpaug. If the demain for this functionality is large enough, I will consider adding a special case for nlpaug's contextual augmentation in `TextDataController`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         content_augmentations=contextual_aug_func,\n",
    "                         seed=42,\n",
    "                         is_batched=False,\n",
    "                         num_proc=1 # set this to 1 when running augmentation on GPU\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Capability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Source', 'Content', 'L1', 'L2'],\n",
       "    num_rows: 2269\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main_ddict = load_dataset('secret_data',data_files=['buyer_listening_with_all_raw_data_w28.csv','buyer_listening_with_all_raw_data_w28.csv'],split='train')\n",
    "# main_ddict\n",
    "\n",
    "# main_ddict = load_dataset('sample_data',data_files=['sample_large.csv','sample_large.csv','sample_large.csv'])\n",
    "# main_ddict\n",
    "\n",
    "main_ddict = load_dataset('sample_data',data_files=['sample_large.csv'],split='train')\n",
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Source': ['Google Play', 'Non Owned', 'Google Play', 'Owned', 'Google Play'],\n",
       " 'Content': ['App ncc lúc nào cx lag đơ, phần tìm kiếm thì viết kiểu gì sp đó vẫn ko ra, thế phải ghi đúng tên mới chịu à? Lỡ quên tên ngta ghi mé mé như v cx phải gợi ý sp tương tự chứ??? ☻',\n",
       "  '..❗️ GÓC THANH LÝ Tính ra rẻ hơn cả mua #Shopee Mong 1 lần đc check ib mỏi tay 😆😆😆   Em chuyển cửa hàng nên dọn lại có thừa vài chục tấm nệm xuất nhật này.   1mx2m : 1m2x2 : 1m4x2m : 1m6x2m : 1m8x2m :2mx2m Đệm dày 7-8 phân Nhưng vì còn ít nên topic này em bán thanh lý giá rẻ ạ.   Ship cod nhận hàng được kiểm tra thoải mái.  Miễn ship toàn quốc. Nên đừng bom tội nghiệp em nhé dày 7-8 phân Nhắn tin em gửi mẫu nhé🥰',\n",
       "  'Mắc gì người ta đặt hàng toàn lỗi 😃????',\n",
       "  '#GhienShopeePayawardT8 Khi bạn chơi shopee quá lâu thì không thể nào không biết đến với ShopeePay . Liên kết thanh toán được cho các đơn hàng Shopee và ShopeeFood luôn nè.',\n",
       "  'Rất bức xúc khi dùng . mã giảm giá người dùng thì m02 vậy cho ưu đãi đấy làm gì ạ . Mua đc mấy đơn thì bị m04 vậy tải về để mua hàng nhưng bh lại để làm cảnh . Thật sự là ko đáng để đc đánh giá một sao'],\n",
       " 'L1': ['Feature', 'Commercial', 'Feature', 'Commercial', 'Feature'],\n",
       " 'L2': ['App performance',\n",
       "  'Items/price',\n",
       "  'App performance',\n",
       "  'Shopee Programs',\n",
       "  'Apply Voucher']}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 210 µs, sys: 118 µs, total: 328 µs\n",
      "Wall time: 227 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Source': 'Google Play',\n",
       " 'Content': 'App ncc lúc nào cx lag đơ, phần tìm kiếm thì viết kiểu gì sp đó vẫn ko ra, thế phải ghi đúng tên mới chịu à? Lỡ quên tên ngta ghi mé mé như v cx phải gợi ý sp tương tự chứ??? ☻',\n",
       " 'L1': 'Feature',\n",
       " 'L2': 'App performance'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "main_ddict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_filter_dict={'L1':lambda x: x!='Others'}\n",
    "\n",
    "_content_tfms = partial(apply_vnmese_word_tokenize,normalize_text=True)\n",
    "_content_tfms.__name__='VNM word segmentation'\n",
    "\n",
    "_upsampling_dict={\n",
    "    'Source': lambda x: x=='hc search' if random.random()<0.5 else False\n",
    "}\n",
    "\n",
    "_aug_tfms=partial(remove_vnmese_accent,prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc = TextDataController(main_ddict,main_text='Content',\n",
    "                         label_names='L1',\n",
    "                         filter_dict=_filter_dict,\n",
    "                         metadatas='Source',\n",
    "                         content_transformations=_content_tfms,\n",
    "                         val_ratio=0.25,\n",
    "                         stratify_cols='Source',\n",
    "                         upsampling_dict=_upsampling_dict,\n",
    "                         content_augmentations=_aug_tfms,\n",
    "                         seed=42,\n",
    "                         is_batched=True,\n",
    "                         is_streamed=False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-e8025282b10ede62_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-0ecebfc0c7f83a1d_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-966fc97b537cb573_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-96e446a75e3f09ba/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-9067e41b077d8eb9_*_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on L1 -----\n",
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Text Transformation --------------------\n",
      "----- VNM word segmentation -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1458 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/1458 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 1, which is 0.09% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/1093 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Upsampling data --------------------\n",
      "----- Do <lambda> on Source -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/1091 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Text Augmentation --------------------\n",
      "----- remove_vnmese_accent -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "CPU times: user 252 ms, sys: 154 ms, total: 406 ms\n",
      "Wall time: 736 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "my_ddict = tdc.do_all_preprocessing(shuffle_trn=True)\n",
    "# 2x big data\n",
    "# is_batched True, shuffle_trn True. Wall time: 1min 6s\n",
    "# is_batched False, shuffle_trn True, Wall time: 1min 18s\n",
    "\n",
    "# 3x sample_large.csv\n",
    "# is_batched True, shuffle_trn True. Wall time: 2.79 s\n",
    "# is_batched False, shuffle_trn True, Wall time: 2.96 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Source', 'Content', 'L1'],\n",
       "        num_rows: 365\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 478 µs, sys: 0 ns, total: 478 µs\n",
      "Wall time: 310 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Source': 'non owned',\n",
       " 'Content': 'non owned . Mã_500K toàn sàn Shopee cho ai cần nè » https://shope.ee/10QzJtpqQi',\n",
       " 'L1': 1}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "my_ddict['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp=iter(my_ddict['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.2 ms, sys: 301 µs, total: 28.5 ms\n",
      "Wall time: 26.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Source': 'google play',\n",
       " 'Content': 'google play . Chán ơi là chánnnn Toàn_bị U02_Mn có biết cách nào chữa ko thì chỉ em với ạ 🥰',\n",
       " 'L1': 3}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "next(_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 63 µs, sys: 53 µs, total: 116 µs\n",
      "Wall time: 118 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Source': 'google play',\n",
       " 'Content': 'google play . Ứng_dụng quá là đơ . Càng dùng càng đơ thì mua_bán cái mẹ j .',\n",
       " 'L1': 3}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "next(_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhobertTokenizer(name_or_path='vinai/phobert-base', vocab_size=64000, model_max_length=256, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Source', 'Content', 'L1'],\n",
       "        num_rows: 365\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tdc.main_ddict['validation'] = tdc.main_ddict['validation'].to_iterable_dataset(num_shards=tdc.num_shards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/365 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_ddict_tok = tdc.do_tokenization(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Source', 'Content', 'L1', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 365\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_ddict_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 287 µs, sys: 140 µs, total: 427 µs\n",
      "Wall time: 380 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = my_ddict_tok['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'non owned . Mã_500K toàn sàn Shopee cho ai cần nè » https://shope.ee/10QzJtpqQi'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_ddict_tok['validation'][0]['Content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'non', 'ow@@', 'ned', '.', 'Mã_@@', '500@@', 'K', 'toàn', 'sàn', 'Sho@@', 'pee', 'cho', 'ai', 'cần', 'nè', '»', 'htt@@', 'ps@@', '://@@', 'sho@@', 'pe@@', '.@@', 'ee@@', '/@@', '10@@', 'Q@@', 'z@@', 'J@@', 't@@', 'p@@', 'q@@', 'Q@@', 'i', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(my_ddict_tok['validation'][0]['input_ids'])[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_iter=iter(my_ddict_tok['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 272 ms, sys: 123 µs, total: 272 ms\n",
      "Wall time: 272 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_tmp = next(_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 5 µs, total: 5 µs\n",
      "Wall time: 6.68 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_tmp = next(_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google play . App nhu cc ko lam gi cung xoa tk\n",
      "['<s>', 'google', 'play', '.', 'App', 'nhu', 'cc', 'ko', 'lam', 'gi', 'cung', 'xoa', 't@@', 'k', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(_tmp['Content'])\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(_tmp['input_ids'])[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2v = {i:v for i,v in enumerate(tdc.label_lists[0])}\n",
    "\n",
    "vc_l1 = pd.Series(my_ddict['validation']['L1']).value_counts(normalize=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feature</td>\n",
       "      <td>0.369863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Commercial</td>\n",
       "      <td>0.213699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shopee account</td>\n",
       "      <td>0.131507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Delivery</td>\n",
       "      <td>0.128767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Buyer complained seller</td>\n",
       "      <td>0.041096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Return/Refund</td>\n",
       "      <td>0.032877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Services</td>\n",
       "      <td>0.032877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Order/Item</td>\n",
       "      <td>0.027397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Payment</td>\n",
       "      <td>0.021918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     index  proportion\n",
       "0                  Feature    0.369863\n",
       "1               Commercial    0.213699\n",
       "2           Shopee account    0.131507\n",
       "3                 Delivery    0.128767\n",
       "4  Buyer complained seller    0.041096\n",
       "5            Return/Refund    0.032877\n",
       "6                 Services    0.032877\n",
       "7               Order/Item    0.027397\n",
       "8                  Payment    0.021918"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc_l1['index'] = vc_l1['index'].map(i2v)\n",
    "vc_l1\n",
    "# Feature                    0.371056\n",
    "#  Commercial                 0.209191\n",
    "#  Delivery                   0.137174\n",
    "#  Shopee account             0.127572\n",
    "#  Buyer complained seller    0.043896\n",
    "#  Return/Refund              0.030864\n",
    "#  Payment                    0.030178\n",
    "#  Order/Item                 0.028121\n",
    "#  Services                   0.021948"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(google play    271\n",
       " non owned       34\n",
       " owned           26\n",
       " ios             22\n",
       " hc search       12\n",
       " Name: count, dtype: int64,\n",
       " google play    0.742466\n",
       " non owned      0.093151\n",
       " owned          0.071233\n",
       " ios            0.060274\n",
       " hc search      0.032877\n",
       " Name: proportion, dtype: float64)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(my_ddict['validation']['Source']).value_counts(),pd.Series(my_ddict['validation']['Source']).value_counts(normalize=True)\n",
    "#  Google Play    0.743484\n",
    "#  Non Owned      0.093278\n",
    "#  Owned          0.069959\n",
    "#  iOS            0.061043\n",
    "#  HC search      0.032236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp_trn = list(my_ddict['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_ddict['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(_tmp_trn) + len(my_ddict['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1106"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_tmp_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1110"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int((47-12)*0.5) + (1458 - 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datasets.iterable_dataset.IterableDataset>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main_ddict = load_dataset('secret_data',data_files=['buyer_listening_with_all_raw_data_w28.csv','buyer_listening_with_all_raw_data_w28.csv'],split='train')\n",
    "# main_ddict\n",
    "\n",
    "# main_ddict = load_dataset('sample_data',data_files=['sample_large.csv','sample_large.csv','sample_large.csv'],streaming=True)\n",
    "# main_ddict\n",
    "\n",
    "main_ddict = load_dataset('sample_data',data_files=['sample_large.csv'],split='train',streaming=True)\n",
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,v in enumerate(main_ddict):\n",
    "#     print(v)\n",
    "#     if i==4: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=['Feature','Commercial','Delivery',\n",
    "        'Shopee account','Buyer complained seller',\n",
    "        'Return/Refund','Payment','Order/Item',\n",
    "        'Services','Others']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_filter_dict={'L1':lambda x: x!='Others'}\n",
    "\n",
    "_content_tfms = partial(apply_vnmese_word_tokenize,normalize_text=True)\n",
    "_content_tfms.__name__='VNM word segmentation'\n",
    "\n",
    "_upsampling_dict={\n",
    "    'Source': lambda x: x=='hc search' if random.random()<0.5 else False\n",
    "}\n",
    "\n",
    "_aug_tfms=partial(remove_vnmese_accent,prob=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_ddict = load_dataset('sample_data',data_files=['sample_large.csv'],split='train',streaming=True)\n",
    "tdc = TextDataController(main_ddict,main_text='Content',\n",
    "                         label_names='L1',\n",
    "                         class_names_predefined=labels,\n",
    "                         filter_dict=_filter_dict,\n",
    "                         metadatas='Source',\n",
    "                         content_transformations=_content_tfms,\n",
    "                         val_ratio=365, \n",
    "#                          upsampling_dict=_upsampling_dict, # super slow\n",
    "                         content_augmentations=_aug_tfms,\n",
    "                         seed=42,\n",
    "                         is_batched=True,\n",
    "                         is_streamed=True,\n",
    "                         num_shards=512\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on L1 -----\n",
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Text Transformation --------------------\n",
      "----- VNM word segmentation -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "-------------------- Text Augmentation --------------------\n",
      "Done\n",
      "-------------------- Shuffling train set --------------------\n",
      "Done\n",
      "CPU times: user 2.62 s, sys: 2.64 s, total: 5.26 s\n",
      "Wall time: 5.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "my_ddict = tdc.do_all_preprocessing(shuffle_trn=True)\n",
    "# 2x big data\n",
    "# is_batched True, shuffle_trn True. Wall time: 1min 6s\n",
    "# is_batched False, shuffle_trn True, Wall time: 1min 18s\n",
    "\n",
    "# 3x sample_large.csv\n",
    "# is_batched True, shuffle_trn True. Wall time: 2.79 s\n",
    "# is_batched False, shuffle_trn True, Wall time: 2.96 s\n",
    "\n",
    "# TODO: redo\n",
    "# 3x sample_large.csv, streaming, with aug\n",
    "# is_batched True, shuffle_trn True. Wall time: 53.5 s\n",
    "# is_batched False, shuffle_trn True, Wall time: 17.4 s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Source', 'Content', 'L1'],\n",
       "    num_rows: 365\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_ddict['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/365 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_ddict_tok = tdc.do_tokenization(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <datasets.iterable_dataset.IterableDataset object>\n",
       "    validation: Dataset({\n",
       "        features: ['Source', 'Content', 'L1', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 365\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_ddict_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(list(my_ddict_tok['train'])),len(list(my_ddict_tok['validation']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_iter=iter(my_ddict_tok['train'])\n",
    "_tmp = next(_iter)\n",
    "_tmp = next(_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_ddict_tok['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp=iter(my_ddict['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.29 s, sys: 2.93 s, total: 5.22 s\n",
      "Wall time: 5.22 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Source': 'google play',\n",
       " 'Content': 'google play . Bi loi khong hien_hinh_anh , xoa di tai lai app deu ko hien_thi gi het .',\n",
       " 'L1': 3}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "next(_tmp)\n",
    "# 9.44s with upsampling\n",
    "# 4.72s without upsampling\n",
    "# 5.16 without upsampling, with augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
      "Wall time: 8.58 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Source': 'hc search',\n",
       " 'Content': 'hc search . khong cap_nhat duoc sdt',\n",
       " 'L1': 9}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "next(_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp=iter(my_ddict['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 173 µs, sys: 154 µs, total: 327 µs\n",
      "Wall time: 241 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Source': 'google play',\n",
       " 'Content': 'google play . đang chơi mà quảng_cáo',\n",
       " 'L1': 1}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "next(_tmp)\n",
    "# 9.44s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 80 µs, sys: 0 ns, total: 80 µs\n",
      "Wall time: 83 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Source': 'google play',\n",
       " 'Content': 'google play . Cứ quản cáo quá nhiều , app nào củng gặp quản_cáo của shopee 😆',\n",
       " 'L1': 1}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "next(_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1093, 365)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(my_ddict['train'])),len(list(my_ddict['validation']))\n",
    "# (1124, 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1093, 365)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(my_ddict_tok['train'])),len(list(my_ddict_tok['validation']))\n",
    "# (1124, 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Source': 'ios', 'Content': 'ios . Đã chỉnh đi chỉnh lại rất nhiều nhưng nó vẫn ghi là thanh toán ko khả dụng! \\nLà sao hả SHOPEE:)))', 'L1': 6}\n",
      "{'Source': 'hc search', 'Content': 'hc search . kh đc mượt', 'L1': 3}\n",
      "{'Source': 'google play', 'Content': 'google play . Hóng đơn hàng về .khi về thì Shipper chưa giao mà báo k ai nhận. Tự ý huỷ đơn. Shipper Ngũ Hành Sơn Đà Nẵng quá kém.', 'L1': 2}\n",
      "{'Source': 'google play', 'Content': 'google play . Có cái nịt thùng mì 50k phí sip 100', 'L1': 2}\n",
      "{'Source': 'google play', 'Content': 'google play . Alo mới đặt hàng đi vắng có 1 bữa Xong shipper nt chửi ôm xồm vậy Đánh giá 1 sao cho biết', 'L1': 2}\n",
      "{'Source': 'google play', 'Content': 'google play . Đc', 'L1': 5}\n",
      "{'Source': 'google play', 'Content': 'google play . Tôi vưa bị đăng xuất khỏi shoppe một cách vô lý. Bây h k vào lại đc. Có vào đc thì tất cả nhưg đơn hàg trc đó của tui cug đã bị mất. Yêu cầu giai quyết vấn đề', 'L1': 9}\n",
      "{'Source': 'google play', 'Content': 'google play . Giao hàng gì mà tới kho gần nhà 3 km mà k chịu giao 6 ngày r đùa nhau a', 'L1': 2}\n",
      "{'Source': 'non owned', 'Content': 'non owned . # **NHANH TAY VÀO LẤY XU**  1. Thử Thách Shopee  🍒 https://shope.ee/7A1gZ0rOqG  2. Thử Thách Shopee Mall (chọn tab \"chương trình nổi bật\")  🍒 https://shope.ee/1fgk0ulXGK  3. Shopee Máy Gắp Thú - 700xu/lượt  🍒 https://shope.ee/AJyiKqnwiu', 'L1': 1}\n",
      "{'Source': 'google play', 'Content': 'google play . Đơ lag, tìm kiếm không thông minh', 'L1': 3}\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(my_ddict['train']):\n",
    "    print(v)\n",
    "    if i==9: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp=iter(my_ddict_tok['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google play . đang chơi mà quảng_cáo\n",
      "['<s>', 'google', 'play', '.', 'đang', 'chơi', 'mà', 'quảng_cáo', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(my_ddict_tok['validation']['Content'][0])\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens((my_ddict_tok['validation']['input_ids'][0])[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start a step-by-step walkthrough on how to use this class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset,load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('sample_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH/'sample_large.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_ddict = Dataset.from_csv(str(DATA_PATH/'sample_large.csv'))\n",
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_ddict = load_dataset(str(DATA_PATH),data_files={'train':'sample_large.csv'})\n",
    "main_ddict = load_dataset(str(DATA_PATH),data_files='sample_large.csv')\n",
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_ddict = load_dataset(str(DATA_PATH),data_files='sample_large.csv',split='train')\n",
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datasets.iterable_dataset.IterableDataset>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict = load_dataset(str(DATA_PATH),data_files='sample_large.csv',split='train',streaming=True)\n",
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Source': 'Google Play',\n",
       " 'Content': 'App ncc lúc nào cx lag đơ, phần tìm kiếm thì viết kiểu gì sp đó vẫn ko ra, thế phải ghi đúng tên mới chịu à? Lỡ quên tên ngta ghi mé mé như v cx phải gợi ý sp tương tự chứ??? ☻',\n",
       " 'L1': 'Feature',\n",
       " 'L2': 'App performance'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(main_ddict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(main_ddict.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "main_ddict = load_dataset(str(DATA_PATH),data_files=['sample_large.csv','sample_large.csv'])\n",
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_ddict = load_dataset('csv',data_files=str(DATA_PATH/'sample_large.csv'))\n",
    "# main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp = Path('secret_data')/'some_files.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Path('secret_data'), 'some_files.csv')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_tmp.parent,_tmp.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('.')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_tmp = Path('some_files.csv')\n",
    "_tmp.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/secret_data-042d1badc74881bf/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.06 ms, sys: 3.97 ms, total: 5.03 ms\n",
      "Wall time: 4.48 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Week', 'Group', 'Source', 'Content', 'L1', 'L2', 'L3', 'L4', 'is_valid', 'iteration'],\n",
       "    num_rows: 114605\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "main_ddict = load_dataset('secret_data',data_files='buyer_listening_with_all_raw_data_w28.csv',split='train')\n",
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/quan/.cache/huggingface/datasets/csv/default-042d1badc74881bf/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcea26c775af4774b58e4eb77438c80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1fa7b0c91d47548d9ca5d92e1b8a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/quan/.cache/huggingface/datasets/csv/default-042d1badc74881bf/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n",
      "CPU times: user 592 ms, sys: 84.5 ms, total: 677 ms\n",
      "Wall time: 1.72 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Week', 'Group', 'Source', 'Content', 'L1', 'L2', 'L3', 'L4', 'is_valid', 'iteration'],\n",
       "    num_rows: 114605\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "main_ddict = load_dataset('csv',data_files='secret_data/buyer_listening_with_all_raw_data_w28.csv',split='train')\n",
    "main_ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/secret_data-042d1badc74881bf/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e22c59234744b838afd14c7c1df86c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "dict_keys(['train'])\n"
     ]
    }
   ],
   "source": [
    "main_ddict = load_dataset('secret_data',data_files='buyer_listening_with_all_raw_data_w28.csv')\n",
    "if hasattr(main_ddict,'keys'):\n",
    "    print('yes')\n",
    "print(main_ddict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Week', 'Group', 'Source', 'Content', 'L1', 'L2', 'L3', 'L4', 'is_valid', 'iteration'],\n",
       "        num_rows: 114605\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp = main_ddict.pop('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    \n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/secret_data-042d1badc74881bf/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Week', 'Group', 'Source', 'Content', 'L1', 'L2', 'L3', 'L4', 'is_valid', 'iteration'],\n",
       "    num_rows: 114605\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict = load_dataset('secret_data',data_files='buyer_listening_with_all_raw_data_w28.csv',split='train')\n",
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(main_ddict,'keys'):\n",
    "    print('yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process metadatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Week', 'Group', 'Source', 'Content', 'L1', 'L2', 'L3', 'L4', 'is_valid', 'iteration'],\n",
       "    num_rows: 114605\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Week': [28.0, 28.0, 28.0],\n",
       " 'Group': ['Tú Bà Bà',\n",
       "  'Gia Lai-Thanh lý đồ dùng và thời trang',\n",
       "  'thuonghieucongluan.com.vn'],\n",
       " 'Source': ['Non Owned', 'Non Owned', 'Non Owned'],\n",
       " 'Content': ['Riết rồi k biết xài cái quần gì để k bị trừ tiền oan. Bữa trc thì vụ nạp thẻ qua shopee T chửi chưa đã cái miệng. Đổi qua momo cho lành. Ỷ y momo k có số dư nên chắc k sao. Chủ yếu lk tk ngân hàng thôi, cái nó tự liên kết thẻ trừ giao dịch qua apple mỗi tuần 129k/ tuần (ông cố ơi!)🙂Thêm cái chỉnh ảnh 419k/ năm (cái này cũng được đi).Dm bữa h tự trừ hết hơn 1tr trong tk ngân hàng. Ui là trời. 2 hộp sữa của con T ra đi nữa rồi đó.Hiện đại, hại điện.Khỏi cảm ơn, T xoá app rồi.Dm trả lại T 2 hộp Meiji đi rồi T sử dụng lại.!',\n",
       "  'GHN có còn ship cho Sendo không các thím? Em mua shopee, lazada, TIKI thì đơn do best, GHN vs viettel ship nhiều nhất mà mấy bên này chắc ship hầu hết các sàn lớn đúng ko nhờ?',\n",
       "  'Cục QLTT Hà Nội: Kiểm tra, xử lý nhiều vụ hàng lậu, hàng giả'],\n",
       " 'L1': ['Feature', 'Delivery', 'Buyer complained seller'],\n",
       " 'L2': ['Feature Others',\n",
       "  '3PL service (excluded SPX)',\n",
       "  'Illegal/counterfeit products'],\n",
       " 'L3': [None, None, None],\n",
       " 'L4': [None, None, None],\n",
       " 'is_valid': [0.0, 0.0, 0.0],\n",
       " 'iteration': [21, 21, 21]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadatas = ['Week','Source']\n",
    "# metadatas = val2iterable(metadatas)\n",
    "process_metas = True\n",
    "main_text='Content'\n",
    "is_batched=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/secret_data-042d1badc74881bf/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Week', 'Group', 'Source', 'Content', 'L1', 'L2', 'L3', 'L4', 'is_valid', 'iteration'],\n",
       "    num_rows: 114605\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict = load_dataset('secret_data',data_files='buyer_listening_with_all_raw_data_w28.csv',split='train')\n",
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _process_metadatas(ds:dict,\n",
    "#                        main_text,\n",
    "#                        metadatas,\n",
    "#                        process_metas=True,\n",
    "#                        sep='.',\n",
    "#                       is_batched=True):\n",
    "#     metadatas = val2iterable(metadatas)\n",
    "#     results={main_text:ds[main_text]}\n",
    "#     for m in metadatas:\n",
    "#         m_data = ds[m]\n",
    "#         if process_metas:\n",
    "#             # just strip and lowercase\n",
    "#             m_data = [str(v).strip().lower() for v in m_data] if is_batched else str(m_data).strip().lower()\n",
    "#         results[m]=m_data\n",
    "#         if is_batched:\n",
    "#             results[main_text] = [f'{m_data[i]}{sep} {results[main_text][i]}' for i in range(len(m_data))]\n",
    "#         else:\n",
    "#             results[main_text] = f'{m_data}{sep} {results[main_text]}'\n",
    "#     return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Metadatas Simple Processing & Concatenating to Main Content -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/114605 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_msg('Metadatas Simple Processing & Concatenating to Main Content')\n",
    "main_ddict_meta = main_ddict.map(partial(_process_metadatas,\n",
    "                                         main_text=main_text,\n",
    "                                         metadatas=metadatas,\n",
    "                                         process_metas=process_metas,\n",
    "                                         is_batched=is_batched),\n",
    "                                batched=is_batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Week': [1.0, 1.0, 1.0],\n",
       " 'Group': ['Google Play', 'Google Play', 'Google Play'],\n",
       " 'Source': ['google play', 'google play', 'google play'],\n",
       " 'Content': ['google play. 1.0. Tại sao cứ hiện thông báo',\n",
       "  'google play. 1.0. Mlem',\n",
       "  'google play. 1.0. 1 số sản phẩm trong giỏ hàng vừa đc cập nhật trong khi giỏ ko còn 1 hàng nào nx 😀'],\n",
       " 'L1': ['Services', 'Others', 'Feature'],\n",
       " 'L2': ['Shopee communication channels', 'Cannot defined', 'Cart & Order'],\n",
       " 'L3': ['Annoying pop-up ads', '-', 'Cart issues/suggestions'],\n",
       " 'L4': ['Non-tech', '-', 'Tech'],\n",
       " 'is_valid': [None, None, None],\n",
       " 'iteration': [1, 1, 1]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict_meta[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/secret_data-042d1badc74881bf/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-cadaba33433fe0f4.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n"
     ]
    }
   ],
   "source": [
    "print_msg('Metadata Simple Processing & Concatenating to Main Content')\n",
    "main_ddict_meta = main_ddict.map(partial(_process_metadatas,\n",
    "                                         main_text=main_text,\n",
    "                                         metadatas=metadatas,\n",
    "                                         process_metas=process_metas,\n",
    "                                         is_batched=False),\n",
    "                                batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Week': [1.0, 1.0, 1.0],\n",
       " 'Group': ['Google Play', 'Google Play', 'Google Play'],\n",
       " 'Source': ['google play', 'google play', 'google play'],\n",
       " 'Content': ['google play. 1.0. Tại sao cứ hiện thông báo',\n",
       "  'google play. 1.0. Mlem',\n",
       "  'google play. 1.0. 1 số sản phẩm trong giỏ hàng vừa đc cập nhật trong khi giỏ ko còn 1 hàng nào nx 😀'],\n",
       " 'L1': ['Services', 'Others', 'Feature'],\n",
       " 'L2': ['Shopee communication channels', 'Cannot defined', 'Cart & Order'],\n",
       " 'L3': ['Annoying pop-up ads', '-', 'Cart issues/suggestions'],\n",
       " 'L4': ['Non-tech', '-', 'Tech'],\n",
       " 'is_valid': [None, None, None],\n",
       " 'iteration': [1, 1, 1]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict_meta[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datasets.iterable_dataset.IterableDataset>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stream\n",
    "main_ddict_stream = load_dataset('secret_data',data_files='buyer_listening_with_all_raw_data_w28.csv',split='train',streaming=True)\n",
    "main_ddict_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_ddict_meta = main_ddict_stream.map(partial(_process_metadatas,\n",
    "                                         main_text=main_text,\n",
    "                                         metadatas=metadatas,\n",
    "                                         process_metas=process_metas,\n",
    "                                         is_batched=True),\n",
    "                                batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Week': '1.0',\n",
       "  'Group': 'Google Play',\n",
       "  'Source': 'google play',\n",
       "  'Content': 'google play. 1.0. Tại sao cứ hiện thông báo',\n",
       "  'L1': 'Services',\n",
       "  'L2': 'Shopee communication channels',\n",
       "  'L3': 'Annoying pop-up ads',\n",
       "  'L4': 'Non-tech',\n",
       "  'is_valid': None,\n",
       "  'iteration': 1},\n",
       " {'Week': '1.0',\n",
       "  'Group': 'Google Play',\n",
       "  'Source': 'google play',\n",
       "  'Content': 'google play. 1.0. Mlem',\n",
       "  'L1': 'Others',\n",
       "  'L2': 'Cannot defined',\n",
       "  'L3': '-',\n",
       "  'L4': '-',\n",
       "  'is_valid': None,\n",
       "  'iteration': 1},\n",
       " {'Week': '1.0',\n",
       "  'Group': 'Google Play',\n",
       "  'Source': 'google play',\n",
       "  'Content': 'google play. 1.0. 1 số sản phẩm trong giỏ hàng vừa đc cập nhật trong khi giỏ ko còn 1 hàng nào nx 😀',\n",
       "  'L1': 'Feature',\n",
       "  'L2': 'Cart & Order',\n",
       "  'L3': 'Cart issues/suggestions',\n",
       "  'L4': 'Tech',\n",
       "  'is_valid': None,\n",
       "  'iteration': 1}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(main_ddict_meta.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Week': '1.0',\n",
       "  'Group': 'Google Play',\n",
       "  'Source': 'google play',\n",
       "  'Content': 'google play. 1.0. Tại sao cứ hiện thông báo',\n",
       "  'L1': 'Services',\n",
       "  'L2': 'Shopee communication channels',\n",
       "  'L3': 'Annoying pop-up ads',\n",
       "  'L4': 'Non-tech',\n",
       "  'is_valid': None,\n",
       "  'iteration': 1},\n",
       " {'Week': '1.0',\n",
       "  'Group': 'Google Play',\n",
       "  'Source': 'google play',\n",
       "  'Content': 'google play. 1.0. Mlem',\n",
       "  'L1': 'Others',\n",
       "  'L2': 'Cannot defined',\n",
       "  'L3': '-',\n",
       "  'L4': '-',\n",
       "  'is_valid': None,\n",
       "  'iteration': 1},\n",
       " {'Week': '1.0',\n",
       "  'Group': 'Google Play',\n",
       "  'Source': 'google play',\n",
       "  'Content': 'google play. 1.0. 1 số sản phẩm trong giỏ hàng vừa đc cập nhật trong khi giỏ ko còn 1 hàng nào nx 😀',\n",
       "  'L1': 'Feature',\n",
       "  'L2': 'Cart & Order',\n",
       "  'L3': 'Cart issues/suggestions',\n",
       "  'L4': 'Tech',\n",
       "  'is_valid': None,\n",
       "  'iteration': 1}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict_meta = main_ddict_stream.map(partial(_process_metadatas,\n",
    "                                         main_text=main_text,\n",
    "                                         metadatas=metadatas,\n",
    "                                         process_metas=process_metas,\n",
    "                                         is_batched=False),\n",
    "                                batched=False)\n",
    "# batched does not matter when using streamed\n",
    "list(main_ddict_meta.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Week', 'Group', 'Source', 'Content', 'L1', 'L2', 'L3', 'L4', 'is_valid', 'iteration'],\n",
       "    num_rows: 114605\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadatas = ['Week','Source']\n",
    "process_metas = True\n",
    "main_text='Content'\n",
    "is_batched=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/secret_data-042d1badc74881bf/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-ac679a0b1f06a8e9.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Metadatas Simple Processing & Concatenating to Main Content -----\n"
     ]
    }
   ],
   "source": [
    "print_msg('Metadatas Simple Processing & Concatenating to Main Content')\n",
    "main_ddict_meta = main_ddict.map(partial(_process_metadatas,\n",
    "                                         main_text=main_text,\n",
    "                                         metadatas=metadatas,\n",
    "                                         process_metas=process_metas,\n",
    "                                         is_batched=is_batched),\n",
    "                                batched=is_batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114605"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(main_ddict_meta['L1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def _encode_labels(self):\n",
    "#         print_msg('Label Encoding')\n",
    "#         if self.label_names is None: \n",
    "#             raise ValueError('Missing label columns!')\n",
    "#         self.label_names = val2iterable(self.label_names)\n",
    "#         if len(self.label_names)>1:\n",
    "#             self.is_multihead=True\n",
    "        \n",
    "#         if self.label_lists is not None and not isinstance(self.label_lists[0],list):\n",
    "#             self.label_lists = [self.label_lists]\n",
    "        \n",
    "#         if isinstance(self.df[self.label_names[0]].iloc[0],list):\n",
    "########               (self.dset[self.label_names[0]][0],list)\n",
    "#             # This is multi-label. Ignore self.label_names[1:]\n",
    "#             self.label_names = [self.label_names[0]]\n",
    "#             self.is_multihead=False\n",
    "#             self.is_multilabel=True\n",
    "            \n",
    "#         encoder_classes=[]\n",
    "#         if not self.is_multilabel:\n",
    "#             for idx,l in enumerate(self.label_names):\n",
    "#                 if self.label_lists is None:\n",
    "#                     train_label = self.df[l].values\n",
    "#                     l_encoder = LabelEncoder()\n",
    "#                     self.df[l] = l_encoder.fit_transform(train_label)\n",
    "#                     encoder_classes.append(list(l_encoder.classes_))\n",
    "#                 else:\n",
    "#                     l_classes = sorted(list(self.label_lists[idx]))\n",
    "#                     label2idx = {v:i for i,v in enumerate(l_classes)}\n",
    "#                     self.df[l] = self.df[l].map(label2idx).values\n",
    "#                     encoder_classes.append(l_classes)\n",
    "#         else:\n",
    "#             # For MultiLabel, we only save the encoder classes without transforming the label itself to one-hot (or actually, few-hot)\n",
    "#             if self.label_lists is None:\n",
    "#                 l_encoder = MultiLabelBinarizer()\n",
    "#                 _ = l_encoder.fit(self.df[self.label_names[0]])\n",
    "#                 encoder_classes.append(list(l_encoder.classes_))\n",
    "#             else:\n",
    "#                 l_classes = sorted(list(self.label_lists[0]))\n",
    "#                 encoder_classes.append(l_classes)\n",
    "                \n",
    "#         self.label_lists = encoder_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructor/ Class Method calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just want to get the dataframe from the csv path, set ```return_df=True```. You still have the input validation precheck functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = TextDataMain.from_csv(DATA_PATH/'sample_large.csv',\n",
    "                            return_df=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```Input Validation Precheck``` will check for missing values and duplicate rows in the csv file. Since there's no such thing in our sample dataset, we won't see anything here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Source.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you are happy with this dataframe (after you did some others preprocessing), then you can start creating a `TextDataMain` object\n",
    "\n",
    "For this dataframe, I want to \n",
    "- Build a text classification model, with main text in ```Content``` column, metadatas is ```Source```, and the label is ```L1```\n",
    "- Perform `apply_word_tokenize` with text normalization (this is \"text transformation\")\n",
    "- For augmentation, I want to perform: Oversampling the ```Owned, Non Owned and HC Search``` from column ```Source```, then add some the Vietnamese no-accent text. Note that all of these are called \"text augmentation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define these transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For Text Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awt_tfm = partial(apply_word_tokenize,normalize_text=True)\n",
    "# You can also set a __name__ to your augmentation function. \n",
    "# This way you will have meaningful text messages as outputs\n",
    "awt_tfm.__name__='UTS Word Tokenization With Normalization'\n",
    "\n",
    "txt_tfms=[awt_tfm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For Text Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply_to_all means I will apply this augmentation to all the data \n",
    "# (including the original data and the augmented data/transformed data from previous augmentation/transformation)\n",
    "over_nonown_tfm = partial(sampling_with_condition,query='Source==\"non owned\"',frac=0.5,seed=42,apply_to_all=False)\n",
    "over_nonown_tfm.__name__ = 'Oversampling Non Owned'\n",
    "\n",
    "over_own_tfm = partial(sampling_with_condition,query='Source==\"owned\"',frac=2,seed=42,apply_to_all=False)\n",
    "over_own_tfm.__name__ = 'Oversampling Owned'\n",
    "\n",
    "over_hc_tfm = partial(sampling_with_condition,query='Source==\"hc search\"',frac=2.5,seed=42,apply_to_all=False)\n",
    "over_hc_tfm.__name__ = 'Oversampling HC search'\n",
    "\n",
    "remove_accent_tfm = partial(remove_vnmese_accent,frac=1,seed=42,apply_to_all=True)\n",
    "remove_accent_tfm.__name__ = 'Add No-Accent Text'\n",
    "\n",
    "aug_tfms = [over_nonown_tfm,over_own_tfm,over_hc_tfm,remove_accent_tfm]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm = TextDataMain(df,\n",
    "                    main_content='Content',\n",
    "                    metadatas='Source', # You can put a list of multiple metadatas\n",
    "                    label_names='L1', # You can put a list of multiple labels\n",
    "                    val_ratio=0.2,\n",
    "                    split_cols='L1', # You can even put a list of multiple columns to be used for validation splitting\n",
    "                    content_tfms = txt_tfms, # You can add multiple content transformation functions ...\n",
    "                    aug_tfms = aug_tfms, # ... as well as augmentation functions\n",
    "                    process_metadatas=True,\n",
    "                    seed=42,\n",
    "                    shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to directly create a ```TextDataMain``` object from our csv file, we can instead use this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm = TextDataMain.from_csv(DATA_PATH/'sample_large.csv',\n",
    "                            return_df=False,\n",
    "                            main_content='Content',\n",
    "                            metadatas='Source',\n",
    "                            label_names='L1',\n",
    "                            val_ratio=0.2,\n",
    "                            split_cols='L1',\n",
    "                            content_tfms = txt_tfms,\n",
    "                            aug_tfms = aug_tfms,\n",
    "                            process_metadatas=True,\n",
    "                            seed=42,\n",
    "                            shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TextDataMain.to_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all the previous constructor calls do not do any heavy processing yet.\n",
    "\n",
    "To actually run all the processes, one can call `TextDataMain.to_df()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = tdm.to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice this?\n",
    "```\n",
    "Previous Validation Percentage: 20.0%\n",
    "- Before leak check\n",
    "Size: 14\n",
    "- After leak check\n",
    "Size: 14\n",
    "- Number of rows leaked: 0, or 0.00% of the original validation (or test) data\n",
    "Current Validation Percentage: 20.0%\n",
    "```\n",
    "After performing train/test split, the ```TextDataMain``` object also perform a \"leak check\": After `text_transformation` is performed, it will compare the text from ```Content``` value in the validation set to the ```Content``` text in the train set. Any duplications (texts that belong to both set) will be removed from validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, since we have metadatas, the metadatas is concatenated to the front of the texture content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.Content.sample(5).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a new dataframe with only the necessary columns (the processed text column, metadatas, label, and ```is_valid``` which tells you which row belongs to the validation set). Notice that our class has also encode our label for us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our TextDataMain object also stores other useful attributes, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entire processed dataframe, similar to the df_processed above\n",
    "tdm.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class names (This will be a list of list, as this class can handle multi-label classification)\n",
    "tdm.label_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary storing unique value for each provided metadata\n",
    "tdm.metadata_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to see how a HuggingFace's tokenizer work on our processed text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will pick a random text from train set to show\n",
    "tdm.tokenizer_explain_single(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing this, we can see how the tokenizer interact with our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TextDataMain.to_datasetdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need to convert our data to HuggingFace's DatasetDict format in order to utilize HuggingFace's model well, we can directly export datasetdict using `TextDataMain.to_datasetdict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict_sample = tdm.to_datasetdict(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict_sample['train']['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ddict_sample['train']['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that PhoBert will auto-pad our sentence to its model max_sequence_length, which is 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ddict_sample['train']['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict_sample['train']['label'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TextDataMain.save_as_pickles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the transformations/augmentations can take time for large dataset, we want to save our TextDataMain object. We can use `TextDataMain.save_as_pickles` to export a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm.save_as_pickles('my_tdm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can load it with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm2 = TextDataMain.from_pickle('my_tdm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and access all the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm2.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm2.label_lists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm2.metadata_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_stats = os.stat(Path('pickle_files/my_tdm.pkl'))\n",
    "print(f'File Size in MegaBytes is {file_stats.st_size / (1024 * 1024)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it saves the entire processed dataframe (and datasetdict if you call ```to_datasetdict```), the pickle size can be large. In some scenario you don't need to store these data attributes (as inference time, or in production). Thus one can save a lighter pickle file by setting ```drop_data_attributes``` to ```True```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm.save_as_pickles('my_lightweight_tdm',drop_data_attributes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_stats = os.stat(Path('pickle_files/my_lightweight_tdm.pkl'))\n",
    "print(f'File Size in MegaBytes is {file_stats.st_size / (1024 * 1024)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see a bigger file size reduction when we work with much larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm_light = TextDataMain.from_pickle('my_lightweight_tdm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can still access some important attributes (except for any data attributes, such as ```df``` or ```main_ddict```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm_light.label_lists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm_light.metadata_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
