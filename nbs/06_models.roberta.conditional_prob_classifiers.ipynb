{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Probability Classifiers\n",
    "\n",
    "> This module contains code to build a conditional probability classifier, which is inspired by this paper: [https://arxiv.org/pdf/1911.06475.pdf](https://arxiv.org/pdf/1911.06475.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.roberta.conditional_prob_classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers.models.roberta.configuration_roberta import RobertaConfig\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "from that_nlp_library.models.roberta.classifiers import ConcatHeadSimple\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
    "from that_nlp_library.model_main import loss_for_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def build_standard_condition_mask(df_labels,\n",
    "                                  label1,label2):\n",
    "    L1_SIZE = df_labels[label1].nunique()\n",
    "    L2_SIZE = df_labels[label2].nunique()\n",
    "\n",
    "    \n",
    "    df_labels = df_labels.drop_duplicates().sort_values([label1,label2])\n",
    "    _d = df_labels.groupby([label1])[label2].apply(list).to_dict()\n",
    "    \n",
    "    mask_l1 = torch.eye(L1_SIZE) ==1\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    mlb.fit([np.arange(L2_SIZE)])\n",
    "    mask_l2 = torch.tensor(mlb.transform(list(_d.values())) == 1)\n",
    "    \n",
    "    mask_final= torch.cat((mask_l1,mask_l2),1)\n",
    "    \n",
    "    return mask_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col_1  col_2\n",
       "0      0      0\n",
       "1      0      1\n",
       "2      0      2\n",
       "3      1      3\n",
       "4      1      4\n",
       "5      2      5\n",
       "6      2      6\n",
       "7      2      7"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df_labels=pd.DataFrame({\n",
    "    'col_1':[0,0,0,1,1,2,2,2],\n",
    "    'col_2':[0,1,2,3,4,5,6,7]\n",
    "})\n",
    "_df_labels\n",
    "\n",
    "# 0 -> (0,1,2), 1 -> (3,4), 2-> (5,6,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True, False, False,  True,  True,  True, False, False, False, False,\n",
      "         False],\n",
      "        [False,  True, False, False, False, False,  True,  True, False, False,\n",
      "         False],\n",
      "        [False, False,  True, False, False, False, False, False,  True,  True,\n",
      "          True]])\n"
     ]
    }
   ],
   "source": [
    "print(build_standard_condition_mask(_df_labels,'col_1','col_2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RobertaHSCCProbSequenceClassification(RobertaPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Roberta Conditional Probability Architecture with Hidden-State-Concatenation for Sequence Classification task\n",
    "    \"\"\"\n",
    "    config_class = RobertaConfig\n",
    "\n",
    "    def __init__(self, \n",
    "                 config, # HuggingFace model configuration\n",
    "                 size_l1=None, # Number of classes for head 1\n",
    "                 size_l2=None, # Number of classes for head 2\n",
    "                 standard_mask=None, # Mask for conditional probability\n",
    "                 layer2concat=4, # number of hidden layer to concatenate (counting from top)\n",
    "                 device=None, # CPU or GPU\n",
    "                 head_class=None, # The class object of the head. You can use RobertaClassificationHeadCustom as default\n",
    "                 **head_class_kwargs, # Keyword arguments for the head class\n",
    "                ):\n",
    "        super().__init__(config)\n",
    "        self.training_device = device if device is not None else torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.size_l1 = size_l1\n",
    "        self.size_l2 = size_l2\n",
    "        self.layer2concat=layer2concat\n",
    "        self.head_class_sizes=[size_l1,size_l2] # will be useful for metric calculation later\n",
    "        \n",
    "        # set num_labels for config\n",
    "        num_labels = size_l1+size_l2\n",
    "        config.num_labels = num_labels\n",
    "        \n",
    "        self.body_model = RobertaModel(config, add_pooling_layer=False)\n",
    "        self.standard_mask = standard_mask.to(self.training_device)\n",
    "        self.classification_head = head_class(config=config,\n",
    "                                              **head_class_kwargs) \n",
    "\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n",
    "                labels=None, **kwargs):\n",
    "        # Use model body to get encoder representations\n",
    "        # the only ones we need for now are input_ids and attention_mask\n",
    "        outputs = self.body_model(input_ids, attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids, **kwargs)\n",
    "        \n",
    "        hidden_states = outputs['hidden_states'] # tuples with len 13 (number of layer/block)\n",
    "        # each with shape: (bs,seq_len,hidden_size_len), e.g. for phobert: (bs,256, 768)\n",
    "        # Note: hidden_size_len = embedding_size\n",
    "        \n",
    "        hidden_concat = torch.cat([hidden_states[i][:,0] for i in range(-1,-self.layer2concat-1,-1)],\n",
    "                                  -1) # (bs,768*4)\n",
    "        \n",
    "        # classification head\n",
    "        logits = self.classification_head(hidden_concat)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # labels shape: (bs,2), first is L1, second is L2\n",
    "            labels_l1 = labels[:,0].view(-1) #(bs,)\n",
    "            labels_l2 = labels[:,1].view(-1) #(bs,)\n",
    "            l1_1hot = torch.nn.functional.one_hot(labels_l1, num_classes=self.size_l1)\n",
    "            l2_1hot = torch.nn.functional.one_hot(labels_l2, num_classes=self.size_l2)\n",
    "            label_concat_1hot = torch.cat((l1_1hot,l2_1hot),1) # (bs,L1+L2)\n",
    "\n",
    "            # the original approach: positives and other children of same parents\n",
    "            _mask = self.standard_mask[labels_l1]\n",
    "            loss_func = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "            loss = loss_func(logits,label_concat_1hot.float())\n",
    "            loss = torch.mul(loss,_mask)\n",
    "            loss = (loss.sum(axis=1)/_mask.sum(axis=1)).mean()\n",
    "            \n",
    "        # Return model output object\n",
    "        return SequenceClassifierOutput(loss=loss, logits=logits,\n",
    "                                     hidden_states=None,\n",
    "                                     attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/models/roberta/conditional_prob_classifiers.py#L39){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RobertaHSCCProbSequenceClassification\n",
       "\n",
       ">      RobertaHSCCProbSequenceClassification (config, size_l1=None,\n",
       ">                                             size_l2=None, standard_mask=None,\n",
       ">                                             layer2concat=4, device=None,\n",
       ">                                             head_class=None,\n",
       ">                                             **head_class_kwargs)\n",
       "\n",
       "Roberta Conditional Probability Architecture with Hidden-State-Concatenation for Sequence Classification task\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| config |  |  | HuggingFace model configuration |\n",
       "| size_l1 | NoneType | None | Number of classes for head 1 |\n",
       "| size_l2 | NoneType | None | Number of classes for head 2 |\n",
       "| standard_mask | NoneType | None | Mask for conditional probability |\n",
       "| layer2concat | int | 4 | number of hidden layer to concatenate (counting from top) |\n",
       "| device | NoneType | None | CPU or GPU |\n",
       "| head_class | NoneType | None | The class object of the head. You can use RobertaClassificationHeadCustom as default |\n",
       "| head_class_kwargs |  |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/models/roberta/conditional_prob_classifiers.py#L39){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RobertaHSCCProbSequenceClassification\n",
       "\n",
       ">      RobertaHSCCProbSequenceClassification (config, size_l1=None,\n",
       ">                                             size_l2=None, standard_mask=None,\n",
       ">                                             layer2concat=4, device=None,\n",
       ">                                             head_class=None,\n",
       ">                                             **head_class_kwargs)\n",
       "\n",
       "Roberta Conditional Probability Architecture with Hidden-State-Concatenation for Sequence Classification task\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| config |  |  | HuggingFace model configuration |\n",
       "| size_l1 | NoneType | None | Number of classes for head 1 |\n",
       "| size_l2 | NoneType | None | Number of classes for head 2 |\n",
       "| standard_mask | NoneType | None | Mask for conditional probability |\n",
       "| layer2concat | int | 4 | number of hidden layer to concatenate (counting from top) |\n",
       "| device | NoneType | None | CPU or GPU |\n",
       "| head_class | NoneType | None | The class object of the head. You can use RobertaClassificationHeadCustom as default |\n",
       "| head_class_kwargs |  |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RobertaHSCCProbSequenceClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
