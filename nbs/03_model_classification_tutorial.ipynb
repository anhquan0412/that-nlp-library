{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Controller Tutorial: Classification\n",
    "\n",
    "> This notebook contains an end-to-end process of preprocess + tokenizing your text, and build a classification model based on Roberta architecture\n",
    "\n",
    "- skip_showdoc: true\n",
    "- skip_exec: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#This will specify a (or a list) of GPUs for training\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from that_nlp_library.text_transformation import *\n",
    "from that_nlp_library.text_augmentation import *\n",
    "from that_nlp_library.text_main import *\n",
    "from that_nlp_library.utils import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import text_normalize\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from importlib.machinery import SourceFileLoader\n",
    "import os\n",
    "from transformers import DataCollatorWithPadding,RobertaTokenizer\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel,RobertaForSequenceClassification\n",
    "import torch\n",
    "import nlpaug.augmenter.char as nac\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the custom augmentation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_aug_stochastic(x,aug=None,p=0.5):\n",
    "    if not isinstance(x,list): \n",
    "        if random.random()<p: return aug.augment(x)[0]\n",
    "        return x\n",
    "    news=[]\n",
    "    originals=[]\n",
    "    for _x in x:\n",
    "        if random.random()<p: news.append(_x)\n",
    "        else: originals.append(_x)\n",
    "    # only perform augmentation when needed\n",
    "    if len(news): news = aug.augment(news)\n",
    "    return news+originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = nac.KeyboardAug(aug_char_max=3,aug_char_p=0.1,aug_word_p=0.07)\n",
    "nearby_aug_func = partial(nlp_aug_stochastic,aug=aug,p=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a TextDataController object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reuse the data and the preprocessings in [this tutorial](https://anhquan0412.github.io/that-nlp-library/text_main.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    22641.000000\n",
       "mean        60.196679\n",
       "std         28.534612\n",
       "min          2.000000\n",
       "25%         36.000000\n",
       "50%         59.000000\n",
       "75%         88.000000\n",
       "max        115.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(list(map(lambda x: len(x.split()),[text for text in dset['Review Text'] if text is not None]))).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         content_augmentations= [nearby_aug_func,str.lower], \n",
    "                         # add \"str.lower\" here because nearby_aug might return uppercase character\n",
    "                         val_ratio=0.2,\n",
    "                         batch_size=1000,\n",
    "                         seed=42,\n",
    "                         num_proc=20,\n",
    "                         verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our tokenizer for Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process and tokenize our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "----- Do <lambda> on Department Name -----\n",
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "Done\n",
      "----- Label Encoding -----\n",
      "Done\n",
      "-------------------- Text Transformation --------------------\n",
      "----- text_normalize -----\n",
      "----- lower -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 0, which is 0.00% of training set\n",
      "-------------------- Text Augmentation --------------------\n",
      "----- nlp_aug_stochastic -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b766ca85cce443b1bcaa39d4c025e1eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- lower -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15294a5d29af47fea1736454fb90104e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94935de422cc40acaa250ac045468171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=20):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b63c3578a64e22ab98fee55ce354d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75752d562a7b41b0afc9e2e5d94d0215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc.process_and_tokenize(_tokenizer,max_length=100,shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ce7c909f614bd9ae377b7da727250a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3551444a8b2447abdb5e5180474d185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "_ = tdc.do_tokenization(_tokenizer,max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Title', 'Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 18102\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Title', 'Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4526\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see one example of how those content transformations and augmentations affect our input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is not what I expected 🤬. I gulped when I put this in my bag during retailer days because the price was still too much ... but thought this has to be wonderful to charge so much,right??\n"
     ]
    }
   ],
   "source": [
    "sample_txt = 'This is not what I expected 🤬. I gulped when I put this in my bag during retailer days because the price was still too much ... but thought this has to be wonderful to charge so much,right??'\n",
    "print(sample_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t------- Text Transformation Explained -------\n",
      "----- Raw sentence -----\n",
      "This is not what I expected 🤬. I gulped when I put this in my bag during retailer days because the price was still too much ... but thought this has to be wonderful to charge so much,right??\n",
      "\n",
      "----- Content Transformations (on both train and test) -----\n",
      "--- text_normalize ---\n",
      "This is not what I expected 🤬 . I gulped when I put this in my bag during retailer days because the price was still too much ... but thought this has to be wonderful to charge so much , right ? ?\n",
      "\n",
      "--- lower ---\n",
      "this is not what i expected 🤬 . i gulped when i put this in my bag during retailer days because the price was still too much ... but thought this has to be wonderful to charge so much , right ? ?\n",
      "\n",
      "\n",
      "----- Augmentations (on train only) -----\n",
      "--- nlp_aug_stochastic ---\n",
      "tMis is not what i expected 🤬. i gulped when i put this in my bag during rrtailer Cays because the price was still too much. .. but thought this has to be wonderful to Vharge so much, right??\n",
      "\n",
      "--- lower ---\n",
      "tmis is not what i expected 🤬. i gulped when i put this in my bag during rrtailer cays because the price was still too much. .. but thought this has to be wonderful to vharge so much, right??\n",
      "\n",
      "\n",
      "\t\t------- Tokenizer Explained -------\n",
      "----- Input -----\n",
      "tmis is not what i expected 🤬. i gulped when i put this in my bag during rrtailer cays because the price was still too much. .. but thought this has to be wonderful to vharge so much, right??\n",
      "\n",
      "----- Tokenized results ----- \n",
      "{'input_ids': [0, 26989, 354, 16, 45, 99, 939, 421, 8103, 10470, 11582, 4, 939, 42445, 9700, 77, 939, 342, 42, 11, 127, 3298, 148, 910, 338, 17624, 254, 740, 4113, 142, 5, 425, 21, 202, 350, 203, 4, 29942, 53, 802, 42, 34, 7, 28, 4613, 7, 748, 298, 16347, 98, 203, 6, 235, 28749, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "----- Results from tokenizer.convert_ids_to_tokens -----\n",
      "['<s>', 'tm', 'is', 'Ġis', 'Ġnot', 'Ġwhat', 'Ġi', 'Ġexpected', 'ĠðŁ', '¤', '¬', '.', 'Ġi', 'Ġgul', 'ped', 'Ġwhen', 'Ġi', 'Ġput', 'Ġthis', 'Ġin', 'Ġmy', 'Ġbag', 'Ġduring', 'Ġr', 'r', 'tail', 'er', 'Ġc', 'ays', 'Ġbecause', 'Ġthe', 'Ġprice', 'Ġwas', 'Ġstill', 'Ġtoo', 'Ġmuch', '.', 'Ġ..', 'Ġbut', 'Ġthought', 'Ġthis', 'Ġhas', 'Ġto', 'Ġbe', 'Ġwonderful', 'Ġto', 'Ġv', 'h', 'arge', 'Ġso', 'Ġmuch', ',', 'Ġright', '??', '</s>']\n",
      "\n",
      "----- Results from tokenizer.decode ----- \n",
      "<s>tmis is not what i expected 🤬. i gulped when i put this in my bag during rrtailer cays because the price was still too much... but thought this has to be wonderful to vharge so much, right??</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "two_steps_tokenization_explain(sample_txt,_tokenizer,\n",
    "                               content_tfms=[text_normalize,str.lower],\n",
    "                               aug_tfms=[partial(nlp_aug_stochastic,aug=aug,p=1),str.lower]\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Experiment: Roberta Vanilla Single-Head Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "from that_nlp_library.models.roberta.classifiers import *\n",
    "from that_nlp_library.model_main import *\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Roberta model using the Model Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.label_lists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(tdc.label_lists[0])\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='roberta-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "_model = RobertaForSequenceClassification.from_pretrained(model_name,num_labels=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model = _model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can define the metrics to used, and the Model Controller object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_funcs = [partial(f1_score,average='macro'),accuracy_score] \n",
    "# we will use both f1_macro and accuracy score as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller = ModelController(_model,\n",
    "                             data_store=tdc,\n",
    "                             metric_funcs=metric_funcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can start training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='566' max='566' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [566/566 02:21, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Score Department name</th>\n",
       "      <th>Accuracy Score Department name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.279182</td>\n",
       "      <td>0.749650</td>\n",
       "      <td>0.918471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.395900</td>\n",
       "      <td>0.259449</td>\n",
       "      <td>0.752880</td>\n",
       "      <td>0.923553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "bs=32\n",
    "wd=0.01\n",
    "epochs= 2\n",
    "\n",
    "controller.fit(epochs,lr,\n",
    "               head_sizes=num_classes,\n",
    "               batch_size=bs,\n",
    "               weight_decay=wd,\n",
    "               save_checkpoint=False,\n",
    "               compute_metrics=compute_metrics_classification,\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging your training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can log your training using HuggingFace:\n",
    "\n",
    "- Supported platforms are \"azure_ml\", \"comet_ml\", \"mlflow\", \"neptune\", \"tensorboard\",\"clearml\" and \"wandb\"\n",
    "\n",
    "- References:\n",
    "\n",
    "    - https://huggingface.co/docs/transformers/v4.28.0/en/main_classes/trainer#transformers.TrainingArguments\n",
    "    \n",
    "    - https://docs.wandb.ai/guides/integrations/huggingface#:~:text=Logging%20your%20Hugging%20Face%20model,every%20save_steps%20in%20the%20TrainingArguments%20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "controller.fit(epochs,lr,\n",
    "               batch_size=bs,\n",
    "               weight_decay=wd,\n",
    "               save_checkpoint=False,\n",
    "               compute_metrics=compute_metrics_classification,\n",
    "               hf_report_to='wandb'\n",
    "              )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save your model weights at the end of your training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "controller.trainer.model.save_pretrained('./sample_weights/model_progress')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can save your weights at every epochs during your training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "controller.fit(epochs,lr,\n",
    "               batch_size=bs,\n",
    "               weight_decay=wd,\n",
    "               save_checkpoint=True,\n",
    "               o_dir='sample_weights',\n",
    "               compute_metrics=compute_metrics_classification,\n",
    "              )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model with tokenized DatasetDict (no TextDataController)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part assumes you already have your tokenized datasetdict, without the use of `TextDataController`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_ddict = copy.deepcopy(tdc.main_ddict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Title', 'Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 18102\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Title', 'Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4526\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that your DatasetDict must contain tokens besides raw text (which typically includes 'input_ids', 'token_type_ids', 'attention_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name='roberta-base'\n",
    "\n",
    "_model = RobertaForSequenceClassification.from_pretrained(model_name,num_labels=num_classes)\n",
    "\n",
    "_model = _model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_funcs = [partial(f1_score,average='macro'),accuracy_score] \n",
    "\n",
    "# note that you omit the `data_store` argument\n",
    "controller = ModelController(_model,metric_funcs=metric_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='849' max='849' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [849/849 02:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Score Department name</th>\n",
       "      <th>Accuracy Score Department name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.305675</td>\n",
       "      <td>0.745469</td>\n",
       "      <td>0.914715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.420700</td>\n",
       "      <td>0.285522</td>\n",
       "      <td>0.749540</td>\n",
       "      <td>0.918692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.420700</td>\n",
       "      <td>0.273176</td>\n",
       "      <td>0.750173</td>\n",
       "      <td>0.919355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "bs=32\n",
    "wd=0.01\n",
    "epochs= 3\n",
    "\n",
    "controller.fit(epochs,lr,\n",
    "               ddict=main_ddict, # Put in your tokenized datasetdict here\n",
    "               label_names='Department Name',\n",
    "               head_sizes=num_classes,\n",
    "               batch_size=bs,\n",
    "               weight_decay=wd,\n",
    "               save_checkpoint=False,\n",
    "               compute_metrics=compute_metrics_classification,\n",
    "               tokenizer=_tokenizer,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.trainer.model.save_pretrained('./sample_weights/model_progress')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using trained model, using TextDataController"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = RobertaForSequenceClassification.from_pretrained('./sample_weights/model_progress',num_labels=6).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_funcs = [partial(f1_score,average='macro'),accuracy_score]\n",
    "controller = ModelController(trained_model,tdc,metric_funcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Train/Validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make prediction on all validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start making predictions --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf2714b852e4e63be9dd806e634a058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b6e37ee50846c8a83202ed72687895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_val = controller.predict_ddict_classification(ds_type='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>label</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>pred_Department Name</th>\n",
       "      <th>pred_prob_Department Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>general petite . . such a fun jacket ! great t...</td>\n",
       "      <td>general petite</td>\n",
       "      <td>Intimate</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 15841, 4716, 1459, 479, 479, 215, 10, 1531...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>Jackets</td>\n",
       "      <td>0.813447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>simple and elegant</td>\n",
       "      <td>general petite . simple and elegant . i though...</td>\n",
       "      <td>general petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 15841, 4716, 1459, 479, 2007, 8, 14878, 47...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>Tops</td>\n",
       "      <td>0.994486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>retro and pretty</td>\n",
       "      <td>general . retro and pretty . this top has a bi...</td>\n",
       "      <td>general</td>\n",
       "      <td>Tops</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 15841, 479, 11299, 8, 1256, 479, 42, 299, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>Tops</td>\n",
       "      <td>0.994513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>summer/fall wear</td>\n",
       "      <td>general petite . summer / fall wear . i first ...</td>\n",
       "      <td>general petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 15841, 4716, 1459, 479, 1035, 1589, 1136, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>0.983874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>perfect except slip</td>\n",
       "      <td>general petite . perfect except slip . this is...</td>\n",
       "      <td>general petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 15841, 4716, 1459, 479, 1969, 4682, 9215, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>0.985444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Title                                        Review Text   \n",
       "0                       general petite . . such a fun jacket ! great t...  \\\n",
       "1   simple and elegant  general petite . simple and elegant . i though...   \n",
       "2     retro and pretty  general . retro and pretty . this top has a bi...   \n",
       "3     summer/fall wear  general petite . summer / fall wear . i first ...   \n",
       "4  perfect except slip  general petite . perfect except slip . this is...   \n",
       "\n",
       "    Division Name Department Name  label   \n",
       "0  general petite        Intimate      2  \\\n",
       "1  general petite            Tops      4   \n",
       "2         general            Tops      4   \n",
       "3  general petite         Dresses      1   \n",
       "4  general petite         Dresses      1   \n",
       "\n",
       "                                           input_ids   \n",
       "0  [0, 15841, 4716, 1459, 479, 479, 215, 10, 1531...  \\\n",
       "1  [0, 15841, 4716, 1459, 479, 2007, 8, 14878, 47...   \n",
       "2  [0, 15841, 479, 11299, 8, 1256, 479, 42, 299, ...   \n",
       "3  [0, 15841, 4716, 1459, 479, 1035, 1589, 1136, ...   \n",
       "4  [0, 15841, 4716, 1459, 479, 1969, 4682, 9215, ...   \n",
       "\n",
       "                                      attention_mask pred_Department Name   \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...              Jackets  \\\n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...                 Tops   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...                 Tops   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...              Dresses   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...              Dresses   \n",
       "\n",
       "   pred_prob_Department Name  \n",
       "0                   0.813447  \n",
       "1                   0.994486  \n",
       "2                   0.994513  \n",
       "3                   0.983874  \n",
       "4                   0.985444  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val = df_val.to_pandas()\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the label index to string, we can use the ```label_lists``` attribute of tdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try to get your metric to see if it matches your last traing epoch's above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7501726809060515"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(df_val['Department Name'],df_val['pred_Department Name'],average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also make predictions on all training set, by changing argument ```ds_type``` to \"train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go through details on how to make a prediction on a completely new and raw dataset using our trained model. For now, let's reuse the sample csv and pretend it's our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig').sample(frac=0.2,random_state=1)\n",
    "# drop NaN values in the label column\n",
    "df_test = df_test[~df_test['Department Name'].isna()].reset_index(drop=True)\n",
    "\n",
    "# save the label, as we will calculate some metrics later. We also filter out labels with NaN Review Text,\n",
    "# as there will be a filtering processing on the test set\n",
    "true_labels = df_test.loc[~df_test['Review Text'].isna(),'Department Name'].values \n",
    "\n",
    "# drop the label (you don't need to, but this is necessary to simulate an actual test set)\n",
    "df_test.drop('Department Name',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4692, 9)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>872</td>\n",
       "      <td>42</td>\n",
       "      <td>Perfect for work and play</td>\n",
       "      <td>This shirt works for both going out and going ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Knits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1033</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I don't know why i had the opposite problem mo...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Jeans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1037</td>\n",
       "      <td>45</td>\n",
       "      <td>Great pants</td>\n",
       "      <td>These cords are great--lightweight for fl wint...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Jeans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>829</td>\n",
       "      <td>35</td>\n",
       "      <td>Surprisingly comfy for a button down</td>\n",
       "      <td>I am a 10 m and got the 10. it fits perfectly ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Blouses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>872</td>\n",
       "      <td>29</td>\n",
       "      <td>Short and small</td>\n",
       "      <td>The shirt is mostly a thick sweatshirt materia...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Knits</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clothing ID  Age                                 Title   \n",
       "0          872   42             Perfect for work and play  \\\n",
       "1         1033   40                                   NaN   \n",
       "2         1037   45                           Great pants   \n",
       "3          829   35  Surprisingly comfy for a button down   \n",
       "4          872   29                       Short and small   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND   \n",
       "0  This shirt works for both going out and going ...       5                1  \\\n",
       "1  I don't know why i had the opposite problem mo...       4                1   \n",
       "2  These cords are great--lightweight for fl wint...       5                1   \n",
       "3  I am a 10 m and got the 10. it fits perfectly ...       5                1   \n",
       "4  The shirt is mostly a thick sweatshirt materia...       3                0   \n",
       "\n",
       "   Positive Feedback Count   Division Name Class Name  \n",
       "0                        0         General      Knits  \n",
       "1                        0  General Petite      Jeans  \n",
       "2                        1  General Petite      Jeans  \n",
       "3                        1  General Petite    Blouses  \n",
       "4                       15  General Petite      Knits  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, you have 2 options\n",
    "\n",
    "1. Use `TextDataController` to process your data, then `ModelController`'s job is to perform prediction\n",
    "2. Convert your dataframe to a HuggingFace Dataset, and let the `ModelController` take care of the preprocessing and the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Input Validation Precheck -\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title          758\n",
      "Review Text    164\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 2 rows\n",
      "-------------------- Start Test Set Transformation --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4a091f0cc946db865fd8ac5603218a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=20):   0%|          | 0/4692 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee3f957c1ab462b9a28758c62d41d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Text Transformation --------------------\n",
      "----- text_normalize -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6780e646a2b441ef836a4142b4b114fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- lower -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112ffc96559040fda274a11a174467c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Tokenization --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6481e118d343fdb1598857f579403d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "_test_dset_processed = tdc.prepare_test_dataset_from_df(df_test,validate=True,do_filtering=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Title', 'Review Text', 'Division Name', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 4528\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_test_dset_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start making predictions --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a2f0f69037471f89446044dfbab3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d2cafb2f0348b2bbadadc86efea7df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_test_dset_predicted = controller.predict_ddict_classification(_test_dset_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>pred_Department Name</th>\n",
       "      <th>pred_prob_Department Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>perfect for work and play</td>\n",
       "      <td>general . perfect for work and play . this shi...</td>\n",
       "      <td>general</td>\n",
       "      <td>[0, 15841, 479, 1969, 13, 173, 8, 310, 479, 42...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>Tops</td>\n",
       "      <td>0.994394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>general petite . . i don't know why i had the ...</td>\n",
       "      <td>general petite</td>\n",
       "      <td>[0, 15841, 4716, 1459, 479, 479, 939, 218, 75,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>0.984001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>great pants</td>\n",
       "      <td>general petite . great pants . thes e cords ar...</td>\n",
       "      <td>general petite</td>\n",
       "      <td>[0, 15841, 4716, 1459, 479, 372, 9304, 479, 5,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>0.989789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>surprisingly comfy for a button down</td>\n",
       "      <td>general petite . surprisingly comfy for a butt...</td>\n",
       "      <td>general petite</td>\n",
       "      <td>[0, 15841, 4716, 1459, 479, 10262, 3137, 24382...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>Tops</td>\n",
       "      <td>0.993930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>short and small</td>\n",
       "      <td>general petite . short and small . the shirt i...</td>\n",
       "      <td>general petite</td>\n",
       "      <td>[0, 15841, 4716, 1459, 479, 765, 8, 650, 479, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>Tops</td>\n",
       "      <td>0.993843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Title   \n",
       "0             perfect for work and play  \\\n",
       "1                                         \n",
       "2                           great pants   \n",
       "3  surprisingly comfy for a button down   \n",
       "4                       short and small   \n",
       "\n",
       "                                         Review Text   Division Name   \n",
       "0  general . perfect for work and play . this shi...         general  \\\n",
       "1  general petite . . i don't know why i had the ...  general petite   \n",
       "2  general petite . great pants . thes e cords ar...  general petite   \n",
       "3  general petite . surprisingly comfy for a butt...  general petite   \n",
       "4  general petite . short and small . the shirt i...  general petite   \n",
       "\n",
       "                                           input_ids   \n",
       "0  [0, 15841, 479, 1969, 13, 173, 8, 310, 479, 42...  \\\n",
       "1  [0, 15841, 4716, 1459, 479, 479, 939, 218, 75,...   \n",
       "2  [0, 15841, 4716, 1459, 479, 372, 9304, 479, 5,...   \n",
       "3  [0, 15841, 4716, 1459, 479, 10262, 3137, 24382...   \n",
       "4  [0, 15841, 4716, 1459, 479, 765, 8, 650, 479, ...   \n",
       "\n",
       "                                      attention_mask pred_Department Name   \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...                 Tops  \\\n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...              Bottoms   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...              Bottoms   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...                 Tops   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...                 Tops   \n",
       "\n",
       "   pred_prob_Department Name  \n",
       "0                   0.994394  \n",
       "1                   0.984001  \n",
       "2                   0.989789  \n",
       "3                   0.993930  \n",
       "4                   0.993843  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_predicted = _test_dset_predicted.to_pandas()\n",
    "df_test_predicted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc168e904424cbfb10df72b5c807827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=20):   0%|          | 0/4692 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84379204787b437ea3f589d16a816fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b9c459d9064bf3b97e2dcae583204e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83b29eab5284d7a9d00410a7e53c96d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7363e1c339f44ad099ecfa782263a2a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb635df0ce643d1a0e51af2d769384f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9268ef98b8f04ac7b07876df95f2d22f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_test_dset = Dataset.from_pandas(df_test)\n",
    "_test_dset_predicted = controller.predict_raw_dset(_test_dset,\n",
    "                                                   do_filtering=True, # since we have some text filtering in the processing\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Title', 'Review Text', 'Division Name', 'input_ids', 'attention_mask', 'pred_Department Name', 'pred_prob_Department Name'],\n",
       "    num_rows: 4528\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_test_dset_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_predicted = _test_dset_predicted.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>pred_Department Name</th>\n",
       "      <th>pred_prob_Department Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>perfect for work and play</td>\n",
       "      <td>general . perfect for work and play . this shi...</td>\n",
       "      <td>general</td>\n",
       "      <td>[0, 15841, 479, 1969, 13, 173, 8, 310, 479, 42...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>Tops</td>\n",
       "      <td>0.994394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>general petite . . i don't know why i had the ...</td>\n",
       "      <td>general petite</td>\n",
       "      <td>[0, 15841, 4716, 1459, 479, 479, 939, 218, 75,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>0.984001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>great pants</td>\n",
       "      <td>general petite . great pants . thes e cords ar...</td>\n",
       "      <td>general petite</td>\n",
       "      <td>[0, 15841, 4716, 1459, 479, 372, 9304, 479, 5,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>0.989789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>surprisingly comfy for a button down</td>\n",
       "      <td>general petite . surprisingly comfy for a butt...</td>\n",
       "      <td>general petite</td>\n",
       "      <td>[0, 15841, 4716, 1459, 479, 10262, 3137, 24382...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>Tops</td>\n",
       "      <td>0.993930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>short and small</td>\n",
       "      <td>general petite . short and small . the shirt i...</td>\n",
       "      <td>general petite</td>\n",
       "      <td>[0, 15841, 4716, 1459, 479, 765, 8, 650, 479, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>Tops</td>\n",
       "      <td>0.993843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Title   \n",
       "0             perfect for work and play  \\\n",
       "1                                         \n",
       "2                           great pants   \n",
       "3  surprisingly comfy for a button down   \n",
       "4                       short and small   \n",
       "\n",
       "                                         Review Text   Division Name   \n",
       "0  general . perfect for work and play . this shi...         general  \\\n",
       "1  general petite . . i don't know why i had the ...  general petite   \n",
       "2  general petite . great pants . thes e cords ar...  general petite   \n",
       "3  general petite . surprisingly comfy for a butt...  general petite   \n",
       "4  general petite . short and small . the shirt i...  general petite   \n",
       "\n",
       "                                           input_ids   \n",
       "0  [0, 15841, 479, 1969, 13, 173, 8, 310, 479, 42...  \\\n",
       "1  [0, 15841, 4716, 1459, 479, 479, 939, 218, 75,...   \n",
       "2  [0, 15841, 4716, 1459, 479, 372, 9304, 479, 5,...   \n",
       "3  [0, 15841, 4716, 1459, 479, 10262, 3137, 24382...   \n",
       "4  [0, 15841, 4716, 1459, 479, 765, 8, 650, 479, ...   \n",
       "\n",
       "                                      attention_mask pred_Department Name   \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...                 Tops  \\\n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...              Bottoms   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...              Bottoms   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...                 Tops   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...                 Tops   \n",
       "\n",
       "   pred_prob_Department Name  \n",
       "0                   0.994394  \n",
       "1                   0.984001  \n",
       "2                   0.989789  \n",
       "3                   0.993930  \n",
       "4                   0.993843  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_predicted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly check the f1 score to make sure everything works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7534947919324427"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(true_labels,df_test_predicted['pred_Department Name'],average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not too far off from the validation F1 score. Notice that the 'test set' is just a sample from the original dataset, not the entire new set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even predict top k results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b2edad5d904358aa3a0c6d5ebd0c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=20):   0%|          | 0/4692 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f58f48b1584e13921ffaa00584be51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3a1f18d7064c2ebd48c97c78de4216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284becfb563c44778928b6b40750465c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743e22938144492c8553f479907fae9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34d7665bd4c471285bec4ca890e459e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7911cadc7b44d6830150c8d888d630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m _test_dset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(df_test)\n\u001b[0;32m----> 2\u001b[0m _test_dset_predicted \u001b[38;5;241m=\u001b[39m \u001b[43mcontroller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_raw_dset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_test_dset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mdo_filtering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mtopk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/kwon/that-nlp-library/that_nlp_library/model_main.py:516\u001b[0m, in \u001b[0;36mModelController.predict_raw_dset\u001b[0;34m(self, dset, batch_size, do_filtering, is_regression, is_multilabel, multilabel_threshold, topk, is_dhc)\u001b[0m\n\u001b[1;32m    511\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_ddict_regression(test_ddict,\n\u001b[1;32m    512\u001b[0m                                                    ds_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    513\u001b[0m                                                    batch_size\u001b[38;5;241m=\u001b[39mbatch_size\n\u001b[1;32m    514\u001b[0m                                                   )\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 516\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_ddict_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_ddict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mds_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mis_multilabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_multilabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mmultilabel_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultilabel_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mtopk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mis_dhc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_dhc\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/kwon/that-nlp-library/that_nlp_library/model_main.py:616\u001b[0m, in \u001b[0;36mModelController.predict_ddict_classification\u001b[0;34m(self, ddict, ds_type, batch_size, is_multilabel, multilabel_threshold, topk, tokenizer, data_collator, label_names, class_names_predefined, device, is_dhc)\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[38;5;66;03m# this will create features: pred_classname and pred_prob_classname\u001b[39;00m\n\u001b[1;32m    601\u001b[0m         results \u001b[38;5;241m=\u001b[39m ddict\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    602\u001b[0m                             partial(_forward_pass_classification,model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    603\u001b[0m                                     topk\u001b[38;5;241m=\u001b[39mtopk,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    613\u001b[0m                             batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m    614\u001b[0m                             batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m--> 616\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[43m_convert_pred_id_to_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabel_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabel_lists\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtopk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mis_multilabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;43;03m#                                                    num_proc=1 # Cannot make it work with num_proc>1\u001b[39;49;00m\n\u001b[1;32m    620\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/kwon/that-nlp-library/that_nlp_library/model_main.py:387\u001b[0m, in \u001b[0;36m_convert_pred_id_to_label\u001b[0;34m(dset, label_names, label_lists, topk, is_multilabel, batch_size, num_proc)\u001b[0m\n\u001b[1;32m    381\u001b[0m     _func1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m xs: label_lists[i][\u001b[38;5;28mint\u001b[39m(xs)] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(xs,(\u001b[38;5;28mlist\u001b[39m,\u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m [label_lists[i][\u001b[38;5;28mint\u001b[39m(x)] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m xs]\n\u001b[1;32m    382\u001b[0m     _func2 \u001b[38;5;241m=\u001b[39m partial(lambda_map_batch,\n\u001b[1;32m    383\u001b[0m                      feature\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel_names[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    384\u001b[0m                     func\u001b[38;5;241m=\u001b[39m_func1,\n\u001b[1;32m    385\u001b[0m                     is_batched\u001b[38;5;241m=\u001b[39mis_batched\n\u001b[1;32m    386\u001b[0m                    )\n\u001b[0;32m--> 387\u001b[0m     dset \u001b[38;5;241m=\u001b[39m \u001b[43mhf_map_dset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m_func2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mis_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_batched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m                       \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "File \u001b[0;32m~/kwon/that-nlp-library/that_nlp_library/utils.py:201\u001b[0m, in \u001b[0;36mhf_map_dset\u001b[0;34m(dset, func, is_batched, batch_size, num_proc)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_streamed:\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dset\u001b[38;5;241m.\u001b[39mmap(func,\n\u001b[1;32m    198\u001b[0m                     batched\u001b[38;5;241m=\u001b[39mis_batched,\n\u001b[1;32m    199\u001b[0m                     batch_size\u001b[38;5;241m=\u001b[39mbatch_size\n\u001b[1;32m    200\u001b[0m                    )\n\u001b[0;32m--> 201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_batched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m               \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/datasets/arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/datasets/arrow_dataset.py:3097\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3090\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3091\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[1;32m   3092\u001b[0m         disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3093\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3094\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3095\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3096\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3097\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3098\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3099\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/datasets/arrow_dataset.py:3474\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3470\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3471\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3472\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3473\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3474\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3478\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3479\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3480\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3481\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3483\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/datasets/arrow_dataset.py:3353\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3352\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3353\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3355\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3356\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3357\u001b[0m     }\n",
      "File \u001b[0;32m~/kwon/that-nlp-library/that_nlp_library/utils.py:158\u001b[0m, in \u001b[0;36mlambda_map_batch\u001b[0;34m(inp, feature, func, is_batched, output_feature, is_func_batched)\u001b[0m\n\u001b[1;32m    156\u001b[0m results\u001b[38;5;241m=\u001b[39m{}\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_func_batched:\n\u001b[0;32m--> 158\u001b[0m     results[output_feature] \u001b[38;5;241m=\u001b[39m \u001b[43mlambda_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43mis_batched\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     results[output_feature] \u001b[38;5;241m=\u001b[39m func(inp[feature]) \u001b[38;5;28;01mif\u001b[39;00m is_batched \u001b[38;5;28;01melse\u001b[39;00m func([inp[feature]])\n",
      "File \u001b[0;32m~/kwon/that-nlp-library/that_nlp_library/utils.py:142\u001b[0m, in \u001b[0;36mlambda_batch\u001b[0;34m(inp, feature, func, is_batched)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlambda_batch\u001b[39m(inp, \u001b[38;5;66;03m# HuggingFace Dataset\u001b[39;00m\n\u001b[1;32m    138\u001b[0m                      feature, \u001b[38;5;66;03m# Feature name.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m                      func, \u001b[38;5;66;03m# The function to apply\u001b[39;00m\n\u001b[1;32m    140\u001b[0m                      is_batched, \u001b[38;5;66;03m# Whether batching is applied\u001b[39;00m\n\u001b[1;32m    141\u001b[0m                     ):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [func(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inp[feature]] \u001b[38;5;28;01mif\u001b[39;00m is_batched \u001b[38;5;28;01melse\u001b[39;00m func(inp[feature])\n",
      "File \u001b[0;32m~/kwon/that-nlp-library/that_nlp_library/utils.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlambda_batch\u001b[39m(inp, \u001b[38;5;66;03m# HuggingFace Dataset\u001b[39;00m\n\u001b[1;32m    138\u001b[0m                      feature, \u001b[38;5;66;03m# Feature name.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m                      func, \u001b[38;5;66;03m# The function to apply\u001b[39;00m\n\u001b[1;32m    140\u001b[0m                      is_batched, \u001b[38;5;66;03m# Whether batching is applied\u001b[39;00m\n\u001b[1;32m    141\u001b[0m                     ):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inp[feature]] \u001b[38;5;28;01mif\u001b[39;00m is_batched \u001b[38;5;28;01melse\u001b[39;00m func(inp[feature])\n",
      "File \u001b[0;32m~/kwon/that-nlp-library/that_nlp_library/model_main.py:381\u001b[0m, in \u001b[0;36m_convert_pred_id_to_label.<locals>.<lambda>\u001b[0;34m(xs)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dset\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(label_names)):\n\u001b[0;32m--> 381\u001b[0m     _func1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m xs: label_lists[i][\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(xs,(\u001b[38;5;28mlist\u001b[39m,\u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m [label_lists[i][\u001b[38;5;28mint\u001b[39m(x)] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m xs]\n\u001b[1;32m    382\u001b[0m     _func2 \u001b[38;5;241m=\u001b[39m partial(lambda_map_batch,\n\u001b[1;32m    383\u001b[0m                      feature\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel_names[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    384\u001b[0m                     func\u001b[38;5;241m=\u001b[39m_func1,\n\u001b[1;32m    385\u001b[0m                     is_batched\u001b[38;5;241m=\u001b[39mis_batched\n\u001b[1;32m    386\u001b[0m                    )\n\u001b[1;32m    387\u001b[0m     dset \u001b[38;5;241m=\u001b[39m hf_map_dset(dset,_func2,\n\u001b[1;32m    388\u001b[0m                        is_batched\u001b[38;5;241m=\u001b[39mis_batched,\n\u001b[1;32m    389\u001b[0m                        batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    390\u001b[0m                        num_proc\u001b[38;5;241m=\u001b[39mnum_proc\n\u001b[1;32m    391\u001b[0m                        )\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "_test_dset = Dataset.from_pandas(df_test)\n",
    "_test_dset_predicted = controller.predict_raw_dset(_test_dset,\n",
    "                                                   do_filtering=True,\n",
    "                                                   topk=3\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/quan/kwon/that-nlp-library/that_nlp_library/model_main.py\u001b[0m(381)\u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    379 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    380 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 381 \u001b[0;31m        \u001b[0m_func1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel_lists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel_lists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    382 \u001b[0;31m        _func2 = partial(lambda_map_batch,\n",
      "\u001b[0m\u001b[0;32m    383 \u001b[0;31m                         \u001b[0mfeature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'pred_{label_names[i]}'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> dset['pred_Department Name'][:3]\n",
      "*** NameError: name 'dset' is not defined\n",
      "ipdb> dset\n",
      "*** NameError: name 'dset' is not defined\n",
      "ipdb> list\n",
      "\u001b[1;32m    376 \u001b[0m                           \u001b[0mnum_proc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_proc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    377 \u001b[0m                          )\n",
      "\u001b[1;32m    378 \u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    379 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    380 \u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 381 \u001b[0;31m        \u001b[0m_func1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel_lists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel_lists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    382 \u001b[0m        _func2 = partial(lambda_map_batch,\n",
      "\u001b[1;32m    383 \u001b[0m                         \u001b[0mfeature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'pred_{label_names[i]}'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    384 \u001b[0m                        \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_func1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    385 \u001b[0m                        \u001b[0mis_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_batched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    386 \u001b[0m                       )\n",
      "\n",
      "ipdb> list\n",
      "\u001b[1;32m    387 \u001b[0m        dset = hf_map_dset(dset,_func2,\n",
      "\u001b[1;32m    388 \u001b[0m                           \u001b[0mis_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_batched\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    389 \u001b[0m                           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    390 \u001b[0m                           \u001b[0mnum_proc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_proc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    391 \u001b[0m                           )\n",
      "\u001b[1;32m    392 \u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    393 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    394 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    395 \u001b[0m\u001b[0;31m# %% ../nbs/03_model_main.ipynb 17\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    396 \u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0mModelController\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    397 \u001b[0m    def __init__(self,\n",
      "\n",
      "ipdb> dset\n",
      "*** NameError: name 'dset' is not defined\n",
      "ipdb> label_names\n",
      "*** NameError: name 'label_names' is not defined\n",
      "ipdb> quit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start making predictions --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2269 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Source</th>\n",
       "      <th>pred_L1</th>\n",
       "      <th>pred_prob_L1</th>\n",
       "      <th>pred_L1_top1</th>\n",
       "      <th>pred_L1_top2</th>\n",
       "      <th>pred_L1_top3</th>\n",
       "      <th>pred_prob_L1_top1</th>\n",
       "      <th>pred_prob_L1_top2</th>\n",
       "      <th>pred_prob_L1_top3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google play - App ncc lúc nào cx lag đơ , phần...</td>\n",
       "      <td>google play</td>\n",
       "      <td>[3, 5, 9]</td>\n",
       "      <td>[0.87822074, 0.023822138, 0.02159522]</td>\n",
       "      <td>Feature</td>\n",
       "      <td>Others</td>\n",
       "      <td>Shopee account</td>\n",
       "      <td>0.878221</td>\n",
       "      <td>0.023822</td>\n",
       "      <td>0.021595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non owned - .. ❗ ️ GÓC THANH LÝ Tính ra rẻ hơn...</td>\n",
       "      <td>non owned</td>\n",
       "      <td>[5, 1, 0]</td>\n",
       "      <td>[0.9309808, 0.015578598, 0.009805982]</td>\n",
       "      <td>Others</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>Buyer complained seller</td>\n",
       "      <td>0.930981</td>\n",
       "      <td>0.015579</td>\n",
       "      <td>0.009806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>google play - Mắc gì người ta đặt hàng toàn lỗ...</td>\n",
       "      <td>google play</td>\n",
       "      <td>[3, 5, 9]</td>\n",
       "      <td>[0.8493735, 0.050054528, 0.021759989]</td>\n",
       "      <td>Feature</td>\n",
       "      <td>Others</td>\n",
       "      <td>Shopee account</td>\n",
       "      <td>0.849374</td>\n",
       "      <td>0.050055</td>\n",
       "      <td>0.021760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>owned - # GhienShopeePayawardT8 Khi bạn chơi s...</td>\n",
       "      <td>owned</td>\n",
       "      <td>[1, 6, 7]</td>\n",
       "      <td>[0.9155516, 0.01255093, 0.010521941]</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>Payment</td>\n",
       "      <td>Return/Refund</td>\n",
       "      <td>0.915552</td>\n",
       "      <td>0.012551</td>\n",
       "      <td>0.010522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google play - Rất bức xúc khi dùng . mã giảm g...</td>\n",
       "      <td>google play</td>\n",
       "      <td>[9, 3, 1]</td>\n",
       "      <td>[0.57147133, 0.25687057, 0.03061041]</td>\n",
       "      <td>Shopee account</td>\n",
       "      <td>Feature</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>0.571471</td>\n",
       "      <td>0.256871</td>\n",
       "      <td>0.030610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       Source    pred_L1   \n",
       "0  google play - App ncc lúc nào cx lag đơ , phần...  google play  [3, 5, 9]  \\\n",
       "1  non owned - .. ❗ ️ GÓC THANH LÝ Tính ra rẻ hơn...    non owned  [5, 1, 0]   \n",
       "2  google play - Mắc gì người ta đặt hàng toàn lỗ...  google play  [3, 5, 9]   \n",
       "3  owned - # GhienShopeePayawardT8 Khi bạn chơi s...        owned  [1, 6, 7]   \n",
       "4  google play - Rất bức xúc khi dùng . mã giảm g...  google play  [9, 3, 1]   \n",
       "\n",
       "                            pred_prob_L1    pred_L1_top1 pred_L1_top2   \n",
       "0  [0.87822074, 0.023822138, 0.02159522]         Feature       Others  \\\n",
       "1  [0.9309808, 0.015578598, 0.009805982]          Others   Commercial   \n",
       "2  [0.8493735, 0.050054528, 0.021759989]         Feature       Others   \n",
       "3   [0.9155516, 0.01255093, 0.010521941]      Commercial      Payment   \n",
       "4   [0.57147133, 0.25687057, 0.03061041]  Shopee account      Feature   \n",
       "\n",
       "              pred_L1_top3  pred_prob_L1_top1  pred_prob_L1_top2   \n",
       "0           Shopee account           0.878221           0.023822  \\\n",
       "1  Buyer complained seller           0.930981           0.015579   \n",
       "2           Shopee account           0.849374           0.050055   \n",
       "3            Return/Refund           0.915552           0.012551   \n",
       "4               Commercial           0.571471           0.256871   \n",
       "\n",
       "   pred_prob_L1_top3  \n",
       "0           0.021595  \n",
       "1           0.009806  \n",
       "2           0.021760  \n",
       "3           0.010522  \n",
       "4           0.030610  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result = controller.predict_ddict(ddict=test_ddict,ds_type='test',topk=3)\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just want to make a prediction on a small amount of data (single sentence, or a few sentences), we can use `ModelController.predict_raw_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have some metadatas, we need to define a dictionary (to imitate a DatasetDict)\n",
    "raw_content={\n",
    "    'Source': 'Google play',\n",
    "    'Content':'Tôi không thích Shopee.Tại vì dùng app rất chậm,lag banh nhà lầu, thậm chí log in còn không đc'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't use metadata, we can use something like this: \n",
    "\n",
    "```raw_content='Tôi không thích Shopee.Tại vì dùng app rất chậm,lag banh nhà lầu, thậm chí log in còn không đc'```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 4718.00it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Source</th>\n",
       "      <th>pred_L1</th>\n",
       "      <th>pred_prob_L1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google play - Tôi không thích Shopee . Tại vì ...</td>\n",
       "      <td>google play</td>\n",
       "      <td>Feature</td>\n",
       "      <td>0.876843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       Source  pred_L1   \n",
       "0  google play - Tôi không thích Shopee . Tại vì ...  google play  Feature  \\\n",
       "\n",
       "   pred_prob_L1  \n",
       "0      0.876843  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result = controller.predict_raw_text(raw_content,topk=1)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using trained model, using tokenized DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "model_name='nguyenvulebinh/envibert'\n",
    "_model_kwargs={\n",
    "    # overall model hyperparams\n",
    "    'is_multilabel':False, # False\n",
    "    'is_multihead':False, # False\n",
    "    'head_class_sizes':num_classes,\n",
    "    'head_class': SimpleClassificationHead,\n",
    "    # classfication head hyperparams\n",
    "    'classifier_dropout':0.1 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sample_weights/model_progress were not used when initializing RobertaBaseForSequenceClassification: ['body_model.pooler.dense.weight', 'body_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaBaseForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaBaseForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "trained_model = model_init_classification(model_class = RobertaBaseForSequenceClassification,\n",
    "                                          cpoint_path = Path('./sample_weights/model_progress'), \n",
    "                                          output_hidden_states=False,\n",
    "                                          seed=42,\n",
    "                                          model_kwargs = _model_kwargs)\n",
    "\n",
    "\n",
    "metric_funcs = [partial(f1_score,average='macro'),accuracy_score]\n",
    "controller = ModelController(trained_model,metric_funcs) # notice that we don't use tdm here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 4780\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask', 'pred_L1', 'pred_prob_L1'],\n",
       "        num_rows: 447\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_label_name = 'L1'\n",
    "my_class_predefined = ['Buyer complained seller',\n",
    " 'Commercial',\n",
    " 'Delivery',\n",
    " 'Feature',\n",
    " 'Order/Item',\n",
    " 'Others',\n",
    " 'Payment',\n",
    " 'Return/Refund',\n",
    " 'Services',\n",
    " 'Shopee account']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start making predictions --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_val = controller.predict_ddict(main_ddict,\n",
    "                                  ds_type='validation',\n",
    "                                  is_multilabel=False,\n",
    "                                  tokenizer=tokenizer,\n",
    "                                  label_names = my_label_name,\n",
    "                                  class_names_predefined=my_class_predefined\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>Source</th>\n",
       "      <th>pred_L1</th>\n",
       "      <th>pred_prob_L1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google play - Chơi gam rất là lác</td>\n",
       "      <td>3</td>\n",
       "      <td>google play</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>0.837496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google play - Zq</td>\n",
       "      <td>5</td>\n",
       "      <td>google play</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.927602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non owned - Làn sóng kỹ thuật số và sự lựa chọ...</td>\n",
       "      <td>5</td>\n",
       "      <td>non owned</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.918241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google play - Hàng quốc tế không còn ship COD ...</td>\n",
       "      <td>6</td>\n",
       "      <td>google play</td>\n",
       "      <td>Delivery</td>\n",
       "      <td>0.804539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google play - Quá tệ . Giao hàng chậm như rùa ...</td>\n",
       "      <td>2</td>\n",
       "      <td>google play</td>\n",
       "      <td>Delivery</td>\n",
       "      <td>0.758327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label       Source   \n",
       "0                  google play - Chơi gam rất là lác      3  google play  \\\n",
       "1                                   google play - Zq      5  google play   \n",
       "2  non owned - Làn sóng kỹ thuật số và sự lựa chọ...      5    non owned   \n",
       "3  google play - Hàng quốc tế không còn ship COD ...      6  google play   \n",
       "4  google play - Quá tệ . Giao hàng chậm như rùa ...      2  google play   \n",
       "\n",
       "      pred_L1  pred_prob_L1  \n",
       "0  Commercial      0.837496  \n",
       "1      Others      0.927602  \n",
       "2      Others      0.918241  \n",
       "3    Delivery      0.804539  \n",
       "4    Delivery      0.758327  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text', 'Source', 'input_ids', 'token_type_ids', 'attention_mask', 'pred_L1', 'pred_prob_L1'],\n",
       "        num_rows: 2269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be cumbersome to preprocess your test data the same way you preprocess your validation set, without the use of tdm (which stores the preprocess pipeline). In short, you need to produce the test datasetdict `test_ddict` containing processed `'input_ids', 'token_type_ids', 'attention_mask'`, then call\n",
    "\n",
    "```python\n",
    "df_results = controller.predict_ddict(ddict=test_ddict,\n",
    "                                      ds_type='test',\n",
    "                                      is_multilabel=False,\n",
    "                                      tokenizer=tokenizer,\n",
    "                                      label_names = my_label_name,\n",
    "                                      class_names_predefined=my_class_predefined     \n",
    "                                     )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Experiment: Roberta Custom Single-Head Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "from that_nlp_library.models.roberta.classifiers import *\n",
    "from that_nlp_library.model_main import *\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will specify a (or a list) of GPUs for training\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train EnviBert using TDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.label_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(tdc.label_lists[0])\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our model and model controller. First, we will initialize the pretrained `body` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.roberta.modeling_roberta import RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='nguyenvulebinh/envibert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nguyenvulebinh/envibert were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "envibert_body = RobertaModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can define a simple class as the head for our classification task, something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassificationHead(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 config, # HuggingFace model configuration\n",
    "                 classifier_dropout=0.1, # Dropout ratio (for dropout layer right before the last nn.Linear)\n",
    "                 num_labels=None, # Number of label output. Every classification class must have this exact variable\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(classifier_dropout)\n",
    "        self.out_proj = torch.nn.Linear(config.hidden_size, num_labels)\n",
    "    def forward(self, inp, **kwargs):\n",
    "        x = inp\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading body weights. This assumes the body is the very first first-layer block of your custom architecture\n"
     ]
    }
   ],
   "source": [
    "_model_kwargs={\n",
    "    # overall model hyperparams\n",
    "    'is_multilabel':tdm.is_multilabel, # False\n",
    "    'is_multihead':tdm.is_multihead, # False\n",
    "    'head_class_sizes':num_classes,\n",
    "    'head_class': SimpleClassificationHead,\n",
    "    # classfication head hyperparams\n",
    "    'classifier_dropout':0.1 \n",
    "}\n",
    "\n",
    "model = model_init_classification(model_class = RobertaBaseForSequenceClassification,\n",
    "                                  cpoint_path = 'nguyenvulebinh/envibert', \n",
    "                                  output_hidden_states=False, # since we are not using 'hidden layer contatenation' technique\n",
    "                                  seed=42,\n",
    "                                  body_model=envibert_body,\n",
    "                                  model_kwargs = _model_kwargs)\n",
    "\n",
    "metric_funcs = [partial(f1_score,average='macro'),accuracy_score] # we will use both f1_macro and accuracy score as metrics\n",
    "controller = ModelController(model,tdm,metric_funcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can start training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1194' max='1194' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1194/1194 01:04, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Score L1</th>\n",
       "      <th>Accuracy Score L1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.059704</td>\n",
       "      <td>0.350748</td>\n",
       "      <td>0.677852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.007712</td>\n",
       "      <td>0.462641</td>\n",
       "      <td>0.697987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 8e-5\n",
    "bs=4\n",
    "wd=0.01\n",
    "epochs= 2\n",
    "\n",
    "controller.fit(epochs,lr,\n",
    "               batch_size=bs,\n",
    "               weight_decay=wd,\n",
    "               save_checkpoint=False,\n",
    "               compute_metrics=compute_metrics_classification,\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging your training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can log your training using HuggingFace:\n",
    "\n",
    "- Supported platforms are \"azure_ml\", \"comet_ml\", \"mlflow\", \"neptune\", \"tensorboard\",\"clearml\" and \"wandb\"\n",
    "\n",
    "- References:\n",
    "\n",
    "    - https://huggingface.co/docs/transformers/v4.28.0/en/main_classes/trainer#transformers.TrainingArguments\n",
    "    \n",
    "    - https://docs.wandb.ai/guides/integrations/huggingface#:~:text=Logging%20your%20Hugging%20Face%20model,every%20save_steps%20in%20the%20TrainingArguments%20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "controller.fit(epochs,lr,\n",
    "               batch_size=bs,\n",
    "               weight_decay=wd,\n",
    "               save_checkpoint=False,\n",
    "               compute_metrics=compute_metrics_classification,\n",
    "               hf_report_to='wandb'\n",
    "              )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save your model weights at the end of your training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "controller.trainer.model.save_pretrained('./sample_weights/model_progress')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can save your weights at every epochs during your training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "controller.fit(epochs,lr,\n",
    "               batch_size=bs,\n",
    "               weight_decay=wd,\n",
    "               save_checkpoint=True,\n",
    "               o_dir='sample_weights',\n",
    "               compute_metrics=compute_metrics_classification,\n",
    "              )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train EnviBert with tokenized DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part assumes you already have your tokenized datasetdict. You must have your tokenizer as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaTokenizer(name_or_path='', vocab_size=59993, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that your DatasetDict must contain tokens besides raw text (which typically includes 'input_ids', 'token_type_ids', 'attention_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 4780\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 447\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='nguyenvulebinh/envibert'\n",
    "_model_kwargs={\n",
    "    # overall model hyperparams\n",
    "    'is_multilabel':False, # False\n",
    "    'is_multihead':False, # False\n",
    "    'head_class_sizes':num_classes,\n",
    "    'head_class': SimpleClassificationHead,\n",
    "    # classfication head hyperparams\n",
    "    'classifier_dropout':0.1 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envibert_body = RobertaModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nguyenvulebinh/envibert were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading body weights. This assumes the body is the very first first-layer block of your custom architecture\n"
     ]
    }
   ],
   "source": [
    "model = model_init_classification(model_class = RobertaBaseForSequenceClassification,\n",
    "                                  cpoint_path = 'nguyenvulebinh/envibert', \n",
    "                                  output_hidden_states=False, # since we are not using 'hidden layer contatenation' technique\n",
    "                                  seed=42,\n",
    "                                  body_model=envibert_body,\n",
    "                                  model_kwargs = _model_kwargs)\n",
    "\n",
    "metric_funcs = [partial(f1_score,average='macro'),accuracy_score] # we will use both f1_macro and accuracy score as metrics\n",
    "controller = ModelController(model,\n",
    "                             metric_funcs=metric_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1194' max='1194' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1194/1194 01:06, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Score L1</th>\n",
       "      <th>Accuracy Score L1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.059704</td>\n",
       "      <td>0.350748</td>\n",
       "      <td>0.677852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.007712</td>\n",
       "      <td>0.462641</td>\n",
       "      <td>0.697987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 8e-5\n",
    "bs=4\n",
    "wd=0.01\n",
    "epochs= 2\n",
    "\n",
    "controller.fit(epochs,lr,\n",
    "               ddict=main_ddict, # Put in your tokenized datasetdict here\n",
    "               batch_size=bs,\n",
    "               weight_decay=wd,\n",
    "               save_checkpoint=False,\n",
    "               compute_metrics=compute_metrics_classification,\n",
    "               tokenizer=tokenizer,\n",
    "               label_names='L1'\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.trainer.model.save_pretrained('./sample_weights/model_progress')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using trained model, using TDM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_multilabel': False,\n",
       " 'is_multihead': False,\n",
       " 'head_class_sizes': 10,\n",
       " 'head_class': __main__.SimpleClassificationHead,\n",
       " 'classifier_dropout': 0.1}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_model_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sample_weights/model_progress were not used when initializing RobertaBaseForSequenceClassification: ['body_model.pooler.dense.weight', 'body_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaBaseForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaBaseForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "trained_model = model_init_classification(model_class = RobertaBaseForSequenceClassification,\n",
    "                                          cpoint_path = Path('./sample_weights/model_progress'), \n",
    "                                          output_hidden_states=False,\n",
    "                                          seed=42,\n",
    "                                          model_kwargs = _model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaBaseForSequenceClassification(\n",
       "  (body_model): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(59993, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classification_head): SimpleClassificationHead(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_funcs = [partial(f1_score,average='macro'),accuracy_score] # we will use both f1_macro and accuracy score as metrics\n",
    "controller = ModelController(trained_model,tdm,metric_funcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Train/Validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make prediction on all validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start making predictions --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_val = controller.predict_ddict(ds_type='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>Source</th>\n",
       "      <th>pred_L1</th>\n",
       "      <th>pred_prob_L1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google play - Chơi gam rất là lác</td>\n",
       "      <td>3</td>\n",
       "      <td>google play</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>0.837496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google play - Zq</td>\n",
       "      <td>5</td>\n",
       "      <td>google play</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.927602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non owned - Làn sóng kỹ thuật số và sự lựa chọ...</td>\n",
       "      <td>5</td>\n",
       "      <td>non owned</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.918241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google play - Hàng quốc tế không còn ship COD ...</td>\n",
       "      <td>6</td>\n",
       "      <td>google play</td>\n",
       "      <td>Delivery</td>\n",
       "      <td>0.804539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google play - Quá tệ . Giao hàng chậm như rùa ...</td>\n",
       "      <td>2</td>\n",
       "      <td>google play</td>\n",
       "      <td>Delivery</td>\n",
       "      <td>0.758327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label       Source   \n",
       "0                  google play - Chơi gam rất là lác      3  google play  \\\n",
       "1                                   google play - Zq      5  google play   \n",
       "2  non owned - Làn sóng kỹ thuật số và sự lựa chọ...      5    non owned   \n",
       "3  google play - Hàng quốc tế không còn ship COD ...      6  google play   \n",
       "4  google play - Quá tệ . Giao hàng chậm như rùa ...      2  google play   \n",
       "\n",
       "      pred_L1  pred_prob_L1  \n",
       "0  Commercial      0.837496  \n",
       "1      Others      0.927602  \n",
       "2      Others      0.918241  \n",
       "3    Delivery      0.804539  \n",
       "4    Delivery      0.758327  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the label index to string, we can use the ```label_lists``` attribute of tdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>Source</th>\n",
       "      <th>pred_L1</th>\n",
       "      <th>pred_prob_L1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google play - Chơi gam rất là lác</td>\n",
       "      <td>Feature</td>\n",
       "      <td>google play</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>0.837496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google play - Zq</td>\n",
       "      <td>Others</td>\n",
       "      <td>google play</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.927602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non owned - Làn sóng kỹ thuật số và sự lựa chọ...</td>\n",
       "      <td>Others</td>\n",
       "      <td>non owned</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.918241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google play - Hàng quốc tế không còn ship COD ...</td>\n",
       "      <td>Payment</td>\n",
       "      <td>google play</td>\n",
       "      <td>Delivery</td>\n",
       "      <td>0.804539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google play - Quá tệ . Giao hàng chậm như rùa ...</td>\n",
       "      <td>Delivery</td>\n",
       "      <td>google play</td>\n",
       "      <td>Delivery</td>\n",
       "      <td>0.758327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label       Source   \n",
       "0                  google play - Chơi gam rất là lác   Feature  google play  \\\n",
       "1                                   google play - Zq    Others  google play   \n",
       "2  non owned - Làn sóng kỹ thuật số và sự lựa chọ...    Others    non owned   \n",
       "3  google play - Hàng quốc tế không còn ship COD ...   Payment  google play   \n",
       "4  google play - Quá tệ . Giao hàng chậm như rùa ...  Delivery  google play   \n",
       "\n",
       "      pred_L1  pred_prob_L1  \n",
       "0  Commercial      0.837496  \n",
       "1      Others      0.927602  \n",
       "2      Others      0.918241  \n",
       "3    Delivery      0.804539  \n",
       "4    Delivery      0.758327  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val['label']= df_val['label'].apply(lambda x: tdm.label_lists[0][x]).values\n",
    "\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try to get your metric to see if it matches your last traing epoch's above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4634417008698494"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(df_val.label,df_val.pred_L1,average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also make predictions on all training set, by changing argument ```ds_type``` to \"train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go through details on how to make a prediction on a completely new and raw dataset using our trained model. For now, let's reuse the sample csv and pretend it's our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Input Validation Precheck -----\n",
      "DataFrame contains duplicated values!\n",
      "-----> Number of duplications: 16 rows\n"
     ]
    }
   ],
   "source": [
    "df_test = TextDataMain.from_csv(Path('sample_data')/'sample_large.csv',return_df=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove all the labels and unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label = df_test['L1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop(['L1','L2'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google Play</td>\n",
       "      <td>App ncc lúc nào cx lag đơ, phần tìm kiếm thì v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Non Owned</td>\n",
       "      <td>..❗️ GÓC THANH LÝ Tính ra rẻ hơn cả mua #Shope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Google Play</td>\n",
       "      <td>Mắc gì người ta đặt hàng toàn lỗi 😃????</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Owned</td>\n",
       "      <td>#GhienShopeePayawardT8 Khi bạn chơi shopee quá...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Google Play</td>\n",
       "      <td>Rất bức xúc khi dùng . mã giảm giá người dùng ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Source                                            Content\n",
       "0  Google Play  App ncc lúc nào cx lag đơ, phần tìm kiếm thì v...\n",
       "1    Non Owned  ..❗️ GÓC THANH LÝ Tính ra rẻ hơn cả mua #Shope...\n",
       "2  Google Play            Mắc gì người ta đặt hàng toàn lỗi 😃????\n",
       "3        Owned  #GhienShopeePayawardT8 Khi bạn chơi shopee quá...\n",
       "4  Google Play  Rất bức xúc khi dùng . mã giảm giá người dùng ..."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a DatasetDict for this test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Getting Test Set --------------------\n",
      "----- Input Validation Precheck -----\n",
      "DataFrame contains duplicated values!\n",
      "-----> Number of duplications: 19 rows\n",
      "-------------------- Start Test Set Transformation --------------------\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "-------------------- Text Transformation --------------------\n",
      "----- text_normalize -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2269/2269 [00:00<00:00, 3954.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Test Leak Checking --------------------\n",
      "- Before leak check\n",
      "Size: 2269\n",
      "- After leak check\n",
      "Size: 0\n",
      "- Number of rows leaked: 2269, or 100.00% of the original validation (or test) data\n",
      "-------------------- Construct DatasetDict --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2269 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_ddict = tdm.get_test_datasetdict_from_df(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the ***Leak Check*** we did in TextDataMain? Our ```df_test``` only has 70 rows, and it also shows that 70 rows of our data is leaked (100%), which is correct because this test dataset is actually a small sample of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test data has been processed + transformed (but not augmented) the same way as the validation set. Now we can start making the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start making predictions --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2269 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "controller = ModelController(trained_model,tdm)\n",
    "df_result = controller.predict_ddict(ddict=test_ddict,ds_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Source</th>\n",
       "      <th>pred_L1</th>\n",
       "      <th>pred_prob_L1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google play - App ncc lúc nào cx lag đơ , phần...</td>\n",
       "      <td>google play</td>\n",
       "      <td>Feature</td>\n",
       "      <td>0.878221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non owned - .. ❗ ️ GÓC THANH LÝ Tính ra rẻ hơn...</td>\n",
       "      <td>non owned</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.930981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>google play - Mắc gì người ta đặt hàng toàn lỗ...</td>\n",
       "      <td>google play</td>\n",
       "      <td>Feature</td>\n",
       "      <td>0.849374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>owned - # GhienShopeePayawardT8 Khi bạn chơi s...</td>\n",
       "      <td>owned</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>0.915552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google play - Rất bức xúc khi dùng . mã giảm g...</td>\n",
       "      <td>google play</td>\n",
       "      <td>Shopee account</td>\n",
       "      <td>0.571471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       Source   \n",
       "0  google play - App ncc lúc nào cx lag đơ , phần...  google play  \\\n",
       "1  non owned - .. ❗ ️ GÓC THANH LÝ Tính ra rẻ hơn...    non owned   \n",
       "2  google play - Mắc gì người ta đặt hàng toàn lỗ...  google play   \n",
       "3  owned - # GhienShopeePayawardT8 Khi bạn chơi s...        owned   \n",
       "4  google play - Rất bức xúc khi dùng . mã giảm g...  google play   \n",
       "\n",
       "          pred_L1  pred_prob_L1  \n",
       "0         Feature      0.878221  \n",
       "1          Others      0.930981  \n",
       "2         Feature      0.849374  \n",
       "3      Commercial      0.915552  \n",
       "4  Shopee account      0.571471  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly check the f1 score to make sure everything works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5303012712104336"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(true_label,df_result.pred_L1,average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are getting the predictions on the entire training+validation set, the F1 score is expected to be slightly higher than validation's F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even predict top k results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start making predictions --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2269 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Source</th>\n",
       "      <th>pred_L1</th>\n",
       "      <th>pred_prob_L1</th>\n",
       "      <th>pred_L1_top1</th>\n",
       "      <th>pred_L1_top2</th>\n",
       "      <th>pred_L1_top3</th>\n",
       "      <th>pred_prob_L1_top1</th>\n",
       "      <th>pred_prob_L1_top2</th>\n",
       "      <th>pred_prob_L1_top3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google play - App ncc lúc nào cx lag đơ , phần...</td>\n",
       "      <td>google play</td>\n",
       "      <td>[3, 5, 9]</td>\n",
       "      <td>[0.87822074, 0.023822138, 0.02159522]</td>\n",
       "      <td>Feature</td>\n",
       "      <td>Others</td>\n",
       "      <td>Shopee account</td>\n",
       "      <td>0.878221</td>\n",
       "      <td>0.023822</td>\n",
       "      <td>0.021595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non owned - .. ❗ ️ GÓC THANH LÝ Tính ra rẻ hơn...</td>\n",
       "      <td>non owned</td>\n",
       "      <td>[5, 1, 0]</td>\n",
       "      <td>[0.9309808, 0.015578598, 0.009805982]</td>\n",
       "      <td>Others</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>Buyer complained seller</td>\n",
       "      <td>0.930981</td>\n",
       "      <td>0.015579</td>\n",
       "      <td>0.009806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>google play - Mắc gì người ta đặt hàng toàn lỗ...</td>\n",
       "      <td>google play</td>\n",
       "      <td>[3, 5, 9]</td>\n",
       "      <td>[0.8493735, 0.050054528, 0.021759989]</td>\n",
       "      <td>Feature</td>\n",
       "      <td>Others</td>\n",
       "      <td>Shopee account</td>\n",
       "      <td>0.849374</td>\n",
       "      <td>0.050055</td>\n",
       "      <td>0.021760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>owned - # GhienShopeePayawardT8 Khi bạn chơi s...</td>\n",
       "      <td>owned</td>\n",
       "      <td>[1, 6, 7]</td>\n",
       "      <td>[0.9155516, 0.01255093, 0.010521941]</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>Payment</td>\n",
       "      <td>Return/Refund</td>\n",
       "      <td>0.915552</td>\n",
       "      <td>0.012551</td>\n",
       "      <td>0.010522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google play - Rất bức xúc khi dùng . mã giảm g...</td>\n",
       "      <td>google play</td>\n",
       "      <td>[9, 3, 1]</td>\n",
       "      <td>[0.57147133, 0.25687057, 0.03061041]</td>\n",
       "      <td>Shopee account</td>\n",
       "      <td>Feature</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>0.571471</td>\n",
       "      <td>0.256871</td>\n",
       "      <td>0.030610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       Source    pred_L1   \n",
       "0  google play - App ncc lúc nào cx lag đơ , phần...  google play  [3, 5, 9]  \\\n",
       "1  non owned - .. ❗ ️ GÓC THANH LÝ Tính ra rẻ hơn...    non owned  [5, 1, 0]   \n",
       "2  google play - Mắc gì người ta đặt hàng toàn lỗ...  google play  [3, 5, 9]   \n",
       "3  owned - # GhienShopeePayawardT8 Khi bạn chơi s...        owned  [1, 6, 7]   \n",
       "4  google play - Rất bức xúc khi dùng . mã giảm g...  google play  [9, 3, 1]   \n",
       "\n",
       "                            pred_prob_L1    pred_L1_top1 pred_L1_top2   \n",
       "0  [0.87822074, 0.023822138, 0.02159522]         Feature       Others  \\\n",
       "1  [0.9309808, 0.015578598, 0.009805982]          Others   Commercial   \n",
       "2  [0.8493735, 0.050054528, 0.021759989]         Feature       Others   \n",
       "3   [0.9155516, 0.01255093, 0.010521941]      Commercial      Payment   \n",
       "4   [0.57147133, 0.25687057, 0.03061041]  Shopee account      Feature   \n",
       "\n",
       "              pred_L1_top3  pred_prob_L1_top1  pred_prob_L1_top2   \n",
       "0           Shopee account           0.878221           0.023822  \\\n",
       "1  Buyer complained seller           0.930981           0.015579   \n",
       "2           Shopee account           0.849374           0.050055   \n",
       "3            Return/Refund           0.915552           0.012551   \n",
       "4               Commercial           0.571471           0.256871   \n",
       "\n",
       "   pred_prob_L1_top3  \n",
       "0           0.021595  \n",
       "1           0.009806  \n",
       "2           0.021760  \n",
       "3           0.010522  \n",
       "4           0.030610  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result = controller.predict_ddict(ddict=test_ddict,ds_type='test',topk=3)\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just want to make a prediction on a small amount of data (single sentence, or a few sentences), we can use `ModelController.predict_raw_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have some metadatas, we need to define a dictionary (to imitate a DatasetDict)\n",
    "raw_content={\n",
    "    'Source': 'Google play',\n",
    "    'Content':'Tôi không thích Shopee.Tại vì dùng app rất chậm,lag banh nhà lầu, thậm chí log in còn không đc'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't use metadata, we can use something like this: \n",
    "\n",
    "```raw_content='Tôi không thích Shopee.Tại vì dùng app rất chậm,lag banh nhà lầu, thậm chí log in còn không đc'```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 4718.00it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Source</th>\n",
       "      <th>pred_L1</th>\n",
       "      <th>pred_prob_L1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google play - Tôi không thích Shopee . Tại vì ...</td>\n",
       "      <td>google play</td>\n",
       "      <td>Feature</td>\n",
       "      <td>0.876843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       Source  pred_L1   \n",
       "0  google play - Tôi không thích Shopee . Tại vì ...  google play  Feature  \\\n",
       "\n",
       "   pred_prob_L1  \n",
       "0      0.876843  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result = controller.predict_raw_text(raw_content,topk=1)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using trained model, using tokenized DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "model_name='nguyenvulebinh/envibert'\n",
    "_model_kwargs={\n",
    "    # overall model hyperparams\n",
    "    'is_multilabel':False, # False\n",
    "    'is_multihead':False, # False\n",
    "    'head_class_sizes':num_classes,\n",
    "    'head_class': SimpleClassificationHead,\n",
    "    # classfication head hyperparams\n",
    "    'classifier_dropout':0.1 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sample_weights/model_progress were not used when initializing RobertaBaseForSequenceClassification: ['body_model.pooler.dense.weight', 'body_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaBaseForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaBaseForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "trained_model = model_init_classification(model_class = RobertaBaseForSequenceClassification,\n",
    "                                          cpoint_path = Path('./sample_weights/model_progress'), \n",
    "                                          output_hidden_states=False,\n",
    "                                          seed=42,\n",
    "                                          model_kwargs = _model_kwargs)\n",
    "\n",
    "\n",
    "metric_funcs = [partial(f1_score,average='macro'),accuracy_score]\n",
    "controller = ModelController(trained_model,metric_funcs) # notice that we don't use tdm here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 4780\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask', 'pred_L1', 'pred_prob_L1'],\n",
       "        num_rows: 447\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_label_name = 'L1'\n",
    "my_class_predefined = ['Buyer complained seller',\n",
    " 'Commercial',\n",
    " 'Delivery',\n",
    " 'Feature',\n",
    " 'Order/Item',\n",
    " 'Others',\n",
    " 'Payment',\n",
    " 'Return/Refund',\n",
    " 'Services',\n",
    " 'Shopee account']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start making predictions --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_val = controller.predict_ddict(main_ddict,\n",
    "                                  ds_type='validation',\n",
    "                                  is_multilabel=False,\n",
    "                                  tokenizer=tokenizer,\n",
    "                                  label_names = my_label_name,\n",
    "                                  class_names_predefined=my_class_predefined\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>Source</th>\n",
       "      <th>pred_L1</th>\n",
       "      <th>pred_prob_L1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google play - Chơi gam rất là lác</td>\n",
       "      <td>3</td>\n",
       "      <td>google play</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>0.837496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google play - Zq</td>\n",
       "      <td>5</td>\n",
       "      <td>google play</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.927602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non owned - Làn sóng kỹ thuật số và sự lựa chọ...</td>\n",
       "      <td>5</td>\n",
       "      <td>non owned</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.918241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google play - Hàng quốc tế không còn ship COD ...</td>\n",
       "      <td>6</td>\n",
       "      <td>google play</td>\n",
       "      <td>Delivery</td>\n",
       "      <td>0.804539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google play - Quá tệ . Giao hàng chậm như rùa ...</td>\n",
       "      <td>2</td>\n",
       "      <td>google play</td>\n",
       "      <td>Delivery</td>\n",
       "      <td>0.758327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label       Source   \n",
       "0                  google play - Chơi gam rất là lác      3  google play  \\\n",
       "1                                   google play - Zq      5  google play   \n",
       "2  non owned - Làn sóng kỹ thuật số và sự lựa chọ...      5    non owned   \n",
       "3  google play - Hàng quốc tế không còn ship COD ...      6  google play   \n",
       "4  google play - Quá tệ . Giao hàng chậm như rùa ...      2  google play   \n",
       "\n",
       "      pred_L1  pred_prob_L1  \n",
       "0  Commercial      0.837496  \n",
       "1      Others      0.927602  \n",
       "2      Others      0.918241  \n",
       "3    Delivery      0.804539  \n",
       "4    Delivery      0.758327  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text', 'Source', 'input_ids', 'token_type_ids', 'attention_mask', 'pred_L1', 'pred_prob_L1'],\n",
       "        num_rows: 2269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be cumbersome to preprocess your test data the same way you preprocess your validation set, without the use of tdm (which stores the preprocess pipeline). In short, you need to produce the test datasetdict `test_ddict` containing processed `'input_ids', 'token_type_ids', 'attention_mask'`, then call\n",
    "\n",
    "```python\n",
    "df_results = controller.predict_ddict(ddict=test_ddict,\n",
    "                                      ds_type='test',\n",
    "                                      is_multilabel=False,\n",
    "                                      tokenizer=tokenizer,\n",
    "                                      label_names = my_label_name,\n",
    "                                      class_names_predefined=my_class_predefined     \n",
    "                                     )\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
