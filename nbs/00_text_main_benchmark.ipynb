{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2114d60",
   "metadata": {},
   "source": [
    "# Text Processing Benchmark\n",
    "\n",
    "> This module contains some benchmarks for `TextDataController`\n",
    "\n",
    "- skip_showdoc: true\n",
    "- skip_exec: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a60d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda list | grep 'datasets\\|transformers\\|torch\\|accelerate'\n",
    "# accelerate                0.29.3                   pypi_0    pypi\n",
    "# datasets                  2.19.0                   pypi_0    pypi\n",
    "# torch                     2.3.0                    pypi_0    pypi\n",
    "# transformers              4.40.1                   pypi_0    pypi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bf920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from that_nlp_library.text_transformation import *\n",
    "from that_nlp_library.text_augmentation import *\n",
    "from that_nlp_library.text_main import *\n",
    "from that_nlp_library.text_main_streaming import *\n",
    "from datasets import load_dataset,enable_caching,disable_caching\n",
    "from transformers import RobertaTokenizer\n",
    "import os\n",
    "import time\n",
    "from underthesea import text_normalize\n",
    "import nlpaug.augmenter.char as nac\n",
    "from functools import partial\n",
    "import random\n",
    "from memory_profiler import memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a743f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_caching() # disable huggingface caching to get a fair benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38d0cd5",
   "metadata": {},
   "source": [
    "## 1. Benchmark on medium-size dataset (~117k rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64405267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117430"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "len(dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9257acdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9658a866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1174"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs=len(dset)//100\n",
    "bs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1a98e0",
   "metadata": {},
   "source": [
    "### a) Non-streaming dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ae56d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmarking(tdc,tokenizer,n=10,shuffle_trn=True,time_list=[]):\n",
    "    time1 = time.time()\n",
    "    tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=shuffle_trn)\n",
    "    time2 = time.time() \n",
    "    process_time = round(time2-time1,2)\n",
    "    print(f'Time it takes to process + tokenize training texts: {process_time} s')\n",
    "    for i,v in enumerate(tdc.main_ddict['train']):\n",
    "        if n is not None and i==tdc.batch_size*n: break\n",
    "    time3 = time.time()\n",
    "    \n",
    "    iteration_time = round(time3-time2,2)\n",
    "    if n is not None:\n",
    "        print(f'Time it takes to go through {n*tdc.batch_size} items: {iteration_time} s')\n",
    "    else:\n",
    "        print(f'Time it takes to go through all items: {iteration_time} s')\n",
    "    \n",
    "    total_time = round(time3-time1,2)\n",
    "    print(f'Total time: {total_time} s')\n",
    "    \n",
    "    time_list+=process_time,iteration_time,total_time\n",
    "    \n",
    "def benchmarking_and_memory_usage(tdc,tokenizer,n=10,shuffle_trn=True,time_list=[]):\n",
    "    mem_usage = memory_usage((benchmarking,[tdc,tokenizer,n,shuffle_trn,time_list]))\n",
    "    total_usage = round(max(mem_usage),1)\n",
    "    print(f'Maximum memory usage: {total_usage} MiB')\n",
    "    time_list.append(total_usage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763b4e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_aug_stochastic(x,aug=None,p=0.5):\n",
    "    results = aug.augment(x)\n",
    "    if not isinstance(x,list): return results[0] if random.random()<p else x\n",
    "    return [a if random.random()<p else b for a,b in zip(results,x)]\n",
    "\n",
    "aug = nac.KeyboardAug(aug_char_max=3,aug_char_p=0.1,aug_word_p=0.07)\n",
    "nearby_aug_func = partial(nlp_aug_stochastic,aug=aug,p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c169caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "502c512c",
   "metadata": {},
   "source": [
    "#### With filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10237f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "timelist1=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3cba22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 14.37 s\n",
      "Time it takes to go through 11740 items: 1.27 s\n",
      "Total time: 15.64 s\n",
      "Maximum memory usage: 734.9 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         val_ratio=None,\n",
    "                         batch_size=bs,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "benchmarking_and_memory_usage(tdc,tokenizer,time_list=timelist1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb71968",
   "metadata": {},
   "source": [
    "#### With filter + metadatas concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ebe861",
   "metadata": {},
   "outputs": [],
   "source": [
    "timelist2=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27292545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 15.26 s\n",
      "Time it takes to go through 11740 items: 1.46 s\n",
      "Total time: 16.72 s\n",
      "Maximum memory usage: 748.6 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         val_ratio=None,\n",
    "                         batch_size=bs,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "benchmarking_and_memory_usage(tdc,tokenizer,time_list=timelist2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae9448e",
   "metadata": {},
   "source": [
    "#### With filter + metadatas concatenation + content transformation + content augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153f90c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "timelist3=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f71b604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 35.09 s\n",
      "Time it takes to go through 11740 items: 1.52 s\n",
      "Total time: 36.61 s\n",
      "Maximum memory usage: 754.7 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         content_augmentations= [nearby_aug_func,str.lower], \n",
    "                         val_ratio=None,\n",
    "                         batch_size=bs,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "benchmarking_and_memory_usage(tdc,tokenizer,time_list=timelist3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da18bc1e",
   "metadata": {},
   "source": [
    "#### With filter + metadatas concatenation + content transformation + content augmentation + no shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79352a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "timelist4=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1bb308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 34.36 s\n",
      "Time it takes to go through 11740 items: 1.47 s\n",
      "Total time: 35.83 s\n",
      "Maximum memory usage: 777.3 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         content_augmentations= [nearby_aug_func,str.lower], \n",
    "                         val_ratio=None,\n",
    "                         batch_size=bs,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "benchmarking_and_memory_usage(tdc,tokenizer,shuffle_trn=False,time_list=timelist4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e47eaff",
   "metadata": {},
   "source": [
    "#### With filter + metadatas concatenation + content transformation + content augmentation + higher batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600c9f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 35.7 s\n",
      "Time it takes to go through 35220 items: 4.47 s\n",
      "Total time: 40.17 s\n",
      "Maximum memory usage: 761.9 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         content_augmentations= [nearby_aug_func,str.lower], \n",
    "                         val_ratio=None,\n",
    "                         batch_size=bs*3,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "benchmarking_and_memory_usage(tdc,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff12cde",
   "metadata": {},
   "source": [
    "#### With filter + metadatas concatenation + content transformation + content augmentation + higher num proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0df1589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 24.7 s\n",
      "Time it takes to go through 11740 items: 1.46 s\n",
      "Total time: 26.16 s\n",
      "Maximum memory usage: 754.2 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         content_augmentations= [nearby_aug_func,str.lower], \n",
    "                         val_ratio=None,\n",
    "                         batch_size=bs,\n",
    "                         seed=42,\n",
    "                         num_proc=8,\n",
    "                         verbose=False\n",
    "                        )\n",
    "benchmarking_and_memory_usage(tdc,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6854f69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a43fa1f",
   "metadata": {},
   "source": [
    "#### With filter + metadatas concatenation + content transformation + content augmentation + iterate the whole dataset (1 epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05e7ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "timelist5=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e0dd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 35.34 s\n",
      "Time it takes to go through all items: 14.32 s\n",
      "Total time: 49.66 s\n",
      "Maximum memory usage: 869.6 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         content_augmentations= [nearby_aug_func,str.lower], \n",
    "                         val_ratio=None,\n",
    "                         batch_size=bs,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "benchmarking_and_memory_usage(tdc,tokenizer,n=None,time_list=timelist5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17c7e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "168ecf0e",
   "metadata": {},
   "source": [
    "### b) With streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2dff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmarking_streaming(tdc,tokenizer,n=10,time_list=[]):\n",
    "    time1 = time.time()\n",
    "    tdc.process_and_tokenize(tokenizer,max_length=512,line_by_line=True)\n",
    "    time2 = time.time() \n",
    "    process_time = round(time2-time1,2)\n",
    "    print(f'Time it takes to process + tokenize training texts: {process_time} s')\n",
    "    for i,v in enumerate(tdc.main_ddict['train']):\n",
    "        if n is not None and i==tdc.batch_size*n: break\n",
    "    time3 = time.time()\n",
    "    iteration_time = round(time3-time2,2)\n",
    "    if n is not None:\n",
    "        print(f'Time it takes to go through {n*tdc.batch_size} items: {iteration_time} s')\n",
    "    else:\n",
    "        print(f'Time it takes to go through all items: {iteration_time} s')\n",
    "    \n",
    "    total_time = round(time3-time1,2)\n",
    "    print(f'Total time: {total_time} s')\n",
    "    time_list+=process_time,iteration_time,total_time\n",
    "def benchmarking_and_memory_usage_streaming(tdc,tokenizer,n=10,time_list=[]):\n",
    "    mem_usage = memory_usage((benchmarking_streaming,[tdc,tokenizer,n,time_list]))\n",
    "    total_usage = round(max(mem_usage),1)\n",
    "    print(f'Maximum memory usage: {total_usage} MiB')\n",
    "    time_list.append(total_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d981a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_aug_stochastic(x,aug=None,p=0.5):\n",
    "    results = aug.augment(x)\n",
    "    if not isinstance(x,list): return results[0] if random.random()<p else x\n",
    "    return [a if random.random()<p else b for a,b in zip(results,x)]\n",
    "\n",
    "aug = nac.KeyboardAug(aug_char_max=3,aug_char_p=0.1,aug_word_p=0.07)\n",
    "nearby_aug_func = partial(nlp_aug_stochastic,aug=aug,p=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52847799",
   "metadata": {},
   "source": [
    "#### With filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659c559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_timelist1=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624f9815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 0.8 s\n",
      "Time it takes to go through 11740 items: 4.03 s\n",
      "Total time: 4.82 s\n",
      "Maximum memory usage: 743.0 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=True)\n",
    "\n",
    "tdc = TextDataControllerStreaming(dset,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                 sup_types='classification',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                             },\n",
    "                                 class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n",
    "                                 batch_size=bs,\n",
    "                                 num_proc=4,\n",
    "                                 seed=42,\n",
    "                                 verbose=False\n",
    "                                )\n",
    "benchmarking_and_memory_usage_streaming(tdc,tokenizer,time_list=ns_timelist1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bad4cf7",
   "metadata": {},
   "source": [
    "#### With filter + metadatas concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f88f3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_timelist2=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec86893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 0.79 s\n",
      "Time it takes to go through 11740 items: 4.43 s\n",
      "Total time: 5.22 s\n",
      "Maximum memory usage: 745.9 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=True)\n",
    "\n",
    "tdc = TextDataControllerStreaming(dset,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                 sup_types='classification',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                             },\n",
    "                                 class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n",
    "                                 metadatas=['Title','Division Name'],\n",
    "                                 batch_size=bs,\n",
    "                                 num_proc=4,\n",
    "                                 seed=42,\n",
    "                                 verbose=False\n",
    "                                )\n",
    "benchmarking_and_memory_usage_streaming(tdc,tokenizer,time_list=ns_timelist2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a759598",
   "metadata": {},
   "source": [
    "#### With filter + metadatas concatenation + content transformation + content augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d0dc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_timelist3=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29e7769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 0.78 s\n",
      "Time it takes to go through 11740 items: 12.23 s\n",
      "Total time: 13.01 s\n",
      "Maximum memory usage: 743.0 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=True)\n",
    "\n",
    "tdc = TextDataControllerStreaming(dset,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                 sup_types='classification',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                             },\n",
    "                                 class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n",
    "                                 metadatas=['Title','Division Name'],\n",
    "                                 content_transformations=[text_normalize,str.lower],\n",
    "                                 content_augmentations= [nearby_aug_func,str.lower],\n",
    "                                 batch_size=bs,\n",
    "                                 num_proc=4,\n",
    "                                 seed=42,\n",
    "                                 verbose=False\n",
    "                                )\n",
    "benchmarking_and_memory_usage_streaming(tdc,tokenizer,time_list=ns_timelist3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850ecbc3",
   "metadata": {},
   "source": [
    "#### With filter + metadatas concatenation + content transformation + content augmentation + higher batch size (not recorded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63cece0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 0.79 s\n",
      "Time it takes to go through 35220 items: 36.66 s\n",
      "Total time: 37.45 s\n",
      "Maximum memory usage: 887.4 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=True)\n",
    "\n",
    "tdc = TextDataControllerStreaming(dset,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                 sup_types='classification',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                             },\n",
    "                                 class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n",
    "                                 metadatas=['Title','Division Name'],\n",
    "                                 content_transformations=[text_normalize,str.lower],\n",
    "                                 content_augmentations= [nearby_aug_func,str.lower],\n",
    "                                 batch_size=bs*3,\n",
    "                                 num_proc=4,\n",
    "                                 seed=42,\n",
    "                                 verbose=False\n",
    "                                )\n",
    "benchmarking_and_memory_usage_streaming(tdc,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69e0b69",
   "metadata": {},
   "source": [
    "#### With filter + metadatas concatenation + content transformation + content augmentation + iterate the whole dataset (1 epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf19883",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_timelist4=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2d48ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 0.8 s\n",
      "Time it takes to go through all items: 111.93 s\n",
      "Total time: 112.73 s\n",
      "Maximum memory usage: 762.8 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=True)\n",
    "\n",
    "tdc = TextDataControllerStreaming(dset,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                 sup_types='classification',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                             },\n",
    "                                 class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n",
    "                                 metadatas=['Title','Division Name'],\n",
    "                                 content_transformations=[text_normalize,str.lower],\n",
    "                                 content_augmentations= [nearby_aug_func,str.lower],\n",
    "                                 batch_size=bs,\n",
    "                                 num_proc=4,\n",
    "                                 seed=42,\n",
    "                                 verbose=False\n",
    "                                )\n",
    "benchmarking_and_memory_usage_streaming(tdc,tokenizer,n=None,time_list=ns_timelist4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda52efe",
   "metadata": {},
   "source": [
    "## 2. Test the effect of batch size and num proc (parallel process) on Non-streaming dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce0eb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmarking(tdc,tokenizer,n=10,shuffle_trn=True):\n",
    "    time1 = time.time()\n",
    "    tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=shuffle_trn)\n",
    "    time2 = time.time() \n",
    "    print(f'Time it takes to process + tokenize training texts: {(time2-time1):.3f} s')\n",
    "    for i,v in enumerate(tdc.main_ddict['train']):\n",
    "        if n is not None and i==tdc.batch_size*n: break\n",
    "    time3 = time.time()\n",
    "    if n is not None:\n",
    "        print(f'Time it takes to go through {n*tdc.batch_size} items: {(time3-time2):.3f} s')\n",
    "    else:\n",
    "        print(f'Time it takes to go through all items: {(time3-time2):.3f} s')\n",
    "\n",
    "    print(f'Total time: {(time3-time1):.3f} s')\n",
    "def benchmarking_and_memory_usage(tdc,tokenizer,n=10,shuffle_trn=True):\n",
    "    mem_usage = memory_usage((benchmarking,[tdc,tokenizer,n,shuffle_trn]))\n",
    "    print(f'Maximum memory usage: {max(mem_usage):.3f} MiB')\n",
    "\n",
    "\n",
    "def nlp_aug_stochastic(x,aug=None,p=0.5):\n",
    "    results = aug.augment(x)\n",
    "    if not isinstance(x,list): return results[0] if random.random()<p else x\n",
    "    return [a if random.random()<p else b for a,b in zip(results,x)]\n",
    "\n",
    "aug = nac.KeyboardAug(aug_char_max=3,aug_char_p=0.1,aug_word_p=0.07)\n",
    "nearby_aug_func = partial(nlp_aug_stochastic,aug=aug,p=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c944af",
   "metadata": {},
   "source": [
    "For non-streaming dataset, text processing + tokenization are the most time-consuming tasks, thus we will check how different batch size and num proc will affect these tasks' running time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776c0c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 64.098 s\n",
      "Time it takes to go through all items: 13.400 s\n",
      "Total time: 77.499 s\n",
      "Maximum memory usage: 925.188 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         content_augmentations= [nearby_aug_func,str.lower], \n",
    "                         val_ratio=None,\n",
    "                         batch_size=100,\n",
    "                         num_proc=2,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "benchmarking_and_memory_usage(tdc,tokenizer,n=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecaabe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 61.297 s\n",
      "Time it takes to go through all items: 14.427 s\n",
      "Total time: 75.724 s\n",
      "Maximum memory usage: 912.223 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         content_augmentations= [nearby_aug_func,str.lower], \n",
    "                         val_ratio=None,\n",
    "                         batch_size=1000,\n",
    "                         num_proc=2,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "benchmarking_and_memory_usage(tdc,tokenizer,n=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db3328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 25.857 s\n",
      "Time it takes to go through all items: 13.776 s\n",
      "Total time: 39.634 s\n",
      "Maximum memory usage: 928.574 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         content_augmentations= [nearby_aug_func,str.lower], \n",
    "                         val_ratio=None,\n",
    "                         batch_size=100,\n",
    "                         num_proc=8,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "benchmarking_and_memory_usage(tdc,tokenizer,n=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0a1c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 24.933 s\n",
      "Time it takes to go through all items: 14.271 s\n",
      "Total time: 39.204 s\n",
      "Maximum memory usage: 913.266 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         content_augmentations= [nearby_aug_func,str.lower], \n",
    "                         val_ratio=None,\n",
    "                         batch_size=1000,\n",
    "                         num_proc=8,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "benchmarking_and_memory_usage(tdc,tokenizer,n=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351512f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 25.600 s\n",
      "Time it takes to go through all items: 14.465 s\n",
      "Total time: 40.064 s\n",
      "Maximum memory usage: 934.883 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         content_augmentations= [nearby_aug_func,str.lower], \n",
    "                         val_ratio=None,\n",
    "                         batch_size=2000,\n",
    "                         num_proc=8,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "benchmarking_and_memory_usage(tdc,tokenizer,n=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85da8411",
   "metadata": {},
   "source": [
    "Increasing num_proc is more beneficial than increasing processing batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf54267",
   "metadata": {},
   "source": [
    "## 3. Improving processing time with caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c82b9d8",
   "metadata": {},
   "source": [
    "The worst processing time is recorded with non-streaming training set, with the following preprocessing: 2-column filtering, 2-column metadatas, 2 content transformations, 2 content augmentation.\n",
    "\n",
    "With caching, we can significantly reduce the preprocessing time. That means, you only need to do all preprocessings once; all subsequent call will take advatages of this cached result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c113fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6f908f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-0aed8574c094e4fd_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-b781a4a73d06caf5_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-0f85d6db4165d6ef_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-420893192d8b876f_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-ee3f2ca19acd2369_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-27d1b7f9046ec1b4_*_of_00004.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/113140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/113140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/113140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/113140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         content_augmentations= [nearby_aug_func,str.lower], \n",
    "                         val_ratio=None,\n",
    "                         batch_size=1000,\n",
    "                         num_proc=4,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45066585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae102d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-0aed8574c094e4fd_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-b781a4a73d06caf5_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-0f85d6db4165d6ef_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-420893192d8b876f_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-ee3f2ca19acd2369_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-27d1b7f9046ec1b4_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-57d938bbd364f406_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-10afb2ec3cb12852_*_of_00004.arrow\n",
      "Loading cached shuffled indices for dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-5d8840c40fe75896.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-030424c28049222f_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-fe4290971e8d1087_*_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 0.979 s\n",
      "Time it takes to go through all items: 16.824 s\n",
      "Maximum memory usage: 823.531 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         content_augmentations= [nearby_aug_func,str.lower], \n",
    "                         val_ratio=None,\n",
    "                         batch_size=1000,\n",
    "                         num_proc=4,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "benchmarking_and_memory_usage(tdc,tokenizer,n=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc01d84",
   "metadata": {},
   "source": [
    "If you cached, then you only need 0.979s to load the data back from caches, instead of wait for 35s to do the process all over again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861fb9ad",
   "metadata": {},
   "source": [
    "## 4. Time and Space Complexity Comparison (as of 5/3/2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b33e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59ca8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1 = [timelist1,ns_timelist1]\n",
    "exp2 = [timelist2,ns_timelist2]\n",
    "exp3 = [timelist3,ns_timelist3]\n",
    "exp4 = [timelist4,[None,None,None,None]] # no shuffling when streaming\n",
    "exp5 = [timelist5,ns_timelist4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f38f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names=['Filter + Shuffling Train','And 2 metadatas',\n",
    "           'And 2 tfms + 2 augs','Same, but without train shuffling',\n",
    "           'Time to process 1 epoch']\n",
    "idxs=['Non-Streaming','Streaming']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e69a813",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp=[]\n",
    "for i in range(2):\n",
    "    _tmp.append([l[i][0] for l in [exp1,exp2,exp3,exp4,exp5]])\n",
    "df = pd.DataFrame(np.array(_tmp),columns=col_names)\n",
    "df.index = idxs\n",
    "df.index.name= 'Time (s) to process and tokenize 117k records with batch size 1174'\n",
    "\n",
    "_tmp=[]\n",
    "for i in range(2):\n",
    "    _tmp.append([l[i][1] for l in [exp1,exp2,exp3,exp4,exp5]])\n",
    "df2 = pd.DataFrame(np.array(_tmp),columns=col_names[:-1]+['Time to iterate 1 epoch'])\n",
    "df2.index = idxs\n",
    "df2.index.name= 'Time (s) to iterate 10 batches (11740 items)'\n",
    "\n",
    "_tmp=[]\n",
    "for i in range(2):\n",
    "    _tmp.append([l[i][2] for l in [exp1,exp2,exp3,exp4,exp5]])\n",
    "df3 = pd.DataFrame(np.array(_tmp),columns=col_names[:-1]+['Total time to process + tokenize + iterate 1 epoch'])\n",
    "df3.index = idxs\n",
    "df3.index.name= 'Total time (s) to process + tokenize + iterate 10 batches'\n",
    "\n",
    "_tmp=[]\n",
    "for i in range(2):\n",
    "    _tmp.append([l[i][3] for l in [exp1,exp2,exp3,exp4,exp5]])\n",
    "df4 = pd.DataFrame(np.array(_tmp),columns=col_names[:-1]+['Total memory to process + tokenize + iterate 1 epoch'])\n",
    "df4.index = idxs\n",
    "df4.index.name= 'Total memory (MiB) to process + tokenize + iterate 10 batches'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ee8ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a215df29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filter + Shuffling Train</th>\n",
       "      <th>And 2 metadatas</th>\n",
       "      <th>And 2 tfms + 2 augs</th>\n",
       "      <th>Same, but without train shuffling</th>\n",
       "      <th>Time to process 1 epoch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time (s) to process and tokenize 117k records with batch size 1174</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Non-Streaming</th>\n",
       "      <td>14.37</td>\n",
       "      <td>15.26</td>\n",
       "      <td>35.09</td>\n",
       "      <td>34.36</td>\n",
       "      <td>35.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Streaming</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.78</td>\n",
       "      <td>None</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Filter + Shuffling Train  \\\n",
       "Time (s) to process and tokenize 117k records w...                            \n",
       "Non-Streaming                                                         14.37   \n",
       "Streaming                                                               0.8   \n",
       "\n",
       "                                                   And 2 metadatas  \\\n",
       "Time (s) to process and tokenize 117k records w...                   \n",
       "Non-Streaming                                                15.26   \n",
       "Streaming                                                     0.79   \n",
       "\n",
       "                                                   And 2 tfms + 2 augs  \\\n",
       "Time (s) to process and tokenize 117k records w...                       \n",
       "Non-Streaming                                                    35.09   \n",
       "Streaming                                                         0.78   \n",
       "\n",
       "                                                   Same, but without train shuffling  \\\n",
       "Time (s) to process and tokenize 117k records w...                                     \n",
       "Non-Streaming                                                                  34.36   \n",
       "Streaming                                                                       None   \n",
       "\n",
       "                                                   Time to process 1 epoch  \n",
       "Time (s) to process and tokenize 117k records w...                          \n",
       "Non-Streaming                                                        35.34  \n",
       "Streaming                                                              0.8  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60100047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filter + Shuffling Train</th>\n",
       "      <th>And 2 metadatas</th>\n",
       "      <th>And 2 tfms + 2 augs</th>\n",
       "      <th>Same, but without train shuffling</th>\n",
       "      <th>Time to iterate 1 epoch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time (s) to iterate 10 batches (11740 items)</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Non-Streaming</th>\n",
       "      <td>1.27</td>\n",
       "      <td>1.46</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.47</td>\n",
       "      <td>14.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Streaming</th>\n",
       "      <td>4.03</td>\n",
       "      <td>4.43</td>\n",
       "      <td>12.23</td>\n",
       "      <td>None</td>\n",
       "      <td>111.93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Filter + Shuffling Train  \\\n",
       "Time (s) to iterate 10 batches (11740 items)                            \n",
       "Non-Streaming                                                    1.27   \n",
       "Streaming                                                        4.03   \n",
       "\n",
       "                                             And 2 metadatas  \\\n",
       "Time (s) to iterate 10 batches (11740 items)                   \n",
       "Non-Streaming                                           1.46   \n",
       "Streaming                                               4.43   \n",
       "\n",
       "                                             And 2 tfms + 2 augs  \\\n",
       "Time (s) to iterate 10 batches (11740 items)                       \n",
       "Non-Streaming                                               1.52   \n",
       "Streaming                                                  12.23   \n",
       "\n",
       "                                             Same, but without train shuffling  \\\n",
       "Time (s) to iterate 10 batches (11740 items)                                     \n",
       "Non-Streaming                                                             1.47   \n",
       "Streaming                                                                 None   \n",
       "\n",
       "                                             Time to iterate 1 epoch  \n",
       "Time (s) to iterate 10 batches (11740 items)                          \n",
       "Non-Streaming                                                  14.32  \n",
       "Streaming                                                     111.93  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c310bc61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filter + Shuffling Train</th>\n",
       "      <th>And 2 metadatas</th>\n",
       "      <th>And 2 tfms + 2 augs</th>\n",
       "      <th>Same, but without train shuffling</th>\n",
       "      <th>Total time to process + tokenize + iterate 1 epoch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total time (s) to process + tokenize + iterate 10 batches</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Non-Streaming</th>\n",
       "      <td>15.64</td>\n",
       "      <td>16.72</td>\n",
       "      <td>36.61</td>\n",
       "      <td>35.83</td>\n",
       "      <td>49.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Streaming</th>\n",
       "      <td>4.82</td>\n",
       "      <td>5.22</td>\n",
       "      <td>13.01</td>\n",
       "      <td>None</td>\n",
       "      <td>112.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Filter + Shuffling Train  \\\n",
       "Total time (s) to process + tokenize + iterate ...                            \n",
       "Non-Streaming                                                         15.64   \n",
       "Streaming                                                              4.82   \n",
       "\n",
       "                                                   And 2 metadatas  \\\n",
       "Total time (s) to process + tokenize + iterate ...                   \n",
       "Non-Streaming                                                16.72   \n",
       "Streaming                                                     5.22   \n",
       "\n",
       "                                                   And 2 tfms + 2 augs  \\\n",
       "Total time (s) to process + tokenize + iterate ...                       \n",
       "Non-Streaming                                                    36.61   \n",
       "Streaming                                                        13.01   \n",
       "\n",
       "                                                   Same, but without train shuffling  \\\n",
       "Total time (s) to process + tokenize + iterate ...                                     \n",
       "Non-Streaming                                                                  35.83   \n",
       "Streaming                                                                       None   \n",
       "\n",
       "                                                   Total time to process + tokenize + iterate 1 epoch  \n",
       "Total time (s) to process + tokenize + iterate ...                                                     \n",
       "Non-Streaming                                                                                   49.66  \n",
       "Streaming                                                                                      112.73  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4c1691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filter + Shuffling Train</th>\n",
       "      <th>And 2 metadatas</th>\n",
       "      <th>And 2 tfms + 2 augs</th>\n",
       "      <th>Same, but without train shuffling</th>\n",
       "      <th>Total memory to process + tokenize + iterate 1 epoch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total memory (MiB) to process + tokenize + iterate 10 batches</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Non-Streaming</th>\n",
       "      <td>734.9</td>\n",
       "      <td>748.6</td>\n",
       "      <td>754.7</td>\n",
       "      <td>777.3</td>\n",
       "      <td>869.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Streaming</th>\n",
       "      <td>743.0</td>\n",
       "      <td>745.9</td>\n",
       "      <td>743.0</td>\n",
       "      <td>None</td>\n",
       "      <td>762.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Filter + Shuffling Train  \\\n",
       "Total memory (MiB) to process + tokenize + iter...                            \n",
       "Non-Streaming                                                         734.9   \n",
       "Streaming                                                             743.0   \n",
       "\n",
       "                                                   And 2 metadatas  \\\n",
       "Total memory (MiB) to process + tokenize + iter...                   \n",
       "Non-Streaming                                                748.6   \n",
       "Streaming                                                    745.9   \n",
       "\n",
       "                                                   And 2 tfms + 2 augs  \\\n",
       "Total memory (MiB) to process + tokenize + iter...                       \n",
       "Non-Streaming                                                    754.7   \n",
       "Streaming                                                        743.0   \n",
       "\n",
       "                                                   Same, but without train shuffling  \\\n",
       "Total memory (MiB) to process + tokenize + iter...                                     \n",
       "Non-Streaming                                                                  777.3   \n",
       "Streaming                                                                       None   \n",
       "\n",
       "                                                   Total memory to process + tokenize + iterate 1 epoch  \n",
       "Total memory (MiB) to process + tokenize + iter...                                                       \n",
       "Non-Streaming                                                                                   869.6    \n",
       "Streaming                                                                                       762.8    "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4beae3a",
   "metadata": {},
   "source": [
    "## 5. Tips and tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ff203b",
   "metadata": {},
   "source": [
    "- For non-streaming data, the best way to minimize processing and iteration time is:\n",
    "    - Turn on dataset caching, and run the processing step once for it to be cached\n",
    "- The more content transformations and augmentations added, the slower the process + iteration. This is especially true for streaming data\n",
    "- For streaming, be aware of the pros and cons of batch-process and line-by-line process (read more [here](https://anhquan0412.github.io/that-nlp-library/text_main_streaming.html#batch-process-vs-line-by-line-process))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
