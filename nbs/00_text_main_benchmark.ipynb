{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74c74569",
   "metadata": {},
   "source": [
    "# Text Processing Benchmark\n",
    "\n",
    "> This module contains some benchmarks for `TextDataController`\n",
    "\n",
    "- skip_showdoc: true\n",
    "- skip_exec: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ddb26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda list | grep 'datasets\\|transformers\\|torch'\n",
    "# datasets                  2.14.4                   pypi_0    pypi\n",
    "# pytorch-ignite            0.4.11                   pypi_0    pypi\n",
    "# pytorch-lightning         2.0.1.post0              pypi_0    pypi\n",
    "# torch                     2.0.1+cu118              pypi_0    pypi\n",
    "# torchaudio                2.0.2+cu118              pypi_0    pypi\n",
    "# torchmetrics              1.1.1                    pypi_0    pypi\n",
    "# torchvision               0.15.2+cu118             pypi_0    pypi\n",
    "# transformers              4.31.0                   pypi_0    pypi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a94f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from that_nlp_library.text_transformation import *\n",
    "from that_nlp_library.text_augmentation import *\n",
    "from that_nlp_library.text_main import *\n",
    "from that_nlp_library.text_main_streaming import *\n",
    "from datasets import load_dataset,enable_caching,disable_caching\n",
    "from transformers import RobertaTokenizer\n",
    "import os\n",
    "import time\n",
    "from underthesea import text_normalize\n",
    "import nlpaug.augmenter.char as nac\n",
    "from functools import partial\n",
    "import random\n",
    "from memory_profiler import memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b201aaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_caching() # disable huggingface caching to get a fair benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420161a0",
   "metadata": {},
   "source": [
    "## Benchmark on medium-size dataset (~117k rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79713440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117430"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "len(dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b7ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874c3a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1174"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs=len(dset)//100\n",
    "bs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3507e8de",
   "metadata": {},
   "source": [
    "### Non-streaming dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd0bdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmarking(tdc,tokenizer,n=10,shuffle_trn=True):\n",
    "    time1 = time.time()\n",
    "    tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=shuffle_trn)\n",
    "    time2 = time.time() \n",
    "    print(f'Time it takes to process + tokenize training texts: {(time2-time1):.3f} s')\n",
    "    for i,v in enumerate(tdc.main_ddict['train']):\n",
    "        if n is not None and i==tdc.batch_size*n: break\n",
    "    time3 = time.time()\n",
    "    if n is not None:\n",
    "        print(f'Time it takes to go through {n*tdc.batch_size} items: {(time3-time2):.3f} s')\n",
    "    else:\n",
    "        print(f'Time it takes to go through all items: {(time3-time2):.3f} s')\n",
    "\n",
    "#     print(f'Total time: {(time3-time1):.3f} s')\n",
    "def benchmarking_and_memory_usage(tdc,tokenizer,n=10,shuffle_trn=True):\n",
    "    mem_usage = memory_usage((benchmarking,[tdc,tokenizer,n,shuffle_trn]))\n",
    "    print(f'Maximum memory usage: {max(mem_usage):.3f} MiB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7729a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_aug_stochastic(x,aug=None,p=0.5):\n",
    "    results = aug.augment(x)\n",
    "    if not isinstance(x,list): return results[0] if random.random()<p else x\n",
    "    return [a if random.random()<p else b for a,b in zip(results,x)]\n",
    "\n",
    "aug = nac.KeyboardAug(aug_char_max=3,aug_char_p=0.1,aug_word_p=0.07)\n",
    "nearby_aug_func = partial(nlp_aug_stochastic,aug=aug,p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1651e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1383c6c",
   "metadata": {},
   "source": [
    "#### With filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09659591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 15.248 s\n",
      "Time it takes to go through 11740 items: 1.428 s\n",
      "Maximum memory usage: 763.223 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         val_ratio=None,\n",
    "                         batch_size=bs,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "benchmarking_and_memory_usage(tdc,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219c5525",
   "metadata": {},
   "source": [
    "#### With filter + metadatas concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae812cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 15.586 s\n",
      "Time it takes to go through 11740 items: 1.639 s\n",
      "Maximum memory usage: 772.113 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         val_ratio=None,\n",
    "                         batch_size=bs,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "benchmarking_and_memory_usage(tdc,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e09b408",
   "metadata": {},
   "source": [
    "#### With filter + metadatas concatenation + content transformation + content augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3846a6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 35.383 s\n",
      "Time it takes to go through 11740 items: 1.695 s\n",
      "Maximum memory usage: 779.855 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         content_augmentations= [nearby_aug_func,str.lower], \n",
    "                         val_ratio=None,\n",
    "                         batch_size=bs,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "benchmarking_and_memory_usage(tdc,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955b2f55",
   "metadata": {},
   "source": [
    "#### With filter + metadatas concatenation + content transformation + content augmentation + no shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82859ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 34.722 s\n",
      "Time it takes to go through 11740 items: 1.708 s\n",
      "Maximum memory usage: 780.215 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         content_augmentations= [nearby_aug_func,str.lower], \n",
    "                         val_ratio=None,\n",
    "                         batch_size=bs,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "benchmarking_and_memory_usage(tdc,tokenizer,shuffle_trn=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aa9efa",
   "metadata": {},
   "source": [
    "#### With filter + metadatas concatenation + content transformation + content augmentation + higher batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1500ee25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 35.651 s\n",
      "Time it takes to go through 35220 items: 5.246 s\n",
      "Maximum memory usage: 774.805 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         content_augmentations= [nearby_aug_func,str.lower], \n",
    "                         val_ratio=None,\n",
    "                         batch_size=bs*3,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "benchmarking_and_memory_usage(tdc,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95db4cf4",
   "metadata": {},
   "source": [
    "#### With filter + metadatas concatenation + content transformation + content augmentation + iterate the whole dataset (1 epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6bd163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1174"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fa534b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 35.532 s\n",
      "Time it takes to go through all items: 16.601 s\n",
      "Maximum memory usage: 862.285 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         content_augmentations= [nearby_aug_func,str.lower], \n",
    "                         val_ratio=None,\n",
    "                         batch_size=bs,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "benchmarking_and_memory_usage(tdc,tokenizer,n=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34df9190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b4c5046",
   "metadata": {},
   "source": [
    "### With streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54379516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmarking_streaming(tdc,tokenizer,n=10):\n",
    "    time1 = time.time()\n",
    "    tdc.process_and_tokenize(tokenizer,line_by_line=True)\n",
    "    time2 = time.time() \n",
    "    print(f'Time it takes to process + tokenize training texts: {(time2-time1):.3f} s')\n",
    "    for i,v in enumerate(tdc.main_ddict['train']):\n",
    "        if n is not None and i==tdc.batch_size*n: break\n",
    "    time3 = time.time()\n",
    "    if n is not None:\n",
    "        print(f'Time it takes to go through {n*tdc.batch_size} items: {(time3-time2):.3f} s')\n",
    "    else:\n",
    "        print(f'Time it takes to go through all items: {(time3-time2):.3f} s')\n",
    "\n",
    "def benchmarking_and_memory_usage_streaming(tdc,tokenizer,n=10):\n",
    "    mem_usage = memory_usage((benchmarking_streaming,[tdc,tokenizer,n]))\n",
    "    print(f'Maximum memory usage: {max(mem_usage):.3f} MiB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0151085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_aug_stochastic(x,aug=None,p=0.5):\n",
    "    results = aug.augment(x)\n",
    "    if not isinstance(x,list): return results[0] if random.random()<p else x\n",
    "    return [a if random.random()<p else b for a,b in zip(results,x)]\n",
    "\n",
    "aug = nac.KeyboardAug(aug_char_max=3,aug_char_p=0.1,aug_word_p=0.07)\n",
    "nearby_aug_func = partial(nlp_aug_stochastic,aug=aug,p=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2efb69",
   "metadata": {},
   "source": [
    "#### With filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b4c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 0.768 s\n",
      "Time it takes to go through 11740 items: 4.768 s\n",
      "Maximum memory usage: 678.414 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=True)\n",
    "\n",
    "tdc = TextDataControllerStreaming(dset,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                 sup_types='classification',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                             },\n",
    "                                 class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n",
    "                                 batch_size=bs,\n",
    "                                 num_proc=4,\n",
    "                                 seed=42,\n",
    "                                 verbose=False\n",
    "                                )\n",
    "benchmarking_and_memory_usage_streaming(tdc,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53cf9e2",
   "metadata": {},
   "source": [
    "#### With filter + metadatas concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b55f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 0.802 s\n",
      "Time it takes to go through 11740 items: 4.692 s\n",
      "Maximum memory usage: 700.844 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=True)\n",
    "\n",
    "tdc = TextDataControllerStreaming(dset,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                 sup_types='classification',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                             },\n",
    "                                 class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n",
    "                                 metadatas=['Title','Division Name'],\n",
    "                                 batch_size=bs,\n",
    "                                 num_proc=4,\n",
    "                                 seed=42,\n",
    "                                 verbose=False\n",
    "                                )\n",
    "benchmarking_and_memory_usage_streaming(tdc,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26045d3",
   "metadata": {},
   "source": [
    "#### With filter + metadatas concatenation + content transformation + content augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f45c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 0.821 s\n",
      "Time it takes to go through 11740 items: 12.967 s\n",
      "Maximum memory usage: 724.578 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=True)\n",
    "\n",
    "tdc = TextDataControllerStreaming(dset,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                 sup_types='classification',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                             },\n",
    "                                 class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n",
    "                                 metadatas=['Title','Division Name'],\n",
    "                                 content_transformations=[text_normalize,str.lower],\n",
    "                                 content_augmentations= [nearby_aug_func,str.lower],\n",
    "                                 batch_size=bs,\n",
    "                                 num_proc=4,\n",
    "                                 seed=42,\n",
    "                                 verbose=False\n",
    "                                )\n",
    "benchmarking_and_memory_usage_streaming(tdc,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b4abe9",
   "metadata": {},
   "source": [
    "#### With filter + metadatas concatenation + content transformation + content augmentation + higher batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9243d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 0.894 s\n",
      "Time it takes to go through 35220 items: 38.426 s\n",
      "Maximum memory usage: 861.527 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=True)\n",
    "\n",
    "tdc = TextDataControllerStreaming(dset,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                 sup_types='classification',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                             },\n",
    "                                 class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n",
    "                                 metadatas=['Title','Division Name'],\n",
    "                                 content_transformations=[text_normalize,str.lower],\n",
    "                                 content_augmentations= [nearby_aug_func,str.lower],\n",
    "                                 batch_size=bs*3,\n",
    "                                 num_proc=4,\n",
    "                                 seed=42,\n",
    "                                 verbose=False\n",
    "                                )\n",
    "benchmarking_and_memory_usage_streaming(tdc,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37097215",
   "metadata": {},
   "source": [
    "#### With filter + metadatas concatenation + content transformation + content augmentation + iterate the whole dataset (1 epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6af9215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 0.966 s\n",
      "Time it takes to go through all items: 120.615 s\n",
      "Maximum memory usage: 747.059 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=True)\n",
    "\n",
    "tdc = TextDataControllerStreaming(dset,\n",
    "                                 main_text='Review Text',\n",
    "                                 label_names='Department Name',\n",
    "                                 sup_types='classification',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                             },\n",
    "                                 class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n",
    "                                 metadatas=['Title','Division Name'],\n",
    "                                 content_transformations=[text_normalize,str.lower],\n",
    "                                 content_augmentations= [nearby_aug_func,str.lower],\n",
    "                                 batch_size=bs,\n",
    "                                 num_proc=4,\n",
    "                                 seed=42,\n",
    "                                 verbose=False\n",
    "                                )\n",
    "benchmarking_and_memory_usage_streaming(tdc,tokenizer,n=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074a5b75",
   "metadata": {},
   "source": [
    "### Test the effect of batch size and num_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5226a77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmarking(tdc,tokenizer,n=10,shuffle_trn=True):\n",
    "    time1 = time.time()\n",
    "    tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=shuffle_trn)\n",
    "    time2 = time.time() \n",
    "    print(f'Time it takes to process + tokenize training texts: {(time2-time1):.3f} s')\n",
    "    for i,v in enumerate(tdc.main_ddict['train']):\n",
    "        if n is not None and i==tdc.batch_size*n: break\n",
    "    time3 = time.time()\n",
    "    if n is not None:\n",
    "        print(f'Time it takes to go through {n*tdc.batch_size} items: {(time3-time2):.3f} s')\n",
    "    else:\n",
    "        print(f'Time it takes to go through all items: {(time3-time2):.3f} s')\n",
    "\n",
    "#     print(f'Total time: {(time3-time1):.3f} s')\n",
    "def benchmarking_and_memory_usage(tdc,tokenizer,n=10,shuffle_trn=True):\n",
    "    mem_usage = memory_usage((benchmarking,[tdc,tokenizer,n,shuffle_trn]))\n",
    "    print(f'Maximum memory usage: {max(mem_usage):.3f} MiB')\n",
    "\n",
    "\n",
    "def nlp_aug_stochastic(x,aug=None,p=0.5):\n",
    "    results = aug.augment(x)\n",
    "    if not isinstance(x,list): return results[0] if random.random()<p else x\n",
    "    return [a if random.random()<p else b for a,b in zip(results,x)]\n",
    "\n",
    "aug = nac.KeyboardAug(aug_char_max=3,aug_char_p=0.1,aug_word_p=0.07)\n",
    "nearby_aug_func = partial(nlp_aug_stochastic,aug=aug,p=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449df493",
   "metadata": {},
   "source": [
    "For non-streaming dataset, text processing + tokenization are the most time-consuming tasks, thus we will check how different batch size and num proc will affect these tasks' running time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff4e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 37.585 s\n",
      "Time it takes to go through all items: 16.613 s\n",
      "Maximum memory usage: 872.035 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         content_augmentations= [nearby_aug_func,str.lower], \n",
    "                         val_ratio=None,\n",
    "                         batch_size=128,\n",
    "                         num_proc=4,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "benchmarking_and_memory_usage(tdc,tokenizer,n=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3608716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 36.803 s\n",
      "Time it takes to go through all items: 16.858 s\n",
      "Maximum memory usage: 888.293 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         content_augmentations= [nearby_aug_func,str.lower], \n",
    "                         val_ratio=None,\n",
    "                         batch_size=1000,\n",
    "                         num_proc=4,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "benchmarking_and_memory_usage(tdc,tokenizer,n=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b423256d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 36.704 s\n",
      "Time it takes to go through all items: 17.148 s\n",
      "Maximum memory usage: 893.254 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         content_augmentations= [nearby_aug_func,str.lower], \n",
    "                         val_ratio=None,\n",
    "                         batch_size=24*50, #1200\n",
    "                         num_proc=24,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "benchmarking_and_memory_usage(tdc,tokenizer,n=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc1370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 37.287 s\n",
      "Time it takes to go through all items: 17.501 s\n",
      "Maximum memory usage: 914.801 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         content_augmentations= [nearby_aug_func,str.lower], \n",
    "                         val_ratio=None,\n",
    "                         batch_size=24*400, #9600\n",
    "                         num_proc=24,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "benchmarking_and_memory_usage(tdc,tokenizer,n=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dc9014",
   "metadata": {},
   "source": [
    "## Improving processing time with caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed839dd4",
   "metadata": {},
   "source": [
    "The worst processing time is recorded with non-streaming training set, with the following preprocessing: 2-column filtering, 2-column metadatas, 2 content transformations, 2 content augmentation.\n",
    "\n",
    "With caching, we can significantly reduce the preprocessing time. That means, you only need to do all preprocessings once; all subsequent call will take advatages of this cached result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0615497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3227ec02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-0aed8574c094e4fd_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-b781a4a73d06caf5_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-0f85d6db4165d6ef_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-420893192d8b876f_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-ee3f2ca19acd2369_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-27d1b7f9046ec1b4_*_of_00004.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/113140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/113140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/113140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/113140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         content_augmentations= [nearby_aug_func,str.lower], \n",
    "                         val_ratio=None,\n",
    "                         batch_size=1000,\n",
    "                         num_proc=4,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "tdc.process_and_tokenize(tokenizer,max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f911b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff6bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-0aed8574c094e4fd_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-b781a4a73d06caf5_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-0f85d6db4165d6ef_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-420893192d8b876f_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-ee3f2ca19acd2369_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-27d1b7f9046ec1b4_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-57d938bbd364f406_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-10afb2ec3cb12852_*_of_00004.arrow\n",
      "Loading cached shuffled indices for dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-5d8840c40fe75896.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-030424c28049222f_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-fe4290971e8d1087_*_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it takes to process + tokenize training texts: 0.979 s\n",
      "Time it takes to go through all items: 16.824 s\n",
      "Maximum memory usage: 823.531 MiB\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n",
    "                    split='train',\n",
    "                    streaming=False)\n",
    "\n",
    "tdc = TextDataController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         label_names='Department Name',\n",
    "                         sup_types='classification',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                      'Department Name': lambda x: x is not None,\n",
    "                                     },\n",
    "                         metadatas=['Title','Division Name'],\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         content_augmentations= [nearby_aug_func,str.lower], \n",
    "                         val_ratio=None,\n",
    "                         batch_size=1000,\n",
    "                         num_proc=4,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "benchmarking_and_memory_usage(tdc,tokenizer,n=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a0e599",
   "metadata": {},
   "source": [
    "If you cached, then you only need 0.979s to load the data back from caches, instead of wait for 35.284s to do the process all over again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c29380",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a23294",
   "metadata": {},
   "source": [
    "| Process + Tokenize Time (117k records), batchsize = 1174 | Filter, train shuffling | And 2 metadatas | And 2 tfms + 2 augs | Batchsize=1174 x 3 | Processing 1 epoch (Batchsize=1174) |\n",
    "|----------------------------------------------------------|-------------------------|-----------------|---------------------|--------------------|----------------|\n",
    "| Non-streaming training                                   | 15.248                   | 15.586          | 35.383              | 35.651             | 35.532         |\n",
    "| Streaming                                                | 0.768                    | 0.802           | 0.821               | 0.894              | 0.966          |\n",
    "| Ratio Non-streaming/Streaming                            | 19.85                    | 19.43           | 43.1               | 39.88              | 36.78          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f818c2",
   "metadata": {},
   "source": [
    "| Run 10 batches time (1174*10) | Filter, train shuffling | And 2 metadatas | And 2 tfms + 2 augs | Total items iterated: 1174 * 10 * 3 | 1 epoch iterated (117430 items) |\n",
    "|-------------------------------|-------------------------|-----------------|---------------------|---------------------------------|--------------------------------|\n",
    "| Non-streaming training        | 1.428                    | 1.639           | 1.695               | 5.246                            | 16.601                         |\n",
    "| Streaming                     | 4.768                    | 4.692           | 12.967              | 38.426                           | 120.615                        |\n",
    "| Ratio Streaming/Non-Streaming | 3.34                     | 2.86            | 7.65                | 7.32                             | 7.27                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331b7fd9",
   "metadata": {},
   "source": [
    "| Total Time (Process+Tokenize+Iterate) | Filter, train shuffling | And 2 metadatas | And 2 tfms + 2 augs | Total items iterated: 1174 * 10 * 3 | 1 epoch iterated: 117430 items |\n",
    "|---------------------------------------|-------------------------|-----------------|---------------------|---------------------------------|--------------------------------|\n",
    "| Non-streaming training                | 16.676                   | 17.225          | 37.078              | 40.897                           | 52.133                         |\n",
    "| Streaming                             | 5.536                    | 5.494           | 13.788               | 39.32                           | 121.581                        |\n",
    "| Ratio Non-streaming/Streaming         | 3.01                     | 3.14            | 2.69                 | 1.04                             | 0.43                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ed24e7",
   "metadata": {},
   "source": [
    "| Total memory use                    | Filter, train shuffling | And 2 metadatas | And 2 tfms + 2 augs | Total items iterated: 1174 * 10 * 3 | 1 epoch iterated: 117430 items |\n",
    "|-------------------------------------|-------------------------|-----------------|---------------------|---------------------------------|--------------------------------|\n",
    "| Non-streaming training              | 763                      | 772             | 779                 | 774                              | 862                            |\n",
    "| Streaming                           | 678                      | 701             | 724                 | 861                              | 747                            |\n",
    "| Ratio Streaming/Non-Streaming       | 0.89                     | 0.91            | 0.93                | 1.11                             | 0.87                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafe59c5",
   "metadata": {},
   "source": [
    "## Tips and tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed369e",
   "metadata": {},
   "source": [
    "- For non-streaming data, the best way to minimize processing and iteration time is:\n",
    "    - Turn on dataset caching, and run the processing step once for it to be cached\n",
    "- The more content transformations and augmentations added, the slower the process + iteration. This is especially true for streaming data\n",
    "- For streaming, be aware of the pros and cons of batch-process and line-by-line process\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
