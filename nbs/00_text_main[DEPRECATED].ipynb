{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Main\n",
    "\n",
    "> This module contains the main Python class for data control: `TextDataMain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp text_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder,MultiLabelBinarizer\n",
    "from datasets import DatasetDict,Dataset\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from that_nlp_library.utils import *\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from that_nlp_library.text_transformation import *\n",
    "from that_nlp_library.text_augmentation import *\n",
    "from importlib.machinery import SourceFileLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text transformation and Tokenizer Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tokenizer_explain(inp, # Input sentence\n",
    "                      tokenizer, # Tokenizer (preferably from HuggingFace)\n",
    "                      split_word=False # Is input `inp` split into list or not\n",
    "                     ):\n",
    "    \"Display results from tokenizer\"\n",
    "    print('----- Tokenizer Explained -----')\n",
    "    print('--- Input ---')\n",
    "    print(inp)\n",
    "    print()\n",
    "    print('--- Tokenized results --- ')\n",
    "    print(tokenizer(inp,is_split_into_words=split_word))\n",
    "    print()\n",
    "    tok = tokenizer.encode(inp,is_split_into_words=split_word)\n",
    "    print('--- Results from tokenizer.convert_ids_to_tokens ---')\n",
    "    print(tokenizer.convert_ids_to_tokens(tok))\n",
    "    print()\n",
    "    print('--- Results from tokenizer.decode --- ')\n",
    "    print(tokenizer.decode(tok))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L20){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### tokenizer_explain\n",
       "\n",
       ">      tokenizer_explain (inp, tokenizer, split_word=False)\n",
       "\n",
       "Display results from tokenizer\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | Input sentence |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| split_word | bool | False | Is input `inp` split into list or not |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L20){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### tokenizer_explain\n",
       "\n",
       ">      tokenizer_explain (inp, tokenizer, split_word=False)\n",
       "\n",
       "Display results from tokenizer\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | Input sentence |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| split_word | bool | False | Is input `inp` split into list or not |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(tokenizer_explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a tokenizer from EnviBert model. Uncomment the command line below to download necessary files to build this tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gdown 14X9fGijA7kdNfe4dM_8gqfxIWtj1Q-hb -O ./envibert_cache --folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir=Path('./envibert_tokenizer')\n",
    "tokenizer = SourceFileLoader(\"envibert.tokenizer\", \n",
    "                             str(cache_dir/'envibert_tokenizer.py')).load_module().RobertaTokenizer(cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Envibert tokenizer does not required the input to be tokenized using word_tokenize from UnderTheSea library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Tokenizer Explained -----\n",
      "--- Input ---\n",
      "hội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 227, 1033, 191, 664, 1033, 7366, 2615, 13, 664, 1033, 671, 1355, 2294, 993, 413, 2900, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens ---\n",
      "['<s>', '▁hội', '▁cư', '▁dân', '▁chung', '▁cư', '▁sen', '▁hồng', '▁-', '▁chung', '▁cư', '▁lot', 'us', '▁sóng', '▁thần', '▁thủ', '▁đức', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> ▁hội ▁cư ▁dân ▁chung ▁cư ▁sen ▁hồng ▁- ▁chung ▁cư ▁lot us ▁sóng ▁thần ▁thủ ▁đức </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inp = 'hội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức'\n",
    "tokenizer_explain(inp,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Tokenizer Explained -----\n",
      "--- Input ---\n",
      "['hội', 'cư', 'dân', 'chung', 'cư', 'sen', 'hồng', '-', 'chung', 'cư', 'lotus', 'sóng', 'thần', 'thủ', 'đức']\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 227, 1033, 191, 664, 1033, 7366, 2615, 13, 664, 1033, 671, 1355, 2294, 993, 413, 2900, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens ---\n",
      "['<s>', '▁hội', '▁cư', '▁dân', '▁chung', '▁cư', '▁sen', '▁hồng', '▁-', '▁chung', '▁cư', '▁lot', 'us', '▁sóng', '▁thần', '▁thủ', '▁đức', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> ▁hội ▁cư ▁dân ▁chung ▁cư ▁sen ▁hồng ▁- ▁chung ▁cư ▁lot us ▁sóng ▁thần ▁thủ ▁đức </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inp = ['hội', 'cư', 'dân', 'chung', 'cư', 'sen', 'hồng', '-', 'chung', 'cư', 'lotus', 'sóng', 'thần', 'thủ', 'đức']\n",
    "tokenizer_explain(inp,tokenizer,split_word=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try PhoBert tokenizer. PhoBert tokenizer, unlike Envibert tokenizer, requires input to be word tokenized (using UnderTheSea library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức\n"
     ]
    }
   ],
   "source": [
    "inp = apply_word_tokenize('hội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức')\n",
    "print(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Tokenizer Explained -----\n",
      "--- Input ---\n",
      "hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 1093, 1838, 1574, 3330, 2025, 31, 1574, 2029, 4885, 8554, 25625, 7344, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens ---\n",
      "['<s>', 'hội', 'cư_dân', 'chung_cư', 'sen', 'hồng', '-', 'chung_cư', 'lo@@', 'tus', 'sóng_thần', 'thủ_@@', 'đức', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer_explain(inp,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def two_steps_tokenization_explain(inp, # Input sentence\n",
    "                                   tokenizer, # Tokenizer (preferably from HuggingFace)\n",
    "                                   split_word=False, # Is input `inp` split into list or not\n",
    "                                   content_tfms=[] # A list of text transformations\n",
    "                                  ):\n",
    "    \"Display results form each content transformation, then display results from tokenizer\"\n",
    "    print('----- Text Transformation Explained -----')\n",
    "    print('--- Raw sentence ---')\n",
    "    print(inp)\n",
    "    for tfm in content_tfms:\n",
    "        print_msg(callable_name(tfm),3)\n",
    "        inp = tfm(inp)\n",
    "        print(inp)\n",
    "    print()\n",
    "    tokenizer_explain(inp,tokenizer,split_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L41){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### two_steps_tokenization_explain\n",
       "\n",
       ">      two_steps_tokenization_explain (inp, tokenizer, split_word=False,\n",
       ">                                      content_tfms=[])\n",
       "\n",
       "Display results form each content transformation, then display results from tokenizer\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | Input sentence |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| split_word | bool | False | Is input `inp` split into list or not |\n",
       "| content_tfms | list | [] | A list of text transformations |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L41){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### two_steps_tokenization_explain\n",
       "\n",
       ">      two_steps_tokenization_explain (inp, tokenizer, split_word=False,\n",
       ">                                      content_tfms=[])\n",
       "\n",
       "Display results form each content transformation, then display results from tokenizer\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | Input sentence |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| split_word | bool | False | Is input `inp` split into list or not |\n",
       "| content_tfms | list | [] | A list of text transformations |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(two_steps_tokenization_explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load Phobert tokenizer one more time to test out this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import text_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Text Transformation Explained -----\n",
      "--- Raw sentence ---\n",
      "Hội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức\n",
      "--- text_normalize ---\n",
      "Hội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức\n",
      "--- apply_word_tokenize ---\n",
      "Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức\n",
      "\n",
      "----- Tokenizer Explained -----\n",
      "--- Input ---\n",
      "Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 792, 1838, 1574, 3330, 2025, 31, 1574, 2029, 4885, 8554, 25625, 7344, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens ---\n",
      "['<s>', 'Hội', 'cư_dân', 'chung_cư', 'sen', 'hồng', '-', 'chung_cư', 'lo@@', 'tus', 'sóng_thần', 'thủ_@@', 'đức', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inp = 'Hội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức'\n",
    "two_steps_tokenization_explain(inp,tokenizer,content_tfms=[text_normalize,apply_word_tokenize])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bit redundant, as `apply_word_tokenize` also have an option to normalize text. Let's shorten the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Text Transformation Explained -----\n",
      "--- Raw sentence ---\n",
      "Hội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức\n",
      "--- apply_word_tokenize ---\n",
      "Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức\n",
      "\n",
      "----- Tokenizer Explained -----\n",
      "--- Input ---\n",
      "Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 792, 1838, 1574, 3330, 2025, 31, 1574, 2029, 4885, 8554, 25625, 7344, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens ---\n",
      "['<s>', 'Hội', 'cư_dân', 'chung_cư', 'sen', 'hồng', '-', 'chung_cư', 'lo@@', 'tus', 'sóng_thần', 'thủ_@@', 'đức', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inp = 'Hội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức'\n",
    "two_steps_tokenization_explain(inp,tokenizer,content_tfms=[partial(apply_word_tokenize,normalize_text=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tokenize_function(examples:dict,\n",
    "                      tok,\n",
    "                      max_length=None,\n",
    "                      is_split_into_words=True):\n",
    "    if max_length is None:\n",
    "        # pad to model's default max sequence length\n",
    "        return tok(examples[\"text\"], padding=\"max_length\", truncation=True,is_split_into_words=is_split_into_words)\n",
    "    if isinstance(max_length,int) and max_length>0:\n",
    "        # pad to max length of the current batch, and start truncating at max_length\n",
    "        return tok(examples[\"text\"], padding=True, max_length=max_length,truncation=True,is_split_into_words=is_split_into_words)\n",
    "    \n",
    "    # no padding (still truncate at model's default max sequence length)\n",
    "    return tok(examples[\"text\"], truncation=True,is_split_into_words=is_split_into_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def datasetdictize_given_idxs(kv_pairs:dict, # Dictionary; keys can be content, label, metadata. Values are list each.\n",
    "                              trn_idx=None, # Training indices\n",
    "                              val_idx=None, # Validation indices\n",
    "                              tokenizer=None, # HuggingFace tokenizer\n",
    "                              is_split_into_words=False, # Is text (content) split into list or not\n",
    "                              max_length=None # pad to model's allowed max length (default is max_sequence_length)\n",
    "                             ):\n",
    "    \"Create a HuggingFace DatasetDict with given arguments\"\n",
    "    if 'text' not in kv_pairs.keys():\n",
    "        raise ValueError('Dictionary must have `text` (which contains texture contents) as key')\n",
    "    all_dataset = Dataset.from_dict(kv_pairs)\n",
    "    main_ddict = DatasetDict()\n",
    "    if trn_idx is None:\n",
    "        main_ddict['train'] = all_dataset\n",
    "    else:\n",
    "        main_ddict['train'] = all_dataset.select(trn_idx)\n",
    "\n",
    "    if val_idx is not None:  \n",
    "        main_ddict['validation'] = all_dataset.select(val_idx)\n",
    "    \n",
    "    print_msg(\"Map Tokenize Function\",20)\n",
    "    main_ddict_tokenized = main_ddict.map(partial(tokenize_function,\n",
    "                                                  tok=tokenizer,\n",
    "                                                  is_split_into_words=is_split_into_words,\n",
    "                                                  max_length=max_length),batched=True)\n",
    "    \n",
    "    return main_ddict_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L67){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### datasetdictize_given_idxs\n",
       "\n",
       ">      datasetdictize_given_idxs (kv_pairs:dict, trn_idx=None, val_idx=None,\n",
       ">                                 tokenizer=None, is_split_into_words=False,\n",
       ">                                 max_length=None)\n",
       "\n",
       "Create a HuggingFace DatasetDict with given arguments\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| kv_pairs | dict |  | Dictionary; keys can be content, label, metadata. Values are list each. |\n",
       "| trn_idx | NoneType | None | Training indices |\n",
       "| val_idx | NoneType | None | Validation indices |\n",
       "| tokenizer | NoneType | None | HuggingFace tokenizer |\n",
       "| is_split_into_words | bool | False | Is text (content) split into list or not |\n",
       "| max_length | NoneType | None | pad to model's allowed max length (default is max_sequence_length) |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L67){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### datasetdictize_given_idxs\n",
       "\n",
       ">      datasetdictize_given_idxs (kv_pairs:dict, trn_idx=None, val_idx=None,\n",
       ">                                 tokenizer=None, is_split_into_words=False,\n",
       ">                                 max_length=None)\n",
       "\n",
       "Create a HuggingFace DatasetDict with given arguments\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| kv_pairs | dict |  | Dictionary; keys can be content, label, metadata. Values are list each. |\n",
       "| trn_idx | NoneType | None | Training indices |\n",
       "| val_idx | NoneType | None | Validation indices |\n",
       "| tokenizer | NoneType | None | HuggingFace tokenizer |\n",
       "| is_split_into_words | bool | False | Is text (content) split into list or not |\n",
       "| max_length | NoneType | None | pad to model's allowed max length (default is max_sequence_length) |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(datasetdictize_given_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir=Path('./envibert_tokenizer')\n",
    "tokenizer = SourceFileLoader(\"envibert.tokenizer\", \n",
    "                             str(cache_dir/'envibert_tokenizer.py')).load_module().RobertaTokenizer(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Map Tokenize Function --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kv_pairs={\n",
    "    'text':[\n",
    "         'hội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức',\n",
    "         'This is the recommended way to make a Python package importable from anywhere',\n",
    "         'hội cần mở thẻ tín dụng tại hà nội, đà nẵng, tp. hồ chí minh',\n",
    "         \"biti's cao lãnh - đồng tháp\",\n",
    "         'chợ phòng trọ + việc làm...khu lĩnh nam - vĩnh hưng - mai động (hoàng mai)'\n",
    "          ],\n",
    "    'label': [0,1,0,0,1]\n",
    "}\n",
    "\n",
    "ddict = datasetdictize_given_idxs(kv_pairs,\n",
    "                                  trn_idx=[0,1,3],\n",
    "                                  val_idx=[2,4],\n",
    "                                  tokenizer=tokenizer,\n",
    "                                  max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 227, 1033, 191, 664, 1033, 7366, 2615, 13, 664, 1033, 671, 1355, 2294, 993, 413, 2900, 2], [0, 116, 14, 6, 3169, 270, 9, 364, 10, 23963, 5360, 15930, 2003, 51, 5906, 2, 1, 1], [0, 880, 592, 427, 162, 171, 906, 13, 122, 6553, 2, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(ddict['train']['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change max_length (which allow truncation when sentence length is higher than max_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Map Tokenize Function --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ddict = datasetdictize_given_idxs(kv_pairs,\n",
    "                                  trn_idx=[0,1,3],\n",
    "                                  val_idx=[2,4],\n",
    "                                  tokenizer=tokenizer,\n",
    "                                  max_length=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 227, 1033, 191, 2], [0, 116, 14, 6, 2], [0, 880, 592, 427, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(ddict['train']['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allow full dataset to be mapped to DatasetDict if we omit trn_idx argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Map Tokenize Function --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ddict = datasetdictize_given_idxs(kv_pairs,\n",
    "                                  tokenizer=tokenizer,\n",
    "                                  max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 227, 1033, 191, 664, 1033, 7366, 2615, 13, 664, 1033, 671, 1355, 2294, 993, 413, 2900, 2, 1, 1, 1, 1, 1, 1, 1], [0, 116, 14, 6, 3169, 270, 9, 364, 10, 23963, 5360, 15930, 2003, 51, 5906, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 227, 256, 778, 2600, 1074, 144, 76, 5489, 613, 57339, 4820, 27666, 57339, 21422, 244, 872, 635, 841, 2, 1, 1, 1, 1, 1], [0, 880, 592, 427, 162, 171, 906, 13, 122, 6553, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 2299, 315, 5995, 1349, 99, 83, 55025, 244, 6356, 1114, 1213, 1163, 13, 8233, 11051, 13, 3335, 109, 28, 11695, 13377, 3335, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(ddict['train']['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class TextDataMain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TextDataMain():\n",
    "    def __init__(self,\n",
    "                 df: pd.DataFrame, # The main dataframe\n",
    "                 main_content:str, # Name of the text column\n",
    "                 metadatas=[], # Names of the metadata columns\n",
    "                 label_names=None, # Names of the label (dependent variable) columns\n",
    "                 class_names_predefined=None, # (Optional) List of names associated with the labels (same index order)\n",
    "                 val_ratio:list|float|None=0.2, # Ratio of data for validation set. If given a list, validation set will be chosen based on indices in this list\n",
    "                 split_cols:list|str=None, # Column(s) needed to do stratified shuffle split\n",
    "                 content_tfms=[], # A list of text transformations\n",
    "                 aug_tfms=[], # A list of text augmentations\n",
    "                 process_metadatas=True, # Whether to do simmple text processing on the chosen metadatas\n",
    "                 seed=None, # Random seed\n",
    "                 cols_to_keep=None, # Columns to keep after all processings\n",
    "                 shuffle_trn=True # Whether to shuffle the train set\n",
    "                ):\n",
    "        self.df = df.copy()\n",
    "        self.main_content = main_content\n",
    "        self.metadatas = metadatas\n",
    "        self.label_names = label_names\n",
    "        self.label_lists = class_names_predefined\n",
    "        self.content_tfms = content_tfms\n",
    "        self.aug_tfms = aug_tfms\n",
    "        self.process_metadatas = process_metadatas\n",
    "        self.val_ratio=val_ratio\n",
    "        self.split_cols=split_cols\n",
    "        self.seed = seed\n",
    "        self.cols_to_keep = cols_to_keep\n",
    "        self.shuffle_trn=shuffle_trn  \n",
    "        self._main_called=False\n",
    "        self.is_multilabel=False\n",
    "        self.is_multihead=False\n",
    "        check_input_validation(self.df)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_csv(cls,path,return_df=False,encoding='utf-8-sig',**kwargs):\n",
    "        df = pd.read_csv(path,encoding=encoding,engine='pyarrow')\n",
    "        tdm = TextDataMain(df,main_content=None) if return_df else TextDataMain(df,**kwargs)\n",
    "        if return_df:\n",
    "            return df\n",
    "        return tdm\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pickle(cls,\n",
    "                    fname, # Name of the pickle file\n",
    "                    parent='pickle_files' # Parent folder\n",
    "                   ):\n",
    "        return load_pickle(fname,parent=parent)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_gsheet(cls,gs_id,return_df=False,**kwargs):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def save_as_pickles(self,\n",
    "                        fname, # Name of the pickle file\n",
    "                        parent='pickle_files', # Parent folder\n",
    "                        drop_data_attributes=False # Whether to drop all large-size data attributes\n",
    "                       ):\n",
    "        if drop_data_attributes:\n",
    "            if hasattr(self, 'df'):\n",
    "                del self.df\n",
    "            if hasattr(self, 'main_ddict'):\n",
    "                del self.main_ddict\n",
    "        save_to_pickle(self,fname,parent=parent)\n",
    "\n",
    "        \n",
    "    def _check_validation_leaking(self,trn_idxs,val_idxs):\n",
    "        if self.val_ratio is None:\n",
    "            return trn_idxs,None\n",
    "        \n",
    "        df_trn = self.df.loc[trn_idxs]\n",
    "        df_val = self.df.loc[val_idxs]\n",
    "        \n",
    "        #sanity check\n",
    "        assert df_trn.shape[0]+df_val.shape[0]==self.df.shape[0],\"Train + Validation != Total Data\"\n",
    "\n",
    "        \n",
    "        print(f'Previous Validation Percentage: {round(100*len(val_idxs)/self.df.shape[0],3)}%')\n",
    "        val_content_series = check_text_leaking(df_trn[self.main_content],df_val[self.main_content])\n",
    "        val_idxs2 = val_content_series.index.values\n",
    "        trn_idxs2 = self.df[~self.df.index.isin(val_idxs2)].index.values\n",
    "        print(f'Current Validation Percentage: {round(100*len(val_idxs2)/self.df.shape[0],3)}%')\n",
    "        if len(val_idxs2)!=len(val_idxs):\n",
    "            return trn_idxs2,val_idxs2\n",
    "        return trn_idxs,val_idxs\n",
    "    \n",
    "    def _train_test_split(self):\n",
    "        print_msg('Train Test Split',20)\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        if self.val_ratio is None: # no train/val split\n",
    "            trn_idxs = rng.permutation(self.df.shape[0])\n",
    "            return trn_idxs,None\n",
    "        if isinstance(self.val_ratio,list) or isinstance(self.val_ratio,np.ndarray):\n",
    "            val_idxs = np.array(self.val_ratio)\n",
    "            trn_idxs = np.array(set(self.df.index.values) - set(self.val_ratio))\n",
    "            return trn_idxs,val_idxs\n",
    "        if isinstance(self.val_ratio,float) and self.split_cols is None:\n",
    "            _idxs = rng.permutation(self.df.shape[0])\n",
    "            _cutoff = int(self.val_ratio*self.df.shape[0]) \n",
    "            val_idxs = _idxs[:_cutoff]\n",
    "            trn_idxs = _idxs[_cutoff:]\n",
    "            return trn_idxs,val_idxs\n",
    "        \n",
    "        self.split_cols = val2iterable(self.split_cols)\n",
    "        if self.is_multilabel and self.label_names[0] in self.split_cols:\n",
    "            raise ValueError('For MultiLabel classification, you cannot choose the label as your shuffle-split column')\n",
    "        \n",
    "        if len(self.split_cols)>0:\n",
    "            _y = self.df[self.split_cols[0]]\n",
    "            if len(self.split_cols)>1:\n",
    "                for c in self.split_cols[1:]:\n",
    "                    _y= _y.astype(str) + '_' + self.df[c].astype(str)\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=self.val_ratio, \n",
    "                                         random_state=self.seed)\n",
    "            trn_idxs,val_idxs = list(sss.split(self.df,_y))[0]\n",
    "            return trn_idxs,val_idxs\n",
    "        \n",
    "        raise ValueError('No valid keyword arguments for train validation split!')\n",
    "\n",
    "                         \n",
    "    def _encode_labels(self):\n",
    "        print_msg('Label Encoding')\n",
    "        if self.label_names is None: \n",
    "            raise ValueError('Missing label columns!')\n",
    "        self.label_names = val2iterable(self.label_names)\n",
    "        if len(self.label_names)>1:\n",
    "            self.is_multihead=True\n",
    "        \n",
    "        if self.label_lists is not None and not isinstance(self.label_lists[0],list):\n",
    "            self.label_lists = [self.label_lists]\n",
    "        \n",
    "        if isinstance(self.df[self.label_names[0]].iloc[0],list):\n",
    "            # This is multi-label. Ignore self.label_names[1:]\n",
    "            self.label_names = [self.label_names[0]]\n",
    "            self.is_multihead=False\n",
    "            self.is_multilabel=True\n",
    "            \n",
    "        encoder_classes=[]\n",
    "        if not self.is_multilabel:\n",
    "            for idx,l in enumerate(self.label_names):\n",
    "                if self.label_lists is None:\n",
    "                    train_label = self.df[l].values\n",
    "                    l_encoder = LabelEncoder()\n",
    "                    self.df[l] = l_encoder.fit_transform(train_label)\n",
    "                    encoder_classes.append(list(l_encoder.classes_))\n",
    "                else:\n",
    "                    l_classes = sorted(list(self.label_lists[idx]))\n",
    "                    label2idx = {v:i for i,v in enumerate(l_classes)}\n",
    "                    self.df[l] = self.df[l].map(label2idx).values\n",
    "                    encoder_classes.append(l_classes)\n",
    "        else:\n",
    "            # For MultiLabel, we only save the encoder classes without transforming the label itself to one-hot (or actually, few-hot)\n",
    "            if self.label_lists is None:\n",
    "                l_encoder = MultiLabelBinarizer()\n",
    "                _ = l_encoder.fit(self.df[self.label_names[0]])\n",
    "                encoder_classes.append(list(l_encoder.classes_))\n",
    "            else:\n",
    "                l_classes = sorted(list(self.label_lists[0]))\n",
    "                encoder_classes.append(l_classes)\n",
    "                \n",
    "        self.label_lists = encoder_classes\n",
    "            \n",
    "    def _process_metadatas(self,df,override_dict=True):\n",
    "        print_msg('Metadata Simple Processing & Concatenating to Main Content')\n",
    "        self.metadatas = val2iterable(self.metadatas)\n",
    "            \n",
    "        for s in self.metadatas:\n",
    "            if self.process_metadatas:\n",
    "                # just strip and lowercase\n",
    "                df[s] = df[s].astype(str).str.strip().str.lower()\n",
    "            # simple concatenation with '. '\n",
    "            df[self.main_content] = df[s] + ' - ' + df[self.main_content]\n",
    "                \n",
    "        if override_dict:        \n",
    "            self.metadata_dict={}\n",
    "            for s in self.metadatas:\n",
    "                self.metadata_dict[s]=sorted(set(df[s].values))\n",
    "        return df\n",
    "    \n",
    "    def _simplify_df(self):\n",
    "        if self.cols_to_keep is None:\n",
    "            self.cols_to_keep= [self.main_content] + self.metadatas + self.label_names\n",
    "        self.df = self.df[self.cols_to_keep].copy()\n",
    "    \n",
    "    def _do_transformation(self,df):\n",
    "        print_msg('Text Transformation',20)\n",
    "        for tfm in self.content_tfms:\n",
    "            print_msg(callable_name(tfm))\n",
    "            df[self.main_content] = [tfm(s) for s in tqdm(df[self.main_content].values)]\n",
    "        return df\n",
    "    \n",
    "    def _do_augmentation(self,df_trn_org):\n",
    "        df_trn_all = df_trn_org.copy()\n",
    "        print_msg('Text Augmentation',20)\n",
    "        print(f'Train data size before augmentation: {len(df_trn_all)}')\n",
    "        for tfm in self.aug_tfms:\n",
    "            print_msg(callable_name(tfm))\n",
    "            if tfm.keywords['apply_to_all']:\n",
    "                new_content,new_others = tfm(content=df_trn_all[self.main_content].values,others=df_trn_all.iloc[:,1:])\n",
    "            else:\n",
    "                new_content,new_others = tfm(content=df_trn_org[self.main_content].values,others=df_trn_org.iloc[:,1:])\n",
    "            \n",
    "            # add axis to np array in order to do concatenation\n",
    "            if len(new_content.shape)==1:\n",
    "                new_content = new_content[:,None]\n",
    "            if len(new_others.values.shape)==1:\n",
    "                new_others = new_others.values[:,None]\n",
    "                \n",
    "            df_tmp = pd.DataFrame(np.concatenate((new_content,new_others.values),axis=1),columns=df_trn_org.columns.values)\n",
    "            df_trn_all = pd.concat((df_trn_all,df_tmp),axis=0).reset_index(drop=True)\n",
    "            print(f'Train data size after THIS augmentation: {len(df_trn_all)}')       \n",
    "        print(f'Train data size after ALL augmentation: {len(df_trn_all)}')\n",
    "        return df_trn_all\n",
    "    \n",
    "    def _main_text_processing(self):\n",
    "        print_msg('Start Main Text Processing',20)\n",
    "        \n",
    "        # Process metadatas\n",
    "        self.df = self._process_metadatas(self.df)\n",
    "        \n",
    "        # Process labels\n",
    "        self._encode_labels()\n",
    "        \n",
    "        # Content transformation\n",
    "        self.df = self._do_transformation(self.df)\n",
    "        \n",
    "        # Train Test Split\n",
    "        trn_idxs,val_idxs = self._train_test_split()\n",
    "        self._simplify_df()\n",
    "        trn_idxs,val_idxs = self._check_validation_leaking(trn_idxs,val_idxs)\n",
    "        if self.val_ratio is not None:\n",
    "            df_val = self.df.loc[val_idxs].reset_index(drop=True)\n",
    "        \n",
    "        # Augmentation\n",
    "        df_trn_org = self.df.loc[trn_idxs].reset_index(drop=True)\n",
    "        df_trn_all = self._do_augmentation(df_trn_org)\n",
    "        df_trn_all['is_valid']=False\n",
    "        \n",
    "        # Shuffle train\n",
    "        if self.shuffle_trn:\n",
    "            df_trn_all = df_trn_all.sample(frac=1.,random_state=self.seed)\n",
    "            \n",
    "        # Combine augmented train and val\n",
    "        if self.val_ratio is not None:\n",
    "            df_val['is_valid']=True\n",
    "            df_trn_all = pd.concat((df_trn_all,df_val),axis=0)\n",
    "        \n",
    "        self._main_called=True\n",
    "        self.df = df_trn_all.reset_index(drop=True)        \n",
    "    \n",
    "    def set_data_collator(self,data_collator):\n",
    "        self.data_collator = data_collator\n",
    "        \n",
    "    def tokenizer_explain_single(self,tokenizer):\n",
    "        inp = self.df[~self.df['is_valid']][self.main_content].sample(1).values[0]\n",
    "        tokenizer_explain(inp,tokenizer)\n",
    "        \n",
    "    def to_df(self): \n",
    "        \"To execute all the defined processings and return a dataframe\"\n",
    "        if not self._main_called:\n",
    "            self._main_text_processing()\n",
    "        return self.df\n",
    "       \n",
    "    def save_train_data_after_processing(self,output_path,encoding='utf-8-sig'):\n",
    "        if not self._main_called:\n",
    "            print_msg('WARNING')\n",
    "            print('Please process training data (using to_df or to_datasetdict)')\n",
    "            return\n",
    "        self.df.to_csv(Path(output_path),encoding=encoding,index=False)\n",
    "    \n",
    "    def to_datasetdict(self,\n",
    "                       tokenizer, # Tokenizer (preferably from HuggingFace)\n",
    "                       is_split_into_words=False, # Is text split into list or not\n",
    "                       max_length=None, # pad to model's allowed max length (default is max_sequence_length)\n",
    "                       trn_ratio=1., # Portion of training data to be converted to datasetdict. Useful for sample experiments\n",
    "                       seed=42 # Random seed\n",
    "                      ):\n",
    "        if not self._main_called:\n",
    "            self._main_text_processing()\n",
    "        val_idx = self.df[self.df['is_valid']].index.values if self.val_ratio is not None else None\n",
    "        trn_idx = self.df[~self.df['is_valid']].index.values\n",
    "        if trn_ratio<1. and trn_ratio>0.:\n",
    "            rng = np.random.default_rng(self.seed)\n",
    "            _idxs = rng.permutation(len(trn_idx))\n",
    "            _cutoff = int(trn_ratio*len(trn_idx)) \n",
    "            trn_idx = _idxs[:_cutoff]\n",
    "            \n",
    "        _label = self.df[self.label_names].values.tolist()\n",
    "        if not self.is_multilabel:\n",
    "            if len(self.label_names)==1:\n",
    "                _label = np.array(_label).flatten().tolist() # (n,)\n",
    "        else:\n",
    "            # For MultiLabel, this is where the actual label transformation happens\n",
    "            mlb = MultiLabelBinarizer(classes=self.label_lists[0])\n",
    "            _label = self.df[self.label_names[0]].values.tolist()\n",
    "            _label = mlb.fit_transform(_label).tolist() # few-hotted\n",
    "        \n",
    "        kv_pairs = {'text':self.df[self.main_content].tolist(),\n",
    "                    'label':_label,\n",
    "                   }\n",
    "        for c in self.cols_to_keep:\n",
    "            if c not in self.label_names+[self.main_content]: kv_pairs[c] = self.df[c].tolist()\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_split_into_words= is_split_into_words\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        ddict = datasetdictize_given_idxs(kv_pairs,trn_idx,val_idx,self.tokenizer,\n",
    "                                         is_split_into_words=is_split_into_words,max_length=max_length)\n",
    "        self.main_ddict = ddict\n",
    "        return ddict\n",
    "    \n",
    "    def get_test_datasetdict_from_csv(self,path,encoding='utf-8-sig'):\n",
    "        df_test = pd.read_csv(path,encoding=encoding,engine='pyarrow')\n",
    "        return self.get_test_datasetdict_from_df(df_test)\n",
    "\n",
    "    def get_test_datasetdict_from_dict(self,content):\n",
    "        if len(self.metadatas)!=0 and not isinstance(content,dict):\n",
    "            raise ValueError(f'There is/are metadatas in the preprocessing step. Please include a dictionary including these keys for metadatas: {self.metadatas}, and texture content: {self.main_content}')\n",
    "            \n",
    "        _dic = {self.main_content:[content]} if isinstance(content,str) else content\n",
    "        for k in _dic.keys():\n",
    "            _dic[k] = val2iterable(_dic[k])\n",
    "        \n",
    "        df_test = pd.DataFrame.from_dict(_dic)\n",
    "        return self.get_test_datasetdict_from_df(df_test)\n",
    "    \n",
    "    def get_test_datasetdict_from_df(self,df_test):\n",
    "        print_msg('Getting Test Set',20)\n",
    "        check_input_validation(df_test)\n",
    "        \n",
    "        cols_to_keep = [c for c in self.cols_to_keep if c not in self.label_names]\n",
    "        df_test = df_test[cols_to_keep].copy()\n",
    "        \n",
    "        print_msg('Start Test Set Transformation',20)\n",
    "        df_test = self._process_metadatas(df_test,override_dict=False)\n",
    "        df_test = self._do_transformation(df_test)\n",
    "        \n",
    "        if hasattr(self,'df'):\n",
    "            print_msg('Test Leak Checking',20)\n",
    "            _ = check_text_leaking(self.df[self.main_content],df_test[self.main_content])\n",
    "        \n",
    "        print_msg('Construct DatasetDict',20)\n",
    "        test_text = df_test[self.main_content].values\n",
    "        \n",
    "        kv_pairs ={'text':test_text}\n",
    "        for c in self.cols_to_keep:\n",
    "            if c not in self.label_names+[self.main_content]: kv_pairs[c] = df_test[c].tolist()\n",
    "        \n",
    "        test_dataset = Dataset.from_dict(kv_pairs)\n",
    "        test_ddict = DatasetDict()\n",
    "        test_ddict['test'] = test_dataset\n",
    "        test_ddict_tokenized = test_ddict.map(partial(tokenize_function,tok=self.tokenizer,\n",
    "                                                      is_split_into_words=self.is_split_into_words,\n",
    "                                                      max_length=self.max_length),batched=True)\n",
    "        \n",
    "        return test_ddict_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L96){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataMain\n",
       "\n",
       ">      TextDataMain (df:pandas.core.frame.DataFrame, main_content:str,\n",
       ">                    metadatas=[], label_names=None,\n",
       ">                    class_names_predefined=None, val_ratio:list|float|None=0.2,\n",
       ">                    split_cols:list|str=None, content_tfms=[], aug_tfms=[],\n",
       ">                    process_metadatas=True, seed=None, cols_to_keep=None,\n",
       ">                    shuffle_trn=True)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | pd.DataFrame |  | The main dataframe |\n",
       "| main_content | str |  | Name of the text column |\n",
       "| metadatas | list | [] | Names of the metadata columns |\n",
       "| label_names | NoneType | None | Names of the label (dependent variable) columns |\n",
       "| class_names_predefined | NoneType | None | (Optional) List of names associated with the labels (same index order) |\n",
       "| val_ratio | list \\| float \\| None | 0.2 | Ratio of data for validation set. If given a list, validation set will be chosen based on indices in this list |\n",
       "| split_cols | list \\| str | None | Column(s) needed to do stratified shuffle split |\n",
       "| content_tfms | list | [] | A list of text transformations |\n",
       "| aug_tfms | list | [] | A list of text augmentations |\n",
       "| process_metadatas | bool | True | Whether to do simmple text processing on the chosen metadatas |\n",
       "| seed | NoneType | None | Random seed |\n",
       "| cols_to_keep | NoneType | None | Columns to keep after all processings |\n",
       "| shuffle_trn | bool | True | Whether to shuffle the train set |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L96){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataMain\n",
       "\n",
       ">      TextDataMain (df:pandas.core.frame.DataFrame, main_content:str,\n",
       ">                    metadatas=[], label_names=None,\n",
       ">                    class_names_predefined=None, val_ratio:list|float|None=0.2,\n",
       ">                    split_cols:list|str=None, content_tfms=[], aug_tfms=[],\n",
       ">                    process_metadatas=True, seed=None, cols_to_keep=None,\n",
       ">                    shuffle_trn=True)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | pd.DataFrame |  | The main dataframe |\n",
       "| main_content | str |  | Name of the text column |\n",
       "| metadatas | list | [] | Names of the metadata columns |\n",
       "| label_names | NoneType | None | Names of the label (dependent variable) columns |\n",
       "| class_names_predefined | NoneType | None | (Optional) List of names associated with the labels (same index order) |\n",
       "| val_ratio | list \\| float \\| None | 0.2 | Ratio of data for validation set. If given a list, validation set will be chosen based on indices in this list |\n",
       "| split_cols | list \\| str | None | Column(s) needed to do stratified shuffle split |\n",
       "| content_tfms | list | [] | A list of text transformations |\n",
       "| aug_tfms | list | [] | A list of text augmentations |\n",
       "| process_metadatas | bool | True | Whether to do simmple text processing on the chosen metadatas |\n",
       "| seed | NoneType | None | Random seed |\n",
       "| cols_to_keep | NoneType | None | Columns to keep after all processings |\n",
       "| shuffle_trn | bool | True | Whether to shuffle the train set |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataMain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start a step-by-step walkthrough on how to use this class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('sample_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructor/ Class Method calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just want to get the dataframe from the csv path, set ```return_df=True```. You still have the input validation precheck functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Input Validation Precheck -----\n",
      "DataFrame contains duplicated values!\n",
      "-----> Number of duplications: 16 rows\n"
     ]
    }
   ],
   "source": [
    "df = TextDataMain.from_csv(DATA_PATH/'sample_large.csv',\n",
    "                            return_df=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```Input Validation Precheck``` will check for missing values and duplicate rows in the csv file. Since there's no such thing in our sample dataset, we won't see anything here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Content</th>\n",
       "      <th>L1</th>\n",
       "      <th>L2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>Non Owned</td>\n",
       "      <td>Xinh quá mấy chị,e còn dư 50 sét thôi! #35k 1s...</td>\n",
       "      <td>Others</td>\n",
       "      <td>Cannot defined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1318</th>\n",
       "      <td>iOS</td>\n",
       "      <td>k sử dụng dc mã giảm giá, nhập vào rồi kêu lỗi...</td>\n",
       "      <td>Feature</td>\n",
       "      <td>Apply Voucher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852</th>\n",
       "      <td>Google Play</td>\n",
       "      <td>Cách shopee giải quyết vấn đề quá là TỆ</td>\n",
       "      <td>Services</td>\n",
       "      <td>Contact Agent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>Google Play</td>\n",
       "      <td>Lỗi không vào được ứng dụng, nhà điều hành có ...</td>\n",
       "      <td>Feature</td>\n",
       "      <td>App performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>iOS</td>\n",
       "      <td>Cảm thấy tệ</td>\n",
       "      <td>Others</td>\n",
       "      <td>Cannot defined</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Source                                            Content   \n",
       "556     Non Owned  Xinh quá mấy chị,e còn dư 50 sét thôi! #35k 1s...  \\\n",
       "1318          iOS  k sử dụng dc mã giảm giá, nhập vào rồi kêu lỗi...   \n",
       "1852  Google Play            Cách shopee giải quyết vấn đề quá là TỆ   \n",
       "901   Google Play  Lỗi không vào được ứng dụng, nhà điều hành có ...   \n",
       "64            iOS                                        Cảm thấy tệ   \n",
       "\n",
       "            L1               L2  \n",
       "556     Others   Cannot defined  \n",
       "1318   Feature    Apply Voucher  \n",
       "1852  Services    Contact Agent  \n",
       "901    Feature  App performance  \n",
       "64      Others   Cannot defined  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Source\n",
       "Google Play    1434\n",
       "Non Owned       499\n",
       "Owned           139\n",
       "iOS             124\n",
       "HC search        73\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Source.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you are happy with this dataframe (after you did some others preprocessing), then you can start creating a `TextDataMain` object\n",
    "\n",
    "For this dataframe, I want to \n",
    "- Build a text classification model, with main text in ```Content``` column, metadatas is ```Source```, and the label is ```L1```\n",
    "- Perform `apply_word_tokenize` with text normalization (this is \"text transformation\")\n",
    "- For augmentation, I want to perform: Oversampling the ```Owned, Non Owned and HC Search``` from column ```Source```, then add some the Vietnamese no-accent text. Note that all of these are called \"text augmentation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define these transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For Text Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awt_tfm = partial(apply_word_tokenize,normalize_text=True)\n",
    "# You can also set a __name__ to your augmentation function. \n",
    "# This way you will have meaningful text messages as outputs\n",
    "awt_tfm.__name__='UTS Word Tokenization With Normalization'\n",
    "\n",
    "txt_tfms=[awt_tfm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For Text Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply_to_all means I will apply this augmentation to all the data \n",
    "# (including the original data and the augmented data/transformed data from previous augmentation/transformation)\n",
    "over_nonown_tfm = partial(sampling_with_condition,query='Source==\"non owned\"',frac=0.5,seed=42,apply_to_all=False)\n",
    "over_nonown_tfm.__name__ = 'Oversampling Non Owned'\n",
    "\n",
    "over_own_tfm = partial(sampling_with_condition,query='Source==\"owned\"',frac=2,seed=42,apply_to_all=False)\n",
    "over_own_tfm.__name__ = 'Oversampling Owned'\n",
    "\n",
    "over_hc_tfm = partial(sampling_with_condition,query='Source==\"hc search\"',frac=2.5,seed=42,apply_to_all=False)\n",
    "over_hc_tfm.__name__ = 'Oversampling HC search'\n",
    "\n",
    "remove_accent_tfm = partial(remove_vnmese_accent,frac=1,seed=42,apply_to_all=True)\n",
    "remove_accent_tfm.__name__ = 'Add No-Accent Text'\n",
    "\n",
    "aug_tfms = [over_nonown_tfm,over_own_tfm,over_hc_tfm,remove_accent_tfm]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Input Validation Precheck -----\n",
      "DataFrame contains duplicated values!\n",
      "-----> Number of duplications: 16 rows\n"
     ]
    }
   ],
   "source": [
    "tdm = TextDataMain(df,\n",
    "                    main_content='Content',\n",
    "                    metadatas='Source', # You can put a list of multiple metadatas\n",
    "                    label_names='L1', # You can put a list of multiple labels\n",
    "                    val_ratio=0.2,\n",
    "                    split_cols='L1', # You can even put a list of multiple columns to be used for validation splitting\n",
    "                    content_tfms = txt_tfms, # You can add multiple content transformation functions ...\n",
    "                    aug_tfms = aug_tfms, # ... as well as augmentation functions\n",
    "                    process_metadatas=True,\n",
    "                    seed=42,\n",
    "                    shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to directly create a ```TextDataMain``` object from our csv file, we can instead use this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Input Validation Precheck -----\n",
      "DataFrame contains duplicated values!\n",
      "-----> Number of duplications: 16 rows\n"
     ]
    }
   ],
   "source": [
    "tdm = TextDataMain.from_csv(DATA_PATH/'sample_large.csv',\n",
    "                            return_df=False,\n",
    "                            main_content='Content',\n",
    "                            metadatas='Source',\n",
    "                            label_names='L1',\n",
    "                            val_ratio=0.2,\n",
    "                            split_cols='L1',\n",
    "                            content_tfms = txt_tfms,\n",
    "                            aug_tfms = aug_tfms,\n",
    "                            process_metadatas=True,\n",
    "                            seed=42,\n",
    "                            shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L354){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataMain.to_df\n",
       "\n",
       ">      TextDataMain.to_df ()\n",
       "\n",
       "To execute all the defined processings and return a dataframe"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L354){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataMain.to_df\n",
       "\n",
       ">      TextDataMain.to_df ()\n",
       "\n",
       "To execute all the defined processings and return a dataframe"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataMain.to_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all the previous constructor calls do not do any heavy processing yet.\n",
    "\n",
    "To actually run all the processes, one can call `TextDataMain.to_df()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "----- Label Encoding -----\n",
      "-------------------- Text Transformation --------------------\n",
      "----- UTS Word Tokenization With Normalization -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2269/2269 [00:03<00:00, 599.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Train Test Split --------------------\n",
      "Previous Validation Percentage: 20.009%\n",
      "- Before leak check\n",
      "Size: 454\n",
      "- After leak check\n",
      "Size: 447\n",
      "- Number of rows leaked: 7, or 1.54% of the original validation (or test) data\n",
      "Current Validation Percentage: 19.7%\n",
      "-------------------- Text Augmentation --------------------\n",
      "Train data size before augmentation: 1822\n",
      "----- Oversampling Non Owned -----\n",
      "Train data size after THIS augmentation: 2020\n",
      "----- Oversampling Owned -----\n",
      "Train data size after THIS augmentation: 2248\n",
      "----- Oversampling HC search -----\n",
      "Train data size after THIS augmentation: 2390\n",
      "----- Add No-Accent Text -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2390/2390 [00:00<00:00, 19530.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size after THIS augmentation: 4780\n",
      "Train data size after ALL augmentation: 4780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_processed = tdm.to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice this?\n",
    "```\n",
    "Previous Validation Percentage: 20.0%\n",
    "- Before leak check\n",
    "Size: 14\n",
    "- After leak check\n",
    "Size: 14\n",
    "- Number of rows leaked: 0, or 0.00% of the original validation (or test) data\n",
    "Current Validation Percentage: 20.0%\n",
    "```\n",
    "After performing train/test split, the ```TextDataMain``` object also perform a \"leak check\": After `text_transformation` is performed, it will compare the text from ```Content``` value in the validation set to the ```Content``` text in the train set. Any duplications (texts that belong to both set) will be removed from validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Source</th>\n",
       "      <th>L1</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5090</th>\n",
       "      <td>non owned - https://shopee.vn/maybeaty like và...</td>\n",
       "      <td>non owned</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2280</th>\n",
       "      <td>owned -_Địa chị này dám giao ko a . 😂</td>\n",
       "      <td>owned</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>google play - ble</td>\n",
       "      <td>google play</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3743</th>\n",
       "      <td>google play - Ko dừng tự_động cập_nhật được</td>\n",
       "      <td>google play</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4624</th>\n",
       "      <td>owned - Như này là sao ạ</td>\n",
       "      <td>owned</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Content       Source L1   \n",
       "5090  non owned - https://shopee.vn/maybeaty like và...    non owned  5  \\\n",
       "2280              owned -_Địa chị này dám giao ko a . 😂        owned  1   \n",
       "1258                                  google play - ble  google play  5   \n",
       "3743        google play - Ko dừng tự_động cập_nhật được  google play  3   \n",
       "4624                           owned - Như này là sao ạ        owned  5   \n",
       "\n",
       "      is_valid  \n",
       "5090      True  \n",
       "2280     False  \n",
       "1258     False  \n",
       "3743     False  \n",
       "4624     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, since we have metadatas, the metadatas is concatenated to the front of the texture content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['owned - 🚀 *_* ĐÓN_CHÀO ƯU_ĐÃI VÀNG VÀO THỨ_BA VỚI PHIÊN CHỢ VOUCHER_*_*_🚀_*_🎁 Miễn_phí tối_đa 50K cho 1 giờ khi chọn dịch_vụ dọn_dẹp nhà từ bTaskee *_🤗_Giảm ngay 20 % gói học Toán 12 tháng từ VioEdu_*_🤑_ShopBack tặng ngay 25K tiền thưởng cho đơn hàng từ 150K_*_😍_Giảm 70K cho hóa đơn từ 300K , áp_dụng cho tất_cả dịch_vụ làm đẹp tại Lamia 🎉 ️ 🎉_Chỉ từ 1000 xu , SỐ_LƯỢNG CÓ_HẠN ➡_[_http://shopee.vn/ShopeeDoiXu ] ( http://shopee.vn/ShopeeDoiXu ) ___________________________💥_10.10_SIÊU_SALE CHÍNH HÃNG - VẪN CÒN SALE_🎊_▶_️ https://shopee.vn/1010-Sieu-Sale-Chinh-Hang Săn thêm ưu_đãi Xtra duy_nhất 11.10 : 🎁 Miễn_phí vận_chuyển 0 Đ_🎁_Thương_hiệu hoàn xu tới 50 % 🎁 Thu_thập voucher , nhận đến 1.2 triệu 🎁 4 Khung giờ săn sale đậm : 0H - 9H - 12H - 21H_#_Shopee1010SieuSaleChinhHang',\n",
       "       'ios - Dat_hang cu bi tu hoan hang ve rat la buc :)',\n",
       "       'owned - Các bạn từng mua gì đáng đồng_tiền nhất trên Sốp_Pi ?',\n",
       "       'google play - Được',\n",
       "       'google play - Chon thanh_toan ma cu the ghi no roi chuyen_khoan , bay di dau cai thanh_toan khi nhan hang roi'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed.Content.sample(5).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a new dataframe with only the necessary columns (the processed text column, metadatas, label, and ```is_valid``` which tells you which row belongs to the validation set). Notice that our class has also encode our label for us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our TextDataMain object also stores other useful attributes, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Source</th>\n",
       "      <th>L1</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hc search - làm_sao để hết lỗi m02</td>\n",
       "      <td>hc search</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hc search - xin chao</td>\n",
       "      <td>hc search</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>owned - 8   NGAY DUA DON , TRUNG VOUCHER 8 THA...</td>\n",
       "      <td>owned</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google play - 🤬_😡_🤬_😡</td>\n",
       "      <td>google play</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hc search - mua hàng quốc_tế như thế_nào</td>\n",
       "      <td>hc search</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content       Source L1  is_valid\n",
       "0                 hc search - làm_sao để hết lỗi m02    hc search  3     False\n",
       "1                               hc search - xin chao    hc search  5     False\n",
       "2  owned - 8   NGAY DUA DON , TRUNG VOUCHER 8 THA...        owned  1     False\n",
       "3                              google play - 🤬_😡_🤬_😡  google play  5     False\n",
       "4           hc search - mua hàng quốc_tế như thế_nào    hc search  4     False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The entire processed dataframe, similar to the df_processed above\n",
    "tdm.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Buyer complained seller',\n",
       "  'Commercial',\n",
       "  'Delivery',\n",
       "  'Feature',\n",
       "  'Order/Item',\n",
       "  'Others',\n",
       "  'Payment',\n",
       "  'Return/Refund',\n",
       "  'Services',\n",
       "  'Shopee account']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class names (This will be a list of list, as this class can handle multi-label classification)\n",
    "tdm.label_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Source': ['google play', 'hc search', 'ios', 'non owned', 'owned']}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a dictionary storing unique value for each provided metadata\n",
    "tdm.metadata_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to see how a HuggingFace's tokenizer work on our processed text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Tokenizer Explained -----\n",
      "--- Input ---\n",
      "google play - Mang ro manh nhung lai dang nhap khong duoc : <\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 38970, 14015, 31, 8283, 7135, 24136, 12088, 5135, 19058, 2008, 18679, 3014, 2662, 6190, 22899, 27, 8452, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens ---\n",
      "['<s>', 'google', 'play', '-', 'Mang', 'ro', 'manh', 'nhung', 'lai', 'dang', 'nh@@', 'ap', 'kh@@', 'ong', 'du@@', 'oc', ':', '<', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> google play - Mang ro manh nhung lai dang nhap khong duoc : < </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this will pick a random text from train set to show\n",
    "tdm.tokenizer_explain_single(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing this, we can see how the tokenizer interact with our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L367){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataMain.to_datasetdict\n",
       "\n",
       ">      TextDataMain.to_datasetdict (tokenizer, is_split_into_words=False,\n",
       ">                                   max_length=None, trn_ratio=1.0, seed=42)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| is_split_into_words | bool | False | Is text split into list or not |\n",
       "| max_length | NoneType | None | pad to model's allowed max length (default is max_sequence_length) |\n",
       "| trn_ratio | float | 1.0 | Portion of training data to be converted to datasetdict. Useful for sample experiments |\n",
       "| seed | int | 42 | Random seed |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L367){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataMain.to_datasetdict\n",
       "\n",
       ">      TextDataMain.to_datasetdict (tokenizer, is_split_into_words=False,\n",
       ">                                   max_length=None, trn_ratio=1.0, seed=42)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| is_split_into_words | bool | False | Is text split into list or not |\n",
       "| max_length | NoneType | None | pad to model's allowed max length (default is max_sequence_length) |\n",
       "| trn_ratio | float | 1.0 | Portion of training data to be converted to datasetdict. Useful for sample experiments |\n",
       "| seed | int | 42 | Random seed |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataMain.to_datasetdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need to convert our data to HuggingFace's DatasetDict format in order to utilize HuggingFace's model well, we can directly export datasetdict using `TextDataMain.to_datasetdict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Map Tokenize Function --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4780 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ddict_sample = tdm.to_datasetdict(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 4780\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 447\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hc search - làm_sao để hết lỗi m02'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict_sample['train']['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1340, 1894, 51139, 31, 2407, 24, 351, 1210, 1387, 3974, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(ddict_sample['train']['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that PhoBert will auto-pad our sentence to its model max_sequence_length, which is 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ddict_sample['train']['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict_sample['train']['label'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L150){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataMain.save_as_pickles\n",
       "\n",
       ">      TextDataMain.save_as_pickles (fname, parent='pickle_files',\n",
       ">                                    drop_data_attributes=False)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |\n",
       "| drop_data_attributes | bool | False | Whether to drop all large-size data attributes |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#L150){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataMain.save_as_pickles\n",
       "\n",
       ">      TextDataMain.save_as_pickles (fname, parent='pickle_files',\n",
       ">                                    drop_data_attributes=False)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |\n",
       "| drop_data_attributes | bool | False | Whether to drop all large-size data attributes |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataMain.save_as_pickles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the transformations/augmentations can take time for large dataset, we want to save our TextDataMain object. We can use `TextDataMain.save_as_pickles` to export a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm.save_as_pickles('my_tdm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can load it with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm2 = TextDataMain.from_pickle('my_tdm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and access all the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Source</th>\n",
       "      <th>L1</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hc search - làm_sao để hết lỗi m02</td>\n",
       "      <td>hc search</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hc search - xin chao</td>\n",
       "      <td>hc search</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>owned - 8   NGAY DUA DON , TRUNG VOUCHER 8 THA...</td>\n",
       "      <td>owned</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google play - 🤬_😡_🤬_😡</td>\n",
       "      <td>google play</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hc search - mua hàng quốc_tế như thế_nào</td>\n",
       "      <td>hc search</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content       Source L1  is_valid\n",
       "0                 hc search - làm_sao để hết lỗi m02    hc search  3     False\n",
       "1                               hc search - xin chao    hc search  5     False\n",
       "2  owned - 8   NGAY DUA DON , TRUNG VOUCHER 8 THA...        owned  1     False\n",
       "3                              google play - 🤬_😡_🤬_😡  google play  5     False\n",
       "4           hc search - mua hàng quốc_tế như thế_nào    hc search  4     False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm2.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Buyer complained seller',\n",
       " 'Commercial',\n",
       " 'Delivery',\n",
       " 'Feature',\n",
       " 'Order/Item',\n",
       " 'Others',\n",
       " 'Payment',\n",
       " 'Return/Refund',\n",
       " 'Services',\n",
       " 'Shopee account']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm2.label_lists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Source': ['google play', 'hc search', 'ios', 'non owned', 'owned']}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm2.metadata_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Size in MegaBytes is 13.537714004516602\n"
     ]
    }
   ],
   "source": [
    "file_stats = os.stat(Path('pickle_files/my_tdm.pkl'))\n",
    "print(f'File Size in MegaBytes is {file_stats.st_size / (1024 * 1024)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it saves the entire processed dataframe (and datasetdict if you call ```to_datasetdict```), the pickle size can be large. In some scenario you don't need to store these data attributes (as inference time, or in production). Thus one can save a lighter pickle file by setting ```drop_data_attributes``` to ```True```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm.save_as_pickles('my_lightweight_tdm',drop_data_attributes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Size in MegaBytes is 3.1605300903320312\n"
     ]
    }
   ],
   "source": [
    "file_stats = os.stat(Path('pickle_files/my_lightweight_tdm.pkl'))\n",
    "print(f'File Size in MegaBytes is {file_stats.st_size / (1024 * 1024)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see a bigger file size reduction when we work with much larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm_light = TextDataMain.from_pickle('my_lightweight_tdm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can still access some important attributes (except for any data attributes, such as ```df``` or ```main_ddict```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Buyer complained seller',\n",
       " 'Commercial',\n",
       " 'Delivery',\n",
       " 'Feature',\n",
       " 'Order/Item',\n",
       " 'Others',\n",
       " 'Payment',\n",
       " 'Return/Refund',\n",
       " 'Services',\n",
       " 'Shopee account']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm_light.label_lists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Source': ['google play', 'hc search', 'ios', 'non owned', 'owned']}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm_light.metadata_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
