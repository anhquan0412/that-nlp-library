{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils\n",
    "\n",
    "> This module contains some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "from functools import partial, reduce\n",
    "import dill as pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Callable, Any\n",
    "from collections.abc import Iterable\n",
    "from datasets import IterableDataset\n",
    "import random\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HiddenPrints:\n",
    "    \"To hide print command when called\"\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def val2iterable(val,lsize=1,t='list'):\n",
    "    \"Convert an element (nonlist value) to an iterable. Currently support list and nparray\"\n",
    "    if not isinstance(val, Iterable) or isinstance(val,str):\n",
    "        if t=='list': val=[val for i in range(lsize)]\n",
    "        elif t=='nparray': val=np.repeat(val,lsize)\n",
    "        else:\n",
    "            raise ValueError('Unrecognized iterable to convert to')\n",
    "    return val\n",
    "\n",
    "def create_dir(path_dir):\n",
    "    \"Create directory if needed\"\n",
    "    path_dir = Path(path_dir)\n",
    "    if not path_dir.exists():\n",
    "        path_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def check_and_get_attribute(obj,attr_name):\n",
    "    if hasattr(obj,attr_name): return getattr(obj,attr_name)\n",
    "    else: raise ValueError(f\"Missing required argument: {attr_name}\")\n",
    "            \n",
    "def callable_name(any_callable: Callable[..., Any]) -> str:\n",
    "    \"To get name of any callable\"\n",
    "    if hasattr(any_callable, '__name__'):\n",
    "        return any_callable.__name__\n",
    "    if isinstance(any_callable, partial):\n",
    "        return any_callable.func.__name__\n",
    "\n",
    "    return str(any_callable)\n",
    "\n",
    "def print_msg(msg,dash_num=5,verbose=True):\n",
    "    if verbose:\n",
    "        print('-'*dash_num+' '+msg+' '+ '-'*dash_num)\n",
    "\n",
    "def seed_notorch(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "def seed_everything(seed=42):\n",
    "    seed_notorch(seed=seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export    \n",
    "def save_to_pickle(my_list,fname,parent='pickle_files'):\n",
    "    if fname[-4:]!='.pkl': fname+='.pkl'\n",
    "    p = Path(parent)\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    fname = p/fname\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(my_list, f)\n",
    "        \n",
    "def load_pickle(fname,parent='pickle_files'):\n",
    "    if fname[-4:]!='.pkl': fname+='.pkl'\n",
    "    with open(Path(parent)/fname,'rb') as f:\n",
    "        my_list = pickle.load(f)\n",
    "    return my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def check_input_validation(df:pd.DataFrame,verbose=True):\n",
    "    verboseprint = print if verbose else lambda *a, **k: None\n",
    "    print_msg('Input Validation Precheck',verbose)\n",
    "    # check whether index is sorted\n",
    "    correct_idxs = np.arange(df.shape[0])\n",
    "    curr_idxs = df.index.values\n",
    "    if not np.array_equal(correct_idxs,curr_idxs):\n",
    "        verboseprint(\"DataFrame Index is not RangeIndex, and will be converted\")\n",
    "        df.index = correct_idxs\n",
    "\n",
    "    # Do a NA check:\n",
    "    na_check = df.isna().sum(axis=0)\n",
    "    na_check = na_check[na_check!=0]\n",
    "    if na_check.shape[0]!=0:\n",
    "        verboseprint(\"Data contains missing values!\")\n",
    "        verboseprint('-----> List of columns and the number of missing values for each')\n",
    "        verboseprint(na_check)\n",
    "\n",
    "    # Do a row duplication check\n",
    "    _df = df.copy().astype(str)\n",
    "    dup_check = _df.value_counts(dropna=False)\n",
    "    dup_check=dup_check[dup_check>1]\n",
    "    if dup_check.shape[0]!=0:\n",
    "        verboseprint(\"Data contains duplicated values!\")\n",
    "        verboseprint(f'-----> Number of duplications: {(dup_check.values-1).sum()} rows')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def check_text_leaking(trn_txt:list,\n",
    "                       test_txt:list,verbose=True):\n",
    "    verboseprint = print if verbose else lambda *a, **k: None\n",
    "    test_txt_leaked = {i.strip() for i in trn_txt} & {j.strip() for j in test_txt}\n",
    "    len_leaked = len(test_txt_leaked)\n",
    "    verboseprint(f'- Number of rows leaked: {len_leaked}, which is {100*len_leaked/len(trn_txt):.2f}% of training set')\n",
    "    return test_txt_leaked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def none2emptystr(x):\n",
    "    if x is None: return ''\n",
    "    return str(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lambda_batch(inp, # HuggingFace Dataset\n",
    "                 feature, # Feature name.\n",
    "                 func, # The function to apply\n",
    "                 is_batched, # Whether batching is applied\n",
    "                ):\n",
    "    return [func(v) for v in inp[feature]] if is_batched else func(inp[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lambda_map_batch(inp, # HuggingFace Dataset\n",
    "                     feature, # Feature name.\n",
    "                     func, # The function to apply\n",
    "                     is_batched, # Whether batching is applied\n",
    "                     output_feature='same', # New feature output, if different from 'feature'. If none, use function's output feature(s)\n",
    "                     is_func_batched=False # Whether the func above only works with batch (mostly sklearn's)\n",
    "                    ):\n",
    "    \n",
    "    if output_feature is None: \n",
    "        return func(inp[feature]) if is_batched else func([inp[feature]])\n",
    "    if output_feature.strip().lower()=='same': output_feature = feature\n",
    "    results={}\n",
    "    if not is_func_batched:\n",
    "        results[output_feature] = lambda_batch(inp,feature,func,is_batched)\n",
    "    else:\n",
    "        results[output_feature] = func(inp[feature]) if is_batched else func([inp[feature]])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def augmentation_stream_generator(dset,text_name,func):\n",
    "    for inp in dset:\n",
    "        # inp[text_name] will be a single item\n",
    "        inp[text_name]=func(inp[text_name])\n",
    "        yield inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# def augmentation_helper(inp,text_name,func):\n",
    "#     # inp[text_name] will be list\n",
    "#     inp[text_name]=[func(v) for v in val2iterable(inp[text_name])]\n",
    "#     return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def func_all(x, functions):\n",
    "    return reduce(lambda acc, func: func(acc), functions, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_dset_col_names(dset):\n",
    "    if dset.column_names is not None: return dset.column_names\n",
    "    warnings.warn(\"Iterable Dataset might contain multiple mapping functions; getting column names can be time and memory consuming\") \n",
    "    return list(next(iter(dset)).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def hf_map_dset(dset,func,\n",
    "                is_batched=True,\n",
    "                batch_size=1024,\n",
    "                num_proc=1):\n",
    "    is_streamed = isinstance(dset,IterableDataset)\n",
    "    if is_streamed:\n",
    "        return dset.map(func,\n",
    "                        batched=is_batched,\n",
    "                        batch_size=batch_size\n",
    "                       )\n",
    "    return dset.map(func,\n",
    "                    batched=is_batched,\n",
    "                    batch_size=batch_size,\n",
    "                    num_proc=num_proc\n",
    "                   )\n",
    "\n",
    "def hf_filter_dset(dset,func,\n",
    "                   is_batched=True,\n",
    "                   batch_size=1024,\n",
    "                   num_proc=1):\n",
    "    is_streamed = isinstance(dset,IterableDataset)\n",
    "    if is_streamed:\n",
    "        return dset.filter(func,\n",
    "                           batched=is_batched,\n",
    "                           batch_size=batch_size\n",
    "                          )\n",
    "    return dset.filter(func,\n",
    "                       batched=is_batched,\n",
    "                       batch_size=batch_size,\n",
    "                       num_proc=num_proc\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def resize_model_embeddings(model,tokenizer):\n",
    "    embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "    if len(tokenizer) > embedding_size:\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    A numerically stable version of the logistic sigmoid function.\n",
    "    \n",
    "    Source: assignment3 of cs231n\n",
    "    \"\"\"\n",
    "    pos_mask = (x >= 0)\n",
    "    neg_mask = (x < 0)\n",
    "    z = np.zeros_like(x)\n",
    "    z[pos_mask] = np.exp(-x[pos_mask])\n",
    "    z[neg_mask] = np.exp(x[neg_mask])\n",
    "    top = np.ones_like(x)\n",
    "    top[neg_mask] = z[neg_mask]\n",
    "    return top / (1 + z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
