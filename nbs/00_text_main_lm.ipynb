{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Main For Language Model\n",
    "\n",
    "> This module contains the main Python class for Language Model data control: `TextDataLMController`\n",
    "\n",
    "- skip_showdoc: true\n",
    "- skip_exec: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp text_main_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from datasets import Dataset\n",
    "from that_nlp_library.utils import *\n",
    "from that_nlp_library.text_main import *\n",
    "from functools import partial\n",
    "import warnings\n",
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from that_nlp_library.text_transformation import *\n",
    "from that_nlp_library.text_augmentation import *\n",
    "from importlib.machinery import SourceFileLoader\n",
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class TextDataLMController"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TextDataLMController(TextDataController):\n",
    "    def __init__(self,\n",
    "                 inp, # HuggingFainpce Dataset or DatasetDict\n",
    "                 main_text:str, # Name of the main text column\n",
    "                 filter_dict={}, # A dictionary: {feature: filtering_function_for_that_feature}\n",
    "                 metadatas=[], # Names of the metadata columns\n",
    "                 process_metas=True, # Whether to do simple text processing on the chosen metadatas\n",
    "                 metas_sep='.', # Separator, for multiple metadatas concatenation\n",
    "                 content_transformations=[], # A list of text transformations\n",
    "                 val_ratio:int|float|None=0.2, # Ratio of data for validation set\n",
    "                 stratify_cols=[], # Column(s) needed to do stratified shuffle split\n",
    "                 seed=None, # Random seed\n",
    "                 batch_size=1024, # CPU batch size\n",
    "                 num_proc=4, # Number of process for multiprocessing\n",
    "                 cols_to_keep=None, # Columns to keep after all processings\n",
    "                 verbose=True, # Whether to prdint processing information\n",
    "                ):\n",
    "        super().__init__(inp=inp,\n",
    "                         main_text=main_text,\n",
    "                         filter_dict=filter_dict,\n",
    "                         metadatas=metadatas,\n",
    "                         process_metas=process_metas,\n",
    "                         metas_sep=metas_sep,\n",
    "                         content_transformations=content_transformations,\n",
    "                         val_ratio=val_ratio,\n",
    "                         stratify_cols=stratify_cols,\n",
    "                         seed=seed,\n",
    "                         batch_size=batch_size,\n",
    "                         num_proc=num_proc,\n",
    "                         cols_to_keep=cols_to_keep,\n",
    "                         verbose=verbose\n",
    "                        )\n",
    "            \n",
    "    \n",
    "    def _do_label_transformation(self):\n",
    "        raise NotImplementedError(\"There's no classification/regression label in text processing for Language Model\")\n",
    "        \n",
    "    def _encode_labels(self):\n",
    "        raise NotImplementedError(\"There's no classification/regression label in text processing for Language Model\")\n",
    "\n",
    "    \n",
    "    def _upsampling(self):\n",
    "        raise NotImplementedError(\"There's no upsampling in text processing for Language Model\")\n",
    "      \n",
    "    def _do_augmentation(self):\n",
    "        raise NotImplementedError(\"There's no text augmentation in text processing for Language Model\")\n",
    "     \n",
    "    def save_as_pickles(self,\n",
    "                        fname, # Name of the pickle file\n",
    "                        parent='pickle_files', # Parent folder\n",
    "                       ):\n",
    "        \n",
    "        save_to_pickle(self,fname,parent=parent) \n",
    "        \n",
    "    def _do_train_shuffling(self):\n",
    "        print_msg('Shuffling and flattening train set',20,verbose=self.verbose)\n",
    "        self.main_ddict['train'] = self.main_ddict['train'].shuffle(seed=self.seed).flatten_indices(num_proc = self.num_proc)\n",
    "        self.verboseprint('Done')\n",
    "\n",
    "    def _group_texts_with_stride(self,examples):\n",
    "        max_length = self.max_length\n",
    "        if max_length is None: \n",
    "            max_length = self.tokenizer.model_max_length\n",
    "        stride = self.stride\n",
    "        if stride is None: stride=max_length\n",
    "        else: stride = max_length-stride\n",
    "        if stride==0: raise ValueError(f'Stride cannot be equal to max length of {max_length}')\n",
    "            \n",
    "        # Concatenate all texts.\n",
    "        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        result_all={}\n",
    "        for k,t in concatenated_examples.items():\n",
    "            result=[]\n",
    "            i=0\n",
    "            while i+max_length<=total_length:\n",
    "                result.append(t[i:i+max_length])\n",
    "                i+=stride\n",
    "            result_all[k]=result\n",
    "\n",
    "        return result_all  \n",
    "    \n",
    "\n",
    "    def do_all_preprocessing(self,\n",
    "                             shuffle_trn=True # To shuffle the train set before tokenization\n",
    "                            ):\n",
    "        if self._processed_call:\n",
    "            warnings.warn('Your dataset has already been processed. Returning the previous processed DatasetDict...')\n",
    "            return self.main_ddict\n",
    "            \n",
    "        print_msg('Start Main Text Processing',20,verbose=self.verbose)\n",
    "        \n",
    "        # Filtering\n",
    "        self.dset,self.ddict_rest = self._do_filtering(self.dset,self.ddict_rest)\n",
    "        \n",
    "        # Process metadatas\n",
    "        self.dset,self.ddict_rest = self._process_metadatas(self.dset,self.ddict_rest)\n",
    "        \n",
    "        \n",
    "        # Content transformation\n",
    "        self.dset,self.ddict_rest = self._do_transformation(self.dset,self.ddict_rest)\n",
    "         \n",
    "        # Train Test Split.\n",
    "        ### self.main_ddict is created here\n",
    "        self._train_test_split()\n",
    "        \n",
    "        # Dropping unused columns\n",
    "        self._simplify_ddict()\n",
    "        \n",
    "        # Check validation leaking\n",
    "        self._check_validation_leaking()\n",
    "        \n",
    "        # Shuffle train\n",
    "        if shuffle_trn:\n",
    "            self._do_train_shuffling()\n",
    "        \n",
    "        self._processed_call=True\n",
    "        \n",
    "        return self.main_ddict\n",
    "    \n",
    "        \n",
    "    def do_tokenization(self,\n",
    "                        tokenizer, # Tokenizer (preferably from HuggingFace)\n",
    "                        max_length=None, # pad to model's allowed max length (default is max_sequence_length). Use -1 for no padding at all\n",
    "                        line_by_line=True, # To whether tokenize each sentence separately, or concatenate them\n",
    "                        stride=None, # option to do striding when line_by_line is False\n",
    "                        trn_size=None, # The number of training data to be tokenized\n",
    "                        tok_num_proc=None, # Number of processes for tokenization\n",
    "                       ):\n",
    "        # References\n",
    "#         https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
    "#         https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py\n",
    "        \n",
    "        print_msg('Tokenization',20,verbose=self.verbose)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.line_by_line = line_by_line\n",
    "        self.stride = stride\n",
    "        self.tok_num_proc = tok_num_proc if tok_num_proc else self.num_proc\n",
    "        \n",
    "        tok_func = partial(tokenize_function,tok=self.tokenizer,\n",
    "                           max_length=max_length if line_by_line else -1,\n",
    "                           return_special_tokens_mask=True)\n",
    "        \n",
    "        _func = partial(lambda_map_batch,\n",
    "                        feature=self.main_text,\n",
    "                        func=tok_func,\n",
    "                        output_feature=None,\n",
    "                        is_batched=self.is_batched)\n",
    "        \n",
    "        \n",
    "        if trn_size is not None:\n",
    "            if isinstance(trn_size,float):\n",
    "                num_shard = int(1/trn_size)\n",
    "            else: # int\n",
    "                trn_len=len(self.main_ddict['train'])\n",
    "                num_shard = trn_len//trn_size\n",
    "            self.main_ddict['train'] = self.main_ddict['train'].shard(num_shard,0)\n",
    "        \n",
    "        for k in self.main_ddict.keys():\n",
    "            self.main_ddict[k] = hf_map_dset(self.main_ddict[k],_func,self.is_batched,self.batch_size,self.tok_num_proc)\n",
    "            if not line_by_line: self.main_ddict[k] = self.main_ddict[k].remove_columns(self.cols_to_keep)\n",
    "        \n",
    "        if not line_by_line: # token concatenation\n",
    "            for k in self.main_ddict.keys():\n",
    "                self.main_ddict[k] = hf_map_dset(self.main_ddict[k],\n",
    "                                                 self._group_texts_with_stride,\n",
    "                                                 is_batched=True,\n",
    "                                                 batch_size=self.batch_size if self.batch_size>1 else 1024,\n",
    "                                                 num_proc=self.tok_num_proc)\n",
    "                \n",
    "        \n",
    "        self.verboseprint('Done')\n",
    "        return self.main_ddict\n",
    "        \n",
    "    def process_and_tokenize(self,\n",
    "                             tokenizer, # Tokenizer (preferably from HuggingFace)\n",
    "                             max_length=None, # pad to model's allowed max length (default is max_sequence_length)\n",
    "                             line_by_line=True, # To whether tokenize each sentence separately, or concatenate them and then tokenize\n",
    "                             stride=None, # option to do striding when line_by_line is False\n",
    "                             trn_size=None, # The number of training data to be tokenized\n",
    "                             tok_num_proc=None, # Number of processes for tokenization\n",
    "                             shuffle_trn=True, # To shuffle the train set before tokenization\n",
    "                            ):\n",
    "        \"\"\"\n",
    "        This will perform `do_all_processing` then `do_tokenization`\n",
    "        \"\"\"\n",
    "        if self.seed:\n",
    "            seed_everything(self.seed)\n",
    "        _ = self.do_all_preprocessing(shuffle_trn)\n",
    "        _ = self.do_tokenization(tokenizer,max_length,line_by_line,stride,trn_size,tok_num_proc)\n",
    "        \n",
    "    \n",
    "    def set_data_collator(self,\n",
    "                          is_mlm=True, # Is this masked language model (True) or causal language model (False)\n",
    "                          mlm_prob=0.15, # Mask probability for masked language model\n",
    "                         ):\n",
    "        if not hasattr(self,'max_length'):\n",
    "            raise ValueError(\"Please call `process_and_tokenize' or `do_tokenization` to tokenize your dataset\")\n",
    "        \n",
    "        self.is_mlm = is_mlm\n",
    "        pad_to_multiple_of_8 = (self.max_length<0) # get data collator to pad\n",
    "        self.data_collator = DataCollatorForLanguageModeling(tokenizer=self.tokenizer,\n",
    "                                                             mlm=is_mlm,\n",
    "                                                             mlm_probability=mlm_prob,\n",
    "                                                             pad_to_multiple_of=8 if pad_to_multiple_of_8 else None\n",
    "                                                            )\n",
    "                                               \n",
    "    \n",
    "    def prepare_test_dataset_from_raws(self,\n",
    "                                       content, # Either a single sentence, list of sentence or a dictionary with keys are metadata columns and values are list\n",
    "                                       do_tokenize=False, # Whether to tokenize text\n",
    "                                      ):\n",
    "        if len(self.metadatas) and not isinstance(content,dict):\n",
    "            raise ValueError(f'There is/are metadatas in the preprocessing step. Please include a dictionary including these keys for metadatas: {self.metadatas}, and texture content: {self.main_text}')\n",
    "        if not self.line_by_line:\n",
    "            raise ValueError(f'This TextDataLMController does not tokenize text line-by-line, thus no test set processing is provided)')\n",
    "        \n",
    "        _dic = {self.main_text:[content]} if isinstance(content,str) else content\n",
    "        for k in _dic.keys():\n",
    "            _dic[k] = val2iterable(_dic[k])\n",
    "        \n",
    "        test_dict = Dataset.from_dict(_dic)\n",
    "        \n",
    "        # set num_proc to 1 for small data processing\n",
    "        _tmp1 = self.num_proc\n",
    "        _tmp2 = self.tok_num_proc\n",
    "        self.num_proc=1\n",
    "        self.tok_num_proc=1\n",
    "        results = self.prepare_test_dataset(test_dict,do_tokenize)\n",
    "        self.num_proc = _tmp1\n",
    "        self.tok_num_proc=_tmp2\n",
    "        return results\n",
    "        \n",
    "    def prepare_test_dataset(self,\n",
    "                             test_dset, # The HuggingFace Dataset as Test set\n",
    "                             do_tokenize, # Whether to tokenize text\n",
    "                            ):\n",
    "        if not self.line_by_line:\n",
    "            raise ValueError(f'This TextDataLMController does not tokenize text line-by-line, thus no test set processing is provided)')\n",
    "            \n",
    "        test_cols = set(get_dset_col_names(test_dset))\n",
    "        missing_cols = set(self.cols_to_keep) - test_cols\n",
    "        if len(missing_cols):\n",
    "            raise ValueError(f'Test set does not have these columns required for preprocessings: {missing_cols}')\n",
    "            \n",
    "        print_msg('Start Test Set Transformation',20,verbose=self.verbose)\n",
    "        \n",
    "        # Process metadatas\n",
    "        test_dset = self._process_metadatas(test_dset)\n",
    "        \n",
    "        # Content transformation\n",
    "        test_dset = self._do_transformation(test_dset)\n",
    "        \n",
    "        # Drop unused columns\n",
    "        cols_to_remove = test_cols - set(self.cols_to_keep)\n",
    "        test_dset = test_dset.remove_columns(list(cols_to_remove))\n",
    "        \n",
    "        if do_tokenize:         \n",
    "            print_msg('Tokenization',20,verbose=self.verbose)\n",
    "            tok_func = partial(tokenize_function,\n",
    "                           tok=self.tokenizer,\n",
    "                           max_length=self.max_length if self.line_by_line else -1,\n",
    "                           return_special_tokens_mask=True\n",
    "                          )\n",
    "            \n",
    "            _func = partial(lambda_map_batch,\n",
    "                        feature=self.main_text,\n",
    "                        func=tok_func,\n",
    "                        output_feature=None,\n",
    "                        is_batched=self.is_batched)\n",
    "            test_dset = hf_map_dset(test_dset,_func,self.is_batched,self.batch_size,self.tok_num_proc)\n",
    "            \n",
    "        self.verboseprint('Done')\n",
    "        return test_dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_lm.py#L16){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataLMController\n",
       "\n",
       ">      TextDataLMController (inp, main_text:str, filter_dict={}, metadatas=[],\n",
       ">                            process_metas=True, metas_sep='.',\n",
       ">                            content_transformations=[],\n",
       ">                            val_ratio:int|float|None=0.2, stratify_cols=[],\n",
       ">                            seed=None, batch_size=1024, num_proc=4,\n",
       ">                            cols_to_keep=None, verbose=True)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | HuggingFainpce Dataset or DatasetDict |\n",
       "| main_text | str |  | Name of the main text column |\n",
       "| filter_dict | dict | {} | A dictionary: {feature: filtering_function_for_that_feature} |\n",
       "| metadatas | list | [] | Names of the metadata columns |\n",
       "| process_metas | bool | True | Whether to do simple text processing on the chosen metadatas |\n",
       "| metas_sep | str | . | Separator, for multiple metadatas concatenation |\n",
       "| content_transformations | list | [] | A list of text transformations |\n",
       "| val_ratio | int \\| float \\| None | 0.2 | Ratio of data for validation set |\n",
       "| stratify_cols | list | [] | Column(s) needed to do stratified shuffle split |\n",
       "| seed | NoneType | None | Random seed |\n",
       "| batch_size | int | 1024 | CPU batch size |\n",
       "| num_proc | int | 4 | Number of process for multiprocessing |\n",
       "| cols_to_keep | NoneType | None | Columns to keep after all processings |\n",
       "| verbose | bool | True | Whether to prdint processing information |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_lm.py#L16){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataLMController\n",
       "\n",
       ">      TextDataLMController (inp, main_text:str, filter_dict={}, metadatas=[],\n",
       ">                            process_metas=True, metas_sep='.',\n",
       ">                            content_transformations=[],\n",
       ">                            val_ratio:int|float|None=0.2, stratify_cols=[],\n",
       ">                            seed=None, batch_size=1024, num_proc=4,\n",
       ">                            cols_to_keep=None, verbose=True)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| inp |  |  | HuggingFainpce Dataset or DatasetDict |\n",
       "| main_text | str |  | Name of the main text column |\n",
       "| filter_dict | dict | {} | A dictionary: {feature: filtering_function_for_that_feature} |\n",
       "| metadatas | list | [] | Names of the metadata columns |\n",
       "| process_metas | bool | True | Whether to do simple text processing on the chosen metadatas |\n",
       "| metas_sep | str | . | Separator, for multiple metadatas concatenation |\n",
       "| content_transformations | list | [] | A list of text transformations |\n",
       "| val_ratio | int \\| float \\| None | 0.2 | Ratio of data for validation set |\n",
       "| stratify_cols | list | [] | Column(s) needed to do stratified shuffle split |\n",
       "| seed | NoneType | None | Random seed |\n",
       "| batch_size | int | 1024 | CPU batch size |\n",
       "| num_proc | int | 4 | Number of process for multiprocessing |\n",
       "| cols_to_keep | NoneType | None | Columns to keep after all processings |\n",
       "| verbose | bool | True | Whether to prdint processing information |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataLMController)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data + Basic use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.from_csv\n",
       "\n",
       ">      TextDataController.from_csv (file_path, **kwargs)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.from_csv\n",
       "\n",
       ">      TextDataController.from_csv (file_path, **kwargs)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataLMController.from_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.from_df\n",
       "\n",
       ">      TextDataController.from_df (df, validate=True, **kwargs)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.from_df\n",
       "\n",
       ">      TextDataController.from_df (df, validate=True, **kwargs)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataLMController.from_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a `TextDataLMController` from a csv, pandas DataFrame, or directly from a HuggingFace dataset object. Currently, `TextDataLMController` is designed for processing text in order to train a language model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset source: https://www.kaggle.com/datasets/kavita5/review_ecommerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23486, 10)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5858</th>\n",
       "      <td>1033</td>\n",
       "      <td>34</td>\n",
       "      <td>Great black trousers</td>\n",
       "      <td>Trousers fit tts with nice thick fabric and gr...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Jeans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17403</th>\n",
       "      <td>860</td>\n",
       "      <td>46</td>\n",
       "      <td>Gorgeous jewel tone color</td>\n",
       "      <td>The other reviews stated this was a shapeless ...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6070</th>\n",
       "      <td>828</td>\n",
       "      <td>46</td>\n",
       "      <td>Test like fit</td>\n",
       "      <td>This shirt was tent like on me and there was r...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21826</th>\n",
       "      <td>1086</td>\n",
       "      <td>21</td>\n",
       "      <td>Exquisite and timeless</td>\n",
       "      <td>This dress is beautiful. i cannot begin to exp...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7872</th>\n",
       "      <td>872</td>\n",
       "      <td>38</td>\n",
       "      <td>Love it</td>\n",
       "      <td>Love this top, you can really fess up, i order...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Clothing ID  Age                      Title   \n",
       "5858          1033   34       Great black trousers  \\\n",
       "17403          860   46  Gorgeous jewel tone color   \n",
       "6070           828   46              Test like fit   \n",
       "21826         1086   21     Exquisite and timeless   \n",
       "7872           872   38                    Love it   \n",
       "\n",
       "                                             Review Text  Rating   \n",
       "5858   Trousers fit tts with nice thick fabric and gr...       5  \\\n",
       "17403  The other reviews stated this was a shapeless ...       4   \n",
       "6070   This shirt was tent like on me and there was r...       1   \n",
       "21826  This dress is beautiful. i cannot begin to exp...       5   \n",
       "7872   Love this top, you can really fess up, i order...       5   \n",
       "\n",
       "       Recommended IND  Positive Feedback Count   Division Name   \n",
       "5858                 1                        0  General Petite  \\\n",
       "17403                1                        3  General Petite   \n",
       "6070                 0                        0         General   \n",
       "21826                1                        2         General   \n",
       "7872                 1                        3  General Petite   \n",
       "\n",
       "      Department Name Class Name  \n",
       "5858          Bottoms      Jeans  \n",
       "17403            Tops      Knits  \n",
       "6070             Tops    Blouses  \n",
       "21826         Dresses    Dresses  \n",
       "7872             Tops      Knits  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a `TextDataLMController` from a dataframe. This also provides a quick input validation check (NaN check and Duplication check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Input Validation Precheck -\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataLMController.from_df(df,main_text='Review Text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create a `TextDataLMController` directly from the csv file. The good thing about using HuggingFace Dataset as the main backend  is that you can utilize lots of its useful functionality, such as caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc = TextDataLMController.from_csv('sample_data/Womens_Clothing_Reviews.csv',main_text='Review Text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create a `TextDataLMController` from a HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'],\n",
       "    num_rows: 23486\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc = TextDataLMController(dset,main_text='Review Text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the \"Input Validation Precheck\" above, we notice that our dataset has missing values in the text field and the label field. For now, let's load the data as a Pandas' DataFrame, perform some cleaning, and create our `TextDataLMController`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(~df['Review Text'].isna()) & (~df['Department Name'].isna())].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Input Validation Precheck -\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title    2966\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 1 rows\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataLMController.from_df(df,main_text='Review Text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you can start perform 2 important steps on your data\n",
    "\n",
    "1. Text preprocessings + Train/Validation Split\n",
    "2. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 3, which is 0.02% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9048b68718a408e8cbafb27cee3b3a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012b31af7dc84b318dc3a953e6a8e232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "ddict = tdc.do_all_preprocessing(shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Review Text'],\n",
       "        num_rows: 18099\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text'],\n",
       "        num_rows: 4526\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our DatasetDict now has two split: train and validation. Note that train split is now IterableDataset, for processing efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Review Text': ['Love this blazer. you can wear it with anything. fits well and lightweight. perfect for south florida.',\n",
       "  \"I love the mustard color and fun boho details of this shirt, however the sizing was a little weird. i ordered the small petite first and it was just too short (which is rarely a problem i have), so i ordered it in the regular small. it's perfect in length, but a little loose on top, which i'm fine with. overall, very happy with this purchase and have received many compliments on it.\",\n",
       "  'I have no idea why this has bad reviews! how is there any left?! this is a beautiful piece that i had to have in both the navy and green. i love that it comes with a cami. it&#39;s very flattering and not too wide or boxy. the lace detail is amazing! i bought my usual size and it fits perfect. love this top!!']}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict['train'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Review Text': ['They fit perfectly and the stretch makes them very comfortable.',\n",
       "  \"I love the metallic colors of this top and figured i could wear it under a ruched jacket and circle skirt for work. welp, that's out the window. this design is poor. for one, this is not a piece for a petite woman with no torso and i don't know how anyone with a longer torso wears t his. this hits above my belly botton on and i got apetite 2. i have no torso. so, without a jacket, i would never wear this. it's very low cut..the back is very low..it's a little loose but i run between a 2 and a 4.\",\n",
       "  'I am 5\\'10\" 150lbs and size 8 fits me perfectly. \\r\\nthe print is lovely and the fabric has a nice flow to it. \\r\\nat this price point the dress should have a lining.']}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict['validation'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This preprocessing step allow you to filter out certain values of a certain column in your dataset. Let's say I want to filter out any None value in the column 'Review Text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Clothing ID                   0\n",
       "Age                           0\n",
       "Title                      2966\n",
       "Review Text                   0\n",
       "Rating                        0\n",
       "Recommended IND               0\n",
       "Positive Feedback Count       0\n",
       "Division Name                13\n",
       "Department Name              13\n",
       "Class Name                   13\n",
       "dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\n",
    "df[(~df['Review Text'].isna())].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will provide a dictionary containing the name of the column and the filtering function to apply on that column. Note that **the filtering function will receive an item from the column, and the function should return a boolean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Input Validation Precheck -\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataLMController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                 seed=42\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f8e186f90e479383f1322db673095d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/23486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 1, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2128c5aa5740f5b1c3912b79faf237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339c9cc765764fa4bcc05b825915c33b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18111 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "ddict = tdc.do_all_preprocessing(shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Review Text'],\n",
       "        num_rows: 18111\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text'],\n",
       "        num_rows: 4529\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we have filtered out all NaN/None value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ddict['train']['Review Text']:\n",
    "    assert i is not None\n",
    "for i in ddict['validation']['Review Text']:\n",
    "    assert i is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even add multiple filtering functions. Remember from our precheck, there are also None values in 'Department Name'. While we are at it, let's filter out any rating that is less than 3 (just to showcase what our filtering can do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rating\n",
       "5    13131\n",
       "4     5077\n",
       "3     2871\n",
       "2     1565\n",
       "1      842\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Rating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `TextDataLMController` will only keep the text and the metadatas columns; any other column will be dropped. To double-check our result, we need to define the `cols_to_keep` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Input Validation Precheck -\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\n",
    "tdc = TextDataLMController.from_df(df,\n",
    "                                   main_text='Review Text',\n",
    "                                   filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                                'Department Name': lambda x: x is not None,\n",
    "                                                'Rating': lambda x: x>=3\n",
    "                                               },\n",
    "                                   cols_to_keep=['Review Text','Rating','Department Name'],\n",
    "                                   seed=42\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4b6d8cf26b4597925581248865e74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/23486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68d959c1af7440a9821e1b2431ff871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/22641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Rating -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e815b660a64478d88c5407b19409268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 1, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c016bd9f3e6418180f8083aca35d556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/16206 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ca52e730724f19afc78bf33fb1223c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/16205 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "ddict = tdc.do_all_preprocessing(shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ddict['train']['Department Name']:\n",
    "    assert i is not None\n",
    "for i in ddict['validation']['Department Name']:\n",
    "    assert i is not None\n",
    "\n",
    "for i in ddict['train']['Rating']:\n",
    "    assert i is not None\n",
    "for i in ddict['validation']['Rating']:\n",
    "    assert i >= 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadatas concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we think metadatas can be helpful, we can concatenate them into the front of your text, so that our text classification model is aware of it.\n",
    "\n",
    "In this example, Let's add 'Title' as our metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Input Validation Precheck -\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\n",
    "tdc = TextDataLMController.from_df(df,\n",
    "                                   main_text='Review Text',\n",
    "                                   filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                   metadatas='Title',\n",
    "                                   process_metas=True, # to preprocess the metadata (currently it's just empty space stripping and lowercasing),\n",
    "                                   seed=42\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a06c99b0d3c421ba8990de146bf20ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/23486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8547016fe5c84b7593f670e204574ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 0, which is 0.00% of training set\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3bc1122ad9742ff8ddb54a2afe1ac46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "ddict = tdc.do_all_preprocessing(shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Title': ['not flattering on me', '', ''],\n",
       " 'Review Text': ['not flattering on me . I ordered this online and was disappointed with the fit when it arrived. i ordered the xs and it was still oversize to the point of being unflattering. i am tall 5\\'9\" about 130 pounds and have a fairly thin torso and look best in cloths that have some shape. if you like a loose fit this might be for you. the material is thicker and warm and comfortable. i would suggest ordering down a size.',\n",
       "  \" . So unflattering! really disappointed. made me look 6 month pregnant and i'm a petite size 2.\",\n",
       "  ' . This t-shirt does a great job of elevating the basic t-shirt in to one with a touch of flair. i typically wear a medium but luckily read earlier reviews and went with the small.']}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict['train'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Title': ['', '', ''],\n",
       " 'Review Text': [\" . This picture doesn't do the skirt justice. i paired it with a creme colored cashmere cowlneck sweater and a silver jeweled belt. it is really pretty and flattering on.\",\n",
       "  ' . Easy to wear! cute, comfy...will be a go to for summer.',\n",
       "  ' . Nice sweater, just did not look good on me. sorry, going back.']}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict['validation'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This processing allows you to **alter the text content in your dataset**. You need to define a function that accepts a single string and returns a new, processed string. Note that this transformation will be applied to ALL of your dataset (both train and validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to normalize our text, because the text might contain some extra spaces between words, or not follow the \"single space after a period\" rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp = \"This is a      sentence,which doesn't follow any rule!No single space is provided after period or punctuation marks.    Maybe there are too many spaces!?!   \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import text_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is a sentence , which doesn't follow any rule ! No single space is provided after period or punctuation marks . Maybe there are too many spaces ! ? !\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_normalize(_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataLMController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None},\n",
    "                         content_transformations=text_normalize,\n",
    "                         seed=42\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "Done\n",
      "-------------------- Text Transformation --------------------\n",
      "----- text_normalize -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 1, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc049e5055c34e2fbab6312196144c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae91ff3314e54c45825d9077e0d47519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18111 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "ddict = tdc.do_all_preprocessing(shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I ordered this online and was disappointed with the fit when it arrived . i ordered the xs and it was still oversize to the point of being unflattering . i am tall 5 \\' 9 \" about 130 pounds and have a fairly thin torso and look best in cloths that have some shape . if you like a loose fit this might be for you . the material is thicker and warm and comfortable . i would suggest ordering down a size .'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict['train']['Review Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This picture doesn't do the skirt justice . i paired it with a creme colored cashmere cowlneck sweater and a silver jeweled belt . it is really pretty and flattering on .\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict['validation']['Review Text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can chain multiple functions. Let's say after text normalizing, I want to lowercase the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is not lowercase'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str.lower('tHis IS NoT lowerCASE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataLMController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None},\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         seed=42\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n",
      "Done\n",
      "-------------------- Text Transformation --------------------\n",
      "----- text_normalize -----\n",
      "----- lower -----\n",
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 1, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f564a246321c4fc9a0d131c2ffd05201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f910df76dc5d4fc3b6dc927e387f700d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18111 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "ddict = tdc.do_all_preprocessing(shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i ordered this online and was disappointed with the fit when it arrived . i ordered the xs and it was still oversize to the point of being unflattering . i am tall 5 \\' 9 \" about 130 pounds and have a fairly thin torso and look best in cloths that have some shape . if you like a loose fit this might be for you . the material is thicker and warm and comfortable . i would suggest ordering down a size .'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict['train']['Review Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this picture doesn't do the skirt justice . i paired it with a creme colored cashmere cowlneck sweater and a silver jeweled belt . it is really pretty and flattering on .\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict['validation']['Review Text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to perform a train/validation split with `TextDataLMController`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first way is when you already have a validation split in your HuggingFace's Dataset. Let's use the Dataset built-in function `train_test_split` to simulate this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "ddict_with_val = dset.train_test_split(test_size=0.1)\n",
    "# This will create a 'test' split instead of 'validation', so we will process a bit to have a validation split\n",
    "ddict_with_val['validation']=ddict_with_val['test']\n",
    "del ddict_with_val['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'],\n",
       "        num_rows: 21137\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'],\n",
       "        num_rows: 2349\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict_with_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffbc78d99d574e81bfc74d669d1fb90c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/21137 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf46a346ae34438a8838587737e0cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/2349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split already exists\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 2, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35786f01cd9e40e6a68107a4d6698fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/20374 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a3764856d5436a89a2af44124da520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/20372 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tdc = TextDataLMController(ddict_with_val,\n",
    "                         main_text='Review Text',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None},\n",
    "                         seed=42\n",
    "                        )\n",
    "ddict = tdc.do_all_preprocessing(shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Review Text'],\n",
       "        num_rows: 20372\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text'],\n",
       "        num_rows: 2267\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second way is to split randomly based on a ratio (a float between 0 and 1), or based on the number of data in your validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Review Text'],\n",
       "        num_rows: 19243\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text'],\n",
       "        num_rows: 3397\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataLMController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None},\n",
    "                         val_ratio=0.15,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "ddict = tdc.do_all_preprocessing(shuffle_trn=True)\n",
    "ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Review Text'],\n",
       "        num_rows: 17640\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataLMController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None},\n",
    "                         val_ratio=5000,\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )\n",
    "ddict = tdc.do_all_preprocessing(shuffle_trn=True)\n",
    "ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A third way is to do a random stratified split (inspired by [sklearn's](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)). Let's do a stratified split based on our label 'Department Name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Department Name\n",
       "Tops        0.445978\n",
       "Dresses     0.269214\n",
       "Bottoms     0.161852\n",
       "Intimate    0.073918\n",
       "Jackets     0.043967\n",
       "Trend       0.005070\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Department Name'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Input Validation Precheck -\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n",
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d79ade65ab94ceebaad9b6fe4efb9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/23486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Do <lambda> on Department Name -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364f24617dda4472a7aa5ec70d0714da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/22641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "Validation split based on val_ratio, with stratifying\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41acb0b85e684baf993eefc942621747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ea7e1af9d04b3b9bcc0ed855f04a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/22628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "- Number of rows leaked: 2, which is 0.01% of training set\n",
      "Filtering leaked data out of training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ca092692a347ebb6134b376cfca350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/18102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec5d5d3ccf04bdea37dc51ee606907c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/18100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Review Text', 'Department Name'],\n",
       "        num_rows: 18100\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text', 'Department Name'],\n",
       "        num_rows: 4526\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc = TextDataLMController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                             },\n",
    "                                 val_ratio=0.2,\n",
    "                                 stratify_cols='Department Name',\n",
    "                                 cols_to_keep=['Review Text','Department Name'],\n",
    "                                 seed=42\n",
    "                                )\n",
    "ddict = tdc.do_all_preprocessing(shuffle_trn=True)\n",
    "ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tops        0.444033\n",
       "Dresses     0.271602\n",
       "Bottoms     0.161878\n",
       "Intimate    0.072983\n",
       "Jackets     0.044309\n",
       "Trend       0.005193\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(ddict['train']['Department Name']).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tops        0.444101\n",
       "Dresses     0.271542\n",
       "Bottoms     0.161732\n",
       "Intimate    0.073133\n",
       "Jackets     0.044189\n",
       "Trend       0.005303\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(ddict['validation']['Department Name']).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use multiple columns for your stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Input Validation Precheck -\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Review Text', 'Rating', 'Department Name'],\n",
       "        num_rows: 18100\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text', 'Rating', 'Department Name'],\n",
       "        num_rows: 4526\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc = TextDataLMController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None,\n",
    "                                              'Department Name': lambda x: x is not None,\n",
    "                                             },\n",
    "                                 val_ratio=0.2,\n",
    "                                 stratify_cols=['Department Name','Rating'],\n",
    "                                 cols_to_keep=['Review Text','Department Name','Rating'],\n",
    "                                 seed=42,\n",
    "                                 verbose=False\n",
    "                                )\n",
    "ddict = tdc.do_all_preprocessing(shuffle_trn=True)\n",
    "ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, you can omit any validation split if you specify `val_ratio` as ```None```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Input Validation Precheck -\n",
      "Data contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "Title              3810\n",
      "Review Text         845\n",
      "Division Name        14\n",
      "Department Name      14\n",
      "Class Name           14\n",
      "dtype: int64\n",
      "Data contains duplicated values!\n",
      "-----> Number of duplications: 21 rows\n",
      "-------------------- Start Main Text Processing --------------------\n",
      "-------------------- Data Filtering --------------------\n",
      "----- Do <lambda> on Review Text -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3a0f2f402f44ae9dabad56b897d9fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/23486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "-------------------- Train Test Split --------------------\n",
      "No validation split defined\n",
      "Done\n",
      "-------------------- Dropping unused features --------------------\n",
      "Done\n",
      "-------------------- Shuffling and flattening train set --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3603900e2cd468cb917147977dc3a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices (num_proc=4):   0%|          | 0/22641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Review Text'],\n",
       "        num_rows: 22641\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc = TextDataLMController.from_df(df,\n",
    "                                 main_text='Review Text',\n",
    "                                 filter_dict={'Review Text': lambda x: x is not None},\n",
    "                                 val_ratio=None,\n",
    "                                 seed=42\n",
    "                                )\n",
    "ddict = tdc.do_all_preprocessing(shuffle_trn=True)\n",
    "ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "from underthesea import text_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_lm.py#L191){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataLMController.process_and_tokenize\n",
       "\n",
       ">      TextDataLMController.process_and_tokenize (tokenizer, max_length=None,\n",
       ">                                                 line_by_line=True,\n",
       ">                                                 stride=None, trn_size=None,\n",
       ">                                                 tok_num_proc=None,\n",
       ">                                                 shuffle_trn=True)\n",
       "\n",
       "This will perform `do_all_processing` then `do_tokenization`\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| max_length | NoneType | None | pad to model's allowed max length (default is max_sequence_length) |\n",
       "| line_by_line | bool | True | To whether tokenize each sentence separately, or concatenate them and then tokenize |\n",
       "| stride | NoneType | None | option to do striding when line_by_line is False |\n",
       "| trn_size | NoneType | None | The number of training data to be tokenized |\n",
       "| tok_num_proc | NoneType | None | Number of processes for tokenization |\n",
       "| shuffle_trn | bool | True | To shuffle the train set before tokenization |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_lm.py#L191){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataLMController.process_and_tokenize\n",
       "\n",
       ">      TextDataLMController.process_and_tokenize (tokenizer, max_length=None,\n",
       ">                                                 line_by_line=True,\n",
       ">                                                 stride=None, trn_size=None,\n",
       ">                                                 tok_num_proc=None,\n",
       ">                                                 shuffle_trn=True)\n",
       "\n",
       "This will perform `do_all_processing` then `do_tokenization`\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tokenizer |  |  | Tokenizer (preferably from HuggingFace) |\n",
       "| max_length | NoneType | None | pad to model's allowed max length (default is max_sequence_length) |\n",
       "| line_by_line | bool | True | To whether tokenize each sentence separately, or concatenate them and then tokenize |\n",
       "| stride | NoneType | None | option to do striding when line_by_line is False |\n",
       "| trn_size | NoneType | None | The number of training data to be tokenized |\n",
       "| tok_num_proc | NoneType | None | Number of processes for tokenization |\n",
       "| shuffle_trn | bool | True | To shuffle the train set before tokenization |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataLMController.process_and_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Tokenize our corpus line-by-line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataLMController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None},\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         cols_to_keep=['Clothing ID','Review Text'],\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With no padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.process_and_tokenize(tokenizer,line_by_line=True,max_length=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Clothing ID', 'Review Text', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 18111\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Clothing ID', 'Review Text', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 4529\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i ordered this online and was disappointed with the fit when it arrived . i ordered the xs and it was still oversize to the point of being unflattering . i am tall 5 ' 9 \" about 130 pounds and have a fairly thin torso and look best in cloths that have some shape . if you like a loose fit this might be for you . the material is thicker and warm and comfortable . i would suggest ordering down a size .\n",
      "this picture doesn't do the skirt justice . i paired it with a creme colored cashmere cowlneck sweater and a silver jeweled belt . it is really pretty and flattering on .\n"
     ]
    }
   ],
   "source": [
    "print(tdc.main_ddict['train']['Review Text'][0])\n",
    "print(tdc.main_ddict['validation']['Review Text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>i ordered this online and was disappointed with the fit when it arrived. i ordered the xs and it was still oversize to the point of being unflattering. i am tall 5'9 \" about 130 pounds and have a fairly thin torso and look best in cloths that have some shape. if you like a loose fit this might be for you. the material is thicker and warm and comfortable. i would suggest ordering down a size.</s>\n",
      "<s>this picture doesn't do the skirt justice. i paired it with a creme colored cashmere cowlneck sweater and a silver jeweled belt. it is really pretty and flattering on.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tdc.main_ddict['train']['input_ids'][0]))\n",
    "print(tokenizer.decode(tdc.main_ddict['validation']['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With padding (set `max_length` to `None` if you want to pad to model's maximum sequence length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataLMController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None},\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         cols_to_keep=['Clothing ID','Review Text'],\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.process_and_tokenize(tokenizer,line_by_line=True,max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>i ordered this online and was disappointed with the fit when it arrived. i ordered the xs and it was still oversize to the point of being unflattering. i am tall 5'9 \" about 130 pounds and have a fairly thin torso and look best in cloths that have some shape. if you like a loose fit this might be for you. the material is thicker and warm and comfortable. i would suggest ordering down a size.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<s>this picture doesn't do the skirt justice. i paired it with a creme colored cashmere cowlneck sweater and a silver jeweled belt. it is really pretty and flattering on.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tdc.main_ddict['train']['input_ids'][0]))\n",
    "print(tokenizer.decode(tdc.main_ddict['validation']['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Tokenize every text, then concatenate them together before splitting them in smaller parts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataLMController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None},\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         cols_to_keep=['Clothing ID','Review Text'],\n",
    "                         seed=42,\n",
    "                         verbose=False,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.process_and_tokenize(tokenizer,line_by_line=False,max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'special_tokens_mask', 'attention_mask'],\n",
       "        num_rows: 13573\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'special_tokens_mask', 'attention_mask'],\n",
       "        num_rows: 3446\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.main_ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that even when I put in the `cols_to_keep` parameters, the returned DatasetDict still does not keep them, because it wouldn't make sense to retain them when the tokens are concatenated. Normally, you will leave `cols_to_keep` as ```None```, which is the default, when `line_by_line` is ```False```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>i ordered this online and was disappointed with the fit when it arrived. i ordered the xs and it was still oversize to the point of being unflattering. i am tall 5'9 \" about 130 pounds and have a fairly thin torso and look best in cloths that have some shape. if you like a loose fit this might be for you. the material is thicker and warm and comfortable. i would suggest ordering down a size.</s><s>so unflattering! really disappointed. made\n",
      "----------------------------------------------------------------------------------------------------\n",
      " me look 6 month pregnant and i'm a petite size 2.</s><s>i love rompers and this one is really cute. i usually wear size 12 but should have got a 10, it runs big. it seems too long, and i'm 5'9 \". the prints cute but a little blah. i paid $ 158 which is too much, since i haven't worn it yet, i should have waited for it to go on sale.</s><s>... the print is so\n",
      "----------------------------------------------------------------------------------------------------\n",
      " sharking, and i love the way it looks on the model -- but i'm a more curvy figure, and the boxy-ish cut plus rather stuff fabric in front is incredibly unflattering. ordinarily i love everything made by maeve, but this one sadly must be returned... on a thinner / straighter-shaped person i expect it would be great.</s><s>i've had my eye on this poncho for weeks and finally scored the olive green one over thanksgiving /\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in tdc.main_ddict['train']['input_ids'][:3]:\n",
    "    print(tokenizer.decode(i))\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>this picture doesn't do the skirt justice. i paired it with a creme colored cashmere cowlneck sweater and a silver jeweled belt. it is really pretty and flattering on.</s><s>easy to wear! cute, comfy... will be a go to for summer.</s><s>nice sweater, just did not look good on me. sorry, going back.</s><s>this jacket was a little shorter than i had expected, but i still really enjoy the cut and fit of it\n",
      "----------------------------------------------------------------------------------------------------\n",
      ".</s><s>i wasn't planning on loving this dress when i tried it on. i loved the the color which is what prompted me to buy it. this dress fit perfectly. it hugs my body without feeling tight. the ruching is perfect. i didn't want to take it off! it's also very comfortable. i'm 5'1 \", 107 lbs and the xs petite fit perfectly. the dress hits me at the same length that is pictured. i think it would\n",
      "----------------------------------------------------------------------------------------------------\n",
      " be easy to hem if you wanted it to be shorter. i have a short torso and saw no issues with that as some reviewer</s><s>i like flowy tops because i have a bit of a belly and i like to camouflage it but this top was really flowy. the fabric is great and the embroidery is beautiful, i was hoping for this to be a holiday staple this year. it has to go back though, just too large. i don't love it quite enough to order\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in tdc.main_ddict['validation']['input_ids'][:3]:\n",
    "    print(tokenizer.decode(i))\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Striding (For Concatenation of tokens)\n",
    "\n",
    "If your sentences (or paragraphs) are larger than `max_length`, after concatenation, they will be broken apart; your long paragraph will be incompleted in terms of meaning. **Striding** is a way to somewhat preserve the sentence's meaning, by getting part of the sentence back. We will demonstrate it with an example, and you can compare it with the previous one (without striding) to see the differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataLMController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None},\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         seed=42,\n",
    "                         verbose=False,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.process_and_tokenize(tokenizer,line_by_line=False,max_length=100,stride=20)\n",
    "# Stride is 20, meaning for the next entry, we go back 20 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>i ordered this online and was disappointed with the fit when it arrived. i ordered the xs and it was still oversize to the point of being unflattering. i am tall 5'9 \" about 130 pounds and have a fairly thin torso and look best in cloths that have some shape. if you like a loose fit this might be for you. the material is thicker and warm and comfortable. i would suggest ordering down a size.</s><s>so unflattering! really disappointed. made\n",
      "----------------------------------------------------------------------------------------------------\n",
      " comfortable. i would suggest ordering down a size.</s><s>so unflattering! really disappointed. made me look 6 month pregnant and i'm a petite size 2.</s><s>i love rompers and this one is really cute. i usually wear size 12 but should have got a 10, it runs big. it seems too long, and i'm 5'9 \". the prints cute but a little blah. i paid $ 158 which is too much, since i haven't worn it\n",
      "----------------------------------------------------------------------------------------------------\n",
      " but a little blah. i paid $ 158 which is too much, since i haven't worn it yet, i should have waited for it to go on sale.</s><s>... the print is so sharking, and i love the way it looks on the model -- but i'm a more curvy figure, and the boxy-ish cut plus rather stuff fabric in front is incredibly unflattering. ordinarily i love everything made by maeve, but this one sadly must be returned... on\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in tdc.main_ddict['train']['input_ids'][:3]:\n",
    "    print(tokenizer.decode(i))\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second entry, we can see it starts with the last 20 tokens of the previous entry: `comfortable. i would suggest ordering down a size.</s><s>so unflattering! really disappointed. made`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>this picture doesn't do the skirt justice. i paired it with a creme colored cashmere cowlneck sweater and a silver jeweled belt. it is really pretty and flattering on.</s><s>easy to wear! cute, comfy... will be a go to for summer.</s><s>nice sweater, just did not look good on me. sorry, going back.</s><s>this jacket was a little shorter than i had expected, but i still really enjoy the cut and fit of it\n",
      "----------------------------------------------------------------------------------------------------\n",
      " was a little shorter than i had expected, but i still really enjoy the cut and fit of it.</s><s>i wasn't planning on loving this dress when i tried it on. i loved the the color which is what prompted me to buy it. this dress fit perfectly. it hugs my body without feeling tight. the ruching is perfect. i didn't want to take it off! it's also very comfortable. i'm 5'1 \", 107 lbs and the xs pet\n",
      "----------------------------------------------------------------------------------------------------\n",
      " it's also very comfortable. i'm 5'1 \", 107 lbs and the xs petite fit perfectly. the dress hits me at the same length that is pictured. i think it would be easy to hem if you wanted it to be shorter. i have a short torso and saw no issues with that as some reviewer</s><s>i like flowy tops because i have a bit of a belly and i like to camouflage it but this top was really flowy. the fabric is great and\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in tdc.main_ddict['validation']['input_ids'][:3]:\n",
    "    print(tokenizer.decode(i))\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import text_normalize\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For masked language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our text controller first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataLMController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None},\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         cols_to_keep=['Clothing ID','Review Text'],\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will tokenize our corpus line-by-line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.process_and_tokenize(tokenizer,line_by_line=True,max_length=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.set_data_collator(is_mlm=True,mlm_prob=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForLanguageModeling(tokenizer=RobertaTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)}, clean_up_tokenization_spaces=True), mlm=True, mlm_probability=0.15, pad_to_multiple_of=8, tf_experimental_compile=False, return_tensors='pt')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc.data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying the collator...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Clothing ID': 937, 'Review Text': 'i ordered this online and was disappointed with the fit when it arrived . i ordered the xs and it was still oversize to the point of being unflattering . i am tall 5 \\' 9 \" about 130 pounds and have a fairly thin torso and look best in cloths that have some shape . if you like a loose fit this might be for you . the material is thicker and warm and comfortable . i would suggest ordering down a size .', 'input_ids': [0, 118, 2740, 42, 804, 8, 21, 5779, 19, 5, 2564, 77, 24, 2035, 479, 939, 2740, 5, 3023, 29, 8, 24, 21, 202, 81, 10799, 7, 5, 477, 9, 145, 29747, 24203, 479, 939, 524, 6764, 195, 128, 361, 22, 59, 8325, 2697, 8, 33, 10, 5342, 7174, 28762, 8, 356, 275, 11, 21543, 29, 14, 33, 103, 3989, 479, 114, 47, 101, 10, 7082, 2564, 42, 429, 28, 13, 47, 479, 5, 1468, 16, 33997, 8, 3279, 8, 3473, 479, 939, 74, 3608, 12926, 159, 10, 1836, 479, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}, {'Clothing ID': 870, 'Review Text': \"so unflattering ! really disappointed . made me look 6 month pregnant and i'm a petite size 2 .\", 'input_ids': [0, 2527, 29747, 24203, 27785, 269, 5779, 479, 156, 162, 356, 231, 353, 5283, 8, 939, 437, 10, 4716, 1459, 1836, 132, 479, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}]\n"
     ]
    }
   ],
   "source": [
    "print([tdc.main_ddict['train'][i] for i in range(2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the length of each token list is different from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[91, 24, 79, 82, 121]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(len,tdc.main_ddict['train']['input_ids'][:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only the required keys\n",
    "inp_keys = tokenizer.model_input_names\n",
    "_inp = [{k:tdc.main_ddict['train'][i][k] for k in inp_keys} for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_ids': [0, 118, 2740, 42, 804, 8, 21, 5779, 19, 5, 2564, 77, 24, 2035, 479, 939, 2740, 5, 3023, 29, 8, 24, 21, 202, 81, 10799, 7, 5, 477, 9, 145, 29747, 24203, 479, 939, 524, 6764, 195, 128, 361, 22, 59, 8325, 2697, 8, 33, 10, 5342, 7174, 28762, 8, 356, 275, 11, 21543, 29, 14, 33, 103, 3989, 479, 114, 47, 101, 10, 7082, 2564, 42, 429, 28, 13, 47, 479, 5, 1468, 16, 33997, 8, 3279, 8, 3473, 479, 939, 74, 3608, 12926, 159, 10, 1836, 479, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [0, 2527, 29747, 24203, 27785, 269, 5779, 479, 156, 162, 356, 231, 353, 5283, 8, 939, 437, 10, 4716, 1459, 1836, 132, 479, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}]\n"
     ]
    }
   ],
   "source": [
    "print(_inp[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tdc.data_collator(_inp) # simulation with batch size 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all token lists have the same length, which is 128: a multiple of 8 and larger than the longest list in the batch (which is 121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 128])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   118,  2740,    42,   804,     8,    21,  5779,    19, 50264,\n",
       "          2564,    77,    24,  2035,   479,   939,  2740,     5,  3023,    29,\n",
       "             8,    24,    21,   202, 50264, 10799,     7,     5,   477, 50264,\n",
       "           145, 50264, 24203,   479,   939,   524,  6764,   195,   128,   361,\n",
       "            22,    59,  8325,  2697,     8,    33,    10,  5342,  7174, 28762,\n",
       "         50264,   356, 50264,    11, 21543,    29,    14,    33,   103, 38941,\n",
       "           479,   114,    47,   101,    10,  7082,  2564,    42,   429,    28,\n",
       "            13,    47,   479, 50264,  1468, 44089, 33997,     8,  3279,     8,\n",
       "          3473,   479,   939,    74,  3608, 12926,   159, 50264,  1836,   479,\n",
       "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1],\n",
       "        [    0,  2527, 29747, 24203, 50264,   269,  5779,   479, 50264, 50264,\n",
       "           356,   231,   353, 50264,     8, 50264,   437,    10,  4716,  1459,\n",
       "          1836, 49943,   479,     2,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['input_ids'][:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `labels` have also been constructed, which shows the \"mask\" tokens (non -100) in which the model has to predict. To increase the amount of masked tokens, increase the `mlm_prob`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,     5,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,    81,  -100,  -100,  -100,  -100,     9,\n",
       "          -100, 29747,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "             8,  -100,   275,  -100,  -100,  -100,  -100,  -100,  -100,  3989,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,     5,  1468,    16,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,    10,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [ -100,  -100,  -100,  -100, 27785,  -100,  -100,  -100,   156,   162,\n",
       "          -100,  -100,  -100,  5283,  -100,   939,  -100,  -100,  -100,  -100,\n",
       "          -100,   132,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['labels'][:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you apply padding in the tokenization step (by adjusting the `max_length` argument), no matter whether it's line-by-line tokenization or not, the data collator will skip the padding step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataLMController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None},\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         cols_to_keep=['Clothing ID','Review Text'],\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.process_and_tokenize(tokenizer,line_by_line=False,max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.set_data_collator(is_mlm=True,mlm_prob=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 100, 100, 100, 100]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(len,tdc.main_ddict['train']['input_ids'][:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_keys = tokenizer.model_input_names\n",
    "_inp = [{k:tdc.main_ddict['train'][i][k] for k in inp_keys} for i in range(5)]\n",
    "out = tdc.data_collator(_inp) # simulation with batch size 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 100])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   118,  2740,    42,   804,     8,    21,  5779,    19, 50264,\n",
       "          2564,    77,    24,  2035,   479,   939,  2740,     5,  3023,    29,\n",
       "             8,    24,    21,   202,    81, 10799,     7,     5,   477, 50264,\n",
       "           145, 50264, 24203,   479,   939,   524,  6764,   195,   128,   361,\n",
       "            22,    59,  8325,  2697,     8,    33,    10,  5342,  7174, 28762,\n",
       "         50264,   356, 50264,    11, 21543,    29,    14,    33,   103, 41316,\n",
       "           479,   114,    47,   101,    10,  7082,  2564,    42,   429,    28,\n",
       "            13,    47,   479, 50264, 17204, 50264, 33997,     8,  3279,     8,\n",
       "          3473,   479,   939,    74,  3608, 12926,   159, 50264,  1836,   479,\n",
       "             2,     0,  2527, 29747, 50264, 27785,   269,  5779,   479, 50264],\n",
       "        [  162,   356,   231, 50264,  5283,     8, 50264,   437, 23781,  4716,\n",
       "          1459,  1836,   132,   479,     2,     0,   118, 50264,   910,  7474,\n",
       "           268,     8,    42,    65,    16,   269, 11962, 50264,   939,  2333,\n",
       "          3568,  1836, 50264,    53,   197,    33, 50264, 50264,   158,  2156,\n",
       "            24, 50264,   380, 44224,    24,  1302,   350,   251,  2156, 50264,\n",
       "           939,   437,   195, 50264,   361,    22,   479,     5, 19553, 11962,\n",
       "            53,    10,   410, 50264,   479,   939,  1199,    68, 26498,    61,\n",
       "            16,   350,   203,  2156,   187,   939, 50264,    75, 10610, 50264,\n",
       "           648,  2156,   939,   197,    33,  9010,    13,    24,     7,   213,\n",
       "            15,  1392,   479,     2,     0,   734,     5,  5780,    16,    98]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['input_ids'][:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,     5,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,    81,  -100,  -100,  -100,  -100,     9,\n",
       "          -100, 29747,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "             8,  -100,   275,  -100,  -100,  -100,  -100,  -100,  -100,  3989,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,     5,  1468,    16,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,    10,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100, 24203,  -100,  -100,  -100,  -100,   156],\n",
       "        [ -100,  -100,  -100,   353,  -100,  -100,   939,  -100,    10,  -100,\n",
       "          -100,  -100,  -100,   479,  -100,  -100,  -100,   657,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,   479,  -100,  -100,\n",
       "          -100,  -100,   316,  -100,  -100,  -100,   300,    10,  -100,  -100,\n",
       "          -100,  1237,  -100,   479,  -100,  -100,  -100,  -100,  -100,     8,\n",
       "          -100,  -100,  -100,   128,  -100,    22,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100, 38596,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  2220,  -100,  -100,    24,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['labels'][:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using the concatenation-of-tokenization technique, one smart thing that the HuggingFace's `DataCollatorForLanguageModeling` (which is the data collator we use) does is to allow maskings at every position, at opposed to to the previous cases (with line-by-line tokenization), there's no masking near the end tokens of each list, because those end tokens are padding tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For causal language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from tokenizers import processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our GPT2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT2 does not use start/end-of-sentence token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'text', '.', 'That', 'is', 'a', 'second', 'text', '.', 'But', 'there', \"'s\", 'a', 'third', 'one']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(tokenizer(\"this is a text. That is a second text.But there's a third one\")['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to perform concatenation-of-token, and you want your causal LM to differentiate between sentences, you can add a special token to separate sentences, as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"$A \" + tokenizer.eos_token,\n",
    "    special_tokens=[(tokenizer.eos_token, tokenizer.eos_token_id)],\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'text', '.', 'That', 'is', 'a', 'second', 'text', '.', 'But', 'there', \"'s\", 'a', 'third', 'one', '<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(tokenizer(\"this is a text. That is a second text.But there's a third one\")['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this modified tokenizer, let's perform concatenation-of-tokenization using GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataLMController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None},\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.process_and_tokenize(tokenizer,line_by_line=False,max_length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's casual language modeling, let's turn off `is_mlm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.set_data_collator(is_mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 100, 100, 100, 100]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(len,tdc.main_ddict['train']['input_ids'][:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "out = tdc.data_collator([tdc.main_ddict['train'][i] for i in range(5)]) # simulation with batch size 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 100])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   72,  6149,   428,  2691,   290,   373, 11679,   351,   262,  4197,\n",
       "           618,   340,  5284,   764,  1312,  6149,   262,  2124,    82,   290,\n",
       "           340,   373,   991,   625,  7857,   284,   262,   966,   286,   852,\n",
       "         42880, 16475,   764,  1312,   716,  7331,   642,   705,   860,   366,\n",
       "           546, 11323,  8059,   290,   423,   257,  6547,  7888, 28668,   290,\n",
       "           804,  1266,   287, 16270,    82,   326,   423,   617,  5485,   764,\n",
       "           611,   345,   588,   257,  9155,  4197,   428,  1244,   307,   329,\n",
       "           345,   764,   262,  2587,   318, 29175,   290,  5814,   290,  6792,\n",
       "           764,  1312,   561,  1950, 16216,   866,   257,  2546,   764, 50256,\n",
       "           568, 42880, 16475,  5145,  1107, 11679,   764,   925,   502,   804],\n",
       "        [  718,  1227, 10423,   290,  1312,  1101,   257,  4273,   578,  2546,\n",
       "           362,   764, 50256,    72,  1842,   374,  3361,   364,   290,   428,\n",
       "           530,   318,  1107, 13779,   764,  1312,  3221,  5806,  2546,  1105,\n",
       "           475,   815,   423,  1392,   257,   838,   837,   340,  4539,  1263,\n",
       "           764,   340,  2331,  1165,   890,   837,   290,  1312,  1101,   642,\n",
       "           705,   860,   366,   764,   262, 20842, 13779,   475,   257,  1310,\n",
       "         33367,   764,  1312,  3432,   720, 24063,   543,   318,  1165,   881,\n",
       "           837,  1201,  1312,  4398,   470, 12666,   340,  1865,   837,  1312,\n",
       "           815,   423, 13488,   329,   340,   284,   467,   319,  5466,   764,\n",
       "         50256,   986,   262,  3601,   318,   523, 21027,   278,   837,   290]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['input_ids'][:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   72,  6149,   428,  2691,   290,   373, 11679,   351,   262,  4197,\n",
       "           618,   340,  5284,   764,  1312,  6149,   262,  2124,    82,   290,\n",
       "           340,   373,   991,   625,  7857,   284,   262,   966,   286,   852,\n",
       "         42880, 16475,   764,  1312,   716,  7331,   642,   705,   860,   366,\n",
       "           546, 11323,  8059,   290,   423,   257,  6547,  7888, 28668,   290,\n",
       "           804,  1266,   287, 16270,    82,   326,   423,   617,  5485,   764,\n",
       "           611,   345,   588,   257,  9155,  4197,   428,  1244,   307,   329,\n",
       "           345,   764,   262,  2587,   318, 29175,   290,  5814,   290,  6792,\n",
       "           764,  1312,   561,  1950, 16216,   866,   257,  2546,   764,  -100,\n",
       "           568, 42880, 16475,  5145,  1107, 11679,   764,   925,   502,   804],\n",
       "        [  718,  1227, 10423,   290,  1312,  1101,   257,  4273,   578,  2546,\n",
       "           362,   764,  -100,    72,  1842,   374,  3361,   364,   290,   428,\n",
       "           530,   318,  1107, 13779,   764,  1312,  3221,  5806,  2546,  1105,\n",
       "           475,   815,   423,  1392,   257,   838,   837,   340,  4539,  1263,\n",
       "           764,   340,  2331,  1165,   890,   837,   290,  1312,  1101,   642,\n",
       "           705,   860,   366,   764,   262, 20842, 13779,   475,   257,  1310,\n",
       "         33367,   764,  1312,  3432,   720, 24063,   543,   318,  1165,   881,\n",
       "           837,  1201,  1312,  4398,   470, 12666,   340,  1865,   837,  1312,\n",
       "           815,   423, 13488,   329,   340,   284,   467,   319,  5466,   764,\n",
       "          -100,   986,   262,  3601,   318,   523, 21027,   278,   837,   290]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['labels'][:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CLM, the `labels` are essentially the same as `input_ids`. From HuggingFace documentation:\n",
    "```\n",
    "`DataCollatorForLanguageModeling` will take care of creating the language model labels  in causal language modeling the inputs serve as labels too (just shifted by one element), and this data collator creates them on the fly during training.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load TextDataController"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_lm.py#L63){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataLMController.save_as_pickles\n",
       "\n",
       ">      TextDataLMController.save_as_pickles (fname, parent='pickle_files')\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main_lm.py#L63){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataLMController.save_as_pickles\n",
       "\n",
       ">      TextDataLMController.save_as_pickles (fname, parent='pickle_files')\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataLMController.save_as_pickles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.from_pickle\n",
       "\n",
       ">      TextDataController.from_pickle (fname, parent='pickle_files')\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_main.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextDataController.from_pickle\n",
       "\n",
       ">      TextDataController.from_pickle (fname, parent='pickle_files')\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| fname |  |  | Name of the pickle file |\n",
       "| parent | str | pickle_files | Parent folder |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextDataLMController.from_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextDataLMController object can be saved and loaded with ease. This is especially useful after text processing and/or tokenization have been done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import disable_caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n",
    "tdc = TextDataLMController(dset,\n",
    "                         main_text='Review Text',\n",
    "                         filter_dict={'Review Text': lambda x: x is not None},\n",
    "                         content_transformations=[text_normalize,str.lower],\n",
    "                         seed=42,\n",
    "                         verbose=False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.process_and_tokenize(tokenizer,line_by_line=True,max_length=-1)\n",
    "\n",
    "tdc.set_data_collator(is_mlm=True,mlm_prob=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.save_as_pickles('my_lm_tdc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load back our object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc2 = TextDataLMController.from_pickle('my_lm_tdc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can still access all its attributes, data, preprocessings, transformations ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Review Text', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 18111\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Review Text', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 4529\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc2.main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Review Text': <function __main__.<lambda>(x)>},\n",
       " [<function underthesea.pipeline.text_normalize.text_normalize(text, tokenizer='underthesea')>,\n",
       "  <method 'lower' of 'str' objects>])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdc2.filter_dict,tdc2.content_tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
