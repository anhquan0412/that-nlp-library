{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Transformation\n",
    "\n",
    "> This contains some text transformation functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp text_transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from underthesea import word_tokenize, sent_tokenize, text_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def apply_vnmese_word_tokenize(sentence:str, # Input sentence\n",
    "                        normalize_text=False, # To 'normalize' the text before tokenization\n",
    "                        fixed_words=[]\n",
    "                       ):\n",
    "    \"Applying UnderTheSea Vietnamese word tokenization\"\n",
    "    if normalize_text:\n",
    "        sentence = text_normalize(sentence)\n",
    "    sens = sent_tokenize(sentence)\n",
    "\n",
    "    tokenized_sen = []\n",
    "    for sen in sens:\n",
    "        tokenized_sen.append(word_tokenize(sen,format='text',fixed_words=fixed_words))\n",
    "    return ' '.join(tokenized_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_transformation.py#L11){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### apply_vnmese_word_tokenize\n",
       "\n",
       ">      apply_vnmese_word_tokenize (sentence:str, normalize_text=False,\n",
       ">                                  fixed_words=[])\n",
       "\n",
       "Applying UnderTheSea Vietnamese word tokenization\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| sentence | str |  | Input sentence |\n",
       "| normalize_text | bool | False | To 'normalize' the text before tokenization |\n",
       "| fixed_words | list | [] |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/text_transformation.py#L11){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### apply_vnmese_word_tokenize\n",
       "\n",
       ">      apply_vnmese_word_tokenize (sentence:str, normalize_text=False,\n",
       ">                                  fixed_words=[])\n",
       "\n",
       "Applying UnderTheSea Vietnamese word tokenization\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| sentence | str |  | Input sentence |\n",
       "| normalize_text | bool | False | To 'normalize' the text before tokenization |\n",
       "| fixed_words | list | [] |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(apply_vnmese_word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For non-Vietnamese word, it's a hit-or-miss since UnderTheSea works best for Vietnamese sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a_cat . New_York city . San_Francisco . New_York and_San_Francisco Bay area . George Bush , Barrack Obama'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'This is a cat. New York city. San Francisco. New York and San Francisco Bay area. George Bush, Barrack Obama'\n",
    "apply_vnmese_word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example on a clean Vietnamese sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ch√†ng trai 9X Qu·∫£ng_Tr·ªã kh·ªüi_nghi·ªáp t·ª´ n·∫•m s√≤'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Ch√†ng trai 9X Qu·∫£ng Tr·ªã kh·ªüi nghi·ªáp t·ª´ n·∫•m s√≤'\n",
    "apply_vnmese_word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if the sentence is not cleaned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Ch√†ng trai 9X Qu·∫£ng Tr·ªã kh·ªüi nghi·ªáp t·ª´ n·∫•m s√≤.Anh ·∫•y kh√¥ng nu√¥i   n·∫•müòä. nh∆∞ng anh n√†y nu√¥i. Ch·ªã ·∫•y l·∫°i kh√¥ng nu√¥i?(ai bi·∫øt t·∫°i sao üòäüòä? )R·ªìi? R·ªìi sao?r·ªìi ?R·ªìi ·ªßa...ch·ª© ch·ªã ·∫•y nu√¥i g√¨, #m·ªôthaiba c≈©ng kh√¥ng r√µ =)) üòä. Haha :) üòä hehe üòä.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ch√†ng trai 9X Qu·∫£ng_Tr·ªã kh·ªüi_nghi·ªáp t·ª´ n·∫•m s√≤ . Anh ·∫•y kh√¥ng nu√¥i n·∫•m üòä . nh∆∞ng anh n√†y nu√¥i . Ch·ªã ·∫•y l·∫°i kh√¥ng nu√¥i ? ( ai bi·∫øt t·∫°i_sao üòä_üòä ? ) R·ªìi ? R·ªìi sao ? r·ªìi ? R·ªìi ·ªßa ... ch·ª© ch·ªã ·∫•y nu√¥i g√¨ , #_m·ªôthaiba c≈©ng kh√¥ng r√µ =))_üòä . Haha :) üòä hehe üòä .'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_vnmese_word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to normalize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ch√†ng trai 9X Qu·∫£ng_Tr·ªã kh·ªüi_nghi·ªáp t·ª´ n·∫•m s√≤ . Anh ·∫•y kh√¥ng nu√¥i n·∫•m üòä . nh∆∞ng anh n√†y nu√¥i . Ch·ªã ·∫•y l·∫°i kh√¥ng nu√¥i ? ( ai bi·∫øt t·∫°i_sao üòä_üòä ? ) R·ªìi ? R·ªìi sao ? r·ªìi ? R·ªìi ·ªßa ... ch·ª© ch·ªã ·∫•y nu√¥i g√¨ , #_m·ªôthaiba c≈©ng kh√¥ng r√µ =))_üòä . Haha :) üòä hehe üòä .'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_vnmese_word_tokenize(text,normalize_text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add a list of specific words to tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vi·ªán Nghi√™n_C·ª©u chi·∫øn_l∆∞·ª£c qu·ªëc_gia v·ªÅ h·ªçc m√°y'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Vi·ªán Nghi√™n C·ª©u chi·∫øn l∆∞·ª£c qu·ªëc gia v·ªÅ h·ªçc m√°y\"\n",
    "apply_vnmese_word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vi·ªán_Nghi√™n_C·ª©u chi·∫øn_l∆∞·ª£c qu·ªëc_gia v·ªÅ h·ªçc_m√°y'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_vnmese_word_tokenize(text,fixed_words=[\"Vi·ªán Nghi√™n C·ª©u\", \"h·ªçc m√°y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
