{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2 Classifiers\n",
    "\n",
    "> This module contains code to build a text classification model using GPT2-related model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.gpt2.classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import torch\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model,GPT2PreTrainedModel\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.modeling_outputs import SequenceClassifierOutputWithPast\n",
    "from that_nlp_library.model_main import loss_for_classification\n",
    "from torch.nn import MSELoss\n",
    "from that_nlp_library.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main classification architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GPT2BaseForSequenceClassification(GPT2PreTrainedModel):\n",
    "    \"\"\"\n",
    "    GPT2 Architecture for Sequence Classification task\n",
    "    Based on: https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L1376\n",
    "    \"\"\"\n",
    "    config_class = GPT2Config\n",
    "\n",
    "    def __init__(self,\n",
    "                 config, # HuggingFace model configuration\n",
    "                 is_multilabel=False, # Whether this is a multilabel classification\n",
    "                 is_multihead=False, # Whether this is a multihead (multi-level) classification\n",
    "                 head_class_sizes=[], # Class size for each head\n",
    "                 head_weights=[], # loss weight for each head. This will be multiplied to the loss of each head's output\n",
    "                 head_class=None, # The class object of the head. \n",
    "                 **head_class_kwargs, # Keyword arguments for the head class\n",
    "                ):\n",
    "        super().__init__(config)\n",
    "        self.is_multilabel = is_multilabel\n",
    "        self.is_multihead = is_multihead\n",
    "        self.head_class_sizes = val2iterable(head_class_sizes)\n",
    "        self.head_weights = val2iterable(head_weights,lsize=len(self.head_class_sizes))\n",
    "        \n",
    "        # set num_labels for config\n",
    "        num_labels = sum(self.head_class_sizes)\n",
    "        config.num_labels = num_labels\n",
    "        \n",
    "        self.body_model = GPT2Model(config)\n",
    "        \n",
    "        # Set up token classification head\n",
    "        if head_class is None:\n",
    "            self.head = torch.nn.Linear(config.n_embd, num_labels, bias=False)\n",
    "        else:\n",
    "            self.head = head_class(config,**head_class_kwargs)\n",
    "\n",
    "        \n",
    "        # Model parallel\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "        \n",
    "        self.config.pad_token_id = self.config.eos_token_id\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids= None,\n",
    "        past_key_values= None,\n",
    "        attention_mask= None,\n",
    "        token_type_ids= None,\n",
    "        position_ids= None,\n",
    "        head_mask= None,\n",
    "        inputs_embeds= None,\n",
    "        labels= None,\n",
    "        use_cache= None,\n",
    "        output_attentions= None,\n",
    "        output_hidden_states= None,\n",
    "        return_dict= None,\n",
    "    ):\n",
    "        # Use model body to get encoder representations\n",
    "        # the only ones we need for now are input_ids and attention_mask\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "\n",
    "        outputs = self.body_model(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "              \n",
    "        sequence_output = outputs[0] # last hidden state: (bs,sequence_length,hidden_size: 768)\n",
    "        \n",
    "        # get the idx of the last token (typically just -1), to be used for classification\n",
    "        if input_ids is not None:\n",
    "            batch_size, sequence_length = input_ids.shape[:2]\n",
    "        else:\n",
    "            batch_size, sequence_length = inputs_embeds.shape[:2]\n",
    "\n",
    "        assert (\n",
    "            self.config.pad_token_id is not None or batch_size == 1\n",
    "        ), \"Cannot handle batch sizes > 1 if no padding token is defined.\"\n",
    "        if self.config.pad_token_id is None:\n",
    "            sequence_lengths = -1\n",
    "        else:\n",
    "            if input_ids is not None:\n",
    "                sequence_lengths = (torch.eq(input_ids, self.config.pad_token_id).long().argmax(-1) - 1).to(\n",
    "                    sequence_output.device\n",
    "                )\n",
    "            else:\n",
    "                sequence_lengths = -1\n",
    "                logger.warning(\n",
    "                    f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n",
    "                    \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n",
    "                )\n",
    "                \n",
    "        # sequence length at this point is just the idx (or indices) of the last token        \n",
    "        sequence_output = sequence_output[torch.arange(batch_size, device=sequence_output.device), \n",
    "                                          sequence_lengths,:] # (bs,hidden_sizes)\n",
    "        \n",
    "        logits = self.head(sequence_output) # (bs,sum of all class sizes)\n",
    "        \n",
    "        # Calculate losses\n",
    "        if labels is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            if self.config.num_labels==1:\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.squeeze(),labels.squeeze())\n",
    "            else:\n",
    "                loss = loss_for_classification(logits, labels, \n",
    "                                       self.is_multilabel,\n",
    "                                       self.is_multihead, \n",
    "                                       self.head_class_sizes,\n",
    "                                       self.head_weights)\n",
    "            \n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "        \n",
    "        # Return model output object\n",
    "        \n",
    "        return SequenceClassifierOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=None,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/models/gpt2/classifiers.py#L16){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### GPT2BaseForSequenceClassification\n",
       "\n",
       ">      GPT2BaseForSequenceClassification (config, is_multilabel=False,\n",
       ">                                         is_multihead=False,\n",
       ">                                         head_class_sizes=[], head_weights=[],\n",
       ">                                         head_class=None, **head_class_kwargs)\n",
       "\n",
       "GPT2 Architecture for Sequence Classification task\n",
       "Based on: https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L1376\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| config |  |  | HuggingFace model configuration |\n",
       "| is_multilabel | bool | False | Whether this is a multilabel classification |\n",
       "| is_multihead | bool | False | Whether this is a multihead (multi-level) classification |\n",
       "| head_class_sizes | list | [] | Class size for each head |\n",
       "| head_weights | list | [] | loss weight for each head. This will be multiplied to the loss of each head's output |\n",
       "| head_class | NoneType | None | The class object of the head. |\n",
       "| head_class_kwargs |  |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/models/gpt2/classifiers.py#L16){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### GPT2BaseForSequenceClassification\n",
       "\n",
       ">      GPT2BaseForSequenceClassification (config, is_multilabel=False,\n",
       ">                                         is_multihead=False,\n",
       ">                                         head_class_sizes=[], head_weights=[],\n",
       ">                                         head_class=None, **head_class_kwargs)\n",
       "\n",
       "GPT2 Architecture for Sequence Classification task\n",
       "Based on: https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L1376\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| config |  |  | HuggingFace model configuration |\n",
       "| is_multilabel | bool | False | Whether this is a multilabel classification |\n",
       "| is_multihead | bool | False | Whether this is a multihead (multi-level) classification |\n",
       "| head_class_sizes | list | [] | Class size for each head |\n",
       "| head_weights | list | [] | loss weight for each head. This will be multiplied to the loss of each head's output |\n",
       "| head_class | NoneType | None | The class object of the head. |\n",
       "| head_class_kwargs |  |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(GPT2BaseForSequenceClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GPT2HiddenStateConcatForSequenceClassification(GPT2PreTrainedModel):\n",
    "    \"\"\"\n",
    "    GPT2 Architecture for Sequence Classification task\n",
    "    Based on: https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L1376\n",
    "    \"\"\"\n",
    "    config_class = GPT2Config\n",
    "\n",
    "    def __init__(self,config, # HuggingFace model configuration\n",
    "                 layer2concat=4, # number of hidden layer to concatenate (counting from top)\n",
    "                 is_multilabel=False, # Whether this is a multilabel classification\n",
    "                 is_multihead=False, # Whether this is a multihead (multi-level) classification\n",
    "                 head_class_sizes=[], # Class size for each head\n",
    "                 head_weights=[], # loss weight for each head. This will be multiplied to the loss of each head's output\n",
    "                 head_class=None, # The class object of the head\n",
    "                 **head_class_kwargs, # Keyword arguments for the head class\n",
    "                ):\n",
    "        super().__init__(config)\n",
    "        self.is_multilabel = is_multilabel\n",
    "        self.is_multihead = is_multihead\n",
    "        self.head_class_sizes = val2iterable(head_class_sizes)\n",
    "        self.head_weights = val2iterable(head_weights,lsize=len(self.head_class_sizes))\n",
    "        self.layer2concat=layer2concat\n",
    "\n",
    "        # set num_labels for config\n",
    "        num_labels = sum(self.head_class_sizes)\n",
    "        config.num_labels = num_labels\n",
    "        \n",
    "        self.body_model = GPT2Model(config)\n",
    "        \n",
    "        # Set up classification head\n",
    "        self.head = head_class(config=config,layer2concat=layer2concat,\n",
    "                                              **head_class_kwargs)\n",
    "\n",
    "        # Model parallel\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "\n",
    "        self.config.pad_token_id = self.config.eos_token_id\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids= None,\n",
    "        past_key_values= None,\n",
    "        attention_mask= None,\n",
    "        token_type_ids= None,\n",
    "        position_ids= None,\n",
    "        head_mask= None,\n",
    "        inputs_embeds= None,\n",
    "        labels= None,\n",
    "        use_cache= None,\n",
    "        output_attentions= None,\n",
    "        output_hidden_states= None,\n",
    "        return_dict= None):\n",
    "        \n",
    "        \n",
    "        # Use model body to get encoder representations\n",
    "        # the only ones we need for now are input_ids and attention_mask\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        \n",
    "        outputs = self.body_model(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "              \n",
    "        sequence_output = outputs[0] # (bs,sequence_length,hidden_size)\n",
    "        hidden_states = outputs['hidden_states'] # tuples with 12 layers\n",
    "        \n",
    "        # get the idx of the last token (typically just -1), to be used for classification\n",
    "        if input_ids is not None:\n",
    "            batch_size, sequence_length = input_ids.shape[:2]\n",
    "        else:\n",
    "            batch_size, sequence_length = inputs_embeds.shape[:2]\n",
    "\n",
    "        assert (\n",
    "            self.config.pad_token_id is not None or batch_size == 1\n",
    "        ), \"Cannot handle batch sizes > 1 if no padding token is defined.\"\n",
    "        if self.config.pad_token_id is None:\n",
    "            sequence_lengths = -1\n",
    "        else:\n",
    "            if input_ids is not None:\n",
    "#                 sequence_lengths = torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1\n",
    "                sequence_lengths = (torch.eq(input_ids, self.config.pad_token_id).long().argmax(-1) - 1).to(\n",
    "                                    sequence_output.device\n",
    "                                )\n",
    "            else:\n",
    "                sequence_lengths = -1\n",
    "                logger.warning(\n",
    "                    f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n",
    "                    \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n",
    "                )\n",
    "        # sequence length at this point is just the idx (or indices) of the last token        \n",
    "        \n",
    "        hidden_concat = torch.cat([hidden_states[i][torch.arange(batch_size, device=sequence_output.device), sequence_lengths,:] for i in range(-1,-self.layer2concat-1,-1)],\n",
    "                                  -1)        \n",
    "        logits = self.head(hidden_concat) # (bs,sum of all class sizes)\n",
    "        \n",
    "    \n",
    "        # Calculate losses\n",
    "        if labels is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            if self.config.num_labels==1:\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.squeeze(),labels.squeeze())\n",
    "            else:\n",
    "                loss = loss_for_classification(logits, labels, \n",
    "                                       self.is_multilabel,\n",
    "                                       self.is_multihead, \n",
    "                                       self.head_class_sizes,\n",
    "                                       self.head_weights)\n",
    "            \n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "        \n",
    "        # Return model output object\n",
    "        return SequenceClassifierOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=None,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/models/gpt2/classifiers.py#L147){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### GPT2HiddenStateConcatForSequenceClassification\n",
       "\n",
       ">      GPT2HiddenStateConcatForSequenceClassification (config, layer2concat=4,\n",
       ">                                                      is_multilabel=False,\n",
       ">                                                      is_multihead=False,\n",
       ">                                                      head_class_sizes=[],\n",
       ">                                                      head_weights=[],\n",
       ">                                                      head_class=None,\n",
       ">                                                      **head_class_kwargs)\n",
       "\n",
       "GPT2 Architecture for Sequence Classification task\n",
       "Based on: https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L1376\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| config |  |  | HuggingFace model configuration |\n",
       "| layer2concat | int | 4 | number of hidden layer to concatenate (counting from top) |\n",
       "| is_multilabel | bool | False | Whether this is a multilabel classification |\n",
       "| is_multihead | bool | False | Whether this is a multihead (multi-level) classification |\n",
       "| head_class_sizes | list | [] | Class size for each head |\n",
       "| head_weights | list | [] | loss weight for each head. This will be multiplied to the loss of each head's output |\n",
       "| head_class | NoneType | None | The class object of the head |\n",
       "| head_class_kwargs |  |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/anhquan0412/that-nlp-library/blob/main/that_nlp_library/models/gpt2/classifiers.py#L147){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### GPT2HiddenStateConcatForSequenceClassification\n",
       "\n",
       ">      GPT2HiddenStateConcatForSequenceClassification (config, layer2concat=4,\n",
       ">                                                      is_multilabel=False,\n",
       ">                                                      is_multihead=False,\n",
       ">                                                      head_class_sizes=[],\n",
       ">                                                      head_weights=[],\n",
       ">                                                      head_class=None,\n",
       ">                                                      **head_class_kwargs)\n",
       "\n",
       "GPT2 Architecture for Sequence Classification task\n",
       "Based on: https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L1376\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| config |  |  | HuggingFace model configuration |\n",
       "| layer2concat | int | 4 | number of hidden layer to concatenate (counting from top) |\n",
       "| is_multilabel | bool | False | Whether this is a multilabel classification |\n",
       "| is_multihead | bool | False | Whether this is a multihead (multi-level) classification |\n",
       "| head_class_sizes | list | [] | Class size for each head |\n",
       "| head_weights | list | [] | loss weight for each head. This will be multiplied to the loss of each head's output |\n",
       "| head_class | NoneType | None | The class object of the head |\n",
       "| head_class_kwargs |  |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(GPT2HiddenStateConcatForSequenceClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
