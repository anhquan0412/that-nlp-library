{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Controller Tutorial: EnviBert model\n",
    "\n",
    "> This notebook contains some example of how to use the EnviBert-based models in this NLP library\n",
    "\n",
    "- skip_showdoc: true\n",
    "- skip_exec: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reuse the sample data in [this tutorial](https://anhquan0412.github.io/that-nlp-library/text_main.html) to experiment with the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from that_nlp_library.text_transformation import *\n",
    "from that_nlp_library.text_augmentation import *\n",
    "from that_nlp_library.text_main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import text_normalize\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from importlib.machinery import SourceFileLoader\n",
    "import os\n",
    "from transformers import DataCollatorWithPadding\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some necessary text augmentations and text transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For Text Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_tfms=[text_normalize]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For Text Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_nonown_tfm = partial(sampling_with_condition,query='Source==\"non owned\"',frac=0.5,seed=42,apply_to_all=False)\n",
    "over_nonown_tfm.__name__ = 'Oversampling Non Owned'\n",
    "\n",
    "over_own_tfm = partial(sampling_with_condition,query='Source==\"owned\"',frac=2,seed=42,apply_to_all=False)\n",
    "over_own_tfm.__name__ = 'Oversampling Owned'\n",
    "\n",
    "over_hc_tfm = partial(sampling_with_condition,query='Source==\"hc search\"',frac=2.5,seed=42,apply_to_all=False)\n",
    "over_hc_tfm.__name__ = 'Oversampling HC search'\n",
    "\n",
    "remove_accent_tfm = partial(remove_vnmese_accent,frac=1,seed=42,apply_to_all=True)\n",
    "remove_accent_tfm.__name__ = 'Add No-Accent Text'\n",
    "\n",
    "aug_tfms = [over_nonown_tfm,over_own_tfm,over_hc_tfm,remove_accent_tfm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a TextDataMain object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('sample_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Input Validation Precheck -----\n",
      "DataFrame contains duplicated values!\n",
      "-----> Number of duplications: 16 rows\n"
     ]
    }
   ],
   "source": [
    "tdm = TextDataMain.from_csv(DATA_PATH/'sample_large.csv',\n",
    "                            return_df=False,\n",
    "                            main_content='Content',\n",
    "                            metadatas='Source',\n",
    "                            label_names='L1',\n",
    "                            val_ratio=0.2,\n",
    "                            split_cols='L1',\n",
    "                            content_tfms = txt_tfms,\n",
    "                            aug_tfms = aug_tfms,\n",
    "                            process_metadatas=True,\n",
    "                            seed=42,\n",
    "                            shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Content</th>\n",
       "      <th>L1</th>\n",
       "      <th>L2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2264</th>\n",
       "      <td>Google Play</td>\n",
       "      <td>ƒêƒÉng xu·∫•t t√†i kho·∫£n th√¨ kh√¥ng ƒëƒÉng nh·∫≠p l·∫°i ƒë∆∞·ª£c.</td>\n",
       "      <td>Shopee account</td>\n",
       "      <td>Sign up/Log in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2265</th>\n",
       "      <td>Non Owned</td>\n",
       "      <td>Tri·ªÉn l√£m Th∆∞∆°ng m·∫°i ƒêi·ªán t·ª≠ Vi·ªát Nam v·ªõi s·ª± t...</td>\n",
       "      <td>Others</td>\n",
       "      <td>Branding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2266</th>\n",
       "      <td>Google Play</td>\n",
       "      <td>Nh∆∞ c·ª©tttttttt</td>\n",
       "      <td>Others</td>\n",
       "      <td>Cannot defined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2267</th>\n",
       "      <td>HC search</td>\n",
       "      <td>√°o kho√°c</td>\n",
       "      <td>Others</td>\n",
       "      <td>Cannot defined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2268</th>\n",
       "      <td>Non Owned</td>\n",
       "      <td>[https://shopee.vn/jocastore.vn](https://shope...</td>\n",
       "      <td>Others</td>\n",
       "      <td>Cannot defined</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Source                                            Content   \n",
       "2264  Google Play  ƒêƒÉng xu·∫•t t√†i kho·∫£n th√¨ kh√¥ng ƒëƒÉng nh·∫≠p l·∫°i ƒë∆∞·ª£c.  \\\n",
       "2265    Non Owned  Tri·ªÉn l√£m Th∆∞∆°ng m·∫°i ƒêi·ªán t·ª≠ Vi·ªát Nam v·ªõi s·ª± t...   \n",
       "2266  Google Play                                     Nh∆∞ c·ª©tttttttt   \n",
       "2267    HC search                                           √°o kho√°c   \n",
       "2268    Non Owned  [https://shopee.vn/jocastore.vn](https://shope...   \n",
       "\n",
       "                  L1              L2  \n",
       "2264  Shopee account  Sign up/Log in  \n",
       "2265          Others        Branding  \n",
       "2266          Others  Cannot defined  \n",
       "2267          Others  Cannot defined  \n",
       "2268          Others  Cannot defined  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm.df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our tokenizer for EnviBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir=Path('./envibert_tokenizer')\n",
    "tokenizer = SourceFileLoader(\"envibert.tokenizer\", \n",
    "                             str(cache_dir/'envibert_tokenizer.py')).load_module().RobertaTokenizer(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EnviBert a data collator to work. We will save this as an attribute in TDM\n",
    "# data_collator = DataCollatorWithPadding(tokenizer,padding=True,max_length=512)\n",
    "# tdm.set_data_collator(data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our DatasetDict from TextDataMain (as our `ModelController` class can also work with DatasetDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "----- Label Encoding -----\n",
      "-------------------- Text Transformation --------------------\n",
      "----- text_normalize -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2269/2269 [00:00<00:00, 3928.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Train Test Split --------------------\n",
      "Previous Validation Percentage: 20.009%\n",
      "- Before leak check\n",
      "Size: 454\n",
      "- After leak check\n",
      "Size: 447\n",
      "- Number of rows leaked: 7, or 1.54% of the original validation (or test) data\n",
      "Current Validation Percentage: 19.7%\n",
      "-------------------- Text Augmentation --------------------\n",
      "Train data size before augmentation: 1822\n",
      "----- Oversampling Non Owned -----\n",
      "Train data size after THIS augmentation: 2020\n",
      "----- Oversampling Owned -----\n",
      "Train data size after THIS augmentation: 2248\n",
      "----- Oversampling HC search -----\n",
      "Train data size after THIS augmentation: 2390\n",
      "----- Add No-Accent Text -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2390/2390 [00:00<00:00, 19058.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size after THIS augmentation: 4780\n",
      "Train data size after ALL augmentation: 4780\n",
      "-------------------- Map Tokenize Function --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4780 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main_ddict= tdm.to_datasetdict(tokenizer,\n",
    "                                max_length=512,\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 4780\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 447\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some examples of outputs the TDM produces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Text Transformation Explained -----\n",
      "--- Raw sentence ---\n",
      "T√¥i ƒë·∫∑t h√†ng m√† ch·∫≥ng th·∫•y giao 1 nƒÉm r·ªìi.L√†m v·ªõi ch·∫£ ƒÉn,ch√°n ü§¨ ü§¨ ü§¨ ü§¨ ü§¨ ü§¨\n",
      "--- text_normalize ---\n",
      "T√¥i ƒë·∫∑t h√†ng m√† ch·∫≥ng th·∫•y giao 1 nƒÉm r·ªìi . L√†m v·ªõi ch·∫£ ƒÉn , ch√°n ü§¨ ü§¨ ü§¨ ü§¨ ü§¨ ü§¨\n",
      "\n",
      "----- Tokenizer Explained -----\n",
      "--- Input ---\n",
      "T√¥i ƒë·∫∑t h√†ng m√† ch·∫≥ng th·∫•y giao 1 nƒÉm r·ªìi . L√†m v·ªõi ch·∫£ ƒÉn , ch√°n ü§¨ ü§¨ ü§¨ ü§¨ ü§¨ ü§¨\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 842, 642, 114, 145, 1371, 289, 363, 139, 93, 539, 5, 3798, 39, 7225, 380, 4, 5925, 3529, 3, 3529, 3, 3529, 3, 3529, 3, 3529, 3, 3529, 3, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens ---\n",
      "['<s>', '‚ñÅT√¥i', '‚ñÅƒë·∫∑t', '‚ñÅh√†ng', '‚ñÅm√†', '‚ñÅch·∫≥ng', '‚ñÅth·∫•y', '‚ñÅgiao', '‚ñÅ1', '‚ñÅnƒÉm', '‚ñÅr·ªìi', '‚ñÅ.', '‚ñÅL√†m', '‚ñÅv·ªõi', '‚ñÅch·∫£', '‚ñÅƒÉn', '‚ñÅ,', '‚ñÅch√°n', '‚ñÅ', '<unk>', '‚ñÅ', '<unk>', '‚ñÅ', '<unk>', '‚ñÅ', '<unk>', '‚ñÅ', '<unk>', '‚ñÅ', '<unk>', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> ‚ñÅT√¥i ‚ñÅƒë·∫∑t ‚ñÅh√†ng ‚ñÅm√† ‚ñÅch·∫≥ng ‚ñÅth·∫•y ‚ñÅgiao ‚ñÅ1 ‚ñÅnƒÉm ‚ñÅr·ªìi ‚ñÅ. ‚ñÅL√†m ‚ñÅv·ªõi ‚ñÅch·∫£ ‚ñÅƒÉn ‚ñÅ, ‚ñÅch√°n ‚ñÅ <unk> ‚ñÅ <unk> ‚ñÅ <unk> ‚ñÅ <unk> ‚ñÅ <unk> ‚ñÅ <unk> </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "two_steps_tokenization_explain('T√¥i ƒë·∫∑t h√†ng m√† ch·∫≥ng th·∫•y giao 1 nƒÉm r·ªìi.L√†m v·ªõi ch·∫£ ƒÉn,ch√°n ü§¨ ü§¨ ü§¨ ü§¨ ü§¨ ü§¨',\n",
    "                               tokenizer,content_tfms=[text_normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Tokenizer Explained -----\n",
      "--- Input ---\n",
      "owned - # ShopeePaychamguibaiSS3 # ShopeePay1111 link : https://bit.ly/UuDaiShopeePay11-11 Username : chipheo2306 Th∆∞∆°ng hi·ªáu l‚Äôoreal r·∫•t hay ch·∫°y sale nh·ªØng ng√†y l·ªÖ l·ªõn v√† nh·∫•t ng√†y 11.11 c√≥ th·ªÉ s·∫Ω c√≥ voucher gi·∫£m 50 % gi·∫£m t·ªëi ƒëa 100 k n·∫øu sƒÉn ƒë∆∞·ª£c voucher m√¨nh r·∫•t mu·ªën s·∫£n ph·∫©m Ôøº N∆∞·ªõc t·∫©y trang cho m·ªçi lo·∫°i da L'Oreal Paris 3 in1 Micellar Water 400 ml b·ªüi s·∫£n ph·∫©m gi√° th√†nh b√¨nh d√¢n n·∫øu sƒÉn sale n·ªØa th√¨ c√†ng r·∫ª cho dung t√≠ch l·ªõn 400 ml l·∫≠n v√† s·∫£n ph·∫©m c√≥ ch·ª©a c√¥ng ngh·ªá mi-xen ƒë·ªôt ph√° Chi·∫øt xu·∫•t th·∫£o d∆∞·ª£c v√† Glycerin b·ªï sung ƒë·ªô ·∫©m cho da H√∫t t·∫•t c·∫£ b·ª•i b·∫©n , c·∫∑n d∆° c·ªßa l·ªõp make-up m√† kh√¥ng g√¢y kh√¥ da . V·ªõi c√¥ng ngh·ªá m·ªõi , mang ƒë·∫øn c√°c t·∫©y trang , l√†m s·∫°ch , gi·ªØ ·∫©m v√† d∆∞·ª°ng m·ªÅm da ƒë·ªìng th·ªùi ch·ªâ trong m·ªôt s·∫£n ph·∫©m . M√¨nh lu√¥n ∆∞u ti√™n thanh to√°n qua ShopeePay ƒë·ªÉ thanh to√°n ƒë∆∞·ª£c ti·ªán l·ª£i v√† nhanh ch√≥ng\n",
      "\n",
      "--- Tokenized results --- \n",
      "{'input_ids': [0, 3507, 13, 2481, 1888, 51603, 53097, 1501, 1509, 10976, 4020, 2327, 31682, 1245, 2481, 1888, 51603, 53097, 4894, 4894, 1599, 46, 47048, 5513, 5713, 428, 428, 12651, 244, 1142, 428, 2057, 1114, 1477, 2327, 17803, 51603, 53097, 4894, 7343, 10578, 7103, 6460, 46, 6613, 58185, 54439, 1603, 2332, 325, 1546, 3, 47666, 173, 292, 1065, 4331, 53, 125, 1204, 300, 17, 134, 125, 704, 10890, 22, 61, 75, 22, 25734, 455, 918, 213, 455, 876, 930, 825, 1817, 513, 4738, 34, 25734, 177, 173, 452, 165, 280, 3529, 3, 4460, 5540, 430, 35, 584, 342, 680, 748, 427, 2187, 21633, 2759, 186, 11, 1147, 18100, 27237, 6499, 3198, 20332, 688, 165, 280, 157, 119, 565, 191, 513, 4738, 4331, 685, 132, 840, 1617, 35, 936, 448, 300, 3198, 20332, 10166, 17, 165, 280, 22, 1880, 59, 552, 5297, 142, 1780, 883, 2198, 1167, 18729, 228, 1425, 3789, 17, 31062, 12035, 829, 1334, 1790, 276, 3075, 35, 680, 29574, 781, 206, 4199, 5749, 4, 16823, 29324, 20, 1028, 364, 142, 4281, 145, 38, 533, 2557, 680, 5, 816, 59, 552, 183, 4, 434, 62, 29, 5540, 430, 4, 83, 1610, 4, 679, 3075, 17, 1187, 1794, 680, 122, 155, 127, 37, 40, 165, 280, 5, 4910, 612, 1261, 597, 656, 1081, 204, 1888, 51603, 53097, 58, 656, 1081, 34, 957, 579, 17, 687, 1519, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "--- Results from tokenizer.convert_ids_to_tokens ---\n",
      "['<s>', '‚ñÅowned', '‚ñÅ-', '‚ñÅ#', '‚ñÅSh', 'opee', 'Pay', 'ch', 'am', 'gu', 'ib', 'ai', 'SS', '3', '‚ñÅ#', '‚ñÅSh', 'opee', 'Pay', '11', '11', '‚ñÅlink', '‚ñÅ:', '‚ñÅhtt', 'ps', ':', '/', '/', 'bit', '.', 'ly', '/', 'U', 'u', 'D', 'ai', 'Sh', 'opee', 'Pay', '11', '-11', '‚ñÅUs', 'ern', 'ame', '‚ñÅ:', '‚ñÅchip', 'heo', '230', '6', '‚ñÅTh∆∞∆°ng', '‚ñÅhi·ªáu', '‚ñÅl', '<unk>', 'oreal', '‚ñÅr·∫•t', '‚ñÅhay', '‚ñÅch·∫°y', '‚ñÅsale', '‚ñÅnh·ªØng', '‚ñÅng√†y', '‚ñÅl·ªÖ', '‚ñÅl·ªõn', '‚ñÅv√†', '‚ñÅnh·∫•t', '‚ñÅng√†y', '‚ñÅ11', '.11', '‚ñÅc√≥', '‚ñÅth·ªÉ', '‚ñÅs·∫Ω', '‚ñÅc√≥', '‚ñÅvoucher', '‚ñÅgi·∫£m', '‚ñÅ50', '‚ñÅ%', '‚ñÅgi·∫£m', '‚ñÅt·ªëi', '‚ñÅƒëa', '‚ñÅ100', '‚ñÅk', '‚ñÅn·∫øu', '‚ñÅsƒÉn', '‚ñÅƒë∆∞·ª£c', '‚ñÅvoucher', '‚ñÅm√¨nh', '‚ñÅr·∫•t', '‚ñÅmu·ªën', '‚ñÅs·∫£n', '‚ñÅph·∫©m', '‚ñÅ', '<unk>', '‚ñÅN∆∞·ªõc', '‚ñÅt·∫©y', '‚ñÅtrang', '‚ñÅcho', '‚ñÅm·ªçi', '‚ñÅlo·∫°i', '‚ñÅda', '‚ñÅL', \"'\", 'O', 'real', '‚ñÅParis', '‚ñÅ3', '‚ñÅin', '1', '‚ñÅMic', 'ellar', '‚ñÅWater', '‚ñÅ400', '‚ñÅml', '‚ñÅb·ªüi', '‚ñÅs·∫£n', '‚ñÅph·∫©m', '‚ñÅgi√°', '‚ñÅth√†nh', '‚ñÅb√¨nh', '‚ñÅd√¢n', '‚ñÅn·∫øu', '‚ñÅsƒÉn', '‚ñÅsale', '‚ñÅn·ªØa', '‚ñÅth√¨', '‚ñÅc√†ng', '‚ñÅr·∫ª', '‚ñÅcho', '‚ñÅdung', '‚ñÅt√≠ch', '‚ñÅl·ªõn', '‚ñÅ400', '‚ñÅml', '‚ñÅl·∫≠n', '‚ñÅv√†', '‚ñÅs·∫£n', '‚ñÅph·∫©m', '‚ñÅc√≥', '‚ñÅch·ª©a', '‚ñÅc√¥ng', '‚ñÅngh·ªá', '‚ñÅmi', '-', 'x', 'en', '‚ñÅƒë·ªôt', '‚ñÅph√°', '‚ñÅChi·∫øt', '‚ñÅxu·∫•t', '‚ñÅth·∫£o', '‚ñÅd∆∞·ª£c', '‚ñÅv√†', '‚ñÅGly', 'cer', 'in', '‚ñÅb·ªï', '‚ñÅsung', '‚ñÅƒë·ªô', '‚ñÅ·∫©m', '‚ñÅcho', '‚ñÅda', '‚ñÅH√∫t', '‚ñÅt·∫•t', '‚ñÅc·∫£', '‚ñÅb·ª•i', '‚ñÅb·∫©n', '‚ñÅ,', '‚ñÅc·∫∑n', '‚ñÅd∆°', '‚ñÅc·ªßa', '‚ñÅl·ªõp', '‚ñÅmake', '-', 'up', '‚ñÅm√†', '‚ñÅkh√¥ng', '‚ñÅg√¢y', '‚ñÅkh√¥', '‚ñÅda', '‚ñÅ.', '‚ñÅV·ªõi', '‚ñÅc√¥ng', '‚ñÅngh·ªá', '‚ñÅm·ªõi', '‚ñÅ,', '‚ñÅmang', '‚ñÅƒë·∫øn', '‚ñÅc√°c', '‚ñÅt·∫©y', '‚ñÅtrang', '‚ñÅ,', '‚ñÅl√†m', '‚ñÅs·∫°ch', '‚ñÅ,', '‚ñÅgi·ªØ', '‚ñÅ·∫©m', '‚ñÅv√†', '‚ñÅd∆∞·ª°ng', '‚ñÅm·ªÅm', '‚ñÅda', '‚ñÅƒë·ªìng', '‚ñÅth·ªùi', '‚ñÅch·ªâ', '‚ñÅtrong', '‚ñÅm·ªôt', '‚ñÅs·∫£n', '‚ñÅph·∫©m', '‚ñÅ.', '‚ñÅM√¨nh', '‚ñÅlu√¥n', '‚ñÅ∆∞u', '‚ñÅti√™n', '‚ñÅthanh', '‚ñÅto√°n', '‚ñÅqua', '‚ñÅSh', 'opee', 'Pay', '‚ñÅƒë·ªÉ', '‚ñÅthanh', '‚ñÅto√°n', '‚ñÅƒë∆∞·ª£c', '‚ñÅti·ªán', '‚ñÅl·ª£i', '‚ñÅv√†', '‚ñÅnhanh', '‚ñÅch√≥ng', '</s>']\n",
      "\n",
      "--- Results from tokenizer.decode --- \n",
      "<s> ‚ñÅowned ‚ñÅ- ‚ñÅ# ‚ñÅSh opee Pay ch am gu ib ai SS 3 ‚ñÅ# ‚ñÅSh opee Pay 11 11 ‚ñÅlink ‚ñÅ: ‚ñÅhtt ps : / / bit. ly / U u D ai Sh opee Pay 11 -11 ‚ñÅUs ern ame ‚ñÅ: ‚ñÅchip heo 230 6 ‚ñÅTh∆∞∆°ng ‚ñÅhi·ªáu ‚ñÅl <unk> oreal ‚ñÅr·∫•t ‚ñÅhay ‚ñÅch·∫°y ‚ñÅsale ‚ñÅnh·ªØng ‚ñÅng√†y ‚ñÅl·ªÖ ‚ñÅl·ªõn ‚ñÅv√† ‚ñÅnh·∫•t ‚ñÅng√†y ‚ñÅ11.11 ‚ñÅc√≥ ‚ñÅth·ªÉ ‚ñÅs·∫Ω ‚ñÅc√≥ ‚ñÅvoucher ‚ñÅgi·∫£m ‚ñÅ50 ‚ñÅ% ‚ñÅgi·∫£m ‚ñÅt·ªëi ‚ñÅƒëa ‚ñÅ100 ‚ñÅk ‚ñÅn·∫øu ‚ñÅsƒÉn ‚ñÅƒë∆∞·ª£c ‚ñÅvoucher ‚ñÅm√¨nh ‚ñÅr·∫•t ‚ñÅmu·ªën ‚ñÅs·∫£n ‚ñÅph·∫©m ‚ñÅ <unk> ‚ñÅN∆∞·ªõc ‚ñÅt·∫©y ‚ñÅtrang ‚ñÅcho ‚ñÅm·ªçi ‚ñÅlo·∫°i ‚ñÅda ‚ñÅL'O real ‚ñÅParis ‚ñÅ3 ‚ñÅin 1 ‚ñÅMic ellar ‚ñÅWater ‚ñÅ400 ‚ñÅml ‚ñÅb·ªüi ‚ñÅs·∫£n ‚ñÅph·∫©m ‚ñÅgi√° ‚ñÅth√†nh ‚ñÅb√¨nh ‚ñÅd√¢n ‚ñÅn·∫øu ‚ñÅsƒÉn ‚ñÅsale ‚ñÅn·ªØa ‚ñÅth√¨ ‚ñÅc√†ng ‚ñÅr·∫ª ‚ñÅcho ‚ñÅdung ‚ñÅt√≠ch ‚ñÅl·ªõn ‚ñÅ400 ‚ñÅml ‚ñÅl·∫≠n ‚ñÅv√† ‚ñÅs·∫£n ‚ñÅph·∫©m ‚ñÅc√≥ ‚ñÅch·ª©a ‚ñÅc√¥ng ‚ñÅngh·ªá ‚ñÅmi - x en ‚ñÅƒë·ªôt ‚ñÅph√° ‚ñÅChi·∫øt ‚ñÅxu·∫•t ‚ñÅth·∫£o ‚ñÅd∆∞·ª£c ‚ñÅv√† ‚ñÅGly cer in ‚ñÅb·ªï ‚ñÅsung ‚ñÅƒë·ªô ‚ñÅ·∫©m ‚ñÅcho ‚ñÅda ‚ñÅH√∫t ‚ñÅt·∫•t ‚ñÅc·∫£ ‚ñÅb·ª•i ‚ñÅb·∫©n ‚ñÅ, ‚ñÅc·∫∑n ‚ñÅd∆° ‚ñÅc·ªßa ‚ñÅl·ªõp ‚ñÅmake - up ‚ñÅm√† ‚ñÅkh√¥ng ‚ñÅg√¢y ‚ñÅkh√¥ ‚ñÅda ‚ñÅ. ‚ñÅV·ªõi ‚ñÅc√¥ng ‚ñÅngh·ªá ‚ñÅm·ªõi ‚ñÅ, ‚ñÅmang ‚ñÅƒë·∫øn ‚ñÅc√°c ‚ñÅt·∫©y ‚ñÅtrang ‚ñÅ, ‚ñÅl√†m ‚ñÅs·∫°ch ‚ñÅ, ‚ñÅgi·ªØ ‚ñÅ·∫©m ‚ñÅv√† ‚ñÅd∆∞·ª°ng ‚ñÅm·ªÅm ‚ñÅda ‚ñÅƒë·ªìng ‚ñÅth·ªùi ‚ñÅch·ªâ ‚ñÅtrong ‚ñÅm·ªôt ‚ñÅs·∫£n ‚ñÅph·∫©m ‚ñÅ. ‚ñÅM√¨nh ‚ñÅlu√¥n ‚ñÅ∆∞u ‚ñÅti√™n ‚ñÅthanh ‚ñÅto√°n ‚ñÅqua ‚ñÅSh opee Pay ‚ñÅƒë·ªÉ ‚ñÅthanh ‚ñÅto√°n ‚ñÅƒë∆∞·ª£c ‚ñÅti·ªán ‚ñÅl·ª£i ‚ñÅv√† ‚ñÅnhanh ‚ñÅch√≥ng </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tdm.tokenizer_explain_single(tokenizer) # Pick a random text in train set to explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Experiment: EnviBert Single-Head Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "from that_nlp_library.models.roberta.classifiers import *\n",
    "from that_nlp_library.model_main import *\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will specify a (or a list) of GPUs for training\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train EnviBert using TDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Buyer complained seller',\n",
       "  'Commercial',\n",
       "  'Delivery',\n",
       "  'Feature',\n",
       "  'Order/Item',\n",
       "  'Others',\n",
       "  'Payment',\n",
       "  'Return/Refund',\n",
       "  'Services',\n",
       "  'Shopee account']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm.label_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(tdm.label_lists[0]) # 10\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our model and model controller. First, we will initialize the pretrained `body` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.roberta.modeling_roberta import RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='nguyenvulebinh/envibert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nguyenvulebinh/envibert were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "envibert_body = RobertaModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can define a simple class as the head for our classification task, something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassificationHead(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 config, # HuggingFace model configuration\n",
    "                 classifier_dropout=0.1, # Dropout ratio (for dropout layer right before the last nn.Linear)\n",
    "                 num_labels=None, # Number of label output. Every classification class must have this exact variable\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(classifier_dropout)\n",
    "        self.out_proj = torch.nn.Linear(config.hidden_size, num_labels)\n",
    "    def forward(self, inp, **kwargs):\n",
    "        x = inp\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading body weights. This assumes the body is the very first first-layer block of your custom architecture\n"
     ]
    }
   ],
   "source": [
    "_model_kwargs={\n",
    "    # overall model hyperparams\n",
    "    'is_multilabel':tdm.is_multilabel, # False\n",
    "    'is_multihead':tdm.is_multihead, # False\n",
    "    'head_class_sizes':num_classes,\n",
    "    'head_class': SimpleClassificationHead,\n",
    "    # classfication head hyperparams\n",
    "    'classifier_dropout':0.1 \n",
    "}\n",
    "\n",
    "model = model_init_classification(model_class = RobertaBaseForSequenceClassification,\n",
    "                                  cpoint_path = 'nguyenvulebinh/envibert', \n",
    "                                  output_hidden_states=False, # since we are not using 'hidden layer contatenation' technique\n",
    "                                  seed=42,\n",
    "                                  body_model=envibert_body,\n",
    "                                  model_kwargs = _model_kwargs)\n",
    "\n",
    "metric_funcs = [partial(f1_score,average='macro'),accuracy_score] # we will use both f1_macro and accuracy score as metrics\n",
    "controller = ModelController(model,tdm,metric_funcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can start training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1194' max='1194' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1194/1194 01:04, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Score L1</th>\n",
       "      <th>Accuracy Score L1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.059704</td>\n",
       "      <td>0.350748</td>\n",
       "      <td>0.677852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.007712</td>\n",
       "      <td>0.462641</td>\n",
       "      <td>0.697987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 8e-5\n",
    "bs=4\n",
    "wd=0.01\n",
    "epochs= 2\n",
    "\n",
    "controller.fit(epochs,lr,\n",
    "               batch_size=bs,\n",
    "               weight_decay=wd,\n",
    "               save_checkpoint=False,\n",
    "               compute_metrics=compute_metrics_classification,\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging your training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can log your training using HuggingFace:\n",
    "\n",
    "- Supported platforms are \"azure_ml\", \"comet_ml\", \"mlflow\", \"neptune\", \"tensorboard\",\"clearml\" and \"wandb\"\n",
    "\n",
    "- References:\n",
    "\n",
    "    - https://huggingface.co/docs/transformers/v4.28.0/en/main_classes/trainer#transformers.TrainingArguments\n",
    "    \n",
    "    - https://docs.wandb.ai/guides/integrations/huggingface#:~:text=Logging%20your%20Hugging%20Face%20model,every%20save_steps%20in%20the%20TrainingArguments%20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "controller.fit(epochs,lr,\n",
    "               batch_size=bs,\n",
    "               weight_decay=wd,\n",
    "               save_checkpoint=False,\n",
    "               compute_metrics=compute_metrics_classification,\n",
    "               hf_report_to='wandb'\n",
    "              )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save your model weights at the end of your training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "controller.trainer.model.save_pretrained('./sample_weights/model_progress')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can save your weights at every epochs during your training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "controller.fit(epochs,lr,\n",
    "               batch_size=bs,\n",
    "               weight_decay=wd,\n",
    "               save_checkpoint=True,\n",
    "               o_dir='sample_weights',\n",
    "               compute_metrics=compute_metrics_classification,\n",
    "              )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train EnviBert with tokenized DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part assumes you already have your tokenized datasetdict. You must have your tokenizer as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaTokenizer(name_or_path='', vocab_size=59993, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that your DatasetDict must contain tokens besides raw text (which typically includes 'input_ids', 'token_type_ids', 'attention_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 4780\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 447\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='nguyenvulebinh/envibert'\n",
    "_model_kwargs={\n",
    "    # overall model hyperparams\n",
    "    'is_multilabel':False, # False\n",
    "    'is_multihead':False, # False\n",
    "    'head_class_sizes':num_classes,\n",
    "    'head_class': SimpleClassificationHead,\n",
    "    # classfication head hyperparams\n",
    "    'classifier_dropout':0.1 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envibert_body = RobertaModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nguyenvulebinh/envibert were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading body weights. This assumes the body is the very first first-layer block of your custom architecture\n"
     ]
    }
   ],
   "source": [
    "model = model_init_classification(model_class = RobertaBaseForSequenceClassification,\n",
    "                                  cpoint_path = 'nguyenvulebinh/envibert', \n",
    "                                  output_hidden_states=False, # since we are not using 'hidden layer contatenation' technique\n",
    "                                  seed=42,\n",
    "                                  body_model=envibert_body,\n",
    "                                  model_kwargs = _model_kwargs)\n",
    "\n",
    "metric_funcs = [partial(f1_score,average='macro'),accuracy_score] # we will use both f1_macro and accuracy score as metrics\n",
    "controller = ModelController(model,\n",
    "                             metric_funcs=metric_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1194' max='1194' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1194/1194 01:06, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Score L1</th>\n",
       "      <th>Accuracy Score L1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.059704</td>\n",
       "      <td>0.350748</td>\n",
       "      <td>0.677852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.007712</td>\n",
       "      <td>0.462641</td>\n",
       "      <td>0.697987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 8e-5\n",
    "bs=4\n",
    "wd=0.01\n",
    "epochs= 2\n",
    "\n",
    "controller.fit(epochs,lr,\n",
    "               ddict=main_ddict, # Put in your tokenized datasetdict here\n",
    "               batch_size=bs,\n",
    "               weight_decay=wd,\n",
    "               save_checkpoint=False,\n",
    "               compute_metrics=compute_metrics_classification,\n",
    "               tokenizer=tokenizer,\n",
    "               label_names='L1'\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.trainer.model.save_pretrained('./sample_weights/model_progress')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using trained model, using TDM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_multilabel': False,\n",
       " 'is_multihead': False,\n",
       " 'head_class_sizes': 10,\n",
       " 'head_class': __main__.SimpleClassificationHead,\n",
       " 'classifier_dropout': 0.1}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_model_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sample_weights/model_progress were not used when initializing RobertaBaseForSequenceClassification: ['body_model.pooler.dense.weight', 'body_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaBaseForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaBaseForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "trained_model = model_init_classification(model_class = RobertaBaseForSequenceClassification,\n",
    "                                          cpoint_path = Path('./sample_weights/model_progress'), \n",
    "                                          output_hidden_states=False,\n",
    "                                          seed=42,\n",
    "                                          model_kwargs = _model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaBaseForSequenceClassification(\n",
       "  (body_model): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(59993, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classification_head): SimpleClassificationHead(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_funcs = [partial(f1_score,average='macro'),accuracy_score] # we will use both f1_macro and accuracy score as metrics\n",
    "controller = ModelController(trained_model,tdm,metric_funcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Train/Validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make prediction on all validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start making predictions --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_val = controller.predict_ddict(ds_type='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>Source</th>\n",
       "      <th>pred_L1</th>\n",
       "      <th>pred_prob_L1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google play - Ch∆°i gam r·∫•t l√† l√°c</td>\n",
       "      <td>3</td>\n",
       "      <td>google play</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>0.837496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google play - Zq</td>\n",
       "      <td>5</td>\n",
       "      <td>google play</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.927602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non owned - L√†n s√≥ng k·ªπ thu·∫≠t s·ªë v√† s·ª± l·ª±a ch·ªç...</td>\n",
       "      <td>5</td>\n",
       "      <td>non owned</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.918241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google play - H√†ng qu·ªëc t·∫ø kh√¥ng c√≤n ship COD ...</td>\n",
       "      <td>6</td>\n",
       "      <td>google play</td>\n",
       "      <td>Delivery</td>\n",
       "      <td>0.804539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google play - Qu√° t·ªá . Giao h√†ng ch·∫≠m nh∆∞ r√πa ...</td>\n",
       "      <td>2</td>\n",
       "      <td>google play</td>\n",
       "      <td>Delivery</td>\n",
       "      <td>0.758327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label       Source   \n",
       "0                  google play - Ch∆°i gam r·∫•t l√† l√°c      3  google play  \\\n",
       "1                                   google play - Zq      5  google play   \n",
       "2  non owned - L√†n s√≥ng k·ªπ thu·∫≠t s·ªë v√† s·ª± l·ª±a ch·ªç...      5    non owned   \n",
       "3  google play - H√†ng qu·ªëc t·∫ø kh√¥ng c√≤n ship COD ...      6  google play   \n",
       "4  google play - Qu√° t·ªá . Giao h√†ng ch·∫≠m nh∆∞ r√πa ...      2  google play   \n",
       "\n",
       "      pred_L1  pred_prob_L1  \n",
       "0  Commercial      0.837496  \n",
       "1      Others      0.927602  \n",
       "2      Others      0.918241  \n",
       "3    Delivery      0.804539  \n",
       "4    Delivery      0.758327  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the label index to string, we can use the ```label_lists``` attribute of tdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>Source</th>\n",
       "      <th>pred_L1</th>\n",
       "      <th>pred_prob_L1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google play - Ch∆°i gam r·∫•t l√† l√°c</td>\n",
       "      <td>Feature</td>\n",
       "      <td>google play</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>0.837496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google play - Zq</td>\n",
       "      <td>Others</td>\n",
       "      <td>google play</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.927602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non owned - L√†n s√≥ng k·ªπ thu·∫≠t s·ªë v√† s·ª± l·ª±a ch·ªç...</td>\n",
       "      <td>Others</td>\n",
       "      <td>non owned</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.918241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google play - H√†ng qu·ªëc t·∫ø kh√¥ng c√≤n ship COD ...</td>\n",
       "      <td>Payment</td>\n",
       "      <td>google play</td>\n",
       "      <td>Delivery</td>\n",
       "      <td>0.804539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google play - Qu√° t·ªá . Giao h√†ng ch·∫≠m nh∆∞ r√πa ...</td>\n",
       "      <td>Delivery</td>\n",
       "      <td>google play</td>\n",
       "      <td>Delivery</td>\n",
       "      <td>0.758327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label       Source   \n",
       "0                  google play - Ch∆°i gam r·∫•t l√† l√°c   Feature  google play  \\\n",
       "1                                   google play - Zq    Others  google play   \n",
       "2  non owned - L√†n s√≥ng k·ªπ thu·∫≠t s·ªë v√† s·ª± l·ª±a ch·ªç...    Others    non owned   \n",
       "3  google play - H√†ng qu·ªëc t·∫ø kh√¥ng c√≤n ship COD ...   Payment  google play   \n",
       "4  google play - Qu√° t·ªá . Giao h√†ng ch·∫≠m nh∆∞ r√πa ...  Delivery  google play   \n",
       "\n",
       "      pred_L1  pred_prob_L1  \n",
       "0  Commercial      0.837496  \n",
       "1      Others      0.927602  \n",
       "2      Others      0.918241  \n",
       "3    Delivery      0.804539  \n",
       "4    Delivery      0.758327  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val['label']= df_val['label'].apply(lambda x: tdm.label_lists[0][x]).values\n",
    "\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try to get your metric to see if it matches your last traing epoch's above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4634417008698494"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(df_val.label,df_val.pred_L1,average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also make predictions on all training set, by changing argument ```ds_type``` to \"train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go through details on how to make a prediction on a completely new and raw dataset using our trained model. For now, let's reuse the sample csv and pretend it's our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Input Validation Precheck -----\n",
      "DataFrame contains duplicated values!\n",
      "-----> Number of duplications: 16 rows\n"
     ]
    }
   ],
   "source": [
    "df_test = TextDataMain.from_csv(Path('sample_data')/'sample_large.csv',return_df=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove all the labels and unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label = df_test['L1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop(['L1','L2'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google Play</td>\n",
       "      <td>App ncc l√∫c n√†o cx lag ƒë∆°, ph·∫ßn t√¨m ki·∫øm th√¨ v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Non Owned</td>\n",
       "      <td>..‚ùóÔ∏è G√ìC THANH L√ù T√≠nh ra r·∫ª h∆°n c·∫£ mua #Shope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Google Play</td>\n",
       "      <td>M·∫Øc g√¨ ng∆∞·ªùi ta ƒë·∫∑t h√†ng to√†n l·ªói üòÉ????</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Owned</td>\n",
       "      <td>#GhienShopeePayawardT8 Khi b·∫°n ch∆°i shopee qu√°...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Google Play</td>\n",
       "      <td>R·∫•t b·ª©c x√∫c khi d√πng . m√£ gi·∫£m gi√° ng∆∞·ªùi d√πng ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Source                                            Content\n",
       "0  Google Play  App ncc l√∫c n√†o cx lag ƒë∆°, ph·∫ßn t√¨m ki·∫øm th√¨ v...\n",
       "1    Non Owned  ..‚ùóÔ∏è G√ìC THANH L√ù T√≠nh ra r·∫ª h∆°n c·∫£ mua #Shope...\n",
       "2  Google Play            M·∫Øc g√¨ ng∆∞·ªùi ta ƒë·∫∑t h√†ng to√†n l·ªói üòÉ????\n",
       "3        Owned  #GhienShopeePayawardT8 Khi b·∫°n ch∆°i shopee qu√°...\n",
       "4  Google Play  R·∫•t b·ª©c x√∫c khi d√πng . m√£ gi·∫£m gi√° ng∆∞·ªùi d√πng ..."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a DatasetDict for this test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Getting Test Set --------------------\n",
      "----- Input Validation Precheck -----\n",
      "DataFrame contains duplicated values!\n",
      "-----> Number of duplications: 19 rows\n",
      "-------------------- Start Test Set Transformation --------------------\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "-------------------- Text Transformation --------------------\n",
      "----- text_normalize -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2269/2269 [00:00<00:00, 3954.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Test Leak Checking --------------------\n",
      "- Before leak check\n",
      "Size: 2269\n",
      "- After leak check\n",
      "Size: 0\n",
      "- Number of rows leaked: 2269, or 100.00% of the original validation (or test) data\n",
      "-------------------- Construct DatasetDict --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2269 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_ddict = tdm.get_test_datasetdict_from_df(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the ***Leak Check*** we did in TextDataMain? Our ```df_test``` only has 70 rows, and it also shows that 70 rows of our data is leaked (100%), which is correct because this test dataset is actually a small sample of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test data has been processed + transformed (but not augmented) the same way as the validation set. Now we can start making the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start making predictions --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2269 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "controller = ModelController(trained_model,tdm)\n",
    "df_result = controller.predict_ddict(ddict=test_ddict,ds_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Source</th>\n",
       "      <th>pred_L1</th>\n",
       "      <th>pred_prob_L1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google play - App ncc l√∫c n√†o cx lag ƒë∆° , ph·∫ßn...</td>\n",
       "      <td>google play</td>\n",
       "      <td>Feature</td>\n",
       "      <td>0.878221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non owned - .. ‚ùó Ô∏è G√ìC THANH L√ù T√≠nh ra r·∫ª h∆°n...</td>\n",
       "      <td>non owned</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.930981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>google play - M·∫Øc g√¨ ng∆∞·ªùi ta ƒë·∫∑t h√†ng to√†n l·ªó...</td>\n",
       "      <td>google play</td>\n",
       "      <td>Feature</td>\n",
       "      <td>0.849374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>owned - # GhienShopeePayawardT8 Khi b·∫°n ch∆°i s...</td>\n",
       "      <td>owned</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>0.915552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google play - R·∫•t b·ª©c x√∫c khi d√πng . m√£ gi·∫£m g...</td>\n",
       "      <td>google play</td>\n",
       "      <td>Shopee account</td>\n",
       "      <td>0.571471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       Source   \n",
       "0  google play - App ncc l√∫c n√†o cx lag ƒë∆° , ph·∫ßn...  google play  \\\n",
       "1  non owned - .. ‚ùó Ô∏è G√ìC THANH L√ù T√≠nh ra r·∫ª h∆°n...    non owned   \n",
       "2  google play - M·∫Øc g√¨ ng∆∞·ªùi ta ƒë·∫∑t h√†ng to√†n l·ªó...  google play   \n",
       "3  owned - # GhienShopeePayawardT8 Khi b·∫°n ch∆°i s...        owned   \n",
       "4  google play - R·∫•t b·ª©c x√∫c khi d√πng . m√£ gi·∫£m g...  google play   \n",
       "\n",
       "          pred_L1  pred_prob_L1  \n",
       "0         Feature      0.878221  \n",
       "1          Others      0.930981  \n",
       "2         Feature      0.849374  \n",
       "3      Commercial      0.915552  \n",
       "4  Shopee account      0.571471  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly check the f1 score to make sure everything works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5303012712104336"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(true_label,df_result.pred_L1,average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are getting the predictions on the entire training+validation set, the F1 score is expected to be slightly higher than validation's F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even predict top k results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start making predictions --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2269 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Source</th>\n",
       "      <th>pred_L1</th>\n",
       "      <th>pred_prob_L1</th>\n",
       "      <th>pred_L1_top1</th>\n",
       "      <th>pred_L1_top2</th>\n",
       "      <th>pred_L1_top3</th>\n",
       "      <th>pred_prob_L1_top1</th>\n",
       "      <th>pred_prob_L1_top2</th>\n",
       "      <th>pred_prob_L1_top3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google play - App ncc l√∫c n√†o cx lag ƒë∆° , ph·∫ßn...</td>\n",
       "      <td>google play</td>\n",
       "      <td>[3, 5, 9]</td>\n",
       "      <td>[0.87822074, 0.023822138, 0.02159522]</td>\n",
       "      <td>Feature</td>\n",
       "      <td>Others</td>\n",
       "      <td>Shopee account</td>\n",
       "      <td>0.878221</td>\n",
       "      <td>0.023822</td>\n",
       "      <td>0.021595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non owned - .. ‚ùó Ô∏è G√ìC THANH L√ù T√≠nh ra r·∫ª h∆°n...</td>\n",
       "      <td>non owned</td>\n",
       "      <td>[5, 1, 0]</td>\n",
       "      <td>[0.9309808, 0.015578598, 0.009805982]</td>\n",
       "      <td>Others</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>Buyer complained seller</td>\n",
       "      <td>0.930981</td>\n",
       "      <td>0.015579</td>\n",
       "      <td>0.009806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>google play - M·∫Øc g√¨ ng∆∞·ªùi ta ƒë·∫∑t h√†ng to√†n l·ªó...</td>\n",
       "      <td>google play</td>\n",
       "      <td>[3, 5, 9]</td>\n",
       "      <td>[0.8493735, 0.050054528, 0.021759989]</td>\n",
       "      <td>Feature</td>\n",
       "      <td>Others</td>\n",
       "      <td>Shopee account</td>\n",
       "      <td>0.849374</td>\n",
       "      <td>0.050055</td>\n",
       "      <td>0.021760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>owned - # GhienShopeePayawardT8 Khi b·∫°n ch∆°i s...</td>\n",
       "      <td>owned</td>\n",
       "      <td>[1, 6, 7]</td>\n",
       "      <td>[0.9155516, 0.01255093, 0.010521941]</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>Payment</td>\n",
       "      <td>Return/Refund</td>\n",
       "      <td>0.915552</td>\n",
       "      <td>0.012551</td>\n",
       "      <td>0.010522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google play - R·∫•t b·ª©c x√∫c khi d√πng . m√£ gi·∫£m g...</td>\n",
       "      <td>google play</td>\n",
       "      <td>[9, 3, 1]</td>\n",
       "      <td>[0.57147133, 0.25687057, 0.03061041]</td>\n",
       "      <td>Shopee account</td>\n",
       "      <td>Feature</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>0.571471</td>\n",
       "      <td>0.256871</td>\n",
       "      <td>0.030610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       Source    pred_L1   \n",
       "0  google play - App ncc l√∫c n√†o cx lag ƒë∆° , ph·∫ßn...  google play  [3, 5, 9]  \\\n",
       "1  non owned - .. ‚ùó Ô∏è G√ìC THANH L√ù T√≠nh ra r·∫ª h∆°n...    non owned  [5, 1, 0]   \n",
       "2  google play - M·∫Øc g√¨ ng∆∞·ªùi ta ƒë·∫∑t h√†ng to√†n l·ªó...  google play  [3, 5, 9]   \n",
       "3  owned - # GhienShopeePayawardT8 Khi b·∫°n ch∆°i s...        owned  [1, 6, 7]   \n",
       "4  google play - R·∫•t b·ª©c x√∫c khi d√πng . m√£ gi·∫£m g...  google play  [9, 3, 1]   \n",
       "\n",
       "                            pred_prob_L1    pred_L1_top1 pred_L1_top2   \n",
       "0  [0.87822074, 0.023822138, 0.02159522]         Feature       Others  \\\n",
       "1  [0.9309808, 0.015578598, 0.009805982]          Others   Commercial   \n",
       "2  [0.8493735, 0.050054528, 0.021759989]         Feature       Others   \n",
       "3   [0.9155516, 0.01255093, 0.010521941]      Commercial      Payment   \n",
       "4   [0.57147133, 0.25687057, 0.03061041]  Shopee account      Feature   \n",
       "\n",
       "              pred_L1_top3  pred_prob_L1_top1  pred_prob_L1_top2   \n",
       "0           Shopee account           0.878221           0.023822  \\\n",
       "1  Buyer complained seller           0.930981           0.015579   \n",
       "2           Shopee account           0.849374           0.050055   \n",
       "3            Return/Refund           0.915552           0.012551   \n",
       "4               Commercial           0.571471           0.256871   \n",
       "\n",
       "   pred_prob_L1_top3  \n",
       "0           0.021595  \n",
       "1           0.009806  \n",
       "2           0.021760  \n",
       "3           0.010522  \n",
       "4           0.030610  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result = controller.predict_ddict(ddict=test_ddict,ds_type='test',topk=3)\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just want to make a prediction on a small amount of data (single sentence, or a few sentences), we can use `ModelController.predict_raw_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have some metadatas, we need to define a dictionary (to imitate a DatasetDict)\n",
    "raw_content={\n",
    "    'Source': 'Google play',\n",
    "    'Content':'T√¥i kh√¥ng th√≠ch Shopee.T·∫°i v√¨ d√πng app r·∫•t ch·∫≠m,lag banh nh√† l·∫ßu, th·∫≠m ch√≠ log in c√≤n kh√¥ng ƒëc'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't use metadata, we can use something like this: \n",
    "\n",
    "```raw_content='T√¥i kh√¥ng th√≠ch Shopee.T·∫°i v√¨ d√πng app r·∫•t ch·∫≠m,lag banh nh√† l·∫ßu, th·∫≠m ch√≠ log in c√≤n kh√¥ng ƒëc'```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 4718.00it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Source</th>\n",
       "      <th>pred_L1</th>\n",
       "      <th>pred_prob_L1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google play - T√¥i kh√¥ng th√≠ch Shopee . T·∫°i v√¨ ...</td>\n",
       "      <td>google play</td>\n",
       "      <td>Feature</td>\n",
       "      <td>0.876843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       Source  pred_L1   \n",
       "0  google play - T√¥i kh√¥ng th√≠ch Shopee . T·∫°i v√¨ ...  google play  Feature  \\\n",
       "\n",
       "   pred_prob_L1  \n",
       "0      0.876843  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result = controller.predict_raw_text(raw_content,topk=1)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using trained model, using tokenized DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "model_name='nguyenvulebinh/envibert'\n",
    "_model_kwargs={\n",
    "    # overall model hyperparams\n",
    "    'is_multilabel':False, # False\n",
    "    'is_multihead':False, # False\n",
    "    'head_class_sizes':num_classes,\n",
    "    'head_class': SimpleClassificationHead,\n",
    "    # classfication head hyperparams\n",
    "    'classifier_dropout':0.1 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sample_weights/model_progress were not used when initializing RobertaBaseForSequenceClassification: ['body_model.pooler.dense.weight', 'body_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaBaseForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaBaseForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "trained_model = model_init_classification(model_class = RobertaBaseForSequenceClassification,\n",
    "                                          cpoint_path = Path('./sample_weights/model_progress'), \n",
    "                                          output_hidden_states=False,\n",
    "                                          seed=42,\n",
    "                                          model_kwargs = _model_kwargs)\n",
    "\n",
    "\n",
    "metric_funcs = [partial(f1_score,average='macro'),accuracy_score]\n",
    "controller = ModelController(trained_model,metric_funcs) # notice that we don't use tdm here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 4780\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask', 'pred_L1', 'pred_prob_L1'],\n",
       "        num_rows: 447\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_label_name = 'L1'\n",
    "my_class_predefined = ['Buyer complained seller',\n",
    " 'Commercial',\n",
    " 'Delivery',\n",
    " 'Feature',\n",
    " 'Order/Item',\n",
    " 'Others',\n",
    " 'Payment',\n",
    " 'Return/Refund',\n",
    " 'Services',\n",
    " 'Shopee account']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start making predictions --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_val = controller.predict_ddict(main_ddict,\n",
    "                                  ds_type='validation',\n",
    "                                  is_multilabel=False,\n",
    "                                  tokenizer=tokenizer,\n",
    "                                  label_names = my_label_name,\n",
    "                                  class_names_predefined=my_class_predefined\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>Source</th>\n",
       "      <th>pred_L1</th>\n",
       "      <th>pred_prob_L1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google play - Ch∆°i gam r·∫•t l√† l√°c</td>\n",
       "      <td>3</td>\n",
       "      <td>google play</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>0.837496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google play - Zq</td>\n",
       "      <td>5</td>\n",
       "      <td>google play</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.927602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non owned - L√†n s√≥ng k·ªπ thu·∫≠t s·ªë v√† s·ª± l·ª±a ch·ªç...</td>\n",
       "      <td>5</td>\n",
       "      <td>non owned</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.918241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google play - H√†ng qu·ªëc t·∫ø kh√¥ng c√≤n ship COD ...</td>\n",
       "      <td>6</td>\n",
       "      <td>google play</td>\n",
       "      <td>Delivery</td>\n",
       "      <td>0.804539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google play - Qu√° t·ªá . Giao h√†ng ch·∫≠m nh∆∞ r√πa ...</td>\n",
       "      <td>2</td>\n",
       "      <td>google play</td>\n",
       "      <td>Delivery</td>\n",
       "      <td>0.758327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label       Source   \n",
       "0                  google play - Ch∆°i gam r·∫•t l√† l√°c      3  google play  \\\n",
       "1                                   google play - Zq      5  google play   \n",
       "2  non owned - L√†n s√≥ng k·ªπ thu·∫≠t s·ªë v√† s·ª± l·ª±a ch·ªç...      5    non owned   \n",
       "3  google play - H√†ng qu·ªëc t·∫ø kh√¥ng c√≤n ship COD ...      6  google play   \n",
       "4  google play - Qu√° t·ªá . Giao h√†ng ch·∫≠m nh∆∞ r√πa ...      2  google play   \n",
       "\n",
       "      pred_L1  pred_prob_L1  \n",
       "0  Commercial      0.837496  \n",
       "1      Others      0.927602  \n",
       "2      Others      0.918241  \n",
       "3    Delivery      0.804539  \n",
       "4    Delivery      0.758327  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text', 'Source', 'input_ids', 'token_type_ids', 'attention_mask', 'pred_L1', 'pred_prob_L1'],\n",
       "        num_rows: 2269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be cumbersome to preprocess your test data the same way you preprocess your validation set, without the use of tdm (which stores the preprocess pipeline). In short, you need to produce the test datasetdict `test_ddict` containing processed `'input_ids', 'token_type_ids', 'attention_mask'`, then call\n",
    "\n",
    "```python\n",
    "df_results = controller.predict_ddict(ddict=test_ddict,\n",
    "                                      ds_type='test',\n",
    "                                      is_multilabel=False,\n",
    "                                      tokenizer=tokenizer,\n",
    "                                      label_names = my_label_name,\n",
    "                                      class_names_predefined=my_class_predefined     \n",
    "                                     )\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
