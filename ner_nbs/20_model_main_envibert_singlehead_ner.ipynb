{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER tutorial: EnviBert model\n",
    "\n",
    "> This notebook contains some examples of how to use the EnviBert-based model on NER task\n",
    "\n",
    "- skip_showdoc: true\n",
    "- skip_exec: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from that_nlp_library.text_transformation import *\n",
    "# from that_nlp_library.text_augmentation import *\n",
    "# from that_nlp_library.text_main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from underthesea import text_normalize\n",
    "# from functools import partial\n",
    "# from pathlib import Path\n",
    "# from importlib.machinery import SourceFileLoader\n",
    "# from transformers import DataCollatorWithPadding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "from datasets import DatasetDict,Dataset\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import XLMRobertaConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.roberta.configuration_roberta import RobertaConfig\n",
    "\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel # body only\n",
    "from transformers import TrainingArguments\n",
    "#inherit this to load pretrained weight\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
    "from datasets import Features,ClassLabel\n",
    "\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('./data')\n",
    "TRAIN_PATH = DATA_PATH/'train'\n",
    "TEST_PATH = DATA_PATH/'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def read_text_file_line(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return f.readlines()\n",
    "\n",
    "\n",
    "def write_text_file_line(file_path,contents):\n",
    "    with open(file_path,'w') as f:\n",
    "        f.writelines(contents)\n",
    "    print('Write sucessfully to '+ file_path)\n",
    "\n",
    "def process_each_file(file_str,f_name,is_train=True,max_len=360):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - file_str: all texts from f_name\n",
    "        - max_len: maximum len of a token list (to be suitable for BERT max_sequence_len of 514)\n",
    "    \n",
    "    Outputs: \n",
    "        - a list of token list, \n",
    "        - a list of POS tag, \n",
    "        - a list of chunking tag, \n",
    "        - a list of Named-Entity tag\n",
    "        - a list of fname\n",
    "    \"\"\"\n",
    "    pat = r'<s>([\\S\\n\\t\\v ]*?)</s>'\n",
    "    results = re.findall(pat,file_str) # list of raw strings inside <s> tag\n",
    "    token_list,pos_list,chunk_list,ner_list,name_list=[],[],[],[],[]\n",
    "    \n",
    "    \n",
    "    curr_len=0\n",
    "    _token_list,_pos_list,_chunk_list,_ner_list,_name_list=[],[],[],[],[]\n",
    "\n",
    "    for r in results:\n",
    "        _tmp = zip(*[s.strip().split('\\t') for s in r.strip().split('\\n') if s.strip()])\n",
    "        if not is_train:\n",
    "            a,b,c = _tmp\n",
    "        else: a,b,c,d,_ = _tmp # tos,pos,chunk,ner\n",
    "        \n",
    "        # cummulating paragraphs until it reachs max_len\n",
    "        if curr_len+len(a) >= max_len and len(_token_list):\n",
    "            token_list.append(_token_list)\n",
    "            pos_list.append(_pos_list)\n",
    "            chunk_list.append(_chunk_list)\n",
    "            name_list.append(_name_list)\n",
    "            ner_list.append(_ner_list)\n",
    "\n",
    "            _token_list,_pos_list,_chunk_list,_ner_list,_name_list=[],[],[],[],[]\n",
    "            curr_len=0\n",
    "        \n",
    "\n",
    "        _token_list+=a\n",
    "        _pos_list+=b\n",
    "        _chunk_list+=c\n",
    "        _name_list+=[f_name for i in range(len(a))]\n",
    "        if is_train: _ner_list+=d\n",
    "        curr_len+=len(a)\n",
    "        \n",
    "    if len(_token_list):\n",
    "        token_list.append(_token_list)\n",
    "        pos_list.append(_pos_list)\n",
    "        chunk_list.append(_chunk_list)\n",
    "        name_list.append(_name_list)\n",
    "        ner_list.append(_ner_list)\n",
    "            \n",
    "    return token_list,pos_list,chunk_list,ner_list,name_list\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Huggingface dataset for training set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NER vietnamese data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'pos_str', 'chunk_str', 'ner_tags_str', 'fname'],\n",
       "        num_rows: 958\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'pos_str', 'chunk_str', 'ner_tags_str', 'fname'],\n",
       "        num_rows: 240\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# iterate through all train file\n",
    "final_token_list,final_pos_list,final_chunk_list,final_ner_list,final_name_list=[],[],[],[],[]\n",
    "for file in np.sort(os.listdir(TRAIN_PATH)):\n",
    "    # Check whether file is in text format or not\n",
    "#     print(f'process {file}')\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = f\"{TRAIN_PATH}/{file}\"\n",
    "        # call read text file function\n",
    "        file_str = read_text_file(file_path)\n",
    "        token_list,pos_list,chunk_list,ner_list,name_list = process_each_file(file_str,file)\n",
    "        final_token_list+=token_list\n",
    "        final_pos_list+=pos_list\n",
    "        final_chunk_list+=chunk_list\n",
    "        final_ner_list+=ner_list\n",
    "        final_name_list+=name_list\n",
    "\n",
    "train_dataset = Dataset.from_dict(\n",
    "                        {'tokens': final_token_list,\n",
    "                        'pos_str':final_pos_list,\n",
    "                         'chunk_str':final_chunk_list,\n",
    "                         'ner_tags_str':final_ner_list,\n",
    "                         'fname':final_name_list\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "ner_ddict = DatasetDict()\n",
    "ner_ddict['train'] = train_dataset.select(range(int(train_dataset.num_rows*0.8)))\n",
    "ner_ddict['validation'] = train_dataset.select(range(int(train_dataset.num_rows*0.8),train_dataset.num_rows))\n",
    "\n",
    "ner_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿<title>Người xoá \"xóc_chéo\" mùa lũ.</title>\n",
      "<editor>Vietlex team, 8-2016</editor>\n",
      "-DOCSTART-\n",
      "<s>\t\t\t\t\n",
      "Nghe\tV\tB-VP\tO\tO\n",
      "nhiều\tA\tB-AP\tO\tO\n",
      "về\tE\tB-PP\tO\tO\n",
      "ông\tNs\tB-NP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "nhưng\tC\tO\tO\tO\n",
      "đến\tE\tB-PP\tO\tO\n",
      "hôm_nay\tN\tB-NP\tO\tO\n",
      "tôi\tP\tB-NP\tO\tO\n",
      "mới\tR\tO\tO\tO\n",
      "có\tV\tB-VP\tO\tO\n",
      "dịp\tN\tB-NP\tO\tO\n",
      "về\tV\tB-VP\tO\tO\n",
      "ấp\tN\tB-NP\tB-LOC\tO\n",
      "Long_Châu\tNNP\tI-NP\tI-LOC\tO\n",
      "1\tM\tI-NP\tI-LOC\tO\n",
      ",\tCH\tO\tO\tO\n",
      "xã\tN\tB-NP\tB-LOC\tO\n",
      "Thạnh_Mỹ_Tây\tNNP\tI-NP\tI-LOC\tO\n",
      "(\tCH\tO\tO\tO\n",
      "Châu_Phú\tNNP\tB-NP\tB-LOC\tO\n",
      ",\tCH\tO\tO\tO\n",
      "An_Giang\tNNP\tB-NP\tB-LOC\tO\n",
      ")\tCH\tO\tO\tO\n",
      "để\tE\tB-PP\tO\tO\n",
      "gặp\tV\tB-VP\tO\tO\n",
      "ông\tNs\tB-NP\tO\tO\n",
      ".\tCH\tO\tO\tO\n",
      "</s>\t\t\t\t\n",
      "<s>\t\t\t\t\n",
      "Người_ta\tN\tB-NP\tO\tO\n",
      "thường\tR\tO\tO\tO\n",
      "gọi\tV\tB-VP\tO\tO\n",
      "ông\tNs\tB-NP\tO\tO\n",
      "là\tV\tB-VP\tO\tO\n",
      "ông\tNs\tB-NP\tO\tO\n",
      "Ba\tNNP\tB-NP\tB-PER\tO\n",
      "Phước\tNNP\tI-NP\tI-PER\tO\n",
      "(\tCH\tO\tO\tO\n",
      "Trần\tNNP\tB-NP\tB-PER\tO\n",
      "Văn\tNNP\tI-NP\tI-PER\tO\n",
      "Minh\tNNP\tI-NP\tI-PER\tO\n",
      ")\tCH\tO\tO\tO\n",
      "-\tCH\tO\tO\tO\n",
      "người\tN\tB-NP\tO\tO\n",
      "đã\tR\tO\tO\tO\n",
      "bỏ\tV\tB-VP\tO\tO\n",
      "nhiều\tA\tB-AP\tO\tO\n",
      "công\tN\tB-NP\tO\tO\n",
      "của\tN\tB-NP\tO\tO\n",
      "và\tCc\tO\tO\tO\n",
      "ngày_tháng\tN\tB-NP\tO\tO\n",
      "để\tE\tB-PP\tO\tO\n",
      "lo\tV\tB-VP\tO\tO\n",
      "chỗ\tN\tB-NP\tO\tO\n",
      "an_nghỉ\tV\tB-VP\tO\tO\n",
      "cuối\tN\tB-NP\tO\tO\n",
      "đời\tN\tB-NP\tO\tO\n",
      "cho\tE\tB-PP\tO\tO\n",
      "những\tL\tB-NP\tO\tO\n",
      "người\tNs\tB-NP\tO\tO\n",
      "dân_nghèo\tN\tB-NP\tO\tO\n",
      "vùng\tN\tB-NP\tO\tO\n",
      "lũ\tN\tB-NP\tO\tO\n",
      "...\tCH\tO\tO\tO\n",
      "</s>\t\t\t\t\n",
      "<s>\t\t\t\t\n",
      "Cái\tN\tB-NP\tO\tO\n",
      "mất\tV\tB-VP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "cái\tN\tB-NP\tO\tO\n",
      "còn\tV\tB-VP\tO\tO\n",
      ".\tCH\tO\tO\tO\n",
      "</s>\t\t\t\t\n",
      "<s>\t\t\t\t\n",
      "Ông\tNs\tB-NP\tO\tO\n",
      "Ba\tNNP\tB-NP\tB-PER\tO\n",
      "Phước\tNNP\tI-NP\tI-PER\tO\n",
      "vóc\tN\tB-NP\tO\tO\n",
      "người\tN\tB-NP\tO\tO\n",
      "nhỏ_nhắn\tA\tB-AP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "tóc\tN\tB-NP\tO\tO\n",
      "bạc\tA\tB-AP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "da\tN\tB-NP\tO\tO\n",
      "ngăm_ngăm\tA\tB-AP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "mặt\tN\tB-NP\tO\tO\n",
      "hiện\tV\tB-VP\tO\tO\n",
      "rõ\tA\tB-AP\tO\tO\n",
      "nét\tN\tB-NP\tO\tO\n",
      "từng_trải\tV\tB-VP\tO\tO\n",
      "của\tE\tB-PP\tO\tO\n",
      "một\tM\tB-NP\tO\tO\n",
      "ông_già\tN\tB-NP\tO\tO\n",
      "70\tM\tB-NP\tO\tO\n",
      "tuổi\tN\tB-NP\tO\tO\n",
      "đã\tR\tO\tO\tO\n",
      "có\tV\tB-VP\tO\tO\n",
      "cả\tP\tB-NP\tO\tO\n",
      "cuộc_đời\tN\tB-NP\tO\tO\n",
      "cần_cù\tA\tB-AP\tO\tO\n",
      "lao_động\tV\tB-VP\tO\tO\n",
      "miệt_mài\tA\tB-AP\tO\tO\n",
      ".\tCH\tO\tO\tO\n",
      "</s>\t\t\t\t\n",
      "<s>\t\t\t\t\n",
      "Ông\tNs\tB-NP\tO\tO\n",
      "đứng\tV\tB-VP\tO\tO\n",
      "bên\tE\tB-PP\tO\tO\n",
      "tôi\tP\tB-NP\tO\tO\n",
      "nói\tV\tB-VP\tO\tO\n",
      "với\tE\tB-PP\tO\tO\n",
      "giọng\tN\tB-NP\tO\tO\n",
      "tự_hào\tV\tB-VP\tO\tO\n",
      ".\tCH\tO\tO\tO\n",
      "</s>\t\t\t\t\n",
      "<s>\t\t\t\t\n",
      "Chú\tNs\tB-NP\tO\tO\n",
      "xem\tV\tB-VP\tO\tO\n",
      "đó\tP\tB-NP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "cả\tP\tB-NP\tO\tO\n",
      "cơ_sở\tN\tB-NP\tO\tO\n",
      "sản_xuất\tV\tB-VP\tO\tO\n",
      "nước_mắm\tN\tB-NP\tO\tO\n",
      "Phước_Hương\tNNP\tB-NP\tB-ORG\tO\n",
      "một\tM\tB-NP\tO\tO\n",
      "năm\tN\tB-NP\tO\tO\n",
      "làm\tV\tB-VP\tO\tO\n",
      "ra\tV\tB-VP\tO\tO\n",
      "gần\tA\tB-AP\tO\tO\n",
      "2.000\tM\tB-NP\tO\tO\n",
      "tấn\tNu\tB-NP\tO\tO\n",
      "nước_mắm\tN\tB-NP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "cả\tP\tB-NP\tO\tO\n",
      "chục\tL\tB-NP\tO\tO\n",
      "hầm\tN\tB-NP\tO\tO\n",
      "nuôi\tV\tB-VP\tO\tO\n",
      "cá\tN\tB-NP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "hiện\tN\tB-NP\tO\tO\n",
      "có\tV\tB-VP\tO\tO\n",
      "trên\tN\tB-NP\tO\tO\n",
      "600.000\tM\tB-NP\tO\tO\n",
      "con\tNc\tB-NP\tO\tO\n",
      "cá_tra\tN\tB-NP\tO\tO\n",
      "...\tCH\tO\tO\tO\n",
      "</s>\t\t\t\t\n",
      "<s>\t\t\t\t\n",
      "Tất_cả\tP\tB-NP\tO\tO\n",
      "tài_sản\tN\tB-NP\tO\tO\n",
      "của\tE\tB-PP\tO\tO\n",
      "tui\tP\tB-NP\tO\tO\n",
      "giờ_đây\tN\tB-NP\tO\tO\n",
      "trên\tN\tB-NP\tO\tO\n",
      "3\tM\tB-NP\tO\tO\n",
      "tỉ\tM\tB-NP\tO\tO\n",
      "bạc\tN\tB-NP\tO\tO\n",
      "rồi\tR\tO\tO\tO\n",
      "sẽ\tR\tO\tO\tO\n",
      "chẳng\tR\tO\tO\tO\n",
      "còn\tV\tB-VP\tO\tO\n",
      "là\tC\tO\tO\tO\n",
      "của\tE\tB-PP\tO\tO\n",
      "tui\tP\tB-NP\tO\tO\n",
      "nữa\tT\tO\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "nhưng\tC\tO\tO\tO\n",
      "cái\tNc\tB-NP\tO\tO\n",
      "nghĩa_địa\tN\tB-NP\tO\tO\n",
      "đó\tP\tB-NP\tO\tO\n",
      "mới\tR\tO\tO\tO\n",
      "là\tV\tB-VP\tO\tO\n",
      "của\tE\tB-PP\tO\tO\n",
      "tui\tP\tB-NP\tO\tO\n",
      "...\tCH\tO\tO\tO\n",
      "</s>\t\t\t\t\n",
      "<s>\t\t\t\t\n",
      "Tôi\tP\tB-NP\tO\tO\n",
      "nhìn\tV\tB-VP\tO\tO\n",
      "theo\tV\tB-VP\tO\tO\n",
      "hướng\tN\tB-NP\tO\tO\n",
      "cánh_tay\tN\tB-NP\tO\tO\n",
      "của\tE\tB-PP\tO\tO\n",
      "ông\tNs\tB-NP\tO\tO\n",
      "Ba\tNNP\tB-NP\tB-PER\tO\n",
      "Phước\tNNP\tI-NP\tI-PER\tO\n",
      "chỉ\tV\tB-VP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "từ\tE\tB-PP\tO\tO\n",
      "căn\tNc\tB-NP\tO\tO\n",
      "nhà\tN\tB-NP\tO\tO\n",
      "đúc\tV\tB-VP\tO\tO\n",
      "nằm\tV\tB-VP\tO\tO\n",
      "ven\tE\tB-PP\tO\tO\n",
      "con\tNc\tB-NP\tO\tO\n",
      "kênh_xáng\tN\tB-NP\tB-LOC\tO\n",
      "Vịnh_Tre\tNNP\tI-NP\tI-LOC\tO\n",
      "vòng\tV\tB-VP\tO\tO\n",
      "qua\tR\tO\tO\tO\n",
      "mấy\tL\tB-NP\tO\tO\n",
      "dãy\tNe\tB-NP\tO\tO\n",
      "nhà_kho\tN\tB-NP\tO\tO\n",
      "ủ\tV\tB-VP\tO\tO\n",
      "nước_mắm\tN\tB-NP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "mấy\tL\tB-NP\tO\tO\n",
      "hầm\tN\tB-NP\tO\tO\n",
      "nuôi\tV\tB-VP\tO\tO\n",
      "cá_tra\tN\tB-NP\tO\tO\n",
      "đang\tR\tO\tO\tO\n",
      "lố_nhố\tA\tB-AP\tO\tO\n",
      "mấy\tL\tB-NP\tO\tO\n",
      "chục\tL\tB-NP\tO\tO\n",
      "công_nhân\tN\tB-NP\tO\tO\n",
      "làm_việc\tV\tB-VP\tO\tO\n",
      "mà\tC\tO\tO\tO\n",
      "thầm\tA\tB-AP\tO\tO\n",
      "khâm_phục\tV\tB-VP\tO\tO\n",
      "người\tN\tB-NP\tO\tO\n",
      "nông_dân\tN\tB-NP\tO\tO\n",
      "chỉ\tR\tO\tO\tO\n",
      "tốt_nghiệp\tV\tB-VP\tO\tO\n",
      "tiểu_học\tN\tB-NP\tO\tO\n",
      "trường\tN\tB-NP\tO\tO\n",
      "làng\tN\tB-NP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "từ\tE\tB-PP\tO\tO\n",
      "bàn_tay\tN\tB-NP\tO\tO\n",
      "trắng\tA\tB-AP\tO\tO\n",
      "đã\tR\tO\tO\tO\n",
      "tạo_lập\tV\tB-VP\tO\tO\n",
      "nên\tR\tO\tO\tO\n",
      "cơ_nghiệp\tN\tB-NP\tO\tO\n",
      "...\tCH\tO\tO\tO\n",
      "</s>\t\t\t\t\n",
      "<s>\t\t\t\t\n",
      "Ba\tNNP\tB-NP\tB-PER\tO\n",
      "Phước\tNNP\tI-NP\tI-PER\tO\n",
      "đang\tR\tO\tO\tO\n",
      "là\tV\tB-VP\tO\tO\n",
      "chủ_tịch\tN\tB-NP\tO\tO\n",
      "hội_đồng_quản_trị\tN\tB-NP\tO\tO\n",
      "doanh_nghiệp\tN\tB-NP\tO\tO\n",
      "tư_nhân\tN\tB-NP\tO\tO\n",
      "sản_xuất\tV\tB-VP\tO\tO\n",
      "nước_mắm\tN\tB-NP\tO\tO\n",
      "Phước_Hương\tNNP\tB-NP\tB-ORG\tO\n",
      ",\tCH\tO\tO\tO\n",
      "nhưng\tC\tO\tO\tO\n",
      "lại\tR\tO\tO\tO\n",
      "bảo\tV\tB-VP\tO\tO\n",
      "cơ_nghiệp\tN\tB-NP\tO\tO\n",
      "đó\tP\tB-NP\tO\tO\n",
      "sẽ\tR\tO\tO\tO\n",
      "chẳng\tR\tO\tO\tO\n",
      "là\tV\tB-VP\tO\tO\n",
      "của\tE\tB-PP\tO\tO\n",
      "mình\tP\tB-NP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "bởi\tE\tB-PP\tO\tO\n",
      "theo\tE\tB-PP\tO\tO\n",
      "cách\tN\tB-NP\tO\tO\n",
      "nói\tV\tB-VP\tO\tO\n",
      "của\tE\tB-PP\tO\tO\n",
      "ông\tNs\tB-NP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "cơ_nghiệp\tN\tB-NP\tO\tO\n",
      "ông\tNs\tB-NP\tO\tO\n",
      "tạo_lập\tV\tB-VP\tO\tO\n",
      "nên\tR\tO\tO\tO\n",
      "đã\tR\tO\tO\tO\n",
      "được\tV\tB-VP\tO\tO\n",
      "chia\tV\tB-VP\tO\tO\n",
      "cho\tE\tB-PP\tO\tO\n",
      "con_cháu\tN\tB-NP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "chúng\tP\tB-NP\tO\tO\n",
      "đã\tR\tO\tO\tO\n",
      "và\tCc\tO\tO\tO\n",
      "đang\tR\tO\tO\tO\n",
      "trực_tiếp\tV\tB-VP\tO\tO\n",
      "tổ_chức\tV\tB-VP\tO\tO\n",
      "sản_xuất\tV\tB-VP\tO\tO\n",
      "kinh_doanh\tV\tB-VP\tO\tO\n",
      ".\tCH\tO\tO\tO\n",
      "</s>\t\t\t\t\n",
      "<s>\t\t\t\t\n",
      "Mai\tN\tB-NP\tO\tO\n",
      "này\tP\tB-NP\tO\tO\n",
      "chuyện\tN\tB-NP\tO\tO\n",
      "tồn_tại\tV\tB-VP\tO\tO\n",
      "và\tCc\tO\tO\tO\n",
      "phát_triển\tV\tB-VP\tO\tO\n",
      "hay\tCc\tO\tO\tO\n",
      "không\tR\tO\tO\tO\n",
      "ông\tNs\tB-NP\tO\tO\n",
      "không\tR\tO\tO\tO\n",
      "biết\tV\tB-VP\tO\tO\n",
      "chắc\tA\tB-AP\tO\tO\n",
      "được\tR\tO\tO\tO\n",
      ".\tCH\tO\tO\tO\n",
      "</s>\t\t\t\t\n",
      "<s>\t\t\t\t\n",
      "Thế\tT\tO\tO\tO\n",
      "nhưng\tC\tO\tO\tO\n",
      "cái\tNc\tB-NP\tO\tO\n",
      "nghĩa_địa\tN\tB-NP\tO\tO\n",
      "rộng\tA\tB-AP\tO\tO\n",
      "trên\tN\tB-NP\tO\tO\n",
      "7.000\tM\tB-NP\tO\tO\n",
      "m²\tNu\tB-NP\tO\tO\n",
      "ở\tE\tB-PP\tO\tO\n",
      "ấp\tN\tB-NP\tB-LOC\tO\n",
      "Long_Châu\tNNP\tI-NP\tI-LOC\tO\n",
      "1\tM\tI-NP\tI-LOC\tO\n",
      ",\tCH\tO\tO\tO\n",
      "xã\tN\tB-NP\tB-LOC\tO\n",
      "Thạnh_Mỹ_Tây\tNNP\tI-NP\tI-LOC\tO\n",
      ",\tCH\tO\tO\tO\n",
      "huyện\tN\tB-NP\tB-LOC\tO\n",
      "Châu_Phú\tNNP\tI-NP\tI-LOC\tO\n",
      "(\tCH\tO\tO\tO\n",
      "An_Giang\tNNP\tB-NP\tB-LOC\tO\n",
      ")\tCH\tO\tO\tO\n",
      "mang\tV\tB-VP\tO\tO\n",
      "tên\tN\tB-NP\tO\tO\n",
      "Từ_Thiện\tNNP\tB-NP\tB-LOC\tO\n",
      "mà\tC\tO\tO\tO\n",
      "ông\tNs\tB-NP\tO\tO\n",
      "đã\tR\tO\tO\tO\n",
      "tốn\tV\tB-VP\tO\tO\n",
      "biết_bao\tR\tO\tO\tO\n",
      "công_sức\tN\tB-NP\tO\tO\n",
      "xây_dựng\tV\tB-VP\tO\tO\n",
      "chắc_chắn\tA\tB-AP\tO\tO\n",
      "sẽ\tR\tO\tO\tO\n",
      "tồn_tại\tV\tB-VP\tO\tO\n",
      "và\tCc\tO\tO\tO\n",
      "phát_triển\tV\tB-VP\tO\tO\n",
      ".\tCH\tO\tO\tO\n",
      "</s>\t\t\t\t\n",
      "<s>\t\t\t\t\n",
      "Ba\tNNP\tB-NP\tB-PER\tO\n",
      "Phước\tNNP\tI-NP\tI-PER\tO\n",
      "tâm_sự\tV\tB-VP\tO\tO\n",
      ":\tCH\tO\tO\tO\n",
      "\"\tCH\tO\tO\tO\n",
      "-\tCH\tO\tO\tO\n",
      "Chú\tNs\tB-NP\tO\tO\n",
      "biết\tV\tB-VP\tO\tO\n",
      "đó\tP\tB-NP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "đất\tN\tB-NP\tO\tO\n",
      "xứ\tN\tB-NP\tO\tO\n",
      "này\tP\tB-NP\tO\tO\n",
      "thấp\tA\tB-AP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "mùa\tN\tB-NP\tO\tO\n",
      "nước\tN\tB-NP\tO\tO\n",
      "nổi\tV\tB-VP\tO\tO\n",
      "dâng\tV\tB-VP\tO\tO\n",
      "lên\tV\tB-VP\tO\tO\n",
      "có\tV\tB-VP\tO\tO\n",
      "chỗ\tN\tB-NP\tO\tO\n",
      "ngập\tV\tB-VP\tO\tO\n",
      "trên\tN\tB-NP\tO\tO\n",
      "2,5\tM\tB-NP\tO\tO\n",
      "m\tNu\tB-NP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "với\tE\tB-PP\tO\tO\n",
      "nhiều\tA\tB-AP\tO\tO\n",
      "gia_đình\tN\tB-NP\tO\tO\n",
      "nghèo\tA\tB-AP\tO\tO\n",
      "chỗ\tN\tB-NP\tO\tO\n",
      "cao_ráo\tA\tB-AP\tO\tO\n",
      "để\tE\tB-PP\tO\tO\n",
      "cất\tV\tB-VP\tO\tO\n",
      "nhà\tN\tB-NP\tO\tO\n",
      "ở\tV\tB-VP\tO\tO\n",
      "còn\tR\tO\tO\tO\n",
      "không\tR\tO\tO\tO\n",
      "có\tV\tB-VP\tO\tO\n",
      "nói\tV\tB-VP\tO\tO\n",
      "chi\tR\tO\tO\tO\n",
      "đến\tE\tB-PP\tO\tO\n",
      "chỗ\tN\tB-NP\tO\tO\n",
      "an_táng\tV\tB-VP\tO\tO\n",
      "người_thân\tN\tB-NP\tO\tO\n",
      "nếu\tC\tO\tO\tO\n",
      "không\tR\tO\tO\tO\n",
      "may\tA\tB-AP\tO\tO\n",
      "chết\tV\tB-VP\tO\tO\n",
      "vào\tE\tB-PP\tO\tO\n",
      "mùa\tN\tB-NP\tO\tO\n",
      "nước\tN\tB-NP\tO\tO\n",
      "nổi\tV\tB-VP\tO\tO\n",
      ".\tCH\tO\tO\tO\n",
      "</s>\t\t\t\t\n",
      "<s>\t\t\t\t\n",
      "Chú\tNs\tB-NP\tO\tO\n",
      "mà\tC\tO\tO\tO\n",
      "thấy\tV\tB-VP\tO\tO\n",
      "cảnh\tN\tB-NP\tO\tO\n",
      "chôn\tV\tB-VP\tO\tO\n",
      "người\tN\tB-NP\tO\tO\n",
      "chết\tV\tB-VP\tO\tO\n",
      "mùa\tN\tB-NP\tO\tO\n",
      "nước\tN\tB-NP\tO\tO\n",
      "nổi\tV\tB-VP\tO\tO\n",
      "mới\tR\tO\tO\tO\n",
      "xót_xa\tA\tB-AP\tO\tO\n",
      ".\tCH\tO\tO\tO\n",
      "</s>\t\t\t\t\n",
      "<s>\t\t\t\t\n",
      "Lúc\tN\tB-NP\tO\tO\n",
      "đó\tP\tB-NP\tO\tO\n",
      "người_ta\tN\tB-NP\tO\tO\n",
      "phải\tV\tB-VP\tO\tO\n",
      "ra_công\tV\tB-VP\tO\tO\n",
      "ghép\tV\tB-VP\tO\tO\n",
      "ván\tN\tB-NP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "cắm\tV\tB-VP\tO\tO\n",
      "cứng\tA\tB-AP\tO\tO\n",
      "xuống\tV\tB-VP\tO\tO\n",
      "nước\tN\tB-NP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "ém\tV\tB-VP\tO\tO\n",
      "kín\tA\tB-AP\tO\tO\n",
      "rồi\tC\tO\tO\tO\n",
      "bơm\tV\tB-VP\tO\tO\n",
      "nước\tN\tB-NP\tO\tO\n",
      "ra\tV\tB-VP\tO\tO\n",
      "để\tE\tB-PP\tO\tO\n",
      "đào\tV\tB-VP\tO\tO\n",
      "đất\tN\tB-NP\tO\tO\n",
      ".\tCH\tO\tO\tO\n",
      "</s>\t\t\t\t\n",
      "<s>\t\t\t\t\n",
      "Khi\tN\tB-NP\tO\tO\n",
      "đặt\tV\tB-VP\tO\tO\n",
      "quan_tài\tN\tB-NP\tO\tO\n",
      "xuống\tV\tB-VP\tO\tO\n",
      "mà\tC\tO\tO\tO\n",
      "dưới\tE\tB-PP\tO\tO\n",
      "huyệt\tN\tB-NP\tO\tO\n",
      "có\tV\tB-VP\tO\tO\n",
      "nước\tN\tB-NP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "quan_tài\tN\tB-NP\tO\tO\n",
      "cứ\tR\tO\tO\tO\n",
      "nổi\tV\tB-VP\tO\tO\n",
      "lên\tV\tB-VP\tO\tO\n",
      "bập_bềnh\tA\tB-AP\tO\tO\n",
      "chẳng\tR\tO\tO\tO\n",
      "chịu\tV\tB-VP\tO\tO\n",
      "nằm\tV\tB-VP\tO\tO\n",
      "im\tA\tB-AP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "phải\tV\tB-VP\tO\tO\n",
      "đè\tV\tB-VP\tO\tO\n",
      "xuống\tV\tB-VP\tO\tO\n",
      "và\tCc\tO\tO\tO\n",
      "đổ\tV\tB-VP\tO\tO\n",
      "nhanh\tA\tB-AP\tO\tO\n",
      "đất\tN\tB-NP\tO\tO\n",
      "lấp\tV\tB-VP\tO\tO\n",
      "lại\tV\tB-VP\tO\tO\n",
      ".\tCH\tO\tO\tO\n",
      "</s>\t\t\t\t\n",
      "<s>\t\t\t\t\n",
      "Không\tR\tO\tO\tO\n",
      "có\tV\tB-VP\tO\tO\n",
      "đất\tN\tB-NP\tO\tO\n",
      "gần\tA\tB-AP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "người_ta\tN\tB-NP\tO\tO\n",
      "phải\tV\tB-VP\tO\tO\n",
      "mang\tV\tB-VP\tO\tO\n",
      "người\tN\tB-NP\tO\tO\n",
      "chết\tV\tB-VP\tO\tO\n",
      "lên\tE\tB-PP\tO\tO\n",
      "vùng\tN\tB-NP\tO\tO\n",
      "Bảy_Núi\tNNP\tI-NP\tB-LOC\tO\n",
      "để\tE\tB-PP\tO\tO\n",
      "chôn_cất\tV\tB-VP\tO\tO\n",
      "xa_xôi\tA\tB-AP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "tốn_kém\tV\tB-VP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "chỉ\tR\tO\tO\tO\n",
      "nhà\tN\tB-NP\tO\tO\n",
      "giàu\tA\tB-AP\tO\tO\n",
      "mới\tR\tO\tO\tO\n",
      "lo\tV\tB-VP\tO\tO\n",
      "nổi\tR\tO\tO\tO\n",
      "...\tCH\tO\tO\tO\n",
      "</s>\t\t\t\t\n",
      "<s>\t\t\t\t\n",
      "Mùa\tN\tB-NP\tO\tO\n",
      "nước\tN\tB-NP\tO\tO\n",
      "nổi\tV\tB-VP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "có\tV\tB-VP\tO\tO\n",
      "gia_đình\tN\tB-NP\tO\tO\n",
      "nghèo_khổ\tA\tB-AP\tO\tO\n",
      "quá\tT\tO\tO\tO\n",
      "phải\tV\tB-VP\tO\tO\n",
      "để\tV\tB-VP\tO\tO\n",
      "quan_tài\tN\tB-NP\tO\tO\n",
      "trên\tE\tB-PP\tO\tO\n",
      "giàn\tN\tB-NP\tO\tO\n",
      "gỗ\tN\tB-NP\tO\tO\n",
      "giữa\tE\tB-PP\tO\tO\n",
      "đồng\tN\tB-NP\tO\tO\n",
      "nước\tN\tB-NP\tO\tO\n",
      "mênh_mông\tA\tB-AP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "chờ\tV\tB-VP\tO\tO\n",
      "nước\tN\tB-NP\tO\tO\n",
      "rút\tV\tB-VP\tO\tO\n",
      "mới\tR\tO\tO\tO\n",
      "chôn\tV\tB-VP\tO\tO\n",
      ".\tCH\tO\tO\tO\n",
      "</s>\t\t\t\t\n",
      "<s>\t\t\t\t\n",
      "Cách\tN\tB-NP\tO\tO\n",
      "an_táng\tV\tB-VP\tO\tO\n",
      "này\tP\tB-NP\tO\tO\n",
      "người_ta\tN\tB-NP\tO\tO\n",
      "thường\tR\tO\tO\tO\n",
      "gọi\tV\tB-VP\tO\tO\n",
      "bằng\tE\tB-PP\tO\tO\n",
      "từ\tN\tB-NP\tO\tO\n",
      "\"\tCH\tO\tO\tO\n",
      "xóc_chéo\tN\tB-NP\tO\tO\n",
      "\"\tCH\tO\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "thấy\tV\tB-VP\tO\tO\n",
      "thương_tâm\tV\tB-VP\tO\tO\n",
      "lắm\tR\tO\tO\tO\n",
      "chú\tNs\tB-NP\tO\tO\n",
      "...\tCH\tO\tO\tO\n",
      "</s>\t\t\t\t\n",
      "<s>\t\t\t\t\n",
      "Ông\tNs\tB-NP\tO\tO\n",
      "Ba\tNNP\tB-NP\tB-PER\tO\n",
      "Phước\tNNP\tI-NP\tI-PER\tO\n",
      "còn\tR\tO\tO\tO\n",
      "kể\tV\tB-VP\tO\tO\n",
      "cho\tE\tB-PP\tO\tO\n",
      "tôi\tP\tB-NP\tO\tO\n",
      "nghe\tV\tB-VP\tO\tO\n",
      "nguồn_gốc\tN\tB-NP\tO\tO\n",
      "hình_thành\tV\tB-VP\tO\tO\n",
      "nghĩa_địa\tN\tB-NP\tO\tO\n",
      "này\tP\tB-NP\tO\tO\n",
      ".\tCH\tO\tO\tO\n",
      "</s>\t\t\t\t\n",
      "<s>\t\t\t\t\n",
      "Tại\tE\tB-PP\tO\tO\n",
      "nơi\tN\tB-NP\tO\tO\n",
      "đây\tP\tB-NP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "trước_kia\tN\tB-NP\tO\tO\n",
      "khoảng\tN\tB-NP\tO\tO\n",
      "đất\tN\tB-NP\tO\tO\n",
      "chỉ\tR\tO\tO\tO\n",
      "rộng\tA\tB-AP\tO\tO\n",
      "1.634\tM\tB-NP\tO\tO\n",
      "m²\tNu\tB-NP\tO\tO\n",
      "của\tE\tB-PP\tO\tO\n",
      "ông\tNs\tB-NP\tO\tO\n",
      "Nguyễn\tNNP\tB-NP\tB-PER\tO\n",
      "Văn\tNNP\tI-NP\tI-PER\tO\n",
      "Tâm\tNNP\tI-NP\tI-PER\tO\n",
      ",\tCH\tO\tO\tO\n",
      "là\tV\tB-VP\tO\tO\n",
      "gò\tN\tB-NP\tO\tO\n",
      "đất\tN\tB-NP\tO\tO\n",
      "hơi\tR\tO\tO\tO\n",
      "cao\tA\tB-AP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "cho_phép\tV\tB-VP\tO\tO\n",
      "bà_con\tN\tB-NP\tO\tO\n",
      "trong\tE\tB-PP\tO\tO\n",
      "vùng\tN\tB-NP\tO\tO\n",
      "chôn_cất\tV\tB-VP\tO\tO\n",
      "người_thân\tN\tB-NP\tO\tO\n",
      "khi\tN\tB-NP\tO\tO\n",
      "qua_đời\tV\tB-VP\tO\tO\n",
      ",\tCH\tO\tO\tO\n",
      "gọi\tV\tB-VP\tO\tO\n",
      "là\tC\tO\tO\tO\n",
      "nhị_tì\tN\tB-NP\tO\tO\n",
      "...\tCH\tO\tO\tO\n",
      "</s>\t\t\t\t\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(file_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Bởi',\n",
       "  'theo',\n",
       "  'qui_định',\n",
       "  ',',\n",
       "  'họ',\n",
       "  'vẫn',\n",
       "  'còn',\n",
       "  'một',\n",
       "  'cơ_hội',\n",
       "  'sống',\n",
       "  'khi',\n",
       "  'đặt',\n",
       "  'bút',\n",
       "  'viết',\n",
       "  'đơn',\n",
       "  'xin',\n",
       "  'ân_giảm',\n",
       "  'án',\n",
       "  'chết',\n",
       "  '...',\n",
       "  'Các',\n",
       "  'tử_tù',\n",
       "  'như',\n",
       "  'Tân',\n",
       "  ',',\n",
       "  'Ngọc',\n",
       "  '...',\n",
       "  'đã',\n",
       "  'nhảy_cẫng',\n",
       "  'lên',\n",
       "  'và',\n",
       "  'thét',\n",
       "  'to',\n",
       "  'đến',\n",
       "  'mức',\n",
       "  'như',\n",
       "  'vỡ',\n",
       "  'tung',\n",
       "  'cả',\n",
       "  'buồng',\n",
       "  'giam',\n",
       "  'khi',\n",
       "  'bất_ngờ',\n",
       "  'được',\n",
       "  'thông_báo',\n",
       "  'đơn',\n",
       "  'xin',\n",
       "  'ân_giảm',\n",
       "  'đã',\n",
       "  'được',\n",
       "  'Chủ_tịch',\n",
       "  'nước',\n",
       "  'chấp_thuận',\n",
       "  '.',\n",
       "  'Họ',\n",
       "  'cúi',\n",
       "  'lạy',\n",
       "  'trời_đất',\n",
       "  ',',\n",
       "  'cúi',\n",
       "  'lạy',\n",
       "  'Chủ_tịch',\n",
       "  'nước',\n",
       "  ',',\n",
       "  'và',\n",
       "  'hôn',\n",
       "  'cả',\n",
       "  'quản_giáo',\n",
       "  '.',\n",
       "  'Họ',\n",
       "  'mừng',\n",
       "  'như',\n",
       "  'được',\n",
       "  'sống',\n",
       "  'lại',\n",
       "  'một',\n",
       "  'cuộc_đời',\n",
       "  'mới',\n",
       "  '.',\n",
       "  'Và',\n",
       "  'tự_nhiên',\n",
       "  'lúc',\n",
       "  'ấy',\n",
       "  ',',\n",
       "  'Thắng',\n",
       "  'cũng',\n",
       "  'thấy',\n",
       "  'lòng',\n",
       "  'nhẹ_nhàng',\n",
       "  'đến',\n",
       "  'lạ',\n",
       "  '.',\n",
       "  'Những',\n",
       "  'bước',\n",
       "  'tường',\n",
       "  'bêtông',\n",
       "  'hằn',\n",
       "  'sâu',\n",
       "  'bao',\n",
       "  'hình_bóng',\n",
       "  'tử_tù',\n",
       "  'như',\n",
       "  'không',\n",
       "  'còn',\n",
       "  'xám_xịt',\n",
       "  'và',\n",
       "  'lạnh_lẽo',\n",
       "  'vô_hồn',\n",
       "  '.',\n",
       "  'Về',\n",
       "  'nhà',\n",
       "  ',',\n",
       "  'vợ',\n",
       "  'làm',\n",
       "  'cùng',\n",
       "  'trại',\n",
       "  'tạm',\n",
       "  'giam',\n",
       "  'hiểu',\n",
       "  'tâm_trạng',\n",
       "  'chồng',\n",
       "  '.',\n",
       "  'Nhưng',\n",
       "  'đứa',\n",
       "  'con',\n",
       "  'mới',\n",
       "  'học',\n",
       "  'lớp',\n",
       "  '8',\n",
       "  'thấy',\n",
       "  'mặt',\n",
       "  'bố',\n",
       "  'tươi_tỉnh',\n",
       "  'khác',\n",
       "  'với',\n",
       "  'vẻ',\n",
       "  'trầm_ngâm',\n",
       "  'nặng_nề',\n",
       "  'thường_ngày',\n",
       "  ',',\n",
       "  'cứ',\n",
       "  'ngạc_nhiên',\n",
       "  'mãi',\n",
       "  '.',\n",
       "  'Không_thể',\n",
       "  'kể',\n",
       "  'công_việc',\n",
       "  'đặc_biệt',\n",
       "  'của',\n",
       "  'mình',\n",
       "  ',',\n",
       "  'anh',\n",
       "  'chỉ',\n",
       "  'cười_cười',\n",
       "  ':',\n",
       "  '“',\n",
       "  'Đêm',\n",
       "  'qua',\n",
       "  ',',\n",
       "  'khi',\n",
       "  'con',\n",
       "  'ngủ',\n",
       "  ',',\n",
       "  'mặt_trời',\n",
       "  'đã',\n",
       "  'mọc',\n",
       "  'lúc',\n",
       "  'nửa_đêm',\n",
       "  'đấy',\n",
       "  'con',\n",
       "  'à',\n",
       "  '!',\n",
       "  '”',\n",
       "  '.',\n",
       "  'Bây_giờ',\n",
       "  ',',\n",
       "  'quản_giáo',\n",
       "  'Thắng',\n",
       "  'vẫn',\n",
       "  'chưa',\n",
       "  'già',\n",
       "  'với',\n",
       "  'tuổi',\n",
       "  '40',\n",
       "  ',',\n",
       "  'nhưng',\n",
       "  'công_việc',\n",
       "  'nặng_nề',\n",
       "  'đã',\n",
       "  'hằn',\n",
       "  'trên',\n",
       "  'trán',\n",
       "  'nhiều',\n",
       "  'nếp',\n",
       "  'nhăn',\n",
       "  'sớm',\n",
       "  '.',\n",
       "  'Anh',\n",
       "  'có',\n",
       "  'niềm',\n",
       "  'tin',\n",
       "  'tội_ác',\n",
       "  'phải',\n",
       "  'bị',\n",
       "  'trừng_phạt',\n",
       "  ',',\n",
       "  'để',\n",
       "  'đảm_bảo',\n",
       "  'bình_yên',\n",
       "  'cho',\n",
       "  'xã_hội',\n",
       "  '.',\n",
       "  'Nhưng',\n",
       "  'anh',\n",
       "  'cũng',\n",
       "  'có',\n",
       "  'ước_mơ',\n",
       "  'đến',\n",
       "  'một',\n",
       "  'lúc_nào',\n",
       "  'đó',\n",
       "  'sẽ',\n",
       "  'không',\n",
       "  'còn',\n",
       "  'trọng_tội',\n",
       "  ',',\n",
       "  'để',\n",
       "  'không',\n",
       "  'còn',\n",
       "  'những',\n",
       "  'buồng',\n",
       "  'biệt',\n",
       "  'giam',\n",
       "  'tử_tù',\n",
       "  'nữa',\n",
       "  '.',\n",
       "  'Và',\n",
       "  'khi',\n",
       "  'ấy',\n",
       "  ',',\n",
       "  'anh',\n",
       "  'sẽ',\n",
       "  'có_thể',\n",
       "  'nở',\n",
       "  'nụ',\n",
       "  'cười',\n",
       "  'thật',\n",
       "  'tươi',\n",
       "  'với',\n",
       "  'con_cái',\n",
       "  'mình',\n",
       "  '...'],\n",
       " 252)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_ddict['validation']['tokens'][0],len(ner_ddict['validation']['tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_ddict['validation']['ner_tags_str'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load huggingface ner data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import get_dataset_config_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# from datasets import DatasetDict\n",
    "\n",
    "# langs = [\"de\", \"fr\", \"it\", \"en\"]\n",
    "# fracs = [0.629, 0.229, 0.084, 0.059]\n",
    "# # Return a DatasetDict if a key doesn't exist\n",
    "# panx_ch = defaultdict(DatasetDict)\n",
    "\n",
    "# for lang, frac in zip(langs, fracs):\n",
    "#     # Load monolingual corpus\n",
    "#     ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
    "#     # Shuffle and downsample each split according to spoken proportion\n",
    "#     for split in ds:\n",
    "#         panx_ch[lang][split] = (\n",
    "#             ds[split]\n",
    "#             .shuffle(seed=0)\n",
    "#             .select(range(int(frac * ds[split].num_rows))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xtreme (/home/quan/.cache/huggingface/datasets/xtreme/PAN-X.vi/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4769a7b541884517bc05a6b43a36f353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.vi/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-90e8d12eb87708dc.arrow\n",
      "Loading cached shuffled indices for dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.vi/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-83187dc62d826c69.arrow\n",
      "Loading cached shuffled indices for dataset at /home/quan/.cache/huggingface/datasets/xtreme/PAN-X.vi/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-3b3d13f2fe95ebe9.arrow\n"
     ]
    }
   ],
   "source": [
    "tmp = load_dataset(\"xtreme\", name=f\"PAN-X.vi\")\n",
    "v_ddict_sample = DatasetDict()\n",
    "for split in tmp:\n",
    "    v_ddict_sample[split] = tmp[split].shuffle(seed=42).select(range(int(0.5*tmp[split].num_rows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_ddict_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cơ', 'quan', 'Mật', 'vụ', 'Hoa', 'Kỳ', '(', 'USSS', ')']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_ddict_sample['validation']['tokens'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 4, 4, 4, 4, 0, 0, 0]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_ddict_sample['validation']['ner_tags'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags  = v_ddict_sample['train'].features['ner_tags'].feature\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tag_names(batch):\n",
    "    return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_ddict_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v_ddict_sample = v_ddict_sample.map(create_tag_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cơ', 'quan', 'Mật', 'vụ', 'Hoa', 'Kỳ', '(', 'USSS', ')']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_ddict_sample['validation']['tokens'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_ddict_sample['validation']['ner_tags_str'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_tfms=[text_normalize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_nonown_tfm = partial(sampling_with_condition,query='Source==\"non owned\"',frac=0.5,seed=42,apply_to_all=False)\n",
    "over_nonown_tfm.__name__ = 'Oversampling Non Owned'\n",
    "\n",
    "over_own_tfm = partial(sampling_with_condition,query='Source==\"owned\"',frac=2,seed=42,apply_to_all=False)\n",
    "over_own_tfm.__name__ = 'Oversampling Owned'\n",
    "\n",
    "over_hc_tfm = partial(sampling_with_condition,query='Source==\"hc search\"',frac=2.5,seed=42,apply_to_all=False)\n",
    "over_hc_tfm.__name__ = 'Oversampling HC search'\n",
    "\n",
    "remove_accent_tfm = partial(remove_vnmese_accent,frac=1,seed=42,apply_to_all=True)\n",
    "remove_accent_tfm.__name__ = 'Add No-Accent Text'\n",
    "\n",
    "aug_tfms = [over_nonown_tfm,over_own_tfm,over_hc_tfm,remove_accent_tfm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a TextDataMain object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('secret_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Input Validation Precheck -----\n",
      "DataFrame contains missing values!\n",
      "-----> List of columns and the number of missing values for each\n",
      "is_valid    65804\n",
      "dtype: int64\n",
      "DataFrame contains duplicated values!\n",
      "-----> Number of duplications: 7 rows\n"
     ]
    }
   ],
   "source": [
    "tdm = TextDataMain.from_csv(DATA_PATH/'buyer_listening_with_all_raw_data_w151617.csv',\n",
    "                            return_df=False,\n",
    "                            main_content='Content',\n",
    "                            metadatas='Source',\n",
    "                            label_names='L1',\n",
    "                            val_ratio=0.24,\n",
    "                            split_cols='L1',\n",
    "                            content_tfms = txt_tfms,\n",
    "                            aug_tfms = aug_tfms,\n",
    "                            process_metadatas=True,\n",
    "                            seed=42,\n",
    "                            shuffle_trn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our tokenizer for EnviBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir=Path('./envibert_tokenizer')\n",
    "tokenizer = SourceFileLoader(\"envibert.tokenizer\", \n",
    "                             str(cache_dir/'envibert_tokenizer.py')).load_module().RobertaTokenizer(cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EnviBert a data collator to work. We will save this as an attribute in TDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer,padding=True,max_length=512)\n",
    "tdm.set_data_collator(data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our DatasetDict from TextDataMain (as our `ModelController` class can also work with DatasetDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start Main Text Processing --------------------\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "----- Label Encoding -----\n",
      "-------------------- Text Transformation --------------------\n",
      "----- text_normalize -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 112453/112453 [00:28<00:00, 3940.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Train Test Split --------------------\n",
      "Previous Validation Percentage: 24.0%\n",
      "- Before leak check\n",
      "Size: 26989\n",
      "- After leak check\n",
      "Size: 23930\n",
      "- Number of rows leaked: 3059, or 11.33% of the original validation (or test) data\n",
      "Current Validation Percentage: 21.28%\n",
      "-------------------- Text Augmentation --------------------\n",
      "Train data size before augmentation: 88523\n",
      "----- Oversampling Non Owned -----\n",
      "Train data size after THIS augmentation: 98345\n",
      "----- Oversampling Owned -----\n",
      "Train data size after THIS augmentation: 109231\n",
      "----- Oversampling HC search -----\n",
      "Train data size after THIS augmentation: 116233\n",
      "----- Add No-Accent Text -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 116233/116233 [00:06<00:00, 19148.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size after THIS augmentation: 232466\n",
      "Train data size after ALL augmentation: 232466\n",
      "-------------------- Map Tokenize Function --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23246 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23930 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main_ddict= tdm.to_datasetdict(tokenizer,\n",
    "                               max_length=512,\n",
    "                               trn_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 23246\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 23930\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 7, 3, 5, 9]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict['validation']['label'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Experiment: EnviBert Multi-Head Classification (with Hidden Layer Concatenation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "from that_nlp_library.models.classifiers import *\n",
    "from that_nlp_library.model_main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will specify a (or a list) of GPUs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train EnviBert (with hidden layer concatenation), using TDM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our model controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nguyenvulebinh/envibert were not used when initializing RobertaHiddenStateConcatForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaHiddenStateConcatForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaHiddenStateConcatForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaHiddenStateConcatForSequenceClassification were not initialized from the model checkpoint at nguyenvulebinh/envibert and are newly initialized: ['classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name='nguyenvulebinh/envibert'\n",
    "num_classes = len(tdm.label_lists[0])\n",
    "\n",
    "_model_kwargs={\n",
    "    'concathead_class': RobertaConcatHeadSimple,\n",
    "    'classifier_dropout':0.1,\n",
    "    'last_hidden_size':768,  \n",
    "    'is_multilabel':tdm.is_multilabel, \n",
    "    'is_multihead':tdm.is_multihead,\n",
    "    'head_class_sizes': num_classes,\n",
    "}\n",
    "\n",
    "model = model_init_classification(model_class = RobertaHiddenStateConcatForSequenceClassification,\n",
    "                                  cpoint_path = 'nguyenvulebinh/envibert', \n",
    "                                  output_hidden_states=True, # since we are using 'hidden layer contatenation'\n",
    "                                  seed=42,\n",
    "                                  model_kwargs = _model_kwargs)\n",
    "metric_funcs = [partial(f1_score,average='macro'),accuracy_score]\n",
    "controller = ModelController(model,tdm,metric_funcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can start training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 8.2e-5\n",
    "bs=8\n",
    "wd=0.01\n",
    "epochs= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.fit(epochs,lr,\n",
    "               batch_size=bs,\n",
    "               weight_decay=wd,\n",
    "               save_checkpoint=False,\n",
    "#                o_dir='sample_weights',\n",
    "               compute_metrics=compute_metrics_classification,\n",
    "              )\n",
    "\n",
    "# Epoch\tTraining Loss\tValidation Loss\tF1 Score L1\tAccuracy Score L1\n",
    "# 1\tNo log\t0.770289\t0.633258\t0.749269\n",
    "# 2\t0.857600\t0.710960\t0.689167\t0.770079\n",
    "# 3\t0.857600\t0.746624\t0.698602\t0.775512\n",
    "# 4\t0.354300\t0.811047\t0.700496\t0.776139\n",
    "\n",
    "# Equal weights\n",
    "# Epoch\tTraining Loss\tValidation Loss\tF1 Score L1\tAccuracy Score L1\tF1 Score L2\tAccuracy Score L2\n",
    "# 1\tNo log\t2.209617\t0.622977\t0.741307\t0.196366\t0.627549\n",
    "# 2\t2.476700\t1.915091\t0.692587\t0.765965\t0.281379\t0.669843\n",
    "# 3\t2.476700\t1.854167\t0.696627\t0.776204\t0.328412\t0.689694\n",
    "# 4\t1.101100\t1.894282\t0.699866\t0.777666\t0.330808\t0.692578\n",
    "\n",
    "# L1 1 L2 2\n",
    "# Epoch\tTraining Loss\tValidation Loss\tF1 Score L1\tAccuracy Score L1\tF1 Score L2\tAccuracy Score L2\n",
    "# 1\tNo log\t3.735447\t0.614677\t0.723587\t0.196626\t0.615388\n",
    "# 2\t4.016400\t3.096701\t0.683411\t0.762496\t0.304085\t0.669425\n",
    "# 3\t4.016400\t2.957187\t0.698510\t0.777583\t0.341109\t0.694040\n",
    "# 4\t1.739400\t3.008255\t0.700440\t0.775242\t0.349905\t0.695127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.trainer.model.save_pretrained('./sample_weights/my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using trained model, using TDM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_init_classification(model_class = RobertaHiddenStateConcatForSequenceClassification,\n",
    "                                  cpoint_path = 'sample_weights/my_model', \n",
    "                                  output_hidden_states=True,\n",
    "                                  seed=42,\n",
    "                                  model_kwargs = _model_kwargs)\n",
    "metric_funcs = [partial(f1_score,average='macro'),accuracy_score]\n",
    "controller = ModelController(model,tdm,metric_funcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Train/Validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make prediction on all validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start making predictions --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23930 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2372: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df_val = controller.predict_ddict(ds_type='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>Source</th>\n",
       "      <th>pred_L1</th>\n",
       "      <th>pred_prob_L1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>owned - [ Cảnh báo ] bán fa.ke giả mạo Shop Ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>owned</td>\n",
       "      <td>Buyer complained seller</td>\n",
       "      <td>0.789140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google play - Chính sách trả hàng hoàn tiền kh...</td>\n",
       "      <td>7</td>\n",
       "      <td>google play</td>\n",
       "      <td>Return/Refund</td>\n",
       "      <td>0.976054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>google play - Hi vọng shopee kiểm duyệt phản h...</td>\n",
       "      <td>3</td>\n",
       "      <td>google play</td>\n",
       "      <td>Feature</td>\n",
       "      <td>0.582839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google play - Shoppe bị lỗi r ....</td>\n",
       "      <td>5</td>\n",
       "      <td>google play</td>\n",
       "      <td>Feature</td>\n",
       "      <td>0.760949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google play - Hàng không đặt được gì hết một sao</td>\n",
       "      <td>9</td>\n",
       "      <td>google play</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.735695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label       Source   \n",
       "0  owned - [ Cảnh báo ] bán fa.ke giả mạo Shop Ma...      0        owned  \\\n",
       "1  google play - Chính sách trả hàng hoàn tiền kh...      7  google play   \n",
       "2  google play - Hi vọng shopee kiểm duyệt phản h...      3  google play   \n",
       "3                 google play - Shoppe bị lỗi r ....      5  google play   \n",
       "4   google play - Hàng không đặt được gì hết một sao      9  google play   \n",
       "\n",
       "                   pred_L1  pred_prob_L1  \n",
       "0  Buyer complained seller      0.789140  \n",
       "1            Return/Refund      0.976054  \n",
       "2                  Feature      0.582839  \n",
       "3                  Feature      0.760949  \n",
       "4                   Others      0.735695  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the label index to string, we can use the ```label_lists``` attribute of tdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['label']= df_val['label'].apply(lambda x: tdm.label_lists[0][x]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6916331451379791"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(df_val.label,df_val.pred_L1,average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go through details on how to make a prediction on a completely new and raw dataset using our trained model. For now, let's reuse the sample csv and pretend it's our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Input Validation Precheck -----\n"
     ]
    }
   ],
   "source": [
    "df_test = TextDataMain.from_csv(Path('sample_data')/'sample.csv',return_df=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove all the labels and unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop(['L1','L2'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Source</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google Play</td>\n",
       "      <td>Google Play</td>\n",
       "      <td>Mình khuyên các bạn nên mua bên Lazada hoặc Ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google Play</td>\n",
       "      <td>Google Play</td>\n",
       "      <td>Con cc quoảng cáu ít thôi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iOS</td>\n",
       "      <td>iOS</td>\n",
       "      <td>Mình có một vài món hàng shipper ấn giao r mà ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Google Play</td>\n",
       "      <td>Google Play</td>\n",
       "      <td>Mình đã sử dụng shoppe cũng 1 thời gian dài rồ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Google Play</td>\n",
       "      <td>Google Play</td>\n",
       "      <td>Chăm sóc khách hàng quá tệ. Nhân viên hỗ trợ c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Group       Source                                            Content\n",
       "0  Google Play  Google Play  Mình khuyên các bạn nên mua bên Lazada hoặc Ti...\n",
       "1  Google Play  Google Play                          Con cc quoảng cáu ít thôi\n",
       "2          iOS          iOS  Mình có một vài món hàng shipper ấn giao r mà ...\n",
       "3  Google Play  Google Play  Mình đã sử dụng shoppe cũng 1 thời gian dài rồ...\n",
       "4  Google Play  Google Play  Chăm sóc khách hàng quá tệ. Nhân viên hỗ trợ c..."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a DatasetDict for this test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Getting Test Set --------------------\n",
      "----- Input Validation Precheck -----\n",
      "-------------------- Start Test Set Transformation --------------------\n",
      "----- Metadata Simple Processing & Concatenating to Main Content -----\n",
      "-------------------- Text Transformation --------------------\n",
      "----- text_normalize -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 70/70 [00:00<00:00, 5981.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Test Leak Checking --------------------\n",
      "- Before leak check\n",
      "Size: 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- After leak check\n",
      "Size: 0\n",
      "- Number of rows leaked: 70, or 100.00% of the original validation (or test) data\n",
      "-------------------- Construct DatasetDict --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/70 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_ddict = tdm.get_test_datasetdict_from_df(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the ***Leak Check*** we did in TextDataMain? Our ```df_test``` only has 70 rows, and it also shows that 70 rows of our data is leaked (100%), which is correct because this test dataset is actually a small sample of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 70\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test data has been processed + transformed (but not augmented) the same way as the validation set. Now we can start making the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start making predictions --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/70 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2372: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "controller = ModelController(model,tdm)\n",
    "df_result = controller.predict_ddict(ddict=test_ddict,ds_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Source</th>\n",
       "      <th>pred_L1</th>\n",
       "      <th>pred_prob_L1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google play - Mình khuyên các bạn nên mua bên ...</td>\n",
       "      <td>google play</td>\n",
       "      <td>Services</td>\n",
       "      <td>0.749757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google play - Con cc quoảng cáu ít thôi</td>\n",
       "      <td>google play</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.845028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ios - Mình có một vài món hàng shipper ấn giao...</td>\n",
       "      <td>ios</td>\n",
       "      <td>Delivery</td>\n",
       "      <td>0.963248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google play - Mình đã sử dụng shoppe cũng 1 th...</td>\n",
       "      <td>google play</td>\n",
       "      <td>Services</td>\n",
       "      <td>0.702858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google play - Chăm sóc khách hàng quá tệ . Nhâ...</td>\n",
       "      <td>google play</td>\n",
       "      <td>Services</td>\n",
       "      <td>0.943015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       Source   pred_L1   \n",
       "0  google play - Mình khuyên các bạn nên mua bên ...  google play  Services  \\\n",
       "1            google play - Con cc quoảng cáu ít thôi  google play    Others   \n",
       "2  ios - Mình có một vài món hàng shipper ấn giao...          ios  Delivery   \n",
       "3  google play - Mình đã sử dụng shoppe cũng 1 th...  google play  Services   \n",
       "4  google play - Chăm sóc khách hàng quá tệ . Nhâ...  google play  Services   \n",
       "\n",
       "   pred_prob_L1  \n",
       "0      0.749757  \n",
       "1      0.845028  \n",
       "2      0.963248  \n",
       "3      0.702858  \n",
       "4      0.943015  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even predict top k results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start making predictions --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/70 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2372: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Source</th>\n",
       "      <th>pred_L1</th>\n",
       "      <th>pred_prob_L1</th>\n",
       "      <th>pred_L1_top1</th>\n",
       "      <th>pred_L1_top2</th>\n",
       "      <th>pred_L1_top3</th>\n",
       "      <th>pred_prob_L1_top1</th>\n",
       "      <th>pred_prob_L1_top2</th>\n",
       "      <th>pred_prob_L1_top3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google play - Mình khuyên các bạn nên mua bên ...</td>\n",
       "      <td>google play</td>\n",
       "      <td>[8, 7, 5]</td>\n",
       "      <td>[0.7497572, 0.11502659, 0.06754405]</td>\n",
       "      <td>Services</td>\n",
       "      <td>Return/Refund</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.749757</td>\n",
       "      <td>0.115027</td>\n",
       "      <td>0.067544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google play - Con cc quoảng cáu ít thôi</td>\n",
       "      <td>google play</td>\n",
       "      <td>[5, 1, 3]</td>\n",
       "      <td>[0.8450278, 0.11246138, 0.027051244]</td>\n",
       "      <td>Others</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>Feature</td>\n",
       "      <td>0.845028</td>\n",
       "      <td>0.112461</td>\n",
       "      <td>0.027051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ios - Mình có một vài món hàng shipper ấn giao...</td>\n",
       "      <td>ios</td>\n",
       "      <td>[2, 3, 5]</td>\n",
       "      <td>[0.9632478, 0.018235153, 0.007653378]</td>\n",
       "      <td>Delivery</td>\n",
       "      <td>Feature</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.963248</td>\n",
       "      <td>0.018235</td>\n",
       "      <td>0.007653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google play - Mình đã sử dụng shoppe cũng 1 th...</td>\n",
       "      <td>google play</td>\n",
       "      <td>[8, 5, 7]</td>\n",
       "      <td>[0.7028584, 0.10196633, 0.10041263]</td>\n",
       "      <td>Services</td>\n",
       "      <td>Others</td>\n",
       "      <td>Return/Refund</td>\n",
       "      <td>0.702858</td>\n",
       "      <td>0.101966</td>\n",
       "      <td>0.100413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google play - Chăm sóc khách hàng quá tệ . Nhâ...</td>\n",
       "      <td>google play</td>\n",
       "      <td>[8, 5, 7]</td>\n",
       "      <td>[0.9430152, 0.038205713, 0.007924775]</td>\n",
       "      <td>Services</td>\n",
       "      <td>Others</td>\n",
       "      <td>Return/Refund</td>\n",
       "      <td>0.943015</td>\n",
       "      <td>0.038206</td>\n",
       "      <td>0.007925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       Source    pred_L1   \n",
       "0  google play - Mình khuyên các bạn nên mua bên ...  google play  [8, 7, 5]  \\\n",
       "1            google play - Con cc quoảng cáu ít thôi  google play  [5, 1, 3]   \n",
       "2  ios - Mình có một vài món hàng shipper ấn giao...          ios  [2, 3, 5]   \n",
       "3  google play - Mình đã sử dụng shoppe cũng 1 th...  google play  [8, 5, 7]   \n",
       "4  google play - Chăm sóc khách hàng quá tệ . Nhâ...  google play  [8, 5, 7]   \n",
       "\n",
       "                            pred_prob_L1 pred_L1_top1   pred_L1_top2   \n",
       "0    [0.7497572, 0.11502659, 0.06754405]     Services  Return/Refund  \\\n",
       "1   [0.8450278, 0.11246138, 0.027051244]       Others     Commercial   \n",
       "2  [0.9632478, 0.018235153, 0.007653378]     Delivery        Feature   \n",
       "3    [0.7028584, 0.10196633, 0.10041263]     Services         Others   \n",
       "4  [0.9430152, 0.038205713, 0.007924775]     Services         Others   \n",
       "\n",
       "    pred_L1_top3  pred_prob_L1_top1  pred_prob_L1_top2  pred_prob_L1_top3  \n",
       "0         Others           0.749757           0.115027           0.067544  \n",
       "1        Feature           0.845028           0.112461           0.027051  \n",
       "2         Others           0.963248           0.018235           0.007653  \n",
       "3  Return/Refund           0.702858           0.101966           0.100413  \n",
       "4  Return/Refund           0.943015           0.038206           0.007925  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result = controller.predict_ddict(ddict=test_ddict,ds_type='test',topk=3)\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just want to make a prediction on a small amount of data (single sentence, or a few sentences), we can use `ModelController.predict_raw_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have some metadatas, we need to define a dictionary (to imitate a DatasetDict)\n",
    "raw_content={\n",
    "    'Source': 'Google play',\n",
    "    'Content':'Tôi không thích Shopee.Tại vì dùng app rất chậm,lag banh nhà lầu, thậm chí log in còn không đc'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't use metadata, we can use something like this: \n",
    "\n",
    "```raw_content='Tôi không thích Shopee.Tại vì dùng app rất chậm,lag banh nhà lầu, thậm chí log in còn không đc'```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4639.72it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2372: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Source</th>\n",
       "      <th>pred_L1</th>\n",
       "      <th>pred_prob_L1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google play - Tôi không thích Shopee . Tại vì ...</td>\n",
       "      <td>google play</td>\n",
       "      <td>Feature</td>\n",
       "      <td>0.993081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       Source  pred_L1   \n",
       "0  google play - Tôi không thích Shopee . Tại vì ...  google play  Feature  \\\n",
       "\n",
       "   pred_prob_L1  \n",
       "0      0.993081  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result = controller.predict_raw_text(raw_content,topk=1)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 7796.10it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2372: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Source</th>\n",
       "      <th>pred_L1</th>\n",
       "      <th>pred_prob_L1</th>\n",
       "      <th>pred_L1_top1</th>\n",
       "      <th>pred_L1_top2</th>\n",
       "      <th>pred_prob_L1_top1</th>\n",
       "      <th>pred_prob_L1_top2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google play - Tôi không thích Shopee . Tại vì ...</td>\n",
       "      <td>google play</td>\n",
       "      <td>[3, 5]</td>\n",
       "      <td>[0.99308056, 0.002194975]</td>\n",
       "      <td>Feature</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.993081</td>\n",
       "      <td>0.002195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>owned - App này xài được</td>\n",
       "      <td>owned</td>\n",
       "      <td>[5, 1]</td>\n",
       "      <td>[0.8444226, 0.09454699]</td>\n",
       "      <td>Others</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>0.844423</td>\n",
       "      <td>0.094547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       Source pred_L1   \n",
       "0  google play - Tôi không thích Shopee . Tại vì ...  google play  [3, 5]  \\\n",
       "1                           owned - App này xài được        owned  [5, 1]   \n",
       "\n",
       "                pred_prob_L1 pred_L1_top1 pred_L1_top2  pred_prob_L1_top1   \n",
       "0  [0.99308056, 0.002194975]      Feature       Others           0.993081  \\\n",
       "1    [0.8444226, 0.09454699]       Others   Commercial           0.844423   \n",
       "\n",
       "   pred_prob_L1_top2  \n",
       "0           0.002195  \n",
       "1           0.094547  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_content={\n",
    "    'Source': ['Google play','Owned'],\n",
    "    'Content':['Tôi không thích Shopee.Tại vì dùng app rất chậm,lag banh nhà lầu, thậm chí log in còn không đc','App này xài được']\n",
    "            }\n",
    "df_result = controller.predict_raw_text(raw_content,topk=2)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train EnviBert (with hidden layer concatenation), using tokenized DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaTokenizer(name_or_path='', vocab_size=59993, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorWithPadding(tokenizer=RobertaTokenizer(name_or_path='', vocab_size=59993, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True), padding=True, max_length=512, pad_to_multiple_of=None, return_tensors='pt')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that your DatasetDict must contain tokens besides raw text (which typically includes 'input_ids', 'token_type_ids', 'attention_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 23246\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 23930\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = 'L1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nguyenvulebinh/envibert were not used when initializing RobertaHiddenStateConcatForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaHiddenStateConcatForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaHiddenStateConcatForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaHiddenStateConcatForSequenceClassification were not initialized from the model checkpoint at nguyenvulebinh/envibert and are newly initialized: ['classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name='nguyenvulebinh/envibert'\n",
    "num_classes = 10\n",
    "\n",
    "_model_kwargs={\n",
    "    'concathead_class': RobertaConcatHeadSimple,\n",
    "    'classifier_dropout':0.1,\n",
    "    'last_hidden_size':768,  \n",
    "    'is_multilabel':False,\n",
    "    'is_multihead': False,\n",
    "    'head_class_sizes':num_classes\n",
    "}\n",
    "\n",
    "model = model_init_classification(model_class = RobertaHiddenStateConcatForSequenceClassification,\n",
    "                                  cpoint_path = 'nguyenvulebinh/envibert', \n",
    "                                  output_hidden_states=True, # since we are using 'hidden layer contatenation'\n",
    "                                  seed=42,\n",
    "                                  model_kwargs = _model_kwargs)\n",
    "\n",
    "metric_funcs = [partial(f1_score,average='macro'),accuracy_score] # we will use both f1_macro and accuracy score as metrics\n",
    "controller = ModelController(model,\n",
    "                             metric_funcs=metric_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 8.2e-5\n",
    "bs=8\n",
    "wd=0.01\n",
    "epochs= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2372: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2906' max='2906' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2906/2906 03:18, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Score L1</th>\n",
       "      <th>Accuracy Score L1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.736859</td>\n",
       "      <td>0.660113</td>\n",
       "      <td>0.755662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.795700</td>\n",
       "      <td>0.702030</td>\n",
       "      <td>0.691708</td>\n",
       "      <td>0.775512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2372: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "controller.fit(epochs,lr,\n",
    "               ddict=main_ddict,\n",
    "               batch_size=bs,\n",
    "               weight_decay=wd,\n",
    "               save_checkpoint=False,\n",
    "#                o_dir='sample_weights',\n",
    "               compute_metrics=compute_metrics_classification,\n",
    "               tokenizer=tokenizer,\n",
    "               data_collator=data_collator,\n",
    "               label_names=label_names\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.trainer.model.save_pretrained('./sample_weights/my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using trained model, using tokenized DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='nguyenvulebinh/envibert'\n",
    "num_classes = 10\n",
    "\n",
    "_model_kwargs={\n",
    "    'concathead_class': RobertaConcatHeadSimple,\n",
    "    'classifier_dropout':0.1,\n",
    "    'last_hidden_size':768,  \n",
    "    'is_multilabel':False,\n",
    "    'is_multihead': False,\n",
    "    'head_class_sizes':num_classes\n",
    "}\n",
    "\n",
    "model = model_init_classification(model_class = RobertaHiddenStateConcatForSequenceClassification,\n",
    "                                  cpoint_path = './sample_weights/my_model', \n",
    "                                  output_hidden_states=True, # since we are using 'hidden layer contatenation'\n",
    "                                  seed=42,\n",
    "                                  model_kwargs = _model_kwargs)\n",
    "\n",
    "metric_funcs = [partial(f1_score,average='macro'),accuracy_score] # we will use both f1_macro and accuracy score as metrics\n",
    "controller = ModelController(model,\n",
    "                             metric_funcs=metric_funcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 23246\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 23930\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_label_name = 'L1'\n",
    "my_class_predefined = ['Buyer complained seller',\n",
    " 'Commercial',\n",
    " 'Delivery',\n",
    " 'Feature',\n",
    " 'Order/Item',\n",
    " 'Others',\n",
    " 'Payment',\n",
    " 'Return/Refund',\n",
    " 'Services',\n",
    " 'Shopee account']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Start making predictions --------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23930 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2372: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>Source</th>\n",
       "      <th>pred_L1</th>\n",
       "      <th>pred_prob_L1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>owned - [ Cảnh báo ] bán fa.ke giả mạo Shop Ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>owned</td>\n",
       "      <td>Buyer complained seller</td>\n",
       "      <td>0.789140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google play - Chính sách trả hàng hoàn tiền kh...</td>\n",
       "      <td>7</td>\n",
       "      <td>google play</td>\n",
       "      <td>Return/Refund</td>\n",
       "      <td>0.976054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>google play - Hi vọng shopee kiểm duyệt phản h...</td>\n",
       "      <td>3</td>\n",
       "      <td>google play</td>\n",
       "      <td>Feature</td>\n",
       "      <td>0.582839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google play - Shoppe bị lỗi r ....</td>\n",
       "      <td>5</td>\n",
       "      <td>google play</td>\n",
       "      <td>Feature</td>\n",
       "      <td>0.760949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google play - Hàng không đặt được gì hết một sao</td>\n",
       "      <td>9</td>\n",
       "      <td>google play</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.735695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label       Source   \n",
       "0  owned - [ Cảnh báo ] bán fa.ke giả mạo Shop Ma...      0        owned  \\\n",
       "1  google play - Chính sách trả hàng hoàn tiền kh...      7  google play   \n",
       "2  google play - Hi vọng shopee kiểm duyệt phản h...      3  google play   \n",
       "3                 google play - Shoppe bị lỗi r ....      5  google play   \n",
       "4   google play - Hàng không đặt được gì hết một sao      9  google play   \n",
       "\n",
       "                   pred_L1  pred_prob_L1  \n",
       "0  Buyer complained seller      0.789140  \n",
       "1            Return/Refund      0.976054  \n",
       "2                  Feature      0.582839  \n",
       "3                  Feature      0.760949  \n",
       "4                   Others      0.735695  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val = controller.predict_ddict(main_ddict,\n",
    "                                  ds_type='validation',\n",
    "                                  is_multilabel=False,\n",
    "                                  tokenizer=tokenizer,\n",
    "                                  data_collator=data_collator,\n",
    "                                  label_names = my_label_name,\n",
    "                                  class_names_predefined=my_class_predefined\n",
    "                                  )\n",
    "df_val.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
