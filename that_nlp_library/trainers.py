# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/10_trainers.ipynb.

# %% ../nbs/10_trainers.ipynb 3
from __future__ import annotations
from transformers import Trainer,get_cosine_with_hard_restarts_schedule_with_warmup
from torch.optim import AdamW

# %% auto 0
__all__ = ['get_cosine_restart_class']

# %% ../nbs/10_trainers.ipynb 4
def get_cosine_restart_class(warmup_ratio=0.1,num_cycles=2):
    """
    Class getter for a Trainer that consists of Cosine Restart with Muptiple Cycles LR Scheduler
    
    Source: https://discuss.huggingface.co/t/how-do-use-lr-scheduler/4046/8
    """
    class CustomTrainer(Trainer):
        def __init__(self, *args, **kwargs):
            self.warmup_ratio=warmup_ratio
            self.num_cycles=num_cycles
            super().__init__(*args, **kwargs)

        def create_optimizer_and_scheduler(self, num_training_steps):
            self.optimizer = AdamW(self.model.parameters(),
                                   lr=self.args.learning_rate,
                                   weight_decay=self.args.weight_decay)
            self.lr_scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(
                 self.optimizer, int(self.warmup_ratio*num_training_steps), num_training_steps, num_cycles=self.num_cycles)    
    return CustomTrainer
