[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to that-nlp-library",
    "section": "",
    "text": "pip install that_nlp_library\nIt is advised that you manually install torch (with your compatible cuda version if you GPU). Typically it’s\npip3 install torch\nVisit Pytorch page for more information",
    "crumbs": [
      "Welcome to that-nlp-library"
    ]
  },
  {
    "objectID": "index.html#supervised-learning",
    "href": "index.html#supervised-learning",
    "title": "Welcome to that-nlp-library",
    "section": "2.1. Supervised Learning",
    "text": "2.1. Supervised Learning\nFor supervised learning, the main pipeline contains 2 parts:\n\nTextDataController: For High-Speed and Customizable Text Processing\nHere is a list of processings that you can use (in order). You also can skip any processing if you want to.\nHere is an example of the Text Controller for a classification task (predict Division Name), without any text preprocessing. The code will also tokenize your text field.\ntdc = TextDataController.from_csv('sample_data/Womens_Clothing_Reviews.csv',\n                                  main_text='Review Text',\n                                  label_names='Division Name',\n                                  sup_types='classification',                                  \n                                 )\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\ntdc.process_and_tokenize(tokenizer,max_length=100,shuffle_trn=True)\nAnd here is an example when all processings are applied\n\n# define a custom augmentation function\nfrom underthesea import text_normalize\nimport nlpaug.augmenter.char as nac\n\ndef nlp_aug(x,aug=None):\n    results = aug.augment(x)\n    if not isinstance(x,list): return results[0]\n    return results\naug = nac.KeyboardAug(aug_char_max=3,aug_char_p=0.1,aug_word_p=0.07)\nnearby_aug_func = partial(nlp_aug,aug=aug)\n\n# initialize the TextDataController\ntdc = TextDataController.from_csv(dset,\n                                  main_text='Review Text',\n                                  \n                                  # metadatas\n                                  metadatas='Title',\n                                  \n                                  # label\n                                  label_names='Division Name',\n                                  sup_types='classification',\n                                  label_tfm_dict={'Division Name': lambda x: x if x!='Initmates' else 'Intimates'},\n                                  \n                                  # row filter\n                                  filter_dict={'Review Text': lambda x: x is not None,\n                                               'Division Name': lambda x: x is not None,\n                                              },\n                                              \n                                  # text transformation\n                                  content_transformation=[text_normalize,str.lower],\n                                  \n                                  # validation split\n                                  val_ratio=0.2,\n                                  stratify_cols=['Division Name'],\n                                  \n                                  # upsampling\n                                  upsampling_list=[('Division Name',lambda x: x=='Intimates')]\n                                  \n                                  # text augmentation\n                                  content_augmentations=nearby_aug_func\n                                 )\n\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\ntdc.process_and_tokenize(tokenizer,max_length=100,shuffle_trn=True)\nFor an in-depth tutorial on Text Controller for Supervised Learning (TextDataController), please visit here\nThis library also supports a streamed version of Text Controller (TextDataControllerStreaming), allowing you to work with data without having it entirely on your memory. You can still perform all the processings in the non-streamed version, except for Train/Validation split (which means you have to define your validation set beforehand), and Upsampling.\nFor more details on streaming, visit how to create a streamed dataset and how to train a model with a streamed dataset\nIf you are curious on the time and space efficiency between streamed and non-streamed version, visit the benchmark here\n\n\nModelController: For customizable model training/inference/interpretation\nHere is an example of using ModelController to train a simple 6-class classification RoBERTa model, with the data and the preprocessing steps stored in our previous TextDataController object\n\n# Load the model from HuggingFace, with the number of classes defined in our TextDataController object\nnum_labels = len(tdc.label_lists[0])\nmodel = RobertaForSequenceClassification.from_pretrained('roberta-base',num_labels=num_labels)\n\n\n# Create the `ModelController` object\ncontroller = ModelController(model,data_store=tdc,seed=42)\n\n# You can define multiple metrics for model evaluation\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score] \n\n# Training the model for 3 epochs, and save all checkpoints to 'my_saved_weights' directory\ncontroller.fit(epochs = 3,\n               learning_rate = 1e-4,\n               metric_funcs = metric_funcs,\n               batch_size = 32,\n               save_checkpoint=True,\n               o_dir='my_saved_weights',\n               compute_metrics=compute_metrics\n              )\nThe library can perform the following:\n\nClassification\nRegression\nMultilabel classification\nMultiheads, where each head can be either classification or regression\n\n“Multihead” is when your model needs to predict multiple outputs at once, for example, given a sentence (e.g. a review on an e-commerce site), you have to predict what category the sentence is about, and the sentiment of the sentence, and maybe the rating of the sentence.\nFor the above example, this is a 3-head problem: classification (for category), classification (for sentiment), and regression (for rating from 1 to 5)\n\nFor 2-head classification where there’s hierarchical relationship between the first output and the second output (e.g. the first output is level 1 clothing category, and the second output is the level 2 clothing subcategory), you can utilize two specific approaches for this use-case: training with conditional probability, or with deep hierarchical classification\n\n\n\nDecoupling of Text Controller and Model Controller\nIn this library, you can utilize TextDataController only to handle all the text processings, and have the final processed-HuggingFace-DatasetDict returned to you. But if you have your own processed DatasetDict, you can skip the TextDataController and use only the ModelController for training your data. There’s a quick tutorial on this decoupling here",
    "crumbs": [
      "Welcome to that-nlp-library"
    ]
  },
  {
    "objectID": "index.html#language-modeling",
    "href": "index.html#language-modeling",
    "title": "Welcome to that-nlp-library",
    "section": "2.2 Language Modeling",
    "text": "2.2 Language Modeling\nFor language modeling, the main pipeline also contains 2 parts\n\nTextDataLMController: Text Data Controller for Language Model\nSimilarly to TextDataController, TextDataLMController also provide a list of processings (except for Label Processing, Upsampling and Text Augmentation). The controller also allows tokenization line-by-line or by token concatenation.\nVisit the tutorial here\nThere’s also a streamed version (TextDataLMControllerStreaming). Here is a tutorial on how to train a language model with a streamed dataset\n\n\nModelLMController: Language Model Controller\nThe library can train a masked language modeling (BERT, RoBERTa …) or a causal language model (GPT) either from scratch or from existing pretrained language models.\n\n\nHidden States Extraction\nThe library also allow you to extract the hidden states of your choice, for further analysis or other downstream tasks that requires a vector representation of your text input.",
    "crumbs": [
      "Welcome to that-nlp-library"
    ]
  },
  {
    "objectID": "text_main_streaming.html",
    "href": "text_main_streaming.html",
    "title": "Text Main Streaming",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom importlib.machinery import SourceFileLoader\nimport os\nimport random",
    "crumbs": [
      "5. Text Classes",
      "b. Text Controller For Supervised Learning",
      "Text Main Streaming"
    ]
  },
  {
    "objectID": "text_main_streaming.html#class-textdatacontrollerstreaming",
    "href": "text_main_streaming.html#class-textdatacontrollerstreaming",
    "title": "Text Main Streaming",
    "section": "Class TextDataControllerStreaming",
    "text": "Class TextDataControllerStreaming\n\nsource\n\nTextDataControllerStreaming\n\n TextDataControllerStreaming (inp, main_text:str, label_names=[],\n                              sup_types=[], class_names_predefined=[],\n                              filter_dict={}, label_tfm_dict={},\n                              metadatas=[], process_metas=True,\n                              metas_sep='.', content_transformations=[],\n                              content_augmentations=[], seed=None,\n                              batch_size=1024, num_proc=1,\n                              cols_to_keep=None, verbose=True)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninp\n\n\nHuggingFainpce Dataset or DatasetDict\n\n\nmain_text\nstr\n\nName of the main text column\n\n\nlabel_names\nlist\n[]\nNames of the label (dependent variable) columns\n\n\nsup_types\nlist\n[]\nType of supervised learning for each label name (‘classification’ or ‘regression’)\n\n\nclass_names_predefined\nlist\n[]\nList of names associated with the labels (same index order)\n\n\nfilter_dict\ndict\n{}\nA dictionary: {feature: filtering_function_based_on_the_feature}\n\n\nlabel_tfm_dict\ndict\n{}\nA dictionary: {label_name: transform_function_for_that_label}\n\n\nmetadatas\nlist\n[]\nNames of the metadata columns\n\n\nprocess_metas\nbool\nTrue\nWhether to do simple text processing on the chosen metadatas\n\n\nmetas_sep\nstr\n.\nSeparator, for multiple metadatas concatenation\n\n\ncontent_transformations\nlist\n[]\nA list of text transformations\n\n\ncontent_augmentations\nlist\n[]\nA list of text augmentations\n\n\nseed\nNoneType\nNone\nRandom seed\n\n\nbatch_size\nint\n1024\nCPU batch size\n\n\nnum_proc\nint\n1\nNumber of process for multiprocessing. This will be applied on non-streamed validation set\n\n\ncols_to_keep\nNoneType\nNone\nColumns to keep after all processings\n\n\nverbose\nbool\nTrue\nWhether to print processing information\n\n\n\n\nsource\n\n\nTextDataControllerStreaming.process_and_tokenize\n\n TextDataControllerStreaming.process_and_tokenize (tokenizer,\n                                                   max_length=None,\n                                                   tok_num_proc=None,\n                                                   line_by_line=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntokenizer\n\n\nTokenizer (preferably from HuggingFace)\n\n\nmax_length\nNoneType\nNone\npad to model’s allowed max length (default is max_sequence_length)\n\n\ntok_num_proc\nNoneType\nNone\nNumber of processes for tokenization\n\n\nline_by_line\nbool\nFalse\nTo whether process + tokenize each sentence separately. Faster, but no padding applied",
    "crumbs": [
      "5. Text Classes",
      "b. Text Controller For Supervised Learning",
      "Text Main Streaming"
    ]
  },
  {
    "objectID": "text_main_streaming.html#streaming-capability",
    "href": "text_main_streaming.html#streaming-capability",
    "title": "Text Main Streaming",
    "section": "1. Streaming Capability",
    "text": "1. Streaming Capability\nThe majority of streaming capability of TextDataControllerStreaming is adapted from HuggingFace’s stream\nStreaming is a method to let you work with data without having it in your hard drive. This is especially helpful when the dataset size exceeds the amount of disk space you have on your machine.\nHere are a few things to be aware of when using TextDataControllerStreaming streaming functionality (versus TextDataController)\n\nThe list of label names must be available beforehand (except for regression label)\nTo avoid out-of-memory error, reduce batch_size argument.\nThere will NOT be any validation split functionality. If you want to include a validation set, provide a validation split in your HuggingFace DatasetDict beforehand\nThere’s no upsampling, and there’s no shuffling the training set\n\nTo stream, you must provide a streamed HuggingFace dataset.\nLet’s repeat few examples mentioned in this tutorial, but with a streaming dataset\n\nfrom transformers import RobertaTokenizer\n\n\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\nfrom underthesea import text_normalize\nimport nlpaug.augmenter.char as nac\nimport nlpaug.augmenter.word as naw\n\n\ndef nlp_aug_stochastic(x,aug=None,p=0.5):\n    if not isinstance(x,list): \n        if random.random()&lt;p: return aug.augment(x)[0]\n        return x\n    news=[]\n    originals=[]\n    for _x in x:\n        if random.random()&lt;p: news.append(_x)\n        else: originals.append(_x)\n    # only perform augmentation when needed\n    if len(news): news = aug.augment(news)\n    return news+originals\n\n\na) Filtering + Metadatas + Label Transformation + Content Transformation (for Single Head)\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\nddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\nddict_with_val['validation'] = ddict_with_val['test']\nddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\ndel ddict_with_val['test']\n\n\ntdc = TextDataControllerStreaming(ddict_with_val,\n                                  main_text='Review Text',\n                                  label_names='Department Name',\n                                  sup_types='classification',\n                                  class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trending'],\n                                  filter_dict={'Review Text': lambda x: x is not None,\n                                              'Department Name': lambda x: x is not None,\n                                              },\n                                  label_tfm_dict={'Department Name': lambda x: x if x!='Trend' else 'Trending'},\n                                  metadatas=['Title','Division Name'],\n                                  content_transformations=[text_normalize,str.lower],\n                                  process_metas=True,\n                                  batch_size=1000,\n                                  num_proc=4,\n                                  seed=42\n                                 )\n\n\ntdc.process_and_tokenize(tokenizer,max_length=256)\n\n-------------------- Data Filtering --------------------\nDone\n----- Metadata Simple Processing & Concatenating to Main Content -----\nDone\n-------------------- Label Transformation --------------------\nDone\n----- Label Encoding -----\nDone\n-------------------- Dropping unused features --------------------\nDone\n----- Performing Content Transformation and Tokenization on validation set -----\nDone\n----- Creating a generator for content transformation, augmentation and tokenization on train set -----\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor i,v in enumerate(tdc.main_ddict['train']):\n    if i%100==0:\n        print(i)\n    if i==1000-1:\n        break\n    pass\n\n\n\n\n\n\n\n\n\n\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\nCPU times: user 1.85 s, sys: 857 ms, total: 2.7 s\nWall time: 2.69 s\n\n\n\nfor i,v in enumerate(tdc.main_ddict['train']):\n    if i==5:break\n    print(f\"Text: {v['Review Text']}\\nLabel: {v['Department Name']} =&gt; {v['label']}\")\n    print('-'*10)\n\n\n\n\n\n\n\n\n\n\nText: general petite . beautiful top , worth the necessary tailoring . the beautiful bold print drew me to this top and it did not disappoint upon receipt . however , the bottom ruffle belled so far out on each side that it was laughable ! the actual fit is nothing like the picture ; clearly the model's arms are placed in front of all the extra fabric to hold the ruffle back . however , the fabric is beautiful , the fit was perfect ( size 2 , 5 ' 4 \" , 106 lbs . ) , the quality is great and i love the print so i decided to take it to my tailor to \" sew away \" the \" wings \" on both si\nLabel: Tops =&gt; 4\n----------\nText: general . not as short on me ( petite ) . i ordered the xxs p as this dress is not a fitted dress , and that was the right size for me . only thing is the length is a bit linger still 9 lower on calf for me ) , the straps are almost tight , so i would say the dress is a reversed taper shape . color is beautiful , i ordered green as the other color ( plum ) doesn't have petite available . green is rich , and classy , the fabric is surprisingly soft . i love the little details in the velvet . definitely need a strapless bra for this one . 115 lbsm 30 d\nLabel: Dresses =&gt; 1\n----------\nText: general . perfect .... for two wears . ok ladies .... you need to know that this type of fabric is the one that will get holes ( i bought the white one ) . it is super thin and lovely , but i was only able to get two wears out of it . i did wash it and it maintained it's size because i restretched it while wet then hung to dry . i was super disappointed about the wear but appreciated being able to return it without question at my local retailer .\nLabel: Tops =&gt; 4\n----------\nText: initmates . . i love this dress . it is so soft and comfortable , perfect for summer ! ! i wish it came in more colors because i would buy everyone ! !\nLabel: Intimate =&gt; 2\n----------\nText: general petite . adorable and excellent quality . this is such a clean and cute printed dress and i knew that i had to try the dress when i first saw it online . after reading other reviews , i sized up . i am normally a 0 or 2 in retailer . i ordered the 2 and it fits nicely and looks great . however , i feel like the 2 buttons at the lowered rib cage area gape slightly . the tie covers it and holds it in place , unless i sit , then it gapes freely . i am a 32 a so not big chested at all , and yet this fit snug in the chest area . i would worry about\nLabel: Dresses =&gt; 1\n----------\n\n\n\nfor i in range(5):\n    print(f\"Text: {tdc.main_ddict['validation']['Review Text'][i]}\")\n    print(f\"Label: {tdc.main_ddict['validation']['Department Name'][i]} =&gt; {tdc.main_ddict['validation']['label'][i]}\")\n    print('-'*10)\n\nText: general . soft , feminine and fun pockets ! . i love this tunic . purchased the dark orange in medium ( i am 5 ' 9 and 140 lbs ) . tried the small and almost kept it but i felt seams around my arm pits a tad , so went with the medium and glad i did - this top should be comfortable . feels very fall and perfect for casual get-togethers and running around town . only comment is that it is rayon ... and for me anyway rayon doesn't wash too well - so we shall see how this one fairs .\nLabel: Tops =&gt; 4\n----------\nText: general petite . a new staple ! . tried these on out of sheer curiosity -- i've got a long torso & was pleasantly surprised how flattering they are ! they manage to look flowing & sleek without shortening the legs . took a size 6 with my 27 \" waist , 37 \" hips . it's a bit of a generous fit , especially around the waist , but they're extremely comfortable & have room to tuck tops into . i have the cowled sweater tank in gray & it looks fantastic over these ! couldn't resist getting both the rust and black . perfect for a dressy casual look\nLabel: Bottoms =&gt; 0\n----------\nText: general . maybe swing is for me ! . i love swing dresses but they never seem to work out for me . however , lately i have been trying on swing tops like this one and they are super scores ! i love this top ! in my store , they had a rack of test materials where they don't have the full line but they have a look at some online features or clothes that are very new releases . this was on the rack . i knew it wasn't my size but i tried it on anyway and i am absolutely in love . i am waiting for a sale ( as always ) but i am going to get this i\nLabel: Tops =&gt; 4\n----------\nText: general . too flare . too small ... too flare ... nice thick fabric . not my favorite pant .\nLabel: Bottoms =&gt; 0\n----------\nText: general . love . i love this top it is easy to wear fun and very comfortable . i was thinking about it for weeks and kept coming back to it after i read a review about going up a size i decided to go for it and i am very happy i did ! ! ! my new favorite ! ! !\nLabel: Tops =&gt; 4\n----------\n\n\nCompare to non-streamed version\n\n# redefine streaming data controller with verbose=False\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\nddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\nddict_with_val['validation'] = ddict_with_val['test']\nddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\ndel ddict_with_val['test']\n\ntdc = TextDataControllerStreaming(ddict_with_val,\n                                  main_text='Review Text',\n                                  label_names='Department Name',\n                                  sup_types='classification',\n                                  class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trending'],\n                                  filter_dict={'Review Text': lambda x: x is not None,\n                                              'Department Name': lambda x: x is not None,\n                                              },\n                                  label_tfm_dict={'Department Name': lambda x: x if x!='Trend' else 'Trending'},\n                                  metadatas=['Title','Division Name'],\n                                  content_transformations=[text_normalize,str.lower],\n                                  process_metas=True,\n                                  batch_size=1000,\n                                  num_proc=4,\n                                  seed=42,\n                                  verbose=False\n                                 )\n\ntdc.process_and_tokenize(tokenizer,max_length=256,tok_num_proc=1)\n\n\nfrom that_nlp_library.text_main import TextDataController\n\n\ndset2 = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\nddict_with_val2 = dset2.train_test_split(test_size=0.1,seed=42)\nddict_with_val2['validation'] = ddict_with_val2['test']\ndel ddict_with_val2['test']\n\n\ntdc2 = TextDataController(ddict_with_val2,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         sup_types='classification',\n                         class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trending'],\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         label_tfm_dict={'Department Name': lambda x: x if x!='Trend' else 'Trending'},\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         process_metas=True,\n                         batch_size=1000,\n                         num_proc=4,\n                         seed=42,\n                         verbose=False\n                        )\ntdc2.process_and_tokenize(tokenizer,max_length=256,shuffle_trn=False,tok_num_proc=1)\n\n\n# check whether train sets are the same\nassert len(list(tdc.main_ddict['train']))==len(tdc2.main_ddict['train'])\n\n\niter1 = iter(tdc.main_ddict['train'])\niter2 = iter(tdc2.main_ddict['train'])\nfor a,b in zip(iter1,iter2):\n    assert a==b\n\n\n# check whether validation set is the same\nassert len(list(tdc.main_ddict['validation']))==len(tdc2.main_ddict['validation'])\n\niter1 = iter(tdc.main_ddict['validation'])\niter2 = iter(tdc2.main_ddict['validation'])\nfor a,b in zip(iter1,iter2):\n    assert a==b\n\n\n\nb) Filtering + Metadatas + Label Transformation + Content Transformation + Content Augmentation (for Multi Head: Classification + Regression + Classification)\n\naug2 = naw.ContextualWordEmbsAug(model_path='roberta-base', \n                                device='cuda:0', # if you don't have gpu, change to 'cpu'\n                                action=\"substitute\",\n                                top_k=10,\n                               aug_p=0.07)\n\ncontextual_aug_func = partial(nlp_aug_stochastic,aug=aug2,p=0.5)\ncontextual_aug_func.run_on_gpu=True\ncontextual_aug_func.batch_size=32\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\nddict_with_val = dset.train_test_split(test_size=0.1)\nddict_with_val['validation'] = ddict_with_val['test']\nddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\ndel ddict_with_val['test']\n\n\ntdc = TextDataControllerStreaming(ddict_with_val,\n                                  main_text='Review Text',\n                                  label_names=['Division Name','Rating','Department Name'],\n                                  sup_types=['classification','regression','classification'],\n                                  class_names_predefined=[['General', 'General Petite', 'Initmates'],\n                                                          [], # empty list for regression\n                                                          ['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trending']],\n                                  filter_dict={'Review Text': lambda x: x is not None,\n                                               'Department Name': lambda x: x is not None,\n                                              },\n                                  metadatas=['Title'],\n                                  label_tfm_dict={'Department Name': lambda x: x if x!='Trend' else 'Trending'},\n                                  content_transformations=[text_normalize,str.lower],\n                                  content_augmentations=contextual_aug_func,\n                                  process_metas=True,\n                                  batch_size=1000,\n                                  num_proc=1,\n                                  seed=42\n                                 )\ntdc.process_and_tokenize(tokenizer,max_length=256)\n\n-------------------- Data Filtering --------------------\nDone\n----- Metadata Simple Processing & Concatenating to Main Content -----\nDone\n-------------------- Label Transformation --------------------\nDone\n----- Label Encoding -----\nDone\n-------------------- Dropping unused features --------------------\nDone\n----- Performing Content Transformation and Tokenization on validation set -----\nDone\n----- Creating a generator for content transformation, augmentation and tokenization on train set -----\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor i,v in enumerate(tdc.main_ddict['train']):\n    if i==10:break\n    print(f\"Text: {v['Review Text']}\\nLabel: {v['Division Name'],v['Rating'],v['Department Name']} =&gt; {v['label']}\")\n    print('-'*10)\n\n\n\n\n\n\n\n\n\n\n\n\n\nText: not as short on me ( petite ). i ordered black xxs p as this model is not a fitted dress, and that was the right size for me. only thing is the length is 1 bit linger still 9 lower on calf for me ). the straps are almost tight, so i would say the dress is a reversed taper shape. color = beautiful! i ordered green as the other color ( ) ( doesn't have petite available. green is rich, and classy, the fabric is surprisingly soft. i love the little details in the front. definitely need a strapless bra for this one. 115 &lt; 30 d\nLabel: ('General Petite', 4.0, 'Tops') =&gt; [1.0, 4.0, 4.0]\n----------\nText: perfect.... for two wears. ( ladies.... we need to know that this type of fabric is the one that will get holes when i bought the white one ). it is super thin and lovely, but i was only able to get 2 wears out of it. i did wash it and it maintained it's size because i restretched it while wet then hung to dry. i been super disappointed about the wear but appreciated being able to find it without question at my favourite retailer.\nLabel: ('General', 5.0, 'Dresses') =&gt; [0.0, 5.0, 1.0]\n----------\nText: . i love this dress. it is in soft is comfortable, perfect for summer!! i wish it came in more colors ; i would buy everyone!!\nLabel: ('General', 1.0, 'Tops') =&gt; [0.0, 1.0, 4.0]\n----------\nText: great fit and what. love this flowing a cute top! casual, but can be easily dressed up. great fit.\nLabel: ('Initmates', 5.0, 'Intimate') =&gt; [2.0, 5.0, 2.0]\n----------\nText: please bring back more!!!. love this tank! prettier color of yellow in person. love the wide width and length. wish they'd bring back more in lots of colors. only sized up one from my usual size and will have too shorten and length of the strap myself, but it's an amazing alteration and is totally worth it! especially good at sale price. i really hope they make more in the style or lots of colors. great find!\nLabel: ('General Petite', 4.0, 'Dresses') =&gt; [1.0, 4.0, 1.0]\n----------\nText: roomy and flows. comfortable and flows well, does run large one order one size smaller fuck\nLabel: ('General', 5.0, 'Bottoms') =&gt; [0.0, 5.0, 0.0]\n----------\nText: not as pictured. i just received this dress wadded around in a bag. it is nothing like the colors shown off line. in the middle it appears blue and brown. in front it is teal and a funky color of beigey pink. the top is see though but it does see have a slip under the skirt. will be sending it back and will remind myself why i shouldn't try to buy any dress on line.\nLabel: ('General Petite', 2.0, 'Tops') =&gt; [1.0, 2.0, 4.0]\n----------\nText: comfy and pretty. i bought this in red and wear it a lot. it is so soft and comfortable!! i love all the fabric and the fit. the longer sleeve length seems amazing! it is a perfect tee! you can tell from the front there's a lot a fabric and it's supposed to be smooth and drape like that. it fits like in the pics this is true to size. wear a tank underneath just case there's a strong wind.\nLabel: ('General Petite', 5.0, 'Tops') =&gt; [1.0, 5.0, 4.0]\n----------\nText: so so pretty. sunday by brooklyn has bee killing it lately with their tops! i could not decide between this and another of theirs, but i am positive i chose this one out first, the design is subtle but very cunning style : the sleeve length matches the rest of the top tier, which gives this top a very finished look. the color is a beautiful jewel-like raspberry red with a bluish undertone. and, the fabric is a nice washable texture. i wore my usual size xs at retailer.\nLabel: ('Initmates', 5.0, 'Intimate') =&gt; [2.0, 5.0, 2.0]\n----------\nText: great flares. wearing jeans, fit in to size and look as pictured. i recommended those to meet friend and she bought and liked them as well.\nLabel: ('General Petite', 5.0, 'Tops') =&gt; [1.0, 5.0, 4.0]\n----------\n\n\n\nfor i in range(5):\n    print(f\"Text: {tdc.main_ddict['validation']['Review Text'][i]}\")\n    print(f\"Label: {tdc.main_ddict['validation']['Division Name'][i],tdc.main_ddict['validation']['Rating'][i],tdc.main_ddict['validation']['Department Name'][i]} =&gt; {tdc.main_ddict['validation']['label'][i]}\")\n    print('-'*10)\n\nText: soft , feminine and fun pockets ! . i love this tunic . purchased the dark orange in medium ( i am 5 ' 9 and 140 lbs ) . tried the small and almost kept it but i felt seams around my arm pits a tad , so went with the medium and glad i did - this top should be comfortable . feels very fall and perfect for casual get-togethers and running around town . only comment is that it is rayon ... and for me anyway rayon doesn't wash too well - so we shall see how this one fairs .\nLabel: ('General', 5.0, 'Tops') =&gt; [0.0, 5.0, 4.0]\n----------\nText: a new staple ! . tried these on out of sheer curiosity -- i've got a long torso & was pleasantly surprised how flattering they are ! they manage to look flowing & sleek without shortening the legs . took a size 6 with my 27 \" waist , 37 \" hips . it's a bit of a generous fit , especially around the waist , but they're extremely comfortable & have room to tuck tops into . i have the cowled sweater tank in gray & it looks fantastic over these ! couldn't resist getting both the rust and black . perfect for a dressy casual look\nLabel: ('General Petite', 5.0, 'Bottoms') =&gt; [1.0, 5.0, 0.0]\n----------\nText: maybe swing is for me ! . i love swing dresses but they never seem to work out for me . however , lately i have been trying on swing tops like this one and they are super scores ! i love this top ! in my store , they had a rack of test materials where they don't have the full line but they have a look at some online features or clothes that are very new releases . this was on the rack . i knew it wasn't my size but i tried it on anyway and i am absolutely in love . i am waiting for a sale ( as always ) but i am going to get this i\nLabel: ('General', 5.0, 'Tops') =&gt; [0.0, 5.0, 4.0]\n----------\nText: too flare . too small ... too flare ... nice thick fabric . not my favorite pant .\nLabel: ('General', 2.0, 'Bottoms') =&gt; [0.0, 2.0, 0.0]\n----------\nText: love . i love this top it is easy to wear fun and very comfortable . i was thinking about it for weeks and kept coming back to it after i read a review about going up a size i decided to go for it and i am very happy i did ! ! ! my new favorite ! ! !\nLabel: ('General', 5.0, 'Tops') =&gt; [0.0, 5.0, 4.0]\n----------\n\n\n\n\nc) Filtering + Metadatas + Content Transformation + Content Augmentation (for Multi Label)\n\naug2 = naw.ContextualWordEmbsAug(model_path='roberta-base', \n                                device='cuda:0', # if you don't have gpu, change to 'cpu'\n                                action=\"substitute\",\n                                top_k=10,\n                               aug_p=0.07)\n\ncontextual_aug_func = partial(nlp_aug_stochastic,aug=aug2,p=0.5)\ncontextual_aug_func.run_on_gpu=True\ncontextual_aug_func.batch_size=32\n\n\ndf = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\ndf['Fake Label'] = [np.random.choice(df['Department Name'].unique()[:-1],size=np.random.randint(2,6),replace=False) for _ in range(len(df))]\n\n\ndset = Dataset.from_pandas(df)\n\n\nddict_with_val = dset.train_test_split(test_size=0.1)\nddict_with_val['validation'] = ddict_with_val['test']\nddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\ndel ddict_with_val['test']\n\n\nddict_with_val\n\nDatasetDict({\n    train: IterableDataset({\n        features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name', 'Fake Label'],\n        n_shards: 1\n    })\n    validation: Dataset({\n        features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name', 'Fake Label'],\n        num_rows: 2349\n    })\n})\n\n\n\ntdc = TextDataControllerStreaming(ddict_with_val,\n                                  main_text='Review Text',\n                                  label_names='Fake Label',\n                                  sup_types='classification',\n                                  class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n                                  filter_dict={'Review Text': lambda x: x is not None},\n                                  metadatas=['Title','Division Name'],\n                                  content_transformations=[text_normalize,str.lower],\n                                  content_augmentations= contextual_aug_func, \n                                  process_metas=True,\n                                  batch_size=1000,\n                                  num_proc=4,\n                                  seed=42\n                                 )\ntdc.process_and_tokenize(tokenizer,max_length=512)\n\n-------------------- Data Filtering --------------------\nDone\n----- Metadata Simple Processing & Concatenating to Main Content -----\nDone\n----- Label Encoding -----\nDone\n-------------------- Dropping unused features --------------------\nDone\n----- Performing Content Transformation and Tokenization on validation set -----\nDone\n----- Creating a generator for content transformation, augmentation and tokenization on train set -----\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor i,v in enumerate(tdc.main_ddict['train']):\n    if i==10:break\n    print(f\"Text: {v['Review Text']}\\nLabel: {v['Fake Label']} =&gt; {v['label']}\")\n    print('-'*10)\n\n\n\n\n\n\n\n\n\n\n\n\n\nText: general. beautiful, stunning, cozy top!. i checked the first review on here and ordered both a small and a medium as i thought this would run small. i have to totally disagree of the reviewer! i find that this top runs true on size or even generous! the sky color is so pretty and this top can be dressed up with many nice jewels and a necklace or it can be comfy casual, i usually wear a small in hh brand and this one was true to fit ( 5 \" 2 \", broad shoulders, 120 ml )\nLabel: ['Jackets', 'Tops', 'Intimate'] =&gt; [0, 0, 1, 1, 1, 0]\n----------\nText: general. love!. love love love this dress! but, and you are not wearing a slip... you should be like please wear a slip, you can see right through this dress.\"\nLabel: ['Bottoms', 'Dresses'] =&gt; [1, 1, 0, 0, 0, 0]\n----------\nText: general. runs big. i liked the idea on these pair as i've been looking for an updated pair of tuxedo pants. i wear 26 in most of their jeans. i'm not super skinny & find my legs medium ( not too skinny & not too athletic ). i tried these on in xs ( 36 european as marked on them ) & they were big around the waist & hip area. there was so much gap in the back it made them look frumpy! u did however liked the length. the material is nice & heavy which i also loved. sadly enough i didn't work for me though. really wish t\nLabel: ['Bottoms', 'Tops', 'Trend', 'Dresses'] =&gt; [1, 1, 0, 0, 1, 1]\n----------\nText: general petite. so flattering, no need for petite. i just try this top on in xs regular even : i generally wear xspetite in retailer and it fit great ( 34 aa - 35 - 34 ). i think it's flattering on any short narrow arms with the halter neck, fitted waist and peplum? i would guess it would be flattering on many body types ; it highlights shoulders beautifully. it was very hard to put my head through the small, not-too-stretchy opening so you may want to try it on without makeup. i knocked off one star, the neck band wasn't symmetrical,\nLabel: ['Dresses', 'Bottoms', 'Trend', 'Tops', 'Intimate'] =&gt; [1, 1, 1, 0, 1, 1]\n----------\nText: general. a new wardrobe staple!. love this jacket! i purchased both the green and gray versions and will have them constantly this winter! although some reviews were critical of the length of the yarn, i do not find this to be a problem... so happy with these products! for reference to size, i bought the m and am 5'11... the sweater hits me exactly where it should on the model.\nLabel: ['Bottoms', 'Tops'] =&gt; [1, 0, 0, 0, 1, 0]\n----------\nText: general. unique and adorable. the photos don't do the top justice. the split back is very unique and beautiful. i typically take a size 8 in tops, however ordered a 15 since a reviewer suggested it was narrow in the shoulders. the 12 will fit perfectly, but the body is way too big that i was swimming it in. i like it enough that i'm going to visit the tailor to take in the fit. with the sale price, i is worth tailoring.\nLabel: ['Bottoms', 'Intimate', 'Dresses', 'Tops'] =&gt; [1, 1, 1, 0, 1, 0]\n----------\nText: general. great slouchy sweater, perfect color. this sweater has the perfect slouchy coat for fall. i wish I were a little bit softer and heavier - the fabric is pretty lightweight - yet it layers beautifully and will be a highlight for me this season.\nLabel: ['Intimate', 'Bottoms', 'Jackets', 'Trend', 'Tops'] =&gt; [1, 0, 1, 1, 1, 1]\n----------\nText: general. feminine plus makeup. this top is gorgeous and versatile. i wear me with jeans and dress it up with a skirt. so happy to have this in my wardrobe.\nLabel: ['Jackets', 'Bottoms'] =&gt; [1, 0, 0, 1, 0, 0]\n----------\nText: general. great pants that don't get baggy.. e they are shit - please make them in other colors besides black and navy!\nLabel: ['Trend', 'Dresses', 'Intimate'] =&gt; [0, 1, 1, 0, 0, 1]\n----------\nText: general petite. a very blouse-like dress - very simple. this dress is very cute on. very flouncy. i know it looks like a gingham collar, but it's a more like a silk dress with a gingham collar. the one issue i had was that it stood out at the back where you tied it on any size ( large or small ), and i am larger in top, so i don't know what would happen if it with a small chest. i wouldn't be minded if it did the same thing on the front, but it was just the back.\nLabel: ['Trend', 'Jackets', 'Tops', 'Bottoms'] =&gt; [1, 0, 0, 1, 1, 1]\n----------\n\n\n\nfor i in range(5):\n    print(f\"Text: {tdc.main_ddict['validation']['Review Text'][i]}\")\n    print(f\"Label: {tdc.main_ddict['validation']['Fake Label'][i]} =&gt; {tdc.main_ddict['validation']['label'][i]}\")\n    print('-'*10)\n\nText: general petite . . this top has great detailing and color . does run a little big , but adds to the style and movement of the tank . the stitching around the bottom makes it cute for layering .\nLabel: ['Dresses', 'Intimate', 'Trend', 'Tops', 'Bottoms'] =&gt; [1, 1, 1, 0, 1, 1]\n----------\nText: general . . i love this top . i got it on sale and am so glad that i did . it is a short too but still super flattering . it isn't too boxy on me .\nLabel: ['Intimate', 'Trend', 'Jackets', 'Dresses', 'Tops'] =&gt; [0, 1, 1, 1, 1, 1]\n----------\nText: general . beautiful idea ... . i ordered my normal size in this dress . i am 6 foot tall , but the regular sizes were too large and too long ( mid-calf ) . i returned the dress for a size smaller in petite for a more flattering hemline . the dress is lovely , especially on the models in the pictures , but didn't quite work out for me . also , it feels like there are hundreds of closure hooks that make putting on / taking off the dress seem to take an unusually long time !\nLabel: ['Dresses', 'Tops'] =&gt; [0, 1, 0, 0, 1, 0]\n----------\nText: general petite . comfy , but not made to last . this sweater is fine for the casual days . i bought this in cream and i have to say after one wash it looks old . i'm a huge retailer lover and buy a lot of clothes from them . this is just not the best quality and looks tired after a few wears . very soft , but poor material . not my favorite purchase .\nLabel: ['Trend', 'Jackets'] =&gt; [0, 0, 0, 1, 0, 1]\n----------\nText: general . great cool looking jeans . i just bought these jeans today & they are really cute & comfortable on . i love pilcro jeans as they fit really well , they are made well & they are always on style . i did have to go down a size as well but they fit beautifully ! comfy & stylish !\nLabel: ['Tops', 'Intimate', 'Jackets', 'Bottoms'] =&gt; [1, 0, 1, 1, 1, 0]\n----------\n\n\n\ntdc.label_lists\n\n[['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']]",
    "crumbs": [
      "5. Text Classes",
      "b. Text Controller For Supervised Learning",
      "Text Main Streaming"
    ]
  },
  {
    "objectID": "text_main_streaming.html#batch-process-vs-line-by-line-process",
    "href": "text_main_streaming.html#batch-process-vs-line-by-line-process",
    "title": "Text Main Streaming",
    "section": "2. Batch process vs Line-by-line process",
    "text": "2. Batch process vs Line-by-line process\nSo far, we are applying processing functions and tokenization on each batch (of size 1000). There’s a faster way to process: we can apply these functions immediately to each row (instead of waiting for a batch to be formed). Let’s discuss when to use each of these approaches\n\nWhen to use batch process over line-by-line process\n\nYou have some processing functions that perform more efficient when apply to the whole batch (instead of applying line-by-line), e.g. the contextual_aug_func (contextual augmentation) function that utilizes a language model on GPU\nFor tokenization, you need to apply padding (to the whole batch)\n\nWhen to use line-by-line process over batch process\n\nAll your processing functions can perform efficiently line-by-line\nFor tokenization, you don’t need padding (maybe you will do so via DataCollatorWithPadding during training)\nIf you choose DataCollatorWithPadding to pad, be aware that there’s no option to truncate, therefore you have to make sure the maximum length of your token_ids does not exceed the maximum model’s sequence length\n\n\n\na) Line-by-line over batch process\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\nddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\nddict_with_val['validation'] = ddict_with_val['test']\nddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\ndel ddict_with_val['test']\n\ntdc = TextDataControllerStreaming(ddict_with_val,\n                                  main_text='Review Text',\n                                  label_names='Department Name',\n                                  sup_types='classification',\n                                  class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trending'],\n                                  filter_dict={'Review Text': lambda x: x is not None,\n                                              'Department Name': lambda x: x is not None,\n                                              },\n                                  label_tfm_dict={'Department Name': lambda x: x if x!='Trend' else 'Trending'},\n                                  metadatas=['Title','Division Name'],\n                                  content_transformations=[text_normalize,str.lower],\n                                  process_metas=True,\n                                  batch_size=1000,\n                                  num_proc=4,\n                                  seed=42\n                                 )\n\nFor example, for the above data controller, two of the processing functions (text_normalize and str.lower) can perform line-by-line just fine. Therefore, we can utilize line-by-line processing\n\ntdc.process_and_tokenize(tokenizer,line_by_line=True)\n\n-------------------- Data Filtering --------------------\nDone\n----- Metadata Simple Processing & Concatenating to Main Content -----\nDone\n-------------------- Label Transformation --------------------\nDone\n----- Label Encoding -----\nDone\n-------------------- Dropping unused features --------------------\nDone\n----- Performing Content Transformation and Tokenization on validation set -----\nDone\n----- Creating a generator for content transformation, augmentation and tokenization on train set -----\nDone\n\n\n\n\n\n\n# time it takes to go throuch 3 batches (1000 x3)\nfor i,v in enumerate(tdc.main_ddict['train']):\n    if i%500==0:\n        print(i)\n    if i==1000*3-1:\n        break\n    pass\n\n0\n500\n1000\n1500\n2000\n2500\nCPU times: user 2.6 s, sys: 2.53 s, total: 5.13 s\nWall time: 5.12 s\n\n\n\nfor i,v in enumerate(tdc.main_ddict['train']):\n    print(v['input_ids'])\n    print(f\"Length of input_ids: {len(v['input_ids'])}\")\n    if i==1:\n        break\n    print('-'*10)\n\n[0, 15841, 4716, 1459, 479, 2721, 299, 2156, 966, 5, 2139, 7886, 5137, 479, 5, 2721, 7457, 5780, 4855, 162, 7, 42, 299, 8, 24, 222, 45, 17534, 2115, 18245, 479, 959, 2156, 5, 2576, 910, 15315, 28, 9970, 98, 444, 66, 15, 349, 526, 14, 24, 21, 38677, 27785, 5, 3031, 2564, 16, 1085, 101, 5, 2170, 25606, 2563, 5, 1421, 18, 3701, 32, 2325, 11, 760, 9, 70, 5, 1823, 10199, 7, 946, 5, 910, 15315, 124, 479, 959, 2156, 5, 10199, 16, 2721, 2156, 5, 2564, 21, 1969, 36, 1836, 132, 2156, 195, 128, 204, 22, 2156, 13442, 23246, 479, 4839, 2156, 5, 1318, 16, 372, 8, 939, 657, 5, 5780, 98, 939, 1276, 7, 185, 24, 7, 127, 26090, 7, 22, 35043, 409, 22, 5, 22, 11954, 22, 15, 258, 3391, 2]\nLength of input_ids: 136\n----------\n[0, 15841, 479, 45, 25, 765, 15, 162, 36, 4716, 1459, 4839, 479, 939, 2740, 5, 37863, 29, 181, 25, 42, 3588, 16, 45, 10, 15898, 3588, 2156, 8, 14, 21, 5, 235, 1836, 13, 162, 479, 129, 631, 16, 5, 5933, 16, 10, 828, 18277, 202, 361, 795, 15, 16701, 13, 162, 4839, 2156, 5, 31622, 32, 818, 3229, 2156, 98, 939, 74, 224, 5, 3588, 16, 10, 13173, 326, 15888, 3989, 479, 3195, 16, 2721, 2156, 939, 2740, 2272, 25, 5, 97, 3195, 36, 36838, 4839, 630, 75, 33, 4716, 1459, 577, 479, 2272, 16, 4066, 2156, 8, 30228, 2156, 5, 10199, 16, 10262, 3793, 479, 939, 657, 5, 410, 1254, 11, 5, 29986, 479, 2299, 240, 10, 18052, 16979, 11689, 13, 42, 65, 479, 12312, 23246, 119, 389, 385, 2]\nLength of input_ids: 133\n\n\nThere’s no padding at all, just by looking at the first 2 input_ids. Let’s use DataCollatorWithPadding to add padding to these tokens\n\nfrom transformers import DataCollatorWithPadding\n\n\n# no truncation strategy! We will pad to multiple of 8, to utilize NVIDIA hardware\ndata_collator = DataCollatorWithPadding(tokenizer,padding=True,pad_to_multiple_of=8)\ntdc.set_data_collator(data_collator)\n\n\ntrain_ddict = tdc.main_ddict['train'].remove_columns(tdc.cols_to_keep)\n\niter1 = iter(train_ddict)\n\n\nout = tdc.data_collator([next(iter1) for i in range(3000)]) # apply data collator on 3 batches of 1000\n\nCPU times: user 2.46 s, sys: 2.27 s, total: 4.72 s\nWall time: 4.72 s\n\n\n\nout['input_ids'].shape\n\ntorch.Size([3000, 160])\n\n\n\nout['input_ids'][:2]\n\ntensor([[    0, 15841,  4716,  1459,   479,  2721,   299,  2156,   966,     5,\n          2139,  7886,  5137,   479,     5,  2721,  7457,  5780,  4855,   162,\n             7,    42,   299,     8,    24,   222,    45, 17534,  2115, 18245,\n           479,   959,  2156,     5,  2576,   910, 15315,    28,  9970,    98,\n           444,    66,    15,   349,   526,    14,    24,    21, 38677, 27785,\n             5,  3031,  2564,    16,  1085,   101,     5,  2170, 25606,  2563,\n             5,  1421,    18,  3701,    32,  2325,    11,   760,     9,    70,\n             5,  1823, 10199,     7,   946,     5,   910, 15315,   124,   479,\n           959,  2156,     5, 10199,    16,  2721,  2156,     5,  2564,    21,\n          1969,    36,  1836,   132,  2156,   195,   128,   204,    22,  2156,\n         13442, 23246,   479,  4839,  2156,     5,  1318,    16,   372,     8,\n           939,   657,     5,  5780,    98,   939,  1276,     7,   185,    24,\n             7,   127, 26090,     7,    22, 35043,   409,    22,     5,    22,\n         11954,    22,    15,   258,  3391,     2,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n        [    0, 15841,   479,    45,    25,   765,    15,   162,    36,  4716,\n          1459,  4839,   479,   939,  2740,     5, 37863,    29,   181,    25,\n            42,  3588,    16,    45,    10, 15898,  3588,  2156,     8,    14,\n            21,     5,   235,  1836,    13,   162,   479,   129,   631,    16,\n             5,  5933,    16,    10,   828, 18277,   202,   361,   795,    15,\n         16701,    13,   162,  4839,  2156,     5, 31622,    32,   818,  3229,\n          2156,    98,   939,    74,   224,     5,  3588,    16,    10, 13173,\n           326, 15888,  3989,   479,  3195,    16,  2721,  2156,   939,  2740,\n          2272,    25,     5,    97,  3195,    36, 36838,  4839,   630,    75,\n            33,  4716,  1459,   577,   479,  2272,    16,  4066,  2156,     8,\n         30228,  2156,     5, 10199,    16, 10262,  3793,   479,   939,   657,\n             5,   410,  1254,    11,     5, 29986,   479,  2299,   240,    10,\n         18052, 16979, 11689,    13,    42,    65,   479, 12312, 23246,   119,\n           389,   385,     2,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]])\n\n\nOur tokens have been padded\nNow let’s compare the runtime if we use batch-processing instead\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\nddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\nddict_with_val['validation'] = ddict_with_val['test']\nddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\ndel ddict_with_val['test']\n\ntdc = TextDataControllerStreaming(ddict_with_val,\n                                  main_text='Review Text',\n                                  label_names='Department Name',\n                                  sup_types='classification',\n                                  class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trending'],\n                                  filter_dict={'Review Text': lambda x: x is not None,\n                                              'Department Name': lambda x: x is not None,\n                                              },\n                                  label_tfm_dict={'Department Name': lambda x: x if x!='Trend' else 'Trending'},\n                                  metadatas=['Title','Division Name'],\n                                  content_transformations=[text_normalize,str.lower],\n                                  process_metas=True,\n                                  batch_size=1000,\n                                  num_proc=4,\n                                  seed=42\n                                 )\n\ntdc.process_and_tokenize(tokenizer,line_by_line=False,max_length=152)\n\n-------------------- Data Filtering --------------------\nDone\n----- Metadata Simple Processing & Concatenating to Main Content -----\nDone\n-------------------- Label Transformation --------------------\nDone\n----- Label Encoding -----\nDone\n-------------------- Dropping unused features --------------------\nDone\n----- Performing Content Transformation and Tokenization on validation set -----\nDone\n----- Creating a generator for content transformation, augmentation and tokenization on train set -----\nDone\n\n\n\n\n\n\n# time it takes to go throuch 3 batches (1000 x3)\nfor i,v in enumerate(tdc.main_ddict['train']):\n    if i%500==0:\n        print(i)\n    if i==1000*3-1:\n        break\n    pass\n\n\n\n\n\n\n\n\n\n\n0\n500\n1000\n1500\n2000\n2500\nCPU times: user 5.5 s, sys: 2.52 s, total: 8.02 s\nWall time: 7.98 s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis took a bit longer than line-by-line’s time\n\nfor i,v in enumerate(tdc.main_ddict['train']):\n    print(v['input_ids'])\n    print(f\"Length of input_ids: {len(v['input_ids'])}\")\n    if i==1:\n        break\n    print('-'*10)\n\n\n\n\n\n\n\n\n\n\n[0, 15841, 4716, 1459, 479, 2721, 299, 2156, 966, 5, 2139, 7886, 5137, 479, 5, 2721, 7457, 5780, 4855, 162, 7, 42, 299, 8, 24, 222, 45, 17534, 2115, 18245, 479, 959, 2156, 5, 2576, 910, 15315, 28, 9970, 98, 444, 66, 15, 349, 526, 14, 24, 21, 38677, 27785, 5, 3031, 2564, 16, 1085, 101, 5, 2170, 25606, 2563, 5, 1421, 18, 3701, 32, 2325, 11, 760, 9, 70, 5, 1823, 10199, 7, 946, 5, 910, 15315, 124, 479, 959, 2156, 5, 10199, 16, 2721, 2156, 5, 2564, 21, 1969, 36, 1836, 132, 2156, 195, 128, 204, 22, 2156, 13442, 23246, 479, 4839, 2156, 5, 1318, 16, 372, 8, 939, 657, 5, 5780, 98, 939, 1276, 7, 185, 24, 7, 127, 26090, 7, 22, 35043, 409, 22, 5, 22, 11954, 22, 15, 258, 3391, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\nLength of input_ids: 149\n----------\n[0, 15841, 479, 45, 25, 765, 15, 162, 36, 4716, 1459, 4839, 479, 939, 2740, 5, 37863, 29, 181, 25, 42, 3588, 16, 45, 10, 15898, 3588, 2156, 8, 14, 21, 5, 235, 1836, 13, 162, 479, 129, 631, 16, 5, 5933, 16, 10, 828, 18277, 202, 361, 795, 15, 16701, 13, 162, 4839, 2156, 5, 31622, 32, 818, 3229, 2156, 98, 939, 74, 224, 5, 3588, 16, 10, 13173, 326, 15888, 3989, 479, 3195, 16, 2721, 2156, 939, 2740, 2272, 25, 5, 97, 3195, 36, 36838, 4839, 630, 75, 33, 4716, 1459, 577, 479, 2272, 16, 4066, 2156, 8, 30228, 2156, 5, 10199, 16, 10262, 3793, 479, 939, 657, 5, 410, 1254, 11, 5, 29986, 479, 2299, 240, 10, 18052, 16979, 11689, 13, 42, 65, 479, 12312, 23246, 119, 389, 385, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\nLength of input_ids: 149\n\n\nBut at least our tokens has been padded appropriately\n\n\nb) Batch-process over line-by-line\n\naug2 = naw.ContextualWordEmbsAug(model_path='roberta-base', \n                                device='cuda:0', # if you don't have gpu, change to 'cpu'\n                                action=\"substitute\",\n                                top_k=10,\n                               aug_p=0.07)\n\ncontextual_aug_func = partial(nlp_aug_stochastic,aug=aug2,p=0.5)\ncontextual_aug_func.run_on_gpu=True\ncontextual_aug_func.batch_size=32\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\nddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\nddict_with_val['validation'] = ddict_with_val['test']\nddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\ndel ddict_with_val['test']\n\ntdc = TextDataControllerStreaming(ddict_with_val,\n                                  main_text='Review Text',\n                                  label_names='Department Name',\n                                  sup_types='classification',\n                                  class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trending'],\n                                  filter_dict={'Review Text': lambda x: x is not None,\n                                              'Department Name': lambda x: x is not None,\n                                              },\n                                  label_tfm_dict={'Department Name': lambda x: x if x!='Trend' else 'Trending'},\n                                  metadatas=['Title','Division Name'],\n                                  content_augmentations=[contextual_aug_func],\n                                  process_metas=True,\n                                  batch_size=1000,\n                                  num_proc=4,\n                                  seed=42\n                                 )\n\nFor the above data controller, there’s an augmentation function contextual_aug_func that can utilize batch process\n\ntdc.process_and_tokenize(tokenizer,line_by_line=False,max_length=152)\n\n-------------------- Data Filtering --------------------\nDone\n----- Metadata Simple Processing & Concatenating to Main Content -----\nDone\n-------------------- Label Transformation --------------------\nDone\n----- Label Encoding -----\nDone\n-------------------- Dropping unused features --------------------\nDone\n----- Performing Content Transformation and Tokenization on validation set -----\nDone\n----- Creating a generator for content transformation, augmentation and tokenization on train set -----\nDone\n\n\n\n\n\n\n# time it takes to go throuch 3 batches (1000 x3)\nfor i,v in enumerate(tdc.main_ddict['train']):\n    if i%500==0:\n        print(i)\n    if i==1000*3-1:\n        break\n    pass\n\n\n\n\n\n\n\n0\n500\n1000\n1500\n2000\n2500\nCPU times: user 43.7 s, sys: 4.3 s, total: 48 s\nWall time: 47.9 s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor i,v in enumerate(tdc.main_ddict['train']):\n    print(v['input_ids'])\n    print(f\"Length of input_ids: {len(v['input_ids'])}\")\n    if i==1:\n        break\n    print('-'*10)\n\n\n\n\n\n\n\n[0, 15841, 4716, 1459, 4, 2721, 299, 6, 966, 5, 2139, 7886, 5137, 4, 20, 2721, 7457, 5780, 4855, 162, 7, 42, 1836, 8, 24, 222, 45, 17534, 2115, 18245, 4, 2223, 6, 5, 2576, 526, 28, 9970, 21, 444, 66, 11, 349, 526, 6, 24, 21, 38677, 328, 5, 3031, 2564, 21, 1085, 101, 5, 2170, 131, 2563, 5, 1421, 18, 3701, 32, 2325, 11, 760, 9, 70, 5, 1823, 10199, 7, 946, 5, 910, 15315, 124, 4, 50121, 50118, 9178, 6294, 6, 5, 10199, 16, 2721, 6, 5, 2408, 21, 1969, 36, 10799, 132, 6, 195, 108, 306, 1297, 13442, 23246, 12345, 5, 1318, 16, 372, 8, 939, 657, 5, 5780, 98, 939, 1276, 7, 492, 24, 7, 127, 26090, 7, 22, 1090, 605, 409, 113, 5, 22, 42932, 113, 15, 2185, 3391, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\nLength of input_ids: 152\n----------\n[0, 15841, 4, 1969, 17220, 1990, 80, 15033, 4, 5148, 10717, 17220, 6968, 33, 7, 216, 14, 42, 1907, 9, 10199, 16, 5, 65, 14, 40, 120, 6538, 36, 118, 33, 5, 1104, 65, 322, 24, 16, 2422, 11962, 8, 9869, 6, 53, 939, 21, 129, 441, 7, 120, 80, 15033, 66, 9, 24, 4, 939, 393, 10397, 24, 8, 24, 4925, 24, 18, 1836, 142, 939, 1079, 47904, 24, 683, 7727, 172, 10601, 7, 3841, 4, 939, 21, 2422, 5779, 15, 5, 3568, 53, 10874, 145, 441, 7, 671, 24, 396, 864, 23, 103, 400, 6215, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\nLength of input_ids: 152\n\n\nLet’s compare this runtime to line-by-line process\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\nddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\nddict_with_val['validation'] = ddict_with_val['test']\nddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\ndel ddict_with_val['test']\n\ntdc = TextDataControllerStreaming(ddict_with_val,\n                                  main_text='Review Text',\n                                  label_names='Department Name',\n                                  sup_types='classification',\n                                  class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trending'],\n                                  filter_dict={'Review Text': lambda x: x is not None,\n                                              'Department Name': lambda x: x is not None,\n                                              },\n                                  label_tfm_dict={'Department Name': lambda x: x if x!='Trend' else 'Trending'},\n                                  metadatas=['Title','Division Name'],\n#                                   content_transformations=[text_normalize,str.lower],\n                                  content_augmentations=[contextual_aug_func],\n                                  process_metas=True,\n                                  batch_size=1000,\n                                  num_proc=4,\n                                  seed=42\n                                 )\n\ntdc.process_and_tokenize(tokenizer,line_by_line=True)\n\n-------------------- Data Filtering --------------------\nDone\n----- Metadata Simple Processing & Concatenating to Main Content -----\nDone\n-------------------- Label Transformation --------------------\nDone\n----- Label Encoding -----\nDone\n-------------------- Dropping unused features --------------------\nDone\n----- Performing Content Transformation and Tokenization on validation set -----\nDone\n----- Creating a generator for content transformation, augmentation and tokenization on train set -----\nDone\n\n\n\n\n\n\n# time it takes to go throuch 3 batches (1000 x3)\nfor i,v in enumerate(tdc.main_ddict['train']):\n    if i%500==0:\n        print(i)\n    if i==1000*3-1:\n        break\n    pass\n\n0\n500\n1000\n1500\n2000\n2500\nCPU times: user 1min, sys: 2.44 s, total: 1min 2s\nWall time: 1min 2s\n\n\nThis took longer to run than batch-processing\n\nfor i,v in enumerate(tdc.main_ddict['train']):\n    print(v['input_ids'])\n    print(f\"Length of input_ids: {len(v['input_ids'])}\")\n    if i==3:\n        break\n    print('-'*10)\n\n[0, 15841, 4716, 1459, 479, 2721, 299, 6, 966, 5, 2139, 7886, 5137, 479, 20, 2721, 7457, 5780, 4855, 162, 7, 42, 299, 8, 24, 222, 45, 17534, 2115, 18245, 4, 959, 6, 5, 2576, 910, 15315, 28, 9970, 98, 444, 66, 15, 349, 526, 14, 24, 21, 38677, 328, 5, 3031, 2564, 16, 1085, 101, 5, 2170, 131, 2563, 5, 1421, 18, 3701, 32, 2325, 11, 760, 9, 70, 5, 1823, 10199, 7, 946, 5, 910, 15315, 124, 4, 50121, 50118, 9178, 6294, 6, 5, 10199, 16, 2721, 6, 5, 2564, 21, 1969, 36, 10799, 132, 6, 195, 108, 306, 1297, 13442, 23246, 12345, 5, 1318, 16, 372, 8, 939, 657, 5, 5780, 98, 939, 1276, 7, 185, 24, 7, 127, 26090, 7, 22, 1090, 605, 409, 113, 5, 22, 42932, 113, 15, 258, 3391, 2]\nLength of input_ids: 137\n----------\n[0, 15841, 479, 45, 25, 765, 15, 162, 36, 13713, 1459, 43, 479, 38, 2740, 5, 37863, 29, 181, 25, 42, 3588, 16, 45, 10, 15898, 3588, 6, 8, 14, 21, 5, 235, 1836, 13, 162, 4, 129, 631, 16, 5, 5933, 16, 10, 828, 18277, 202, 361, 29668, 15, 16701, 13, 162, 238, 5, 31622, 32, 818, 3229, 6, 98, 939, 74, 224, 5, 3588, 16, 10, 13173, 326, 15888, 3989, 4, 3195, 16, 2721, 6, 939, 2740, 2272, 25, 5, 97, 3195, 36, 2911, 783, 43, 630, 75, 33, 4716, 1459, 577, 4, 2272, 16, 4066, 6, 8, 30228, 6, 5, 10199, 16, 10262, 3793, 4, 939, 657, 5, 410, 1254, 11, 5, 29986, 4, 2299, 240, 10, 18052, 16979, 11689, 13, 42, 65, 4, 50121, 50118, 50121, 50118, 15314, 23246, 119, 389, 417, 2]\nLength of input_ids: 137\n----------\n[0, 15841, 479, 1969, 17220, 1990, 80, 15033, 479, 5148, 10717, 17220, 6968, 240, 7, 216, 14, 42, 1907, 9, 10199, 16, 5, 65, 14, 40, 120, 6538, 36, 118, 2162, 5, 1104, 65, 322, 24, 16, 2422, 7174, 8, 9869, 6, 53, 939, 21, 129, 441, 7, 120, 80, 15033, 66, 9, 24, 4, 939, 222, 10397, 24, 8, 24, 4925, 24, 18, 1836, 142, 939, 1079, 47904, 24, 150, 7727, 172, 10601, 7, 3841, 4, 939, 21, 2422, 5779, 59, 5, 3568, 53, 10874, 145, 441, 7, 671, 24, 396, 864, 23, 127, 400, 6215, 4, 2]\nLength of input_ids: 99\n----------\n[0, 25153, 11139, 328, 479, 38, 657, 42, 3588, 4, 24, 1299, 98, 3793, 8, 3473, 6, 1969, 13, 1035, 12846, 939, 2813, 24, 376, 11, 80, 8089, 142, 939, 74, 907, 961, 12846, 2]\nLength of input_ids: 35",
    "crumbs": [
      "5. Text Classes",
      "b. Text Controller For Supervised Learning",
      "Text Main Streaming"
    ]
  },
  {
    "objectID": "text_main_streaming.html#save-and-load-textdatacontrollerstreaming",
    "href": "text_main_streaming.html#save-and-load-textdatacontrollerstreaming",
    "title": "Text Main Streaming",
    "section": "3. Save and Load TextDataControllerStreaming",
    "text": "3. Save and Load TextDataControllerStreaming\n\nsource\n\nTextDataControllerStreaming.save_as_pickles\n\n TextDataControllerStreaming.save_as_pickles (fname,\n                                              parent='pickle_files',\n                                              drop_attributes=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfname\n\n\nName of the pickle file\n\n\nparent\nstr\npickle_files\nParent folder\n\n\ndrop_attributes\nbool\nFalse\nWhether to drop large-size attributes\n\n\n\n\nsource\n\n\nTextDataControllerStreaming.from_pickle\n\n TextDataControllerStreaming.from_pickle (fname, parent='pickle_files')\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfname\n\n\nName of the pickle file\n\n\nparent\nstr\npickle_files\nParent folder\n\n\n\nTextDataControllerStreaming object can be saved and loaded with ease. This is especially useful after text processing and/or tokenization have been done\n\nfrom datasets import disable_caching\n\n\ndisable_caching() # disable huggingface caching to see data size\n\n\nfrom underthesea import text_normalize\nimport nlpaug.augmenter.char as nac\nimport nlpaug.augmenter.word as naw\n\n\ndef nlp_aug_stochastic(x,aug=None,p=0.5):\n    if not isinstance(x,list): \n        if random.random()&lt;p: return aug.augment(x)[0]\n        return x\n    news=[]\n    originals=[]\n    for _x in x:\n        if random.random()&lt;p: news.append(_x)\n        else: originals.append(_x)\n    # only perform augmentation when needed\n    if len(news): news = aug.augment(news)\n    return news+originals\n\n\naug2 = naw.ContextualWordEmbsAug(model_path='roberta-base', \n                                device='cuda:0', # if you don't have gpu, change to 'cpu'\n                                action=\"substitute\",\n                                top_k=10,\n                               aug_p=0.07)\n\ncontextual_aug_func = partial(nlp_aug_stochastic,aug=aug2,p=0.1)\ncontextual_aug_func.run_on_gpu=True\ncontextual_aug_func.batch_size=32\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\nddict_with_val = dset.train_test_split(test_size=0.2)\nddict_with_val['validation'] = ddict_with_val['test']\nddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\ndel ddict_with_val['test']\n\ntdc = TextDataControllerStreaming(ddict_with_val,\n                                  main_text='Review Text',\n                                  label_names='Department Name',\n                                  sup_types='classification',\n                                  class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n                                  filter_dict={'Review Text': lambda x: x is not None,\n                                               'Department Name': lambda x: x is not None,\n                                              },\n                                  metadatas=['Title','Division Name'],\n                                  content_transformations=[text_normalize,str.lower],\n                                  content_augmentations= contextual_aug_func,\n                                  process_metas=True,\n                                  batch_size=100,\n                                  num_proc=4,\n                                  seed=42\n                                 )\ntdc.process_and_tokenize(tokenizer,max_length=256)\n\n-------------------- Data Filtering --------------------\nDone\n----- Metadata Simple Processing & Concatenating to Main Content -----\nDone\n----- Label Encoding -----\nDone\n-------------------- Dropping unused features --------------------\nDone\n----- Performing Content Transformation and Tokenization on validation set -----\nDone\n----- Creating a generator for content transformation, augmentation and tokenization on train set -----\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntdc.main_ddict\n\nDatasetDict({\n    train: IterableDataset({\n        features: Unknown,\n        n_shards: 3\n    })\n    validation: Dataset({\n        features: ['Title', 'Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 4529\n    })\n})\n\n\n\ntdc.save_as_pickles('my_tdc_stream')\n\nLet’s check the file size\n\nfile_stats = os.stat(Path('pickle_files/my_tdc_stream.pkl'))\nprint(f'File Size in MegaBytes is {round(file_stats.st_size / (1024 * 1024), 3)}')\n\nFile Size in MegaBytes is 479.023\n\n\nLoad back our object\n\ntdc2 = TextDataControllerStreaming.from_pickle('my_tdc_stream')\n\nYou can still access all its attributes, data, preprocessings, transformation/augmentation …\n\ntdc2.main_ddict\n\nDatasetDict({\n    train: IterableDataset({\n        features: Unknown,\n        n_shards: 3\n    })\n    validation: Dataset({\n        features: ['Title', 'Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 4529\n    })\n})\n\n\n\nfor i,v in enumerate(tdc2.main_ddict['train']):\n    if i==3:break\n    print(f\"Text: {v['Review Text']}\\nLabel: {v['Department Name']} =&gt; {v['label']}\")\n    print('-'*10)\n\n\n\n\n\n\n\n\n\n\n\n\n\nText: general petite.. i love it soft brown glistening, flowy beauty! it's my favorite color too! i'm 5'5 \". 34 d, size 6 and a small fit and with room to spare. don't wait!\nLabel: Jackets =&gt; 3\n----------\nText: general. not the same... as i agree a other reviewer, the material of these jeans is not the same! thin, short, and you end up pulling them up all the time. me am a short, curvy girl and would prefer to have the old jean fabric back! this seems to be the trend in jeans? nydj also uses this fabric? probably too much.\nLabel: Dresses =&gt; 1\n----------\nText: general. not for the busty, simple fabric, very versatile but the knit length and style accentuates the bust. probably not an issue for most but if your a d or up it's more attention than you may want.\nLabel: Tops =&gt; 4\n----------\n\n\n\ntdc2.label_lists\n\n[['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']]\n\n\n\ntdc2.filter_dict,tdc2.content_tfms,tdc2.aug_tfms\n\n({'Review Text': &lt;function __main__.&lt;lambda&gt;(x)&gt;,\n  'Department Name': &lt;function __main__.&lt;lambda&gt;(x)&gt;},\n [&lt;function underthesea.pipeline.text_normalize.text_normalize(text, tokenizer='underthesea')&gt;,\n  &lt;method 'lower' of 'str' objects&gt;],\n [functools.partial(&lt;function nlp_aug_stochastic&gt;, aug=&lt;nlpaug.augmenter.word.context_word_embs.ContextualWordEmbsAug object&gt;, p=0.1)])\n\n\nIf you don’t want to store the HuggingFace DatasetDict in your TextDataControllerStreaming, or the augmentation functions (typically when you already have a trained model, and you only use TextDataControllerStreaming to preprocess the test set), you can remove it in the save_as_pickles step\n\ntdc.save_as_pickles('my_lightweight_tdc_stream',drop_attributes=True)\n\nLet’s check the file size\n\nfile_stats = os.stat(Path('pickle_files/my_lightweight_tdc_stream.pkl'))\nprint(f'File Size in MegaBytes is {round(file_stats.st_size / (1024 * 1024), 3)}')\n\nFile Size in MegaBytes is 1.907\n\n\nLoad it back\n\ntdc3 = TextDataControllerStreaming.from_pickle('my_lightweight_tdc_stream')\n\nWe will use this object to demonstrate the Test Set Construction in the next section\n\n\nConstruct a Test Dataset\n\nsource\n\n\nTextDataControllerStreaming.prepare_test_dataset\n\n TextDataControllerStreaming.prepare_test_dataset (test_dset,\n                                                   do_filtering=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntest_dset\n\n\nThe HuggingFace Dataset as Test set\n\n\ndo_filtering\nbool\nFalse\nwhether to perform data filtering on this test set\n\n\n\n\nsource\n\n\nTextDataControllerStreaming.prepare_test_dataset_from_csv\n\n TextDataControllerStreaming.prepare_test_dataset_from_csv (file_path,\n                                                            do_filtering=F\n                                                            alse)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\n\n\npath to csv file\n\n\ndo_filtering\nbool\nFalse\nwhether to perform data filtering on this test set\n\n\n\n\nsource\n\n\nTextDataControllerStreaming.prepare_test_dataset_from_df\n\n TextDataControllerStreaming.prepare_test_dataset_from_df (df,\n                                                           validate=True, \n                                                           do_filtering=Fa\n                                                           lse)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\n\n\nPandas Dataframe\n\n\nvalidate\nbool\nTrue\nwhether to perform input data validation\n\n\ndo_filtering\nbool\nFalse\nwhether to perform data filtering on this test set\n\n\n\n\nsource\n\n\nTextDataControllerStreaming.prepare_test_dataset_from_raws\n\n TextDataControllerStreaming.prepare_test_dataset_from_raws (content)\n\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\ncontent\nEither a single sentence, list of sentence or a dictionary with keys are metadata columns and values are list\n\n\n\nLet’s say you have done your preprocessing and tokenization in your training set, and have a nicely trained model, ready to do inference on new data. Here is how you can use TextDataControllerStreaming to apply all the necessary preprocessings to your new data\nWe will reuse the lightweight tdc object we created in the previous section (since we don’t really need all the training data just to construct new data). Also, we will take a small sample of our training data and pretend it is our test data\n\ntdc = TextDataControllerStreaming.from_pickle('my_lightweight_tdc_stream')\n\n\ndf_test = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig').sample(frac=0.2,random_state=1)\n# drop NaN values in the label column\ndf_test = df_test[~df_test['Department Name'].isna()].reset_index(drop=True)\ndf_test.shape\n\n(4692, 10)\n\n\n\ndf_test.head()\n\n\n\n\n\n\n\n\n\nClothing ID\nAge\nTitle\nReview Text\nRating\nRecommended IND\nPositive Feedback Count\nDivision Name\nDepartment Name\nClass Name\n\n\n\n\n0\n872\n42\nPerfect for work and play\nThis shirt works for both going out and going ...\n5\n1\n0\nGeneral\nTops\nKnits\n\n\n1\n1033\n40\nNaN\nI don't know why i had the opposite problem mo...\n4\n1\n0\nGeneral Petite\nBottoms\nJeans\n\n\n2\n1037\n45\nGreat pants\nThese cords are great--lightweight for fl wint...\n5\n1\n1\nGeneral Petite\nBottoms\nJeans\n\n\n3\n829\n35\nSurprisingly comfy for a button down\nI am a 10 m and got the 10. it fits perfectly ...\n5\n1\n1\nGeneral Petite\nTops\nBlouses\n\n\n4\n872\n29\nShort and small\nThe shirt is mostly a thick sweatshirt materia...\n3\n0\n15\nGeneral Petite\nTops\nKnits\n\n\n\n\n\n\n\n\n\ntest_dset = tdc.prepare_test_dataset_from_df(df_test,validate=True,do_filtering=True)\n\n- Input Validation Precheck -\nData contains missing values!\n-----&gt; List of columns and the number of missing values for each\nTitle          758\nReview Text    164\ndtype: int64\nData contains duplicated values!\n-----&gt; Number of duplications: 2 rows\n-------------------- Start Test Set Transformation --------------------\n-------------------- Data Filtering --------------------\nDone\n----- Metadata Simple Processing & Concatenating to Main Content -----\nDone\n-------------------- Dropping unused features --------------------\nDone\n----- Performing Content Transformation and Tokenization on test set -----\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor i in range(3):\n    print(f\"Text: {test_dset['Review Text'][i]}\")\n    print(f\"Input_ids: {test_dset['input_ids'][i]}\")\n    print('-'*10)\n\nText: general . perfect for work and play . this shirt works for both going out and going to work , and i can wear it with everything . fits perfect , tucked and untucked , tied and untied . i love it .\nInput_ids: [0, 15841, 479, 1969, 13, 173, 8, 310, 479, 42, 6399, 1364, 13, 258, 164, 66, 8, 164, 7, 173, 2156, 8, 939, 64, 3568, 24, 19, 960, 479, 10698, 1969, 2156, 21222, 8, 7587, 23289, 2156, 3016, 8, 7587, 2550, 479, 939, 657, 24, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n----------\nText: general petite . . i don't know why i had the opposite problem most reviewers had with these ..... i tried on the regular length in the store and found that they were just a bit too short with heels . ( i'm 5 ' 5 ) . i had them ordered in a petite and when they came , they were too short with flats ! maybe it's the way i like to wear them , i like my flare jeans to barely skim the ground . i just exchanged them for regular length and will wear them with a small wedge shoe . aside from the length issues , these are super cute\nInput_ids: [0, 15841, 4716, 1459, 479, 479, 939, 218, 75, 216, 596, 939, 56, 5, 5483, 936, 144, 34910, 56, 19, 209, 29942, 734, 939, 1381, 15, 5, 1675, 5933, 11, 5, 1400, 8, 303, 14, 51, 58, 95, 10, 828, 350, 765, 19, 8872, 479, 36, 939, 437, 195, 128, 195, 4839, 479, 939, 56, 106, 2740, 11, 10, 4716, 1459, 8, 77, 51, 376, 2156, 51, 58, 350, 765, 19, 20250, 27785, 2085, 24, 18, 5, 169, 939, 101, 7, 3568, 106, 2156, 939, 101, 127, 24186, 10844, 7, 6254, 28772, 5, 1255, 479, 939, 95, 11024, 106, 13, 1675, 5933, 8, 40, 3568, 106, 19, 10, 650, 27288, 12604, 479, 4364, 31, 5, 5933, 743, 2156, 209, 32, 2422, 11962, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n----------\nText: general petite . great pants . thes e cords are great--lightweight for fl winters , and the bootcut flare bottom is super cute with ballet flats or booties . i am 5 ' 10 \" and typically a size 8 ; the size 29 fit perfectly . they have a little stretch to them , which is great . very flattering--wish i could order in more colors ! !\nInput_ids: [0, 15841, 4716, 1459, 479, 372, 9304, 479, 5, 29, 364, 37687, 32, 372, 5579, 6991, 4301, 13, 2342, 31000, 2156, 8, 5, 9759, 8267, 24186, 2576, 16, 2422, 11962, 19, 22573, 20250, 50, 9759, 918, 479, 939, 524, 195, 128, 158, 22, 8, 3700, 10, 1836, 290, 25606, 5, 1836, 1132, 2564, 6683, 479, 51, 33, 10, 410, 4140, 7, 106, 2156, 61, 16, 372, 479, 182, 34203, 5579, 605, 1173, 939, 115, 645, 11, 55, 8089, 27785, 27785, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n----------\n\n\nLet’s make our test data streamed as well\n\ntest_dset_raw = Dataset.from_pandas(df_test).to_iterable_dataset()\n\nThis test dataset might have some NaN values in the text field (Review Text), thus we will turn on the filtering option to get rid of these NaNs, as this is what we did in the training set. If your test dataset don’t need any filtering, turn off this option\n\ntest_dset = tdc.prepare_test_dataset(test_dset_raw,do_filtering=True)\n\n-------------------- Start Test Set Transformation --------------------\n-------------------- Data Filtering --------------------\nDone\n----- Metadata Simple Processing & Concatenating to Main Content -----\nDone\n-------------------- Dropping unused features --------------------\nDone\n----- Performing Content Transformation and Tokenization on test set -----\nDone\n\n\n\nfor i,v in enumerate(test_dset):\n    if i==3:break\n    print(f\"Text: {v['Review Text']}\\Input_ids: {v['input_ids']}\\nAttention mask: {v['attention_mask']}\")\n    print('-'*10)\n\nText: general . perfect for work and play . this shirt works for both going out and going to work , and i can wear it with everything . fits perfect , tucked and untucked , tied and untied . i love it .\\Input_ids: [0, 15841, 479, 1969, 13, 173, 8, 310, 479, 42, 6399, 1364, 13, 258, 164, 66, 8, 164, 7, 173, 2156, 8, 939, 64, 3568, 24, 19, 960, 479, 10698, 1969, 2156, 21222, 8, 7587, 23289, 2156, 3016, 8, 7587, 2550, 479, 939, 657, 24, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\nAttention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n----------\nText: general petite . . i don't know why i had the opposite problem most reviewers had with these ..... i tried on the regular length in the store and found that they were just a bit too short with heels . ( i'm 5 ' 5 ) . i had them ordered in a petite and when they came , they were too short with flats ! maybe it's the way i like to wear them , i like my flare jeans to barely skim the ground . i just exchanged them for regular length and will wear them with a small wedge shoe . aside from the length issues , these are super cute\\Input_ids: [0, 15841, 4716, 1459, 479, 479, 939, 218, 75, 216, 596, 939, 56, 5, 5483, 936, 144, 34910, 56, 19, 209, 29942, 734, 939, 1381, 15, 5, 1675, 5933, 11, 5, 1400, 8, 303, 14, 51, 58, 95, 10, 828, 350, 765, 19, 8872, 479, 36, 939, 437, 195, 128, 195, 4839, 479, 939, 56, 106, 2740, 11, 10, 4716, 1459, 8, 77, 51, 376, 2156, 51, 58, 350, 765, 19, 20250, 27785, 2085, 24, 18, 5, 169, 939, 101, 7, 3568, 106, 2156, 939, 101, 127, 24186, 10844, 7, 6254, 28772, 5, 1255, 479, 939, 95, 11024, 106, 13, 1675, 5933, 8, 40, 3568, 106, 19, 10, 650, 27288, 12604, 479, 4364, 31, 5, 5933, 743, 2156, 209, 32, 2422, 11962, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\nAttention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n----------\nText: general petite . great pants . thes e cords are great--lightweight for fl winters , and the bootcut flare bottom is super cute with ballet flats or booties . i am 5 ' 10 \" and typically a size 8 ; the size 29 fit perfectly . they have a little stretch to them , which is great . very flattering--wish i could order in more colors ! !\\Input_ids: [0, 15841, 4716, 1459, 479, 372, 9304, 479, 5, 29, 364, 37687, 32, 372, 5579, 6991, 4301, 13, 2342, 31000, 2156, 8, 5, 9759, 8267, 24186, 2576, 16, 2422, 11962, 19, 22573, 20250, 50, 9759, 918, 479, 939, 524, 195, 128, 158, 22, 8, 3700, 10, 1836, 290, 25606, 5, 1836, 1132, 2564, 6683, 479, 51, 33, 10, 410, 4140, 7, 106, 2156, 61, 16, 372, 479, 182, 34203, 5579, 605, 1173, 939, 115, 645, 11, 55, 8089, 27785, 27785, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\nAttention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n----------",
    "crumbs": [
      "5. Text Classes",
      "b. Text Controller For Supervised Learning",
      "Text Main Streaming"
    ]
  },
  {
    "objectID": "deprecated/model_main_envibert_singlehead-copy1.html",
    "href": "deprecated/model_main_envibert_singlehead-copy1.html",
    "title": "Model Controller Tutorial: EnviBert model (Single Head)",
    "section": "",
    "text": "We will walk through other cases of classification: multi-head and multi-label. Since we will showcase the capabiilty of this label in these cases, there won’t be as detailed as this tutorial"
  },
  {
    "objectID": "deprecated/model_main_envibert_singlehead-copy1.html#load-data",
    "href": "deprecated/model_main_envibert_singlehead-copy1.html#load-data",
    "title": "Model Controller Tutorial: EnviBert model (Single Head)",
    "section": "Load data",
    "text": "Load data\n\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom that_nlp_library.text_main import *\n\n\nfrom underthesea import text_normalize\nfrom functools import partial\nfrom pathlib import Path\nfrom importlib.machinery import SourceFileLoader\nfrom transformers import DataCollatorWithPadding\nimport pandas as pd\n\nDefine some necessary text augmentations and text transformations\n\nFor Text Transformation\n\n\ntxt_tfms=[text_normalize]\n\n\nFor Text Augmentation\n\n\nover_nonown_tfm = partial(sampling_with_condition,query='Source==\"non owned\"',frac=0.5,seed=42,apply_to_all=False)\nover_nonown_tfm.__name__ = 'Oversampling Non Owned'\n\nover_own_tfm = partial(sampling_with_condition,query='Source==\"owned\"',frac=2,seed=42,apply_to_all=False)\nover_own_tfm.__name__ = 'Oversampling Owned'\n\nover_hc_tfm = partial(sampling_with_condition,query='Source==\"hc search\"',frac=2.5,seed=42,apply_to_all=False)\nover_hc_tfm.__name__ = 'Oversampling HC search'\n\nremove_accent_tfm = partial(remove_vnmese_accent,frac=1,seed=42,apply_to_all=True)\nremove_accent_tfm.__name__ = 'Add No-Accent Text'\n\naug_tfms = [over_nonown_tfm,over_own_tfm,over_hc_tfm,remove_accent_tfm]\n\nLet’s load and preprocess our data\n\nDATA_PATH = Path('secret_data')\n\n\ndf = TextDataMain.from_csv(DATA_PATH/'buyer_listening_with_all_raw_data_w2223.csv',return_df=True)\n\n----- Input Validation Precheck -----\nDataFrame contains missing values!\n-----&gt; List of columns and the number of missing values for each\nis_valid    65804\ndtype: int64\nDataFrame contains duplicated values!\n-----&gt; Number of duplications: 7 rows\n\n\n\ndf.head(3)\n\n\n\n\n\n\n\n\n\nWeek\nGroup\nSource\nContent\nL1\nL2\nL3\nL4\nis_valid\niteration\n\n\n\n\n0\n1.0\nGoogle Play\nGoogle Play\nTại sao cứ hiện thông báo\nServices\nShopee communication channels\nAnnoying pop-up ads\nNon-tech\nNaN\n1\n\n\n1\n1.0\nGoogle Play\nGoogle Play\nMlem\nOthers\nCannot defined\n-\n-\nNaN\n1\n\n\n2\n1.0\nGoogle Play\nGoogle Play\n1 số sản phẩm trong giỏ hàng vừa đc cập nhật t...\nFeature\nCart & Order\nCart issues/suggestions\nTech\nNaN\n1\n\n\n\n\n\n\n\n\nQuick preprocess of data and train/validation split. Due to custom logic, we will sample our data here instead of using the train_ratio from the to_datasetdict function\n\ndf_rare = df[df.L2.isin(['Chatbot', 'Commercial Others'])].copy()\n\ndf_final = pd.concat([df.query('iteration==1').sample(500,random_state=42),\n                      df.query('iteration&gt;=7 & iteration&lt;13').sample(1200,random_state=42),\n                      df_rare,\n                      df.query('iteration&gt;=13'),\n                     ],axis=0).reset_index(drop=True)\n\nval_idxs = df_final[df_final.iteration&gt;=13].index.values # from week 9\n\n\ntdm = TextDataMain(df_final,\n                    main_content='Content',\n                    metadatas='Source',\n                    label_names='L1',\n                    val_ratio=val_idxs,\n                    split_cols='L1',\n                    content_tfms = txt_tfms,\n                    aug_tfms = aug_tfms,\n                    process_metadatas=True,\n                    seed=42,\n                    cols_to_keep=['Content','Source','iteration','L1'], \n                   # Note that the text column (e.g.`Content`) must be the first item in the `cols_to_keep`\n                    shuffle_trn=True)\n\n----- Input Validation Precheck -----\nDataFrame contains missing values!\n-----&gt; List of columns and the number of missing values for each\nis_valid    498\ndtype: int64\n\n\nDefine our tokenizer for EnviBert\n\ncache_dir=Path('./envibert_tokenizer')\ntokenizer = SourceFileLoader(\"envibert.tokenizer\", \n                             str(cache_dir/'envibert_tokenizer.py')).load_module().RobertaTokenizer(cache_dir)\n\nCreate our DatasetDict from TextDataMain (as our ModelController class can also work with DatasetDict)\n\nmain_ddict= tdm.to_datasetdict(tokenizer,\n                               max_length=512)\n\n-------------------- Start Main Text Processing --------------------\n----- Metadata Simple Processing & Concatenating to Main Content -----\n----- Label Encoding -----\n-------------------- Text Transformation --------------------\n----- text_normalize -----\n-------------------- Train Test Split --------------------\nPrevious Validation Percentage: 74.101%\n- Before leak check\nSize: 4927\n- After leak check\nSize: 4885\n- Number of rows leaked: 42, or 0.85% of the original validation (or test) data\nCurrent Validation Percentage: 73.47%\n-------------------- Text Augmentation --------------------\nTrain data size before augmentation: 1764\n----- Oversampling Non Owned -----\nTrain data size after THIS augmentation: 2229\n----- Oversampling Owned -----\nTrain data size after THIS augmentation: 2789\n----- Oversampling HC search -----\nTrain data size after THIS augmentation: 2904\n----- Add No-Accent Text -----\nTrain data size after THIS augmentation: 5808\nTrain data size after ALL augmentation: 5808\n-------------------- Map Tokenize Function --------------------\n\n\n100%|█████████████████████████████████████| 6649/6649 [00:01&lt;00:00, 3640.53it/s]\n100%|████████████████████████████████████| 2904/2904 [00:00&lt;00:00, 10205.97it/s]\n\n\n\n\n\n\n\n\n\nmain_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'Source', 'iteration', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 5808\n    })\n    validation: Dataset({\n        features: ['text', 'label', 'Source', 'iteration', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 4885\n    })\n})"
  },
  {
    "objectID": "deprecated/model_main_envibert_singlehead-copy1.html#train-envibert-with-hidden-layer-concatenation-using-tdm",
    "href": "deprecated/model_main_envibert_singlehead-copy1.html#train-envibert-with-hidden-layer-concatenation-using-tdm",
    "title": "Model Controller Tutorial: EnviBert model (Single Head)",
    "section": "Train EnviBert (with hidden layer concatenation), using TDM",
    "text": "Train EnviBert (with hidden layer concatenation), using TDM\nLet’s create our model controller\n\nmodel_name='nguyenvulebinh/envibert'\nenvibert_body = RobertaModel.from_pretrained(model_name)\nnum_classes = len(tdm.label_lists[0])\n\nSome weights of the model checkpoint at nguyenvulebinh/envibert were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\n_model_kwargs={\n    # overall model hyperparams\n    'layer2concat':4,\n    'is_multilabel':tdm.is_multilabel, # False\n    'is_multihead':tdm.is_multihead, # False\n    'head_class_sizes':num_classes,\n    'head_class': ConcatHeadSimple,\n    # classfication head hyperparams\n    'classifier_dropout':0.1 \n}\n\nmodel = model_init_classification(model_class = RobertaHiddenStateConcatForSequenceClassification,\n                                  cpoint_path = model_name, \n                                  output_hidden_states=True, # since we are not using 'hidden layer contatenation' technique\n                                  seed=42,\n                                  body_model=envibert_body,\n                                  model_kwargs = _model_kwargs)\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score] # we will use both f1_macro and accuracy score as metrics\ncontroller = ModelController(model,tdm,metric_funcs)\n\nLoading body weights. This assumes the body is the very first first-layer block of your custom architecture\n\n\nAnd we can start training our model\n\nlr = 8.2e-5\nbs=4\nwd=0.01\nepochs= 4\n\ncontroller.fit(epochs,lr,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics_classification,\n              )\n\n/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\n\n\n\n    \n      \n      \n      [2904/2904 03:19, Epoch 4/4]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1 Score L1\nAccuracy Score L1\n\n\n\n\n1\nNo log\n1.372778\n0.344335\n0.621085\n\n\n2\n0.789100\n1.375100\n0.450448\n0.637871\n\n\n3\n0.789100\n1.668144\n0.501494\n0.670420\n\n\n4\n0.136500\n1.773287\n0.503311\n0.671238\n\n\n\n\n\n\n\n\ncontroller.trainer.model.save_pretrained('./sample_weights/my_model')"
  },
  {
    "objectID": "deprecated/model_main_envibert_singlehead-copy1.html#predict-using-trained-model-using-tdm",
    "href": "deprecated/model_main_envibert_singlehead-copy1.html#predict-using-trained-model-using-tdm",
    "title": "Model Controller Tutorial: EnviBert model (Single Head)",
    "section": "Predict using trained model, using TDM",
    "text": "Predict using trained model, using TDM\n\nLoad trained model\n\n_model_kwargs\n\n{'layer2concat': 4,\n 'is_multilabel': False,\n 'is_multihead': False,\n 'head_class_sizes': 10,\n 'head_class': that_nlp_library.models.roberta.classifiers.ConcatHeadSimple,\n 'classifier_dropout': 0.1}\n\n\n\ntrained_model = model_init_classification(model_class = RobertaHiddenStateConcatForSequenceClassification,\n                                          cpoint_path = Path('./sample_weights/my_model'), \n                                          output_hidden_states=True,\n                                          seed=42,\n                                          model_kwargs = _model_kwargs)\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score]\ncontroller = ModelController(trained_model,tdm,metric_funcs)\n\nSome weights of the model checkpoint at sample_weights/my_model were not used when initializing RobertaHiddenStateConcatForSequenceClassification: ['body_model.pooler.dense.weight', 'body_model.pooler.dense.bias']\n- This IS expected if you are initializing RobertaHiddenStateConcatForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaHiddenStateConcatForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\n\nPredict Train/Validation set\nMake prediction on all validation set\n\ndf_val = controller.predict_ddict(ds_type='validation',batch_size=8)\n\n-------------------- Start making predictions --------------------\n\n\n\n\n\n\ndf_val.head()\n\n\n\n\n\n\n\n\n\ntext\nlabel\nSource\niteration\npred_L1\npred_prob_L1\n\n\n\n\n0\ngoogle play - lam phien\n5\ngoogle play\n13\nFeature\n0.991197\n\n\n1\ngoogle play - .. t . À mà họ nữ ưu m\n5\ngoogle play\n13\nOthers\n0.850162\n\n\n2\ngoogle play - Cc lùa dao\n5\ngoogle play\n13\nOthers\n0.998358\n\n\n3\ngoogle play - Mặt hàng sp mình đều nhỡ với Gia...\n2\ngoogle play\n13\nDelivery\n0.984496\n\n\n4\ngoogle play - Chưa tối ưu tốt cho Android Oppo...\n3\ngoogle play\n13\nFeature\n0.983986\n\n\n\n\n\n\n\n\nTo convert the label index to string, we can use the label_lists attribute of tdm\n\ndf_val['label']= df_val['label'].apply(lambda x: tdm.label_lists[0][x]).values\n\n\nf1_score(df_val.label,df_val.pred_L1,average='macro')\n\n0.5034901410417664\n\n\n\n\nPredict Test set\nWe will go through details on how to make a prediction on a completely new and raw dataset using our trained model. For now, let’s reuse the sample csv and pretend it’s our test set\n\ndf_test = TextDataMain.from_csv(Path('sample_data')/'sample_large.csv',return_df=True)\n\n----- Input Validation Precheck -----\nDataFrame contains duplicated values!\n-----&gt; Number of duplications: 16 rows\n\n\nWe added the required columns as we defined in training process, and remove all the labels\n\ndf_test = df_test.drop(['L1','L2'],axis=1)\ndf_test['iteration']=df_val.iteration.max()+1\n\n\ndf_test.head()\n\n\n\n\n\n\n\n\n\nSource\nContent\niteration\n\n\n\n\n0\nGoogle Play\nApp ncc lúc nào cx lag đơ, phần tìm kiếm thì v...\n21\n\n\n1\nNon Owned\n..❗️ GÓC THANH LÝ Tính ra rẻ hơn cả mua #Shope...\n21\n\n\n2\nGoogle Play\nMắc gì người ta đặt hàng toàn lỗi 😃????\n21\n\n\n3\nOwned\n#GhienShopeePayawardT8 Khi bạn chơi shopee quá...\n21\n\n\n4\nGoogle Play\nRất bức xúc khi dùng . mã giảm giá người dùng ...\n21\n\n\n\n\n\n\n\n\nWe will create a DatasetDict for this test dataframe\n\ntest_ddict = tdm.get_test_datasetdict_from_df(df_test)\n\n-------------------- Getting Test Set --------------------\n----- Input Validation Precheck -----\nDataFrame contains duplicated values!\n-----&gt; Number of duplications: 19 rows\n-------------------- Start Test Set Transformation --------------------\n----- Metadata Simple Processing & Concatenating to Main Content -----\n-------------------- Text Transformation --------------------\n----- text_normalize -----\n-------------------- Test Leak Checking --------------------\n- Before leak check\nSize: 2269\n- After leak check\nSize: 2080\n- Number of rows leaked: 189, or 8.33% of the original validation (or test) data\n-------------------- Construct DatasetDict --------------------\n\n\n100%|█████████████████████████████████████| 2269/2269 [00:00&lt;00:00, 3120.31it/s]\n\n\n\n\n\n\ntest_ddict\n\nDatasetDict({\n    test: Dataset({\n        features: ['text', 'Source', 'iteration', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 2269\n    })\n})\n\n\nOur test data has been processed + transformed (but not augmented) the same way as the validation set. Now we can start making the prediction\n\ncontroller = ModelController(model,tdm)\ndf_result = controller.predict_ddict(ddict=test_ddict,ds_type='test')\n\n-------------------- Start making predictions --------------------\n\n\n\n\n\n\ndf_result.head()\n\n\n\n\n\n\n\n\n\ntext\nSource\niteration\npred_L1\npred_prob_L1\n\n\n\n\n0\ngoogle play - App ncc lúc nào cx lag đơ , phần...\ngoogle play\n21\nFeature\n0.997287\n\n\n1\nnon owned - .. ❗ ️ GÓC THANH LÝ Tính ra rẻ hơn...\nnon owned\n21\nOthers\n0.999744\n\n\n2\ngoogle play - Mắc gì người ta đặt hàng toàn lỗ...\ngoogle play\n21\nFeature\n0.959076\n\n\n3\nowned - # GhienShopeePayawardT8 Khi bạn chơi s...\nowned\n21\nCommercial\n0.999123\n\n\n4\ngoogle play - Rất bức xúc khi dùng . mã giảm g...\ngoogle play\n21\nFeature\n0.997365\n\n\n\n\n\n\n\n\nWe can even predict top k results\n\ndf_result = controller.predict_ddict(ddict=test_ddict,ds_type='test',topk=3)\ndf_result.head()\n\n-------------------- Start making predictions --------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\ntext\nSource\niteration\npred_L1\npred_prob_L1\npred_L1_top1\npred_L1_top2\npred_L1_top3\npred_prob_L1_top1\npred_prob_L1_top2\npred_prob_L1_top3\n\n\n\n\n0\ngoogle play - App ncc lúc nào cx lag đơ , phần...\ngoogle play\n21\n[3, 9, 5]\n[0.99728715, 0.0009524937, 0.00058025133]\nFeature\nShopee account\nOthers\n0.997287\n0.000952\n0.000580\n\n\n1\nnon owned - .. ❗ ️ GÓC THANH LÝ Tính ra rẻ hơn...\nnon owned\n21\n[5, 1, 3]\n[0.9997445, 8.3330306e-05, 4.366889e-05]\nOthers\nCommercial\nFeature\n0.999744\n0.000083\n0.000044\n\n\n2\ngoogle play - Mắc gì người ta đặt hàng toàn lỗ...\ngoogle play\n21\n[3, 5, 9]\n[0.95907587, 0.03484495, 0.0030704038]\nFeature\nOthers\nShopee account\n0.959076\n0.034845\n0.003070\n\n\n3\nowned - # GhienShopeePayawardT8 Khi bạn chơi s...\nowned\n21\n[1, 6, 3]\n[0.99912256, 0.0005796181, 6.469468e-05]\nCommercial\nPayment\nFeature\n0.999123\n0.000580\n0.000065\n\n\n4\ngoogle play - Rất bức xúc khi dùng . mã giảm g...\ngoogle play\n21\n[3, 9, 5]\n[0.9973652, 0.0006989358, 0.00043120742]\nFeature\nShopee account\nOthers\n0.997365\n0.000699\n0.000431\n\n\n\n\n\n\n\n\nIf we just want to make a prediction on a small amount of data (single sentence, or a few sentences), we can use ModelController.predict_raw_text\n\n# Since we have some metadatas, we need to define a dictionary (to imitate a DatasetDict)\nraw_content={\n    'Source': 'Google play',\n    'iteration':21,\n    'Content':'Tôi không thích Shopee.Tại vì dùng app rất chậm,lag banh nhà lầu, thậm chí log in còn không đc'\n}\n\nIf we don’t use metadata, we can use something like this:\nraw_content='Tôi không thích Shopee.Tại vì dùng app rất chậm,lag banh nhà lầu, thậm chí log in còn không đc'\n\ndf_result = controller.predict_raw_text(raw_content,topk=1)\ndf_result\n\n100%|███████████████████████████████████████████| 1/1 [00:00&lt;00:00, 4981.36it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntext\nSource\niteration\npred_L1\npred_prob_L1\n\n\n\n\n0\ngoogle play - Tôi không thích Shopee . Tại vì ...\ngoogle play\n21\nFeature\n0.997374\n\n\n\n\n\n\n\n\n\nraw_content={\n    'Source': ['Google play','Owned'],\n    'iteration':[21,21],\n    'Content':['Tôi không thích Shopee.Tại vì dùng app rất chậm,lag banh nhà lầu, thậm chí log in còn không đc',\n               'App này xài được. Mua đồ rẻ ghê, được voucher nhiều nữa']\n            }\ndf_result = controller.predict_raw_text(raw_content,topk=2)\ndf_result\n\n100%|███████████████████████████████████████████| 2/2 [00:00&lt;00:00, 6887.20it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntext\nSource\niteration\npred_L1\npred_prob_L1\npred_L1_top1\npred_L1_top2\npred_prob_L1_top1\npred_prob_L1_top2\n\n\n\n\n0\ngoogle play - Tôi không thích Shopee . Tại vì ...\ngoogle play\n21\n[3, 9]\n[0.99737394, 0.00059575593]\nFeature\nShopee account\n0.997374\n0.000596\n\n\n1\nowned - App này xài được . Mua đồ rẻ ghê , đượ...\nowned\n21\n[1, 3]\n[0.99925727, 0.00019265412]\nCommercial\nFeature\n0.999257\n0.000193"
  },
  {
    "objectID": "deprecated/model_main_envibert_multilabel-copy1.html",
    "href": "deprecated/model_main_envibert_multilabel-copy1.html",
    "title": "Model Controller Tutorial: EnviBert model (Multi Label)",
    "section": "",
    "text": "We will walk through other cases of classification: multi-head and multi-label. Since we will showcase the capabiilty of this label in these cases, there won’t be as detailed as this tutorial"
  },
  {
    "objectID": "deprecated/model_main_envibert_multilabel-copy1.html#load-data",
    "href": "deprecated/model_main_envibert_multilabel-copy1.html#load-data",
    "title": "Model Controller Tutorial: EnviBert model (Multi Label)",
    "section": "Load data",
    "text": "Load data\n\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom that_nlp_library.text_main import *\n\n\nfrom underthesea import text_normalize\nfrom functools import partial\nfrom pathlib import Path\nfrom importlib.machinery import SourceFileLoader\nimport os\nfrom transformers import DataCollatorWithPadding\nimport pandas as pd\n\nDefine some necessary text augmentations and text transformations\n\nFor Text Transformation\n\n\ntxt_tfms=[text_normalize]\n\n\nFor Text Augmentation\n\n\nover_nonown_tfm = partial(sampling_with_condition,query='Source==\"non owned\"',frac=0.5,seed=42,apply_to_all=False)\nover_nonown_tfm.__name__ = 'Oversampling Non Owned'\n\nover_own_tfm = partial(sampling_with_condition,query='Source==\"owned\"',frac=2,seed=42,apply_to_all=False)\nover_own_tfm.__name__ = 'Oversampling Owned'\n\nover_hc_tfm = partial(sampling_with_condition,query='Source==\"hc search\"',frac=2.5,seed=42,apply_to_all=False)\nover_hc_tfm.__name__ = 'Oversampling HC search'\n\nremove_accent_tfm = partial(remove_vnmese_accent,frac=1,seed=42,apply_to_all=True)\nremove_accent_tfm.__name__ = 'Add No-Accent Text'\n\naug_tfms = [over_nonown_tfm,over_own_tfm,over_hc_tfm,remove_accent_tfm]\n\nCreate a TextDataMain object\n\nDATA_PATH = Path('secret_data')\n\ndf = TextDataMain.from_csv(DATA_PATH/'buyer_listening_with_all_raw_data_w2223.csv',return_df=True)\n\n#Quick preprocess of data and train/validation split. \n#Due to custom logic, we will sample our data here instead of using the `train_ratio` from the `to_datasetdict` function\n\ndf_rare = df[df.L2.isin(['Chatbot', 'Commercial Others'])].copy()\n\ndf_final = pd.concat([df.query('iteration==1').sample(500,random_state=42),\n                      df.query('iteration&gt;=7 & iteration&lt;13').sample(1200,random_state=42),\n                      df_rare,\n                      df.query('iteration&gt;=13'),\n                     ],axis=0).reset_index(drop=True)\n\nval_idxs = df_final[df_final.iteration&gt;=13].index.values # from week 9\n\n----- Input Validation Precheck -----\nDataFrame contains missing values!\n-----&gt; List of columns and the number of missing values for each\nis_valid    65804\ndtype: int64\nDataFrame contains duplicated values!\n-----&gt; Number of duplications: 7 rows\n\n\nSince the problem is not multi-label, we will make it into one by concatenate L1 label and L2 label together\n\ndf_final['L1L2'] = df_final[['L1','L2']].values.tolist()\n\n\ndf_final.head(5)\n\n\n\n\n\n\n\n\n\nWeek\nGroup\nSource\nContent\nL1\nL2\nL3\nL4\nis_valid\niteration\nL1L2\n\n\n\n\n0\n46.0\nGoogle Play\nGoogle Play\nKog zô dx xóa tải lại củg kog zô dx luôn vãi c...\nFeature\nApp performance\nLag when browsing in general\nTech\nNaN\n1\n[Feature, App performance]\n\n\n1\n32.0\nGoogle Play\nGoogle Play\nTôi mêt moi voi cái ung dung nay quá lam sao x...\nOthers\nCannot defined\n-\n-\nNaN\n1\n[Others, Cannot defined]\n\n\n2\n10.0\nGoogle Play\nGoogle Play\nLag vc fix đi shoper\nFeature\nApp performance\nLag when browsing in general\nTech\nNaN\n1\n[Feature, App performance]\n\n\n3\n20.0\nGoogle Play\nGoogle Play\nGỡ\nOthers\nCannot defined\n-\n-\nNaN\n1\n[Others, Cannot defined]\n\n\n4\n31.0\nGoogle Play\nGoogle Play\nKo mở đc chán nản, báo update, update ko dc, v...\nFeature\nApp performance\nApp installment problem\nTech\nNaN\n1\n[Feature, App performance]\n\n\n\n\n\n\n\n\n\ntdm = TextDataMain(df_final,\n                    main_content='Content',\n                    metadatas='Source',\n                    label_names='L1L2',\n                    val_ratio=val_idxs,\n                    split_cols='L2',\n                    content_tfms = txt_tfms,\n                    aug_tfms = aug_tfms,\n                    process_metadatas=True,\n                    seed=42,\n                    shuffle_trn=True)\n\n----- Input Validation Precheck -----\nDataFrame contains missing values!\n-----&gt; List of columns and the number of missing values for each\nis_valid    498\ndtype: int64\n\n\nDefine our tokenizer for EnviBert\n\ncache_dir=Path('./envibert_tokenizer')\ntokenizer = SourceFileLoader(\"envibert.tokenizer\", \n                             str(cache_dir/'envibert_tokenizer.py')).load_module().RobertaTokenizer(cache_dir)\n\nCreate our DatasetDict from TextDataMain (as our ModelController class can also work with DatasetDict)\n\nmain_ddict= tdm.to_datasetdict(tokenizer,\n                               max_length=512)\n\n-------------------- Start Main Text Processing --------------------\n----- Metadata Simple Processing & Concatenating to Main Content -----\n----- Label Encoding -----\n-------------------- Text Transformation --------------------\n----- text_normalize -----\n-------------------- Train Test Split --------------------\nPrevious Validation Percentage: 74.101%\n- Before leak check\nSize: 4927\n- After leak check\nSize: 4885\n- Number of rows leaked: 42, or 0.85% of the original validation (or test) data\nCurrent Validation Percentage: 73.47%\n-------------------- Text Augmentation --------------------\nTrain data size before augmentation: 1764\n----- Oversampling Non Owned -----\nTrain data size after THIS augmentation: 2229\n----- Oversampling Owned -----\nTrain data size after THIS augmentation: 2789\n----- Oversampling HC search -----\nTrain data size after THIS augmentation: 2904\n----- Add No-Accent Text -----\nTrain data size after THIS augmentation: 5808\nTrain data size after ALL augmentation: 5808\n-------------------- Map Tokenize Function --------------------\n\n\n100%|█████████████████████████████████████| 6649/6649 [00:01&lt;00:00, 3621.52it/s]\n100%|█████████████████████████████████████| 2904/2904 [00:00&lt;00:00, 9960.57it/s]\n\n\n\n\n\n\n\n\n\nmain_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 5808\n    })\n    validation: Dataset({\n        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 4885\n    })\n})\n\n\n\nprint(main_ddict['validation']['label'][:2])\n\n[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
  },
  {
    "objectID": "deprecated/model_main_envibert_multilabel-copy1.html#train-envibert-with-hidden-layer-concatenation-using-tdm",
    "href": "deprecated/model_main_envibert_multilabel-copy1.html#train-envibert-with-hidden-layer-concatenation-using-tdm",
    "title": "Model Controller Tutorial: EnviBert model (Multi Label)",
    "section": "Train EnviBert (with hidden layer concatenation), using TDM",
    "text": "Train EnviBert (with hidden layer concatenation), using TDM\n\nmodel_name='nguyenvulebinh/envibert'\nenvibert_body = RobertaModel.from_pretrained(model_name)\nnum_classes = len(tdm.label_lists[0])\n\nSome weights of the model checkpoint at nguyenvulebinh/envibert were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\nLet’s create our model controller\n\n_model_kwargs={\n    # overall model hyperparams\n    'layer2concat':4,\n    'is_multilabel':tdm.is_multilabel, # False\n    'is_multihead':tdm.is_multihead, # False\n    'head_class_sizes':num_classes,\n    'head_class': ConcatHeadSimple,\n    # classfication head hyperparams\n    'classifier_dropout':0.1 \n}\n\nmodel = model_init_classification(model_class = RobertaHiddenStateConcatForSequenceClassification,\n                                  cpoint_path = model_name, \n                                  output_hidden_states=True, # since we are not using 'hidden layer contatenation' technique\n                                  seed=42,\n                                  body_model=envibert_body,\n                                  model_kwargs = _model_kwargs)\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score] # we will use both f1_macro and accuracy score as metrics\ncontroller = ModelController(model,tdm,metric_funcs)\n\nLoading body weights. This assumes the body is the very first first-layer block of your custom architecture\n\n\nAnd we can start training our model. Pay attention to the compute_metrics function\n\nlr = 6e-5\nbs=4\nwd=0.01\nepochs= 4\n\ncontroller.fit(epochs,lr,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=partial(compute_metrics_classification,\n                                       is_multilabel=tdm.is_multilabel,\n                                       multilabel_threshold=0.5)\n              )\n# Epoch Training Loss   Validation Loss F1 Score L1l2   Accuracy Score L1l2\n# 1 No log  0.082577    0.028576    0.111771\n# 2 0.090200    0.076037    0.057607    0.223337\n# 3 0.090200    0.072825    0.088066    0.245241\n# 4 0.023500    0.073391    0.095394    0.253019\n\n/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n\n\n\n\n    \n      \n      \n      [2904/2904 03:23, Epoch 4/4]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1 Score L1l2\nAccuracy Score L1l2\n\n\n\n\n1\nNo log\n0.085917\n0.021178\n0.111771\n\n\n2\n0.098200\n0.078571\n0.053714\n0.210850\n\n\n3\n0.098200\n0.074122\n0.078145\n0.231116\n\n\n4\n0.027900\n0.073689\n0.079069\n0.237666\n\n\n\n\n\n\n\n\ncontroller.trainer.model.save_pretrained('./sample_weights/my_model')"
  },
  {
    "objectID": "deprecated/model_main_envibert_multilabel-copy1.html#predict-using-trained-model-using-tdm",
    "href": "deprecated/model_main_envibert_multilabel-copy1.html#predict-using-trained-model-using-tdm",
    "title": "Model Controller Tutorial: EnviBert model (Multi Label)",
    "section": "Predict using trained model, using TDM",
    "text": "Predict using trained model, using TDM\n\nLoad trained model\n\n_model_kwargs\n\n{'layer2concat': 4,\n 'is_multilabel': True,\n 'is_multihead': False,\n 'head_class_sizes': 76,\n 'head_class': that_nlp_library.models.roberta.classifiers.ConcatHeadSimple,\n 'classifier_dropout': 0.1}\n\n\n\ntrained_model = model_init_classification(model_class = RobertaHiddenStateConcatForSequenceClassification,\n                                          cpoint_path = Path('./sample_weights/my_model'), \n                                          output_hidden_states=True,\n                                          seed=42,\n                                          model_kwargs = _model_kwargs)\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score]\ncontroller = ModelController(trained_model,tdm,metric_funcs)\n\nSome weights of the model checkpoint at sample_weights/my_model were not used when initializing RobertaHiddenStateConcatForSequenceClassification: ['body_model.pooler.dense.weight', 'body_model.pooler.dense.bias']\n- This IS expected if you are initializing RobertaHiddenStateConcatForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaHiddenStateConcatForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\n\nPredict Train/Validation set\nMake prediction on all validation set. Note that you can set the probability threshold.\n\ndf_val = controller.predict_ddict(ds_type='validation',multilabel_threshold=0.5,batch_size=8)\n\n-------------------- Start making predictions --------------------\n\n\n\n\n\n\ndf_val.head()\n\n\n\n\n\n\n\n\n\ntext\nlabel\nSource\npred_L1L2\npred_prob_L1L2\npred_L1L2_string\n\n\n\n\n0\ngoogle play - lam phien\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...\ngoogle play\n[False, False, False, False, False, False, Fal...\n[0.00043046146, 0.00057243364, 0.0015474476, 0...\nCannot defined,Others\n\n\n1\ngoogle play - .. t . À mà họ nữ ưu m\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...\ngoogle play\n[False, False, False, False, False, False, Fal...\n[0.00048226005, 0.0006290678, 0.001644549, 0.0...\nCannot defined,Others\n\n\n2\ngoogle play - Cc lùa dao\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...\ngoogle play\n[False, False, False, False, False, False, Fal...\n[0.0004853605, 0.00060451776, 0.0016006823, 0....\nCannot defined,Others\n\n\n3\ngoogle play - Mặt hàng sp mình đều nhỡ với Gia...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\ngoogle play\n[False, False, False, False, False, False, Fal...\n[0.004979573, 0.007575649, 0.0147191025, 0.001...\nDelivery\n\n\n4\ngoogle play - Chưa tối ưu tốt cho Android Oppo...\n[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\ngoogle play\n[False, False, False, False, False, False, Fal...\n[0.00052510685, 0.0007405444, 0.0019072617, 0....\nCannot defined,Others\n\n\n\n\n\n\n\n\nTo convert the label index to string, we can use the label_lists attribute of tdm\n\nimport pandas as pd\nimport numpy as np\n\n\ndef get_label_str_multilabel(row):\n    indices=np.where(row==1)[0]\n    return ','.join([tdm.label_lists[0][i] for i in indices])\n\n\ndf_val['label_str']=df_val['label'].apply(get_label_str_multilabel)\n\n\ndf_val[['label_str','pred_L1L2_string']]\n\n\n\n\n\n\n\n\n\nlabel_str\npred_L1L2_string\n\n\n\n\n0\nCannot defined,Others\nCannot defined,Others\n\n\n1\nCannot defined,Others\nCannot defined,Others\n\n\n2\nCannot defined,Others\nCannot defined,Others\n\n\n3\nDelivery,Delivery time\nDelivery\n\n\n4\nApp performance,Feature\nCannot defined,Others\n\n\n...\n...\n...\n\n\n4880\nCommercial,Flash Sale/Campaigns\nCannot defined,Others\n\n\n4881\nDelivery,Driver\n\n\n\n4882\nOther payment methods,Payment\nCannot defined,Others\n\n\n4883\nCommercial,Flash Sale/Campaigns\nCannot defined,Others\n\n\n4884\nFeature,Shopee coins\nDelivery\n\n\n\n\n4885 rows × 2 columns\n\n\n\n\n\n\nPredict Test set\nWe will go through details on how to make a prediction on a completely new and raw dataset using our trained model. For now, let’s reuse the sample csv and pretend it’s our test set\n\ndf_test = TextDataMain.from_csv(Path('sample_data')/'sample_large.csv',return_df=True)\n\n----- Input Validation Precheck -----\nDataFrame contains duplicated values!\n-----&gt; Number of duplications: 16 rows\n\n\nWe will remove all the labels and unnecessary columns\n\ndf_test = df_test.drop(['L1','L2'],axis=1)\n\n\ndf_test.head()\n\n\n\n\n\n\n\n\n\nSource\nContent\n\n\n\n\n0\nGoogle Play\nApp ncc lúc nào cx lag đơ, phần tìm kiếm thì v...\n\n\n1\nNon Owned\n..❗️ GÓC THANH LÝ Tính ra rẻ hơn cả mua #Shope...\n\n\n2\nGoogle Play\nMắc gì người ta đặt hàng toàn lỗi 😃????\n\n\n3\nOwned\n#GhienShopeePayawardT8 Khi bạn chơi shopee quá...\n\n\n4\nGoogle Play\nRất bức xúc khi dùng . mã giảm giá người dùng ...\n\n\n\n\n\n\n\n\nWe will create a DatasetDict for this test dataframe\n\ntest_ddict = tdm.get_test_datasetdict_from_df(df_test)\n\n-------------------- Getting Test Set --------------------\n----- Input Validation Precheck -----\nDataFrame contains duplicated values!\n-----&gt; Number of duplications: 19 rows\n-------------------- Start Test Set Transformation --------------------\n----- Metadata Simple Processing & Concatenating to Main Content -----\n-------------------- Text Transformation --------------------\n----- text_normalize -----\n-------------------- Test Leak Checking --------------------\n- Before leak check\nSize: 2269\n- After leak check\nSize: 2080\n- Number of rows leaked: 189, or 8.33% of the original validation (or test) data\n-------------------- Construct DatasetDict --------------------\n\n\n100%|█████████████████████████████████████| 2269/2269 [00:00&lt;00:00, 3972.37it/s]\n\n\n\n\n\n\ntest_ddict\n\nDatasetDict({\n    test: Dataset({\n        features: ['text', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 2269\n    })\n})\n\n\nOur test data has been processed + transformed (but not augmented) the same way as the validation set. Now we can start making the prediction\n\n# controller = ModelController(model,tdm)\ndf_result = controller.predict_ddict(ddict=test_ddict,ds_type='test',multilabel_threshold=0.5)\n\n-------------------- Start making predictions --------------------\n\n\n\n\n\n\ndf_result.head()\n\n\n\n\n\n\n\n\n\ntext\nSource\npred_L1L2\npred_prob_L1L2\npred_L1L2_string\n\n\n\n\n0\ngoogle play - App ncc lúc nào cx lag đơ , phần...\ngoogle play\n[False, False, False, False, False, True, Fals...\n[0.0025301736, 0.0055451845, 0.044778068, 0.00...\nApp performance,Feature\n\n\n1\nnon owned - .. ❗ ️ GÓC THANH LÝ Tính ra rẻ hơn...\nnon owned\n[False, False, False, False, False, False, Fal...\n[0.00043563428, 0.00053482514, 0.0008672758, 0...\nCannot defined,Others\n\n\n2\ngoogle play - Mắc gì người ta đặt hàng toàn lỗ...\ngoogle play\n[False, False, False, False, False, False, Fal...\n[0.0018123146, 0.0027692849, 0.0063686953, 0.0...\n\n\n\n3\nowned - # GhienShopeePayawardT8 Khi bạn chơi s...\nowned\n[False, False, False, False, False, False, Fal...\n[0.0011939979, 0.0015298241, 0.0022472697, 0.0...\nCommercial,Shopee Programs\n\n\n4\ngoogle play - Rất bức xúc khi dùng . mã giảm g...\ngoogle play\n[False, False, False, False, False, False, Fal...\n[0.002711329, 0.006989172, 0.05613184, 0.00163...\nFeature\n\n\n\n\n\n\n\n\nIf we just want to make a prediction on a small amount of data (single sentence, or a few sentences), we can use ModelController.predict_raw_text\n\n# Since we have some metadatas, we need to define a dictionary (to imitate a DatasetDict)\nraw_content={\n    'Source': 'Google play',\n    'Content':'Tôi không thích Shopee.Tại vì dùng app rất chậm,lag banh nhà lầu, thậm chí log in còn không đc'\n}\n\nIf we don’t use metadata, we can use something like this:\nraw_content='Tôi không thích Shopee.Tại vì dùng app rất chậm,lag banh nhà lầu, thậm chí log in còn không đc'\n\ndf_result = controller.predict_raw_text(raw_content,multilabel_threshold=0.5)\ndf_result\n\n100%|███████████████████████████████████████████| 1/1 [00:00&lt;00:00, 4951.95it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntext\nSource\npred_L1L2\npred_prob_L1L2\npred_L1L2_string\n\n\n\n\n0\ngoogle play - Tôi không thích Shopee . Tại vì ...\ngoogle play\n[False, False, False, False, False, True, Fals...\n[0.0017681926, 0.003631191, 0.018489178, 0.001...\nApp performance,Feature"
  },
  {
    "objectID": "deprecated/model_main[deprecated].html",
    "href": "deprecated/model_main[deprecated].html",
    "title": "Model Main Functions and Controller",
    "section": "",
    "text": "from nbdev.showdoc import *\n\n\nsource\n\nmodel_init_classification\n\n model_init_classification (model_class, cpoint_path,\n                            output_hidden_states:bool, device=None,\n                            seed=42, body_model=None, model_kwargs={})\n\nTo initialize a classification model, either from an existing HuggingFace model or custom architecture\nCan be used for binary, multi-class single-head, multi-class “two-head”, and multi-label clasisifcation\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel_class\n\n\nModel’s class object, e.g. RobertaHiddenStateConcatForSequenceClassification\n\n\ncpoint_path\n\n\nEither model string name on HuggingFace, or the path to model checkpoint\n\n\noutput_hidden_states\nbool\n\nTo whether output the model hidden states or not. Useful when you try to build a custom classification head\n\n\ndevice\nNoneType\nNone\nDevice to train on\n\n\nseed\nint\n42\nRandom seed\n\n\nbody_model\nNoneType\nNone\nIf not none, we use this to initialize model’s body. If you only want to load the model checkpoint in cpoint_path, leave this as none\n\n\nmodel_kwargs\ndict\n{}\nKeyword arguments for model (both head and body)\n\n\n\n\nsource\n\n\ncompute_metrics_classification\n\n compute_metrics_classification (pred, metric_funcs=[], head_sizes=[],\n                                 label_names=[], is_multilabel=False,\n                                 multilabel_threshold=0.5)\n\nReturn a dictionary of metric name and its values. Can handle both multiclass and multilabel\nReference: https://github.com/huggingface/transformers/blob/dbc12269ed5546b2da9236b9f1078b95b6a4d3d5/src/transformers/trainer_utils.py#LL100C22-L100C22\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npred\n\n\nAn EvalPrediction object from HuggingFace (which is a named tuple with predictions and label_ids attributes)\n\n\nmetric_funcs\nlist\n[]\nA list of metric functions to evaluate\n\n\nhead_sizes\nlist\n[]\nClass size for each head,\n\n\nlabel_names\nlist\n[]\nNames of the label (dependent variable) columns\n\n\nis_multilabel\nbool\nFalse\nWhether this is a multilabel classification\n\n\nmultilabel_threshold\nfloat\n0.5\nThreshold for multilabel (&gt;= threshold is positive)\n\n\n\n\ndef compute_metrics_separate_singleheads(pred, # An EvalPrediction object from HuggingFace (which is a named tuple with ```predictions``` and ```label_ids``` attributes)\n                              metric_funcs=[], # A list of metric functions to evaluate\n                              label_names=[], # Names of the label (dependent variable) columns\n                              **kwargs\n                             ):\n    \"\"\"\n    Return a dictionary of metric name and its values. Can handle both multiclass and multilabel\n    \"\"\"\n    # pred: EvalPrediction object \n    # (which is a named tuple with predictions and label_ids attributes)\n    labels = pred.label_ids # (bs,number of head separately)\n    assert labels.shape[1]==len(label_names)\n    \n    results={}\n    metric_funcs = val2iterable(metric_funcs)\n    \n    for i in range(len(label_names)):\n        _label = labels[:,i]\n        _pred = pred.predictions[i].argmax(-1)\n        for m_func in metric_funcs:\n            m_name = callable_name(m_func)\n            results[f'{m_name}_{label_names[i]}']=m_func(_label,_pred)\n    \n    return results\n\n\nsource\n\n\ncompute_metrics_separate_singleheads\n\n compute_metrics_separate_singleheads (pred, metric_funcs=[],\n                                       label_names=[], **kwargs)\n\nReturn a dictionary of metric name and its values. Can handle both multiclass and multilabel\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npred\n\n\nAn EvalPrediction object from HuggingFace (which is a named tuple with predictions and label_ids attributes)\n\n\nmetric_funcs\nlist\n[]\nA list of metric functions to evaluate\n\n\nlabel_names\nlist\n[]\nNames of the label (dependent variable) columns\n\n\nkwargs\n\n\n\n\n\n\n\ndef loss_for_classification(logits, # output of the last linear layer, before any softmax/sigmoid. Size: (bs,class_size)\n                            labels, # determined by your datasetdict. Size: (bs,number_of_head)\n                            is_multilabel=False, # Whether this is a multilabel classification\n                            is_multihead=False, # Whether this is a multihead (multi-level) classification\n                            head_sizes=[], # class size for each head\n                            head_weights=[], # loss weight for each head\n                           ):\n    \"\"\"\n    The general loss function for classification\n    \n    - If is_multilabel is ```False``` and is_multihead is ```False```: One-Head Classification, e.g. You predict 1 out of n class\n    \n    - If is_multilabel is ```False``` and is_multihead is ```True```: Multi-Head Classification, e.g. You predict 1 out of n classes at Level 1, \n    and 1 out of m classes at Level 2\n    \n    - If is_multilabel is ```True``` and is_multihead is ```False```: One-Head Multi-Label Classification, e.g. You predict x out of n class (x&gt;=1)\n    \n    - If is_multilabel is ```True``` and is_multihead is ```True```: Not supported!\n    \n    \"\"\"\n    if is_multilabel and is_multihead: raise ValueError('Multi-Label and Multi-Head problem is not supported')\n    head_sizes = val2iterable(head_sizes)\n    loss=0\n    if not is_multilabel:\n        if not is_multihead:\n            loss_fct = torch.nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, head_sizes[0]), labels.view(-1))\n        else:\n            assert len(head_sizes)==len(head_weights),\"For MultiHead, make sure len of head_sizes and head_weights equal\"\n            for i,(_size,_weight) in enumerate(zip(head_sizes,head_weights)):\n                start= 0 if i==0 else start+head_sizes[i-1]\n                end = start + _size\n                loss_fct = torch.nn.CrossEntropyLoss()\n                loss = loss + _weight*loss_fct(logits[:,start:end].view(-1,_size),\n                                               labels[:,i].view(-1))\n    else:\n        if not is_multihead:\n            loss_fct = torch.nn.BCEWithLogitsLoss()\n#             label_1hot = torch.nn.functional.one_hot(labels.view(-1),num_classes=head_sizes[0])\n            loss = loss_fct(logits,\n                            labels.float())\n        else:\n            raise ValueError('Multi-Head with Multi-Label classification is not supported. Have you lost your mind?')\n#             assert len(head_sizes)==len(head_weights),\"For MultiHead, make sure len of head_sizes and head_weights equal\"\n#             for i,(_size,_weight) in enumerate(zip(head_sizes,head_weights)):\n#                 start= 0 if i==0 else start+head_sizes[i-1]\n#                 end = start + _size\n#                 loss_fct = torch.nn.BCEWithLogitsLoss()\n#                 loss = loss + _weight*loss_fct(logits[:,start:end].view(-1,_size),\n#                                                torch.nn.functional.one_hot(labels[:,i].view(-1),num_classes=_size).float()\n#                                               )\n            \n    return loss\n\n\nsource\n\n\nloss_for_classification\n\n loss_for_classification (logits, labels, is_multilabel=False,\n                          is_multihead=False, head_sizes=[],\n                          head_weights=[])\n\nThe general loss function for classification\n\nIf is_multilabel is False and is_multihead is False: One-Head Classification, e.g. You predict 1 out of n class\nIf is_multilabel is False and is_multihead is True: Multi-Head Classification, e.g. You predict 1 out of n classes at Level 1, and 1 out of m classes at Level 2\nIf is_multilabel is True and is_multihead is False: One-Head Multi-Label Classification, e.g. You predict x out of n class (x&gt;=1)\nIf is_multilabel is True and is_multihead is True: Not supported!\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlogits\n\n\noutput of the last linear layer, before any softmax/sigmoid. Size: (bs,class_size)\n\n\nlabels\n\n\ndetermined by your datasetdict. Size: (bs,number_of_head)\n\n\nis_multilabel\nbool\nFalse\nWhether this is a multilabel classification\n\n\nis_multihead\nbool\nFalse\nWhether this is a multihead (multi-level) classification\n\n\nhead_sizes\nlist\n[]\nclass size for each head\n\n\nhead_weights\nlist\n[]\nloss weight for each head\n\n\n\n\ndef finetune(lr, # Learning rate\n             bs, # Batch size\n             wd, # Weight decay\n             epochs, # Number of epochs\n             ddict, # The HuggingFace datasetdict\n             tokenizer,# HuggingFace tokenizer\n             o_dir = './tmp_weights', # Directory to save weights\n             save_checkpoint=False, # Whether to save weights (checkpoints) to o_dir\n             model=None, # NLP model\n             model_init=None, # A function to initialize model\n             data_collator=None, # HuggingFace data collator\n             compute_metrics=None, # A function to compute metric, e.g. `compute_metrics_classification`\n             grad_accum_steps=2, # The batch at each step will be divided by this integer and gradient will be accumulated over gradient_accumulation_steps steps.\n             lr_scheduler_type='cosine',  # The scheduler type to use. Including: linear, cosine, cosine_with_restarts, polynomial, constant, constant_with_warmup\n             warmup_ratio=0.1, # The warmup ratio for some lr scheduler\n             no_valid=False, # Whether there is a validation set or not\n             seed=42, # Random seed\n             report_to='none' # The list of integrations to report the results and logs to. Supported platforms are \"azure_ml\", \"comet_ml\", \"mlflow\", \"neptune\", \"tensorboard\",\"clearml\" and \"wandb\". Use \"all\" to report to all integrations installed, \"none\" for no integrations.\n            ):\n    \"The main model training/finetuning function\"\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    seed_everything(seed)\n    training_args = TrainingArguments(o_dir, \n                                learning_rate=lr, \n                                warmup_ratio=warmup_ratio if lr_scheduler_type!='linear' else 0.0, \n                                lr_scheduler_type=lr_scheduler_type, \n                                fp16=True,\n                                do_train=True,\n                                do_eval= not no_valid,\n                                evaluation_strategy=\"no\" if no_valid else \"epoch\", \n                                save_strategy=\"epoch\" if save_checkpoint else 'no',\n                                overwrite_output_dir=True,\n                                gradient_accumulation_steps=grad_accum_steps,\n                                per_device_train_batch_size=bs, \n                                per_device_eval_batch_size=bs,\n                                num_train_epochs=epochs, weight_decay=wd,\n                                report_to=report_to,\n                                logging_dir=os.path.join(o_dir, 'log') if report_to!='none' else None,\n                                logging_steps = len(ddict[\"train\"]) // bs,\n                                )\n\n    # instantiate trainer\n    trainer = Trainer(\n        model=model,\n        model_init=model_init if model is None else None,\n        args=training_args,\n        train_dataset=ddict['train'],#.shard(200, 0),    # Only use subset of the dataset for a quick training. Remove shard for full training\n        eval_dataset=ddict['validation'] if not no_valid else None,#.shard(100, 0), # Only use subset of the dataset for a quick training. Remove shard for full training\n        data_collator=data_collator,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics,\n    )\n    \n    \n    trainer.train()\n    return trainer\n\n\nsource\n\n\nfinetune\n\n finetune (lr, bs, wd, epochs, ddict, tokenizer, o_dir='./tmp_weights',\n           save_checkpoint=False, model=None, model_init=None,\n           data_collator=None, compute_metrics=None, grad_accum_steps=2,\n           lr_scheduler_type='cosine', warmup_ratio=0.1, no_valid=False,\n           seed=42, report_to='none')\n\nThe main model training/finetuning function\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlr\n\n\nLearning rate\n\n\nbs\n\n\nBatch size\n\n\nwd\n\n\nWeight decay\n\n\nepochs\n\n\nNumber of epochs\n\n\nddict\n\n\nThe HuggingFace datasetdict\n\n\ntokenizer\n\n\nHuggingFace tokenizer\n\n\no_dir\nstr\n./tmp_weights\nDirectory to save weights\n\n\nsave_checkpoint\nbool\nFalse\nWhether to save weights (checkpoints) to o_dir\n\n\nmodel\nNoneType\nNone\nNLP model\n\n\nmodel_init\nNoneType\nNone\nA function to initialize model\n\n\ndata_collator\nNoneType\nNone\nHuggingFace data collator\n\n\ncompute_metrics\nNoneType\nNone\nA function to compute metric, e.g. compute_metrics_classification\n\n\ngrad_accum_steps\nint\n2\nThe batch at each step will be divided by this integer and gradient will be accumulated over gradient_accumulation_steps steps.\n\n\nlr_scheduler_type\nstr\ncosine\nThe scheduler type to use. Including: linear, cosine, cosine_with_restarts, polynomial, constant, constant_with_warmup\n\n\nwarmup_ratio\nfloat\n0.1\nThe warmup ratio for some lr scheduler\n\n\nno_valid\nbool\nFalse\nWhether there is a validation set or not\n\n\nseed\nint\n42\nRandom seed\n\n\nreport_to\nstr\nnone\nThe list of integrations to report the results and logs to. Supported platforms are “azure_ml”, “comet_ml”, “mlflow”, “neptune”, “tensorboard”,“clearml” and “wandb”. Use “all” to report to all integrations installed, “none” for no integrations.\n\n\n\n\ndef _forward_pass_for_predictions(batch,\n                                 model=None, # NLP model\n                                 topk=1, # Number of labels to return for each head\n                                 is_multilabel=False, # Is this a multilabel classification?\n                                 multilabel_threshold=0.5, # The threshold for multilabel classification\n                                 tokenizer=None, # HuggingFace tokenizer\n                                 data_collator=None, # HuggingFace data collator\n                                 cols_to_remove=[], # list of keys (columns) to remove from ```batch```\n                                 label_names=[], # Names of the label columns\n                                 label_sizes=[], # Size of each label\n                                 device = None, # device that the model is trained on\n                                 is_dhc=False\n                                 ):\n    if data_collator is not None:\n# --- Convert from  \n# {'input_ids': [tensor([    0, 10444,   244, 14585,   125,  2948,  5925,   368,     2]), \n#                tensor([    0, 16098,  2913,   244,   135,   198, 34629,  6356,     2])]\n# 'attention_mask': [tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), \n#                    tensor([1, 1, 1, 1, 1, 1, 1, 1, 1])]\n#                    }\n# --- to\n#         [{'input_ids': tensor([    0, 10444,   244, 14585,   125,  2948,  5925,   368,     2]),\n#           'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1])},\n#          {'input_ids': tensor([    0, 16098,  2913,   244,   135,   198, 34629,  6356,     2]),\n#           'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1])}]\n\n        # remove string text, due to transformer new version       \n        collator_inp = []\n        ks = [k for k in batch.keys() if k not in cols_to_remove]\n        vs = [batch[k] for k in ks]\n        for pair in zip(*vs):\n            collator_inp.append({k:v for k,v in zip(ks,pair)})\n        \n        batch = data_collator(collator_inp)\n    \n    inputs = {k:v.to(device) for k,v in batch.items()\n              if k in tokenizer.model_input_names}\n    \n    _f = partial(torch.nn.functional.softmax,dim=1) if not is_multilabel else torch.sigmoid\n    \n    # switch to eval mode for evaluation\n    if model.training:\n        model.eval()\n    with torch.no_grad():\n        outputs = model(**inputs)\n        outputs_logits = outputs.logits\n        outputs_list=[]\n        if is_dhc:\n            # outputs_logits will be a list of n (typically 2) heads' logits, each has shape (bs,class_size)\n            for i in range(len(label_names)):\n                outputs_list.append(_f(outputs_logits[i].cpu()))\n        else:\n            # outputs_logits will have shape (bs,sum of all class sizes). We split into each class\n            outputs_logits = outputs_logits.cpu()\n            _s=0\n            _e=label_sizes[0]\n            for i in range(len(label_names)):\n                outputs_list.append(_f(outputs_logits[:,_s:_e]))\n                _s+=label_sizes[i]\n                _e+=label_sizes[i+1] if i+1&lt;len(label_names) else 0\n        \n        # save prediction and probability\n        pred_label_list=[]\n        pred_prob_list=[]\n        if is_multilabel:\n            for i in range(len(label_names)):\n                pred_label_list.append(outputs_list[i]&gt;=multilabel_threshold)\n                pred_prob_list.append(outputs_list[i])\n        else:\n            for i in range(len(label_names)):\n                _p,_l = torch.topk(outputs_list[i],topk,dim=-1)\n                if topk==1:\n                    _l,_p = _l[:,0],_p[:,0]\n                pred_label_list.append(_l)\n                pred_prob_list.append(_p)\n    \n    # Switch back to train mode\n    if not model.training:\n        model.train()\n        \n    results={}\n    for i in range(len(label_names)):\n        results[f'pred_{label_names[i]}']= pred_label_list[i].numpy()\n        results[f'pred_prob_{label_names[i]}']= pred_prob_list[i].numpy()\n    return results\n\n\nclass ModelController():\n    def __init__(self,\n                 model, # NLP model\n                 data_store:TextDataMain=None, # a TextDataMain object\n                 metric_funcs=[accuracy_score], # Metric function (can be from Sklearn)\n                 seed=42, # Random seed\n                ):\n        self.model = model\n        self.data_store = data_store\n        self.metric_funcs = metric_funcs\n        self.seed = seed\n        \n    def fit(self,\n            epochs, # Number of epochs\n            learning_rate, # Learning rate\n            ddict=None, # DatasetDict to fit (will override data_store)\n            batch_size=16, # Batch size\n            weight_decay=0.01, # Weight decay\n            lr_scheduler_type='cosine', # The scheduler type to use. Including: linear, cosine, cosine_with_restarts, polynomial, constant, constant_with_warmup\n            warmup_ratio=0.1, # The warmup ratio for some lr scheduler\n            o_dir = './tmp_weights', # Directory to save weights\n            save_checkpoint=False, # Whether to save weights (checkpoints) to o_dir\n            hf_report_to='none', # The list of HuggingFace-allowed integrations to report the results and logs to\n            compute_metrics=None, # A function to compute metric, e.g. `compute_metrics_classification` which utilizes the given ```metric_funcs``` \n            grad_accum_steps=2, # Gradient will be accumulated over gradient_accumulation_steps steps.\n            tokenizer=None, # Tokenizer (to override one in ```data_store```)\n            data_collator=None, # Data Collator (to override one in ```data_store```)\n            label_names=None, # Names of the label (dependent variable) columns (to override one in ```data_store```)\n            head_sizes=None, # Class size for each head (to override one in ```model```)\n           ):\n        \n        if tokenizer is None: tokenizer=check_and_get_attribute(self.data_store,'tokenizer')\n        if data_collator is None: data_collator=getattr(self.data_store,'data_collator',None)\n        if ddict is None: ddict = check_and_get_attribute(self.data_store,'main_ddict')\n            \n        if label_names is None: label_names=check_and_get_attribute(self.data_store,'label_names')\n        label_names = val2iterable(label_names)\n        \n        if head_sizes is None: head_sizes=check_and_get_attribute(self.model,'head_class_sizes')\n        head_sizes = val2iterable(head_sizes)\n        \n        if len(set(ddict.keys()) & set(['train','training']))==0:\n            raise ValueError(\"Missing the following key for DatasetDict: train/training\")\n        no_valid = len(set(ddict.keys()) & set(['validation','val']))==0\n\n        _compute_metrics = partial(compute_metrics,\n                                   metric_funcs=self.metric_funcs,\n                                   head_sizes=head_sizes,\n                                   label_names=label_names \n                                  )\n        \n\n        trainer = finetune(learning_rate,batch_size,weight_decay,epochs,\n                           ddict,tokenizer,o_dir,\n                           save_checkpoint=save_checkpoint,\n                           model=self.model,\n                           data_collator=data_collator,\n                           compute_metrics=_compute_metrics,\n                           grad_accum_steps=grad_accum_steps,\n                           lr_scheduler_type=lr_scheduler_type,\n                           warmup_ratio=warmup_ratio,\n                           no_valid=no_valid,\n                           seed=self.seed,\n                           report_to=hf_report_to)\n        self.trainer = trainer\n        \n    def predict_raw_text(self,\n                         content:dict|list|str, # Either a single sentence, list of sentence or a dictionary with keys are metadata, values are list\n                         batch_size=1, # Batch size. For a small amount of texts, you might want to keep this small\n                         is_multilabel=None, # Is this a multilabel classification?\n                         multilabel_threshold=0.5, # Threshold for multilabel classification\n                         topk=1, # Number of labels to return for each head\n                         is_dhc=False # Are outpuf (of model) separate heads?\n                        ):\n        if not isinstance(self.data_store,TextDataMain) or not self.data_store._main_called:\n            raise ValueError('This functionality needs a TextDataMain object which has already processed some training data')\n        with HiddenPrints():\n            test_ddict = self.data_store.get_test_datasetdict_from_dict(content)\n            df_result =  self.predict_ddict(ddict=test_ddict,\n                                            ds_type='test',\n                                            batch_size=batch_size,\n                                            is_multilabel=is_multilabel,\n                                            multilabel_threshold=multilabel_threshold,\n                                            topk=topk,\n                                            is_dhc=is_dhc\n                                           )\n        return df_result\n    \n    def predict_ddict(self,\n                      ddict=None, # DatasetDict to predict (will override ```data_store```)\n                      ds_type='test', # Keys of DatasetDict to predict\n                      batch_size=16, # Batch size\n                      is_multilabel=None, # Is this a multilabel classification?\n                      multilabel_threshold=0.5, # Threshold for multilabel classification\n                      topk=1, # Number of labels to return for each head\n                      tokenizer=None, # Tokenizer (to override one in ```data_store```)\n                      data_collator=None, # Data Collator (to override one in ```data_store```)\n                      label_names=None, # Names of the label (dependent variable) columns (to override one in ```data_store```)\n                      class_names_predefined=None, # List of names associated with the labels (same index order) (to override one in ```data_store```)\n                      device=None, # Device that the model is trained on\n                      is_dhc=False # Are outpuf (of model) separate heads?\n                     ):\n        if device is None: device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        if is_multilabel is None: is_multilabel=getattr(self.model,'is_multilabel',False)\n        label_lists = class_names_predefined\n        \n        if tokenizer is None: tokenizer=check_and_get_attribute(self.data_store,'tokenizer')\n        if data_collator is None: data_collator=getattr(self.data_store,'data_collator',None)\n        if label_names is None: label_names=check_and_get_attribute(self.data_store,'label_names')\n        if label_lists is None: label_lists = check_and_get_attribute(self.data_store,'label_lists')\n        if not isinstance(label_names,list):\n            label_names=[label_names]\n        if not isinstance(label_lists[0],list):\n            label_lists=[label_lists]    \n            \n        label_sizes = [len(cs) for cs in label_lists]\n        if ddict is None: ddict = check_and_get_attribute(self.data_store,'main_ddict') \n        if ds_type not in ddict.keys():\n            raise ValueError(f'{ds_type} is not in the given DatasetDict keys')\n        \n        ddict.set_format(\"torch\",\n                        columns=[\"input_ids\", \"attention_mask\"])\n        \n        print_msg('Start making predictions',20)\n        preserved_kws=['input_ids', 'token_type_ids', 'attention_mask','label'] # hard-coded\n        cols_to_remove = (set(ddict[ds_type].features.keys()) - set(preserved_kws)) | {'text'}\n        ddict[ds_type] = ddict[ds_type].map(\n            partial(_forward_pass_for_predictions,model=self.model,\n                    topk=topk,\n                    is_multilabel=is_multilabel,\n                    multilabel_threshold=multilabel_threshold,\n                    tokenizer=tokenizer,\n                    data_collator=data_collator,\n                    cols_to_remove=cols_to_remove,\n                   label_names=label_names,\n                    label_sizes=label_sizes,\n                    is_dhc = is_dhc,\n                    device=device\n                   ), \n            batched=True, batch_size=batch_size)\n    \n        ddict.set_format(\"pandas\")\n        \n        df_result = ddict[ds_type][:]\n        cols_to_keep = [c for c in df_result.columns.values if c not in preserved_kws[:-1]]\n        df_result = df_result.loc[:,cols_to_keep]\n        \n        # convert pred id to string label\n        for i in range(len(label_names)):  \n            if not is_multilabel:\n                if topk==1:\n                    df_result[f'pred_{label_names[i]}'] = df_result[f'pred_{label_names[i]}'].apply(lambda x: label_lists[i][int(x)])\n                else:\n                    df1 = pd.DataFrame(df_result[f'pred_{label_names[i]}'].to_list(),columns=[f'pred_{label_names[i]}_top{j}' for j in range(1,topk+1)])\n                    df1_prob = pd.DataFrame(df_result[f'pred_prob_{label_names[i]}'].to_list(),columns=[f'pred_prob_{label_names[i]}_top{j}' for j in range(1,topk+1)])\n\n                    for j in range(1,topk+1):\n                        df1[f'pred_{label_names[i]}_top{j}'] =  df1[f'pred_{label_names[i]}_top{j}'].apply(lambda x: label_lists[i][int(x)])\n\n                    df_result = pd.concat([df_result,df1,df1_prob],axis=1)\n            else:\n                get_label_str_multilabel = lambda row: ','.join([label_lists[i][int(j)] for j in np.where(row==True)[0]])\n                df_result[f'pred_{label_names[i]}_string'] = df_result[f'pred_{label_names[i]}'].apply(get_label_str_multilabel)\n\n        return df_result\n\n\nsource\n\n\nModelController\n\n ModelController (model,\n                  data_store:that_nlp_library.text_main.TextDataMain=None,\n                  metric_funcs=[&lt;function accuracy_score at\n                  0x7f900acdc700&gt;], seed=42)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel\n\n\nNLP model\n\n\ndata_store\nTextDataMain\nNone\na TextDataMain object\n\n\nmetric_funcs\nlist\n[]\nMetric function (can be from Sklearn)\n\n\nseed\nint\n42\nRandom seed\n\n\n\n\nsource\n\n\nModelController.fit\n\n ModelController.fit (epochs, learning_rate, ddict=None, batch_size=16,\n                      weight_decay=0.01, lr_scheduler_type='cosine',\n                      warmup_ratio=0.1, o_dir='./tmp_weights',\n                      save_checkpoint=False, hf_report_to='none',\n                      compute_metrics=None, grad_accum_steps=2,\n                      tokenizer=None, data_collator=None,\n                      label_names=None, head_sizes=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nepochs\n\n\nNumber of epochs\n\n\nlearning_rate\n\n\nLearning rate\n\n\nddict\nNoneType\nNone\nDatasetDict to fit (will override data_store)\n\n\nbatch_size\nint\n16\nBatch size\n\n\nweight_decay\nfloat\n0.01\nWeight decay\n\n\nlr_scheduler_type\nstr\ncosine\nThe scheduler type to use. Including: linear, cosine, cosine_with_restarts, polynomial, constant, constant_with_warmup\n\n\nwarmup_ratio\nfloat\n0.1\nThe warmup ratio for some lr scheduler\n\n\no_dir\nstr\n./tmp_weights\nDirectory to save weights\n\n\nsave_checkpoint\nbool\nFalse\nWhether to save weights (checkpoints) to o_dir\n\n\nhf_report_to\nstr\nnone\nThe list of HuggingFace-allowed integrations to report the results and logs to\n\n\ncompute_metrics\nNoneType\nNone\nA function to compute metric, e.g. compute_metrics_classification which utilizes the given metric_funcs\n\n\ngrad_accum_steps\nint\n2\nGradient will be accumulated over gradient_accumulation_steps steps.\n\n\ntokenizer\nNoneType\nNone\nTokenizer (to override one in data_store)\n\n\ndata_collator\nNoneType\nNone\nData Collator (to override one in data_store)\n\n\nlabel_names\nNoneType\nNone\nNames of the label (dependent variable) columns (to override one in data_store)\n\n\nhead_sizes\nNoneType\nNone\nClass size for each head (to override one in model)\n\n\n\n\nsource\n\n\nModelController.predict_ddict\n\n ModelController.predict_ddict (ddict=None, ds_type='test', batch_size=16,\n                                is_multilabel=None,\n                                multilabel_threshold=0.5, topk=1,\n                                tokenizer=None, data_collator=None,\n                                label_names=None,\n                                class_names_predefined=None, device=None,\n                                is_dhc=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nddict\nNoneType\nNone\nDatasetDict to predict (will override data_store)\n\n\nds_type\nstr\ntest\nKeys of DatasetDict to predict\n\n\nbatch_size\nint\n16\nBatch size\n\n\nis_multilabel\nNoneType\nNone\nIs this a multilabel classification?\n\n\nmultilabel_threshold\nfloat\n0.5\nThreshold for multilabel classification\n\n\ntopk\nint\n1\nNumber of labels to return for each head\n\n\ntokenizer\nNoneType\nNone\nTokenizer (to override one in data_store)\n\n\ndata_collator\nNoneType\nNone\nData Collator (to override one in data_store)\n\n\nlabel_names\nNoneType\nNone\nNames of the label (dependent variable) columns (to override one in data_store)\n\n\nclass_names_predefined\nNoneType\nNone\nList of names associated with the labels (same index order) (to override one in data_store)\n\n\ndevice\nNoneType\nNone\nDevice that the model is trained on\n\n\nis_dhc\nbool\nFalse\nAre outpuf (of model) separate heads?\n\n\n\n\nsource\n\n\nModelController.predict_raw_text\n\n ModelController.predict_raw_text (content:dict|list|str, batch_size=1,\n                                   is_multilabel=None,\n                                   multilabel_threshold=0.5, topk=1,\n                                   is_dhc=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncontent\ndict | list | str\n\nEither a single sentence, list of sentence or a dictionary with keys are metadata, values are list\n\n\nbatch_size\nint\n1\nBatch size. For a small amount of texts, you might want to keep this small\n\n\nis_multilabel\nNoneType\nNone\nIs this a multilabel classification?\n\n\nmultilabel_threshold\nfloat\n0.5\nThreshold for multilabel classification\n\n\ntopk\nint\n1\nNumber of labels to return for each head\n\n\nis_dhc\nbool\nFalse\nAre outpuf (of model) separate heads?"
  },
  {
    "objectID": "deprecated/text_main[deprecated].html",
    "href": "deprecated/text_main[deprecated].html",
    "title": "Text Main",
    "section": "",
    "text": "from __future__ import annotations\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.preprocessing import LabelEncoder,MultiLabelBinarizer\nfrom datasets import DatasetDict,Dataset\nfrom pathlib import Path\nfrom tqdm import tqdm\nfrom that_nlp_library.utils import *\nfrom functools import partial\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom importlib.machinery import SourceFileLoader\nimport os"
  },
  {
    "objectID": "deprecated/text_main[deprecated].html#text-transformation-and-tokenizer-explanation",
    "href": "deprecated/text_main[deprecated].html#text-transformation-and-tokenizer-explanation",
    "title": "Text Main",
    "section": "Text transformation and Tokenizer Explanation",
    "text": "Text transformation and Tokenizer Explanation\n\ndef tokenizer_explain(inp, # Input sentence\n                      tokenizer, # Tokenizer (preferably from HuggingFace)\n                      split_word=False # Is input `inp` split into list or not\n                     ):\n    \"Display results from tokenizer\"\n    print('----- Tokenizer Explained -----')\n    print('--- Input ---')\n    print(inp)\n    print()\n    print('--- Tokenized results --- ')\n    print(tokenizer(inp,is_split_into_words=split_word))\n    print()\n    tok = tokenizer.encode(inp,is_split_into_words=split_word)\n    print('--- Results from tokenizer.convert_ids_to_tokens ---')\n    print(tokenizer.convert_ids_to_tokens(tok))\n    print()\n    print('--- Results from tokenizer.decode --- ')\n    print(tokenizer.decode(tok))\n    print()\n\n\nsource\n\ntokenizer_explain\n\n tokenizer_explain (inp, tokenizer, split_word=False)\n\nDisplay results from tokenizer\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninp\n\n\nInput sentence\n\n\ntokenizer\n\n\nTokenizer (preferably from HuggingFace)\n\n\nsplit_word\nbool\nFalse\nIs input inp split into list or not\n\n\n\nLet’s load a tokenizer from EnviBert model. Uncomment the command line below to download necessary files to build this tokenizer\n\n# !pip install gdown\n\n\n# !gdown 14X9fGijA7kdNfe4dM_8gqfxIWtj1Q-hb -O ./envibert_cache --folder\n\n\ncache_dir=Path('./envibert_tokenizer')\ntokenizer = SourceFileLoader(\"envibert.tokenizer\", \n                             str(cache_dir/'envibert_tokenizer.py')).load_module().RobertaTokenizer(cache_dir)\n\nNote that Envibert tokenizer does not required the input to be tokenized using word_tokenize from UnderTheSea library\n\ninp = 'hội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức'\ntokenizer_explain(inp,tokenizer)\n\n----- Tokenizer Explained -----\n--- Input ---\nhội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức\n\n--- Tokenized results --- \n{'input_ids': [0, 227, 1033, 191, 664, 1033, 7366, 2615, 13, 664, 1033, 671, 1355, 2294, 993, 413, 2900, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n--- Results from tokenizer.convert_ids_to_tokens ---\n['&lt;s&gt;', '▁hội', '▁cư', '▁dân', '▁chung', '▁cư', '▁sen', '▁hồng', '▁-', '▁chung', '▁cư', '▁lot', 'us', '▁sóng', '▁thần', '▁thủ', '▁đức', '&lt;/s&gt;']\n\n--- Results from tokenizer.decode --- \n&lt;s&gt; ▁hội ▁cư ▁dân ▁chung ▁cư ▁sen ▁hồng ▁- ▁chung ▁cư ▁lot us ▁sóng ▁thần ▁thủ ▁đức &lt;/s&gt;\n\n\n\n\ninp = ['hội', 'cư', 'dân', 'chung', 'cư', 'sen', 'hồng', '-', 'chung', 'cư', 'lotus', 'sóng', 'thần', 'thủ', 'đức']\ntokenizer_explain(inp,tokenizer,split_word=True)\n\n----- Tokenizer Explained -----\n--- Input ---\n['hội', 'cư', 'dân', 'chung', 'cư', 'sen', 'hồng', '-', 'chung', 'cư', 'lotus', 'sóng', 'thần', 'thủ', 'đức']\n\n--- Tokenized results --- \n{'input_ids': [0, 227, 1033, 191, 664, 1033, 7366, 2615, 13, 664, 1033, 671, 1355, 2294, 993, 413, 2900, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n--- Results from tokenizer.convert_ids_to_tokens ---\n['&lt;s&gt;', '▁hội', '▁cư', '▁dân', '▁chung', '▁cư', '▁sen', '▁hồng', '▁-', '▁chung', '▁cư', '▁lot', 'us', '▁sóng', '▁thần', '▁thủ', '▁đức', '&lt;/s&gt;']\n\n--- Results from tokenizer.decode --- \n&lt;s&gt; ▁hội ▁cư ▁dân ▁chung ▁cư ▁sen ▁hồng ▁- ▁chung ▁cư ▁lot us ▁sóng ▁thần ▁thủ ▁đức &lt;/s&gt;\n\n\n\nNow let’s try PhoBert tokenizer. PhoBert tokenizer, unlike Envibert tokenizer, requires input to be word tokenized (using UnderTheSea library)\n\nfrom transformers import AutoTokenizer\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\n\ninp = apply_word_tokenize('hội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức')\nprint(inp)\n\nhội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức\n\n\n\ntokenizer_explain(inp,tokenizer)\n\n----- Tokenizer Explained -----\n--- Input ---\nhội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức\n\n--- Tokenized results --- \n{'input_ids': [0, 1093, 1838, 1574, 3330, 2025, 31, 1574, 2029, 4885, 8554, 25625, 7344, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n--- Results from tokenizer.convert_ids_to_tokens ---\n['&lt;s&gt;', 'hội', 'cư_dân', 'chung_cư', 'sen', 'hồng', '-', 'chung_cư', 'lo@@', 'tus', 'sóng_thần', 'thủ_@@', 'đức', '&lt;/s&gt;']\n\n--- Results from tokenizer.decode --- \n&lt;s&gt; hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức &lt;/s&gt;\n\n\n\n\ndef two_steps_tokenization_explain(inp, # Input sentence\n                                   tokenizer, # Tokenizer (preferably from HuggingFace)\n                                   split_word=False, # Is input `inp` split into list or not\n                                   content_tfms=[] # A list of text transformations\n                                  ):\n    \"Display results form each content transformation, then display results from tokenizer\"\n    print('----- Text Transformation Explained -----')\n    print('--- Raw sentence ---')\n    print(inp)\n    for tfm in content_tfms:\n        print_msg(callable_name(tfm),3)\n        inp = tfm(inp)\n        print(inp)\n    print()\n    tokenizer_explain(inp,tokenizer,split_word)\n\n\nsource\n\n\ntwo_steps_tokenization_explain\n\n two_steps_tokenization_explain (inp, tokenizer, split_word=False,\n                                 content_tfms=[])\n\nDisplay results form each content transformation, then display results from tokenizer\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninp\n\n\nInput sentence\n\n\ntokenizer\n\n\nTokenizer (preferably from HuggingFace)\n\n\nsplit_word\nbool\nFalse\nIs input inp split into list or not\n\n\ncontent_tfms\nlist\n[]\nA list of text transformations\n\n\n\nLet’s load Phobert tokenizer one more time to test out this function\n\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\n\nfrom underthesea import text_normalize\n\n\ninp = 'Hội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức'\ntwo_steps_tokenization_explain(inp,tokenizer,content_tfms=[text_normalize,apply_word_tokenize])\n\n----- Text Transformation Explained -----\n--- Raw sentence ---\nHội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức\n--- text_normalize ---\nHội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức\n--- apply_word_tokenize ---\nHội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức\n\n----- Tokenizer Explained -----\n--- Input ---\nHội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức\n\n--- Tokenized results --- \n{'input_ids': [0, 792, 1838, 1574, 3330, 2025, 31, 1574, 2029, 4885, 8554, 25625, 7344, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n--- Results from tokenizer.convert_ids_to_tokens ---\n['&lt;s&gt;', 'Hội', 'cư_dân', 'chung_cư', 'sen', 'hồng', '-', 'chung_cư', 'lo@@', 'tus', 'sóng_thần', 'thủ_@@', 'đức', '&lt;/s&gt;']\n\n--- Results from tokenizer.decode --- \n&lt;s&gt; Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức &lt;/s&gt;\n\n\n\nThis is a bit redundant, as apply_word_tokenize also have an option to normalize text. Let’s shorten the code:\n\nfrom functools import partial\n\n\ninp = 'Hội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức'\ntwo_steps_tokenization_explain(inp,tokenizer,content_tfms=[partial(apply_word_tokenize,normalize_text=True)])\n\n----- Text Transformation Explained -----\n--- Raw sentence ---\nHội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức\n--- apply_word_tokenize ---\nHội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức\n\n----- Tokenizer Explained -----\n--- Input ---\nHội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức\n\n--- Tokenized results --- \n{'input_ids': [0, 792, 1838, 1574, 3330, 2025, 31, 1574, 2029, 4885, 8554, 25625, 7344, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n--- Results from tokenizer.convert_ids_to_tokens ---\n['&lt;s&gt;', 'Hội', 'cư_dân', 'chung_cư', 'sen', 'hồng', '-', 'chung_cư', 'lo@@', 'tus', 'sóng_thần', 'thủ_@@', 'đức', '&lt;/s&gt;']\n\n--- Results from tokenizer.decode --- \n&lt;s&gt; Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức &lt;/s&gt;"
  },
  {
    "objectID": "deprecated/text_main[deprecated].html#datasetdict",
    "href": "deprecated/text_main[deprecated].html#datasetdict",
    "title": "Text Main",
    "section": "DatasetDict",
    "text": "DatasetDict\n\ndef tokenize_function(examples:dict,\n                      tok,\n                      max_length=None,\n                      is_split_into_words=True):\n    if max_length is None:\n        # pad to model's default max sequence length\n        return tok(examples[\"text\"], padding=\"max_length\", truncation=True,is_split_into_words=is_split_into_words)\n    if isinstance(max_length,int) and max_length&gt;0:\n        # pad to max length of the current batch, and start truncating at max_length\n        return tok(examples[\"text\"], padding=True, max_length=max_length,truncation=True,is_split_into_words=is_split_into_words)\n    \n    # no padding (still truncate at model's default max sequence length)\n    return tok(examples[\"text\"], truncation=True,is_split_into_words=is_split_into_words)\n\n\ndef datasetdictize_given_idxs(kv_pairs:dict, # Dictionary; keys can be content, label, metadata. Values are list each.\n                              trn_idx=None, # Training indices\n                              val_idx=None, # Validation indices\n                              tokenizer=None, # HuggingFace tokenizer\n                              is_split_into_words=False, # Is text (content) split into list or not\n                              max_length=None # pad to model's allowed max length (default is max_sequence_length)\n                             ):\n    \"Create a HuggingFace DatasetDict with given arguments\"\n    if 'text' not in kv_pairs.keys():\n        raise ValueError('Dictionary must have `text` (which contains texture contents) as key')\n    all_dataset = Dataset.from_dict(kv_pairs)\n    main_ddict = DatasetDict()\n    if trn_idx is None:\n        main_ddict['train'] = all_dataset\n    else:\n        main_ddict['train'] = all_dataset.select(trn_idx)\n\n    if val_idx is not None:  \n        main_ddict['validation'] = all_dataset.select(val_idx)\n    \n    print_msg(\"Map Tokenize Function\",20)\n    main_ddict_tokenized = main_ddict.map(partial(tokenize_function,\n                                                  tok=tokenizer,\n                                                  is_split_into_words=is_split_into_words,\n                                                  max_length=max_length),batched=True)\n    \n    return main_ddict_tokenized\n\n\nsource\n\ndatasetdictize_given_idxs\n\n datasetdictize_given_idxs (kv_pairs:dict, trn_idx=None, val_idx=None,\n                            tokenizer=None, is_split_into_words=False,\n                            max_length=None)\n\nCreate a HuggingFace DatasetDict with given arguments\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nkv_pairs\ndict\n\nDictionary; keys can be content, label, metadata. Values are list each.\n\n\ntrn_idx\nNoneType\nNone\nTraining indices\n\n\nval_idx\nNoneType\nNone\nValidation indices\n\n\ntokenizer\nNoneType\nNone\nHuggingFace tokenizer\n\n\nis_split_into_words\nbool\nFalse\nIs text (content) split into list or not\n\n\nmax_length\nNoneType\nNone\npad to model’s allowed max length (default is max_sequence_length)\n\n\n\nExample\n\ncache_dir=Path('./envibert_tokenizer')\ntokenizer = SourceFileLoader(\"envibert.tokenizer\", \n                             str(cache_dir/'envibert_tokenizer.py')).load_module().RobertaTokenizer(cache_dir)\n\n\nkv_pairs={\n    'text':[\n         'hội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức',\n         'This is the recommended way to make a Python package importable from anywhere',\n         'hội cần mở thẻ tín dụng tại hà nội, đà nẵng, tp. hồ chí minh',\n         \"biti's cao lãnh - đồng tháp\",\n         'chợ phòng trọ + việc làm...khu lĩnh nam - vĩnh hưng - mai động (hoàng mai)'\n          ],\n    'label': [0,1,0,0,1]\n}\n\nddict = datasetdictize_given_idxs(kv_pairs,\n                                  trn_idx=[0,1,3],\n                                  val_idx=[2,4],\n                                  tokenizer=tokenizer,\n                                  max_length=512)\n\n-------------------- Map Tokenize Function --------------------\n\n\n\n\n\n\n\n\n\nddict.keys()\n\ndict_keys(['train', 'validation'])\n\n\n\nprint(ddict['train']['input_ids'])\n\n[[0, 227, 1033, 191, 664, 1033, 7366, 2615, 13, 664, 1033, 671, 1355, 2294, 993, 413, 2900, 2], [0, 116, 14, 6, 3169, 270, 9, 364, 10, 23963, 5360, 15930, 2003, 51, 5906, 2, 1, 1], [0, 880, 592, 427, 162, 171, 906, 13, 122, 6553, 2, 1, 1, 1, 1, 1, 1, 1]]\n\n\nYou can change max_length (which allow truncation when sentence length is higher than max_length)\n\nddict = datasetdictize_given_idxs(kv_pairs,\n                                  trn_idx=[0,1,3],\n                                  val_idx=[2,4],\n                                  tokenizer=tokenizer,\n                                  max_length=5)\n\n-------------------- Map Tokenize Function --------------------\n\n\n\n\n\n\n\n\n\nprint(ddict['train']['input_ids'])\n\n[[0, 227, 1033, 191, 2], [0, 116, 14, 6, 2], [0, 880, 592, 427, 2]]\n\n\nAllow full dataset to be mapped to DatasetDict if we omit trn_idx argument\n\nddict = datasetdictize_given_idxs(kv_pairs,\n                                  tokenizer=tokenizer,\n                                  max_length=512)\n\n-------------------- Map Tokenize Function --------------------\n\n\n\n\n\n\nprint(ddict['train']['input_ids'])\n\n[[0, 227, 1033, 191, 664, 1033, 7366, 2615, 13, 664, 1033, 671, 1355, 2294, 993, 413, 2900, 2, 1, 1, 1, 1, 1, 1, 1], [0, 116, 14, 6, 3169, 270, 9, 364, 10, 23963, 5360, 15930, 2003, 51, 5906, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 227, 256, 778, 2600, 1074, 144, 76, 5489, 613, 57339, 4820, 27666, 57339, 21422, 244, 872, 635, 841, 2, 1, 1, 1, 1, 1], [0, 880, 592, 427, 162, 171, 906, 13, 122, 6553, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 2299, 315, 5995, 1349, 99, 83, 55025, 244, 6356, 1114, 1213, 1163, 13, 8233, 11051, 13, 3335, 109, 28, 11695, 13377, 3335, 3, 2]]"
  },
  {
    "objectID": "deprecated/text_main[deprecated].html#class-textdatamain",
    "href": "deprecated/text_main[deprecated].html#class-textdatamain",
    "title": "Text Main",
    "section": "Class TextDataMain",
    "text": "Class TextDataMain\n\nclass TextDataMain():\n    def __init__(self,\n                 df: pd.DataFrame, # The main dataframe\n                 main_content:str, # Name of the text column\n                 metadatas=[], # Names of the metadata columns\n                 label_names=None, # Names of the label (dependent variable) columns\n                 class_names_predefined=None, # (Optional) List of names associated with the labels (same index order)\n                 val_ratio:list|float|None=0.2, # Ratio of data for validation set. If given a list, validation set will be chosen based on indices in this list\n                 split_cols:list|str=None, # Column(s) needed to do stratified shuffle split\n                 content_tfms=[], # A list of text transformations\n                 aug_tfms=[], # A list of text augmentations\n                 process_metadatas=True, # Whether to do simmple text processing on the chosen metadatas\n                 seed=None, # Random seed\n                 cols_to_keep=None, # Columns to keep after all processings\n                 shuffle_trn=True # Whether to shuffle the train set\n                ):\n        self.df = df.copy()\n        self.main_content = main_content\n        self.metadatas = metadatas\n        self.label_names = label_names\n        self.label_lists = class_names_predefined\n        self.content_tfms = content_tfms\n        self.aug_tfms = aug_tfms\n        self.process_metadatas = process_metadatas\n        self.val_ratio=val_ratio\n        self.split_cols=split_cols\n        self.seed = seed\n        self.cols_to_keep = cols_to_keep\n        self.shuffle_trn=shuffle_trn  \n        self._main_called=False\n        self.is_multilabel=False\n        self.is_multihead=False\n        check_input_validation(self.df)\n        \n    @classmethod\n    def from_csv(cls,path,return_df=False,encoding='utf-8-sig',**kwargs):\n        df = pd.read_csv(path,encoding=encoding,engine='pyarrow')\n        tdm = TextDataMain(df,main_content=None) if return_df else TextDataMain(df,**kwargs)\n        if return_df:\n            return df\n        return tdm\n    \n    @classmethod\n    def from_pickle(cls,\n                    fname, # Name of the pickle file\n                    parent='pickle_files' # Parent folder\n                   ):\n        return load_pickle(fname,parent=parent)\n    \n    @classmethod\n    def from_gsheet(cls,gs_id,return_df=False,**kwargs):\n        pass\n\n    \n    def save_as_pickles(self,\n                        fname, # Name of the pickle file\n                        parent='pickle_files', # Parent folder\n                        drop_data_attributes=False # Whether to drop all large-size data attributes\n                       ):\n        if drop_data_attributes:\n            if hasattr(self, 'df'):\n                del self.df\n            if hasattr(self, 'main_ddict'):\n                del self.main_ddict\n        save_to_pickle(self,fname,parent=parent)\n\n        \n    def _check_validation_leaking(self,trn_idxs,val_idxs):\n        if self.val_ratio is None:\n            return trn_idxs,None\n        \n        df_trn = self.df.loc[trn_idxs]\n        df_val = self.df.loc[val_idxs]\n        \n        #sanity check\n        assert df_trn.shape[0]+df_val.shape[0]==self.df.shape[0],\"Train + Validation != Total Data\"\n\n        \n        print(f'Previous Validation Percentage: {round(100*len(val_idxs)/self.df.shape[0],3)}%')\n        val_content_series = check_text_leaking(df_trn[self.main_content],df_val[self.main_content])\n        val_idxs2 = val_content_series.index.values\n        trn_idxs2 = self.df[~self.df.index.isin(val_idxs2)].index.values\n        print(f'Current Validation Percentage: {round(100*len(val_idxs2)/self.df.shape[0],3)}%')\n        if len(val_idxs2)!=len(val_idxs):\n            return trn_idxs2,val_idxs2\n        return trn_idxs,val_idxs\n    \n    def _train_test_split(self):\n        print_msg('Train Test Split',20)\n        rng = np.random.default_rng(self.seed)\n        if self.val_ratio is None: # no train/val split\n            trn_idxs = rng.permutation(self.df.shape[0])\n            return trn_idxs,None\n        if isinstance(self.val_ratio,list) or isinstance(self.val_ratio,np.ndarray):\n            val_idxs = np.array(self.val_ratio)\n            trn_idxs = np.array(set(self.df.index.values) - set(self.val_ratio))\n            return trn_idxs,val_idxs\n        if isinstance(self.val_ratio,float) and self.split_cols is None:\n            _idxs = rng.permutation(self.df.shape[0])\n            _cutoff = int(self.val_ratio*self.df.shape[0]) \n            val_idxs = _idxs[:_cutoff]\n            trn_idxs = _idxs[_cutoff:]\n            return trn_idxs,val_idxs\n        \n        self.split_cols = val2iterable(self.split_cols)\n        if self.is_multilabel and self.label_names[0] in self.split_cols:\n            raise ValueError('For MultiLabel classification, you cannot choose the label as your shuffle-split column')\n        \n        if len(self.split_cols)&gt;0:\n            _y = self.df[self.split_cols[0]]\n            if len(self.split_cols)&gt;1:\n                for c in self.split_cols[1:]:\n                    _y= _y.astype(str) + '_' + self.df[c].astype(str)\n            sss = StratifiedShuffleSplit(n_splits=1, test_size=self.val_ratio, \n                                         random_state=self.seed)\n            trn_idxs,val_idxs = list(sss.split(self.df,_y))[0]\n            return trn_idxs,val_idxs\n        \n        raise ValueError('No valid keyword arguments for train validation split!')\n\n                         \n    def _encode_labels(self):\n        print_msg('Label Encoding')\n        if self.label_names is None: \n            raise ValueError('Missing label columns!')\n        self.label_names = val2iterable(self.label_names)\n        if len(self.label_names)&gt;1:\n            self.is_multihead=True\n        \n        if self.label_lists is not None and not isinstance(self.label_lists[0],list):\n            self.label_lists = [self.label_lists]\n        \n        if isinstance(self.df[self.label_names[0]].iloc[0],list):\n            # This is multi-label. Ignore self.label_names[1:]\n            self.label_names = [self.label_names[0]]\n            self.is_multihead=False\n            self.is_multilabel=True\n            \n        encoder_classes=[]\n        if not self.is_multilabel:\n            for idx,l in enumerate(self.label_names):\n                if self.label_lists is None:\n                    train_label = self.df[l].values\n                    l_encoder = LabelEncoder()\n                    self.df[l] = l_encoder.fit_transform(train_label)\n                    encoder_classes.append(list(l_encoder.classes_))\n                else:\n                    l_classes = sorted(list(self.label_lists[idx]))\n                    label2idx = {v:i for i,v in enumerate(l_classes)}\n                    self.df[l] = self.df[l].map(label2idx).values\n                    encoder_classes.append(l_classes)\n        else:\n            # For MultiLabel, we only save the encoder classes without transforming the label itself to one-hot (or actually, few-hot)\n            if self.label_lists is None:\n                l_encoder = MultiLabelBinarizer()\n                _ = l_encoder.fit(self.df[self.label_names[0]])\n                encoder_classes.append(list(l_encoder.classes_))\n            else:\n                l_classes = sorted(list(self.label_lists[0]))\n                encoder_classes.append(l_classes)\n                \n        self.label_lists = encoder_classes\n            \n    def _process_metadatas(self,df,override_dict=True):\n        print_msg('Metadata Simple Processing & Concatenating to Main Content')\n        self.metadatas = val2iterable(self.metadatas)\n            \n        for s in self.metadatas:\n            if self.process_metadatas:\n                # just strip and lowercase\n                df[s] = df[s].astype(str).str.strip().str.lower()\n            # simple concatenation with '. '\n            df[self.main_content] = df[s] + ' - ' + df[self.main_content]\n                \n        if override_dict:        \n            self.metadata_dict={}\n            for s in self.metadatas:\n                self.metadata_dict[s]=sorted(set(df[s].values))\n        return df\n    \n    def _simplify_df(self):\n        if self.cols_to_keep is None:\n            self.cols_to_keep= [self.main_content] + self.metadatas + self.label_names\n        self.df = self.df[self.cols_to_keep].copy()\n    \n    def _do_transformation(self,df):\n        print_msg('Text Transformation',20)\n        for tfm in self.content_tfms:\n            print_msg(callable_name(tfm))\n            df[self.main_content] = [tfm(s) for s in tqdm(df[self.main_content].values)]\n        return df\n    \n    def _do_augmentation(self,df_trn_org):\n        df_trn_all = df_trn_org.copy()\n        print_msg('Text Augmentation',20)\n        print(f'Train data size before augmentation: {len(df_trn_all)}')\n        for tfm in self.aug_tfms:\n            print_msg(callable_name(tfm))\n            if tfm.keywords['apply_to_all']:\n                new_content,new_others = tfm(content=df_trn_all[self.main_content].values,others=df_trn_all.iloc[:,1:])\n            else:\n                new_content,new_others = tfm(content=df_trn_org[self.main_content].values,others=df_trn_org.iloc[:,1:])\n            \n            # add axis to np array in order to do concatenation\n            if len(new_content.shape)==1:\n                new_content = new_content[:,None]\n            if len(new_others.values.shape)==1:\n                new_others = new_others.values[:,None]\n                \n            df_tmp = pd.DataFrame(np.concatenate((new_content,new_others.values),axis=1),columns=df_trn_org.columns.values)\n            df_trn_all = pd.concat((df_trn_all,df_tmp),axis=0).reset_index(drop=True)\n            print(f'Train data size after THIS augmentation: {len(df_trn_all)}')       \n        print(f'Train data size after ALL augmentation: {len(df_trn_all)}')\n        return df_trn_all\n    \n    def _main_text_processing(self):\n        print_msg('Start Main Text Processing',20)\n        \n        # Process metadatas\n        self.df = self._process_metadatas(self.df)\n        \n        # Process labels\n        self._encode_labels()\n        \n        # Content transformation\n        self.df = self._do_transformation(self.df)\n        \n        # Train Test Split\n        trn_idxs,val_idxs = self._train_test_split()\n        self._simplify_df()\n        trn_idxs,val_idxs = self._check_validation_leaking(trn_idxs,val_idxs)\n        if self.val_ratio is not None:\n            df_val = self.df.loc[val_idxs].reset_index(drop=True)\n        \n        # Augmentation\n        df_trn_org = self.df.loc[trn_idxs].reset_index(drop=True)\n        df_trn_all = self._do_augmentation(df_trn_org)\n        df_trn_all['is_valid']=False\n        \n        # Shuffle train\n        if self.shuffle_trn:\n            df_trn_all = df_trn_all.sample(frac=1.,random_state=self.seed)\n            \n        # Combine augmented train and val\n        if self.val_ratio is not None:\n            df_val['is_valid']=True\n            df_trn_all = pd.concat((df_trn_all,df_val),axis=0)\n        \n        self._main_called=True\n        self.df = df_trn_all.reset_index(drop=True)        \n    \n    def set_data_collator(self,data_collator):\n        self.data_collator = data_collator\n        \n    def tokenizer_explain_single(self,tokenizer):\n        inp = self.df[~self.df['is_valid']][self.main_content].sample(1).values[0]\n        tokenizer_explain(inp,tokenizer)\n        \n    def to_df(self): \n        \"To execute all the defined processings and return a dataframe\"\n        if not self._main_called:\n            self._main_text_processing()\n        return self.df\n       \n    def save_train_data_after_processing(self,output_path,encoding='utf-8-sig'):\n        if not self._main_called:\n            print_msg('WARNING')\n            print('Please process training data (using to_df or to_datasetdict)')\n            return\n        self.df.to_csv(Path(output_path),encoding=encoding,index=False)\n    \n    def to_datasetdict(self,\n                       tokenizer, # Tokenizer (preferably from HuggingFace)\n                       is_split_into_words=False, # Is text split into list or not\n                       max_length=None, # pad to model's allowed max length (default is max_sequence_length)\n                       trn_ratio=1., # Portion of training data to be converted to datasetdict. Useful for sample experiments\n                       seed=42 # Random seed\n                      ):\n        if not self._main_called:\n            self._main_text_processing()\n        val_idx = self.df[self.df['is_valid']].index.values if self.val_ratio is not None else None\n        trn_idx = self.df[~self.df['is_valid']].index.values\n        if trn_ratio&lt;1. and trn_ratio&gt;0.:\n            rng = np.random.default_rng(self.seed)\n            _idxs = rng.permutation(len(trn_idx))\n            _cutoff = int(trn_ratio*len(trn_idx)) \n            trn_idx = _idxs[:_cutoff]\n            \n        _label = self.df[self.label_names].values.tolist()\n        if not self.is_multilabel:\n            if len(self.label_names)==1:\n                _label = np.array(_label).flatten().tolist() # (n,)\n        else:\n            # For MultiLabel, this is where the actual label transformation happens\n            mlb = MultiLabelBinarizer(classes=self.label_lists[0])\n            _label = self.df[self.label_names[0]].values.tolist()\n            _label = mlb.fit_transform(_label).tolist() # few-hotted\n        \n        kv_pairs = {'text':self.df[self.main_content].tolist(),\n                    'label':_label,\n                   }\n        for c in self.cols_to_keep:\n            if c not in self.label_names+[self.main_content]: kv_pairs[c] = self.df[c].tolist()\n        \n        self.tokenizer = tokenizer\n        self.is_split_into_words= is_split_into_words\n        self.max_length = max_length\n        \n        ddict = datasetdictize_given_idxs(kv_pairs,trn_idx,val_idx,self.tokenizer,\n                                         is_split_into_words=is_split_into_words,max_length=max_length)\n        self.main_ddict = ddict\n        return ddict\n    \n    def get_test_datasetdict_from_csv(self,path,encoding='utf-8-sig'):\n        df_test = pd.read_csv(path,encoding=encoding,engine='pyarrow')\n        return self.get_test_datasetdict_from_df(df_test)\n\n    def get_test_datasetdict_from_dict(self,content):\n        if len(self.metadatas)!=0 and not isinstance(content,dict):\n            raise ValueError(f'There is/are metadatas in the preprocessing step. Please include a dictionary including these keys for metadatas: {self.metadatas}, and texture content: {self.main_content}')\n            \n        _dic = {self.main_content:[content]} if isinstance(content,str) else content\n        for k in _dic.keys():\n            _dic[k] = val2iterable(_dic[k])\n        \n        df_test = pd.DataFrame.from_dict(_dic)\n        return self.get_test_datasetdict_from_df(df_test)\n    \n    def get_test_datasetdict_from_df(self,df_test):\n        print_msg('Getting Test Set',20)\n        check_input_validation(df_test)\n        \n        cols_to_keep = [c for c in self.cols_to_keep if c not in self.label_names]\n        df_test = df_test[cols_to_keep].copy()\n        \n        print_msg('Start Test Set Transformation',20)\n        df_test = self._process_metadatas(df_test,override_dict=False)\n        df_test = self._do_transformation(df_test)\n        \n        if hasattr(self,'df'):\n            print_msg('Test Leak Checking',20)\n            _ = check_text_leaking(self.df[self.main_content],df_test[self.main_content])\n        \n        print_msg('Construct DatasetDict',20)\n        test_text = df_test[self.main_content].values\n        \n        kv_pairs ={'text':test_text}\n        for c in self.cols_to_keep:\n            if c not in self.label_names+[self.main_content]: kv_pairs[c] = df_test[c].tolist()\n        \n        test_dataset = Dataset.from_dict(kv_pairs)\n        test_ddict = DatasetDict()\n        test_ddict['test'] = test_dataset\n        test_ddict_tokenized = test_ddict.map(partial(tokenize_function,tok=self.tokenizer,\n                                                      is_split_into_words=self.is_split_into_words,\n                                                      max_length=self.max_length),batched=True)\n        \n        return test_ddict_tokenized\n\n\nsource\n\nTextDataMain\n\n TextDataMain (df:pandas.core.frame.DataFrame, main_content:str,\n               metadatas=[], label_names=None,\n               class_names_predefined=None, val_ratio:list|float|None=0.2,\n               split_cols:list|str=None, content_tfms=[], aug_tfms=[],\n               process_metadatas=True, seed=None, cols_to_keep=None,\n               shuffle_trn=True)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nThe main dataframe\n\n\nmain_content\nstr\n\nName of the text column\n\n\nmetadatas\nlist\n[]\nNames of the metadata columns\n\n\nlabel_names\nNoneType\nNone\nNames of the label (dependent variable) columns\n\n\nclass_names_predefined\nNoneType\nNone\n(Optional) List of names associated with the labels (same index order)\n\n\nval_ratio\nlist | float | None\n0.2\nRatio of data for validation set. If given a list, validation set will be chosen based on indices in this list\n\n\nsplit_cols\nlist | str\nNone\nColumn(s) needed to do stratified shuffle split\n\n\ncontent_tfms\nlist\n[]\nA list of text transformations\n\n\naug_tfms\nlist\n[]\nA list of text augmentations\n\n\nprocess_metadatas\nbool\nTrue\nWhether to do simmple text processing on the chosen metadatas\n\n\nseed\nNoneType\nNone\nRandom seed\n\n\ncols_to_keep\nNoneType\nNone\nColumns to keep after all processings\n\n\nshuffle_trn\nbool\nTrue\nWhether to shuffle the train set\n\n\n\nLet’s start a step-by-step walkthrough on how to use this class\n\nDATA_PATH = Path('sample_data')\n\n\n\nConstructor/ Class Method calls\nIf you just want to get the dataframe from the csv path, set return_df=True. You still have the input validation precheck functionality.\n\ndf = TextDataMain.from_csv(DATA_PATH/'sample_large.csv',\n                            return_df=True)\n\n----- Input Validation Precheck -----\nDataFrame contains duplicated values!\n-----&gt; Number of duplications: 16 rows\n\n\nThe Input Validation Precheck will check for missing values and duplicate rows in the csv file. Since there’s no such thing in our sample dataset, we won’t see anything here\n\ndf.sample(5)\n\n\n\n\n\n\n\n\n\nSource\nContent\nL1\nL2\n\n\n\n\n556\nNon Owned\nXinh quá mấy chị,e còn dư 50 sét thôi! #35k 1s...\nOthers\nCannot defined\n\n\n1318\niOS\nk sử dụng dc mã giảm giá, nhập vào rồi kêu lỗi...\nFeature\nApply Voucher\n\n\n1852\nGoogle Play\nCách shopee giải quyết vấn đề quá là TỆ\nServices\nContact Agent\n\n\n901\nGoogle Play\nLỗi không vào được ứng dụng, nhà điều hành có ...\nFeature\nApp performance\n\n\n64\niOS\nCảm thấy tệ\nOthers\nCannot defined\n\n\n\n\n\n\n\n\n\ndf.Source.value_counts()\n\nSource\nGoogle Play    1434\nNon Owned       499\nOwned           139\niOS             124\nHC search        73\nName: count, dtype: int64\n\n\nLet’s say you are happy with this dataframe (after you did some others preprocessing), then you can start creating a TextDataMain object\nFor this dataframe, I want to - Build a text classification model, with main text in Content column, metadatas is Source, and the label is L1 - Perform apply_word_tokenize with text normalization (this is “text transformation”) - For augmentation, I want to perform: Oversampling the Owned, Non Owned and HC Search from column Source, then add some the Vietnamese no-accent text. Note that all of these are called “text augmentation”\nLet’s define these transformations\n\nFor Text Transformation\n\n\nawt_tfm = partial(apply_word_tokenize,normalize_text=True)\n# You can also set a __name__ to your augmentation function. \n# This way you will have meaningful text messages as outputs\nawt_tfm.__name__='UTS Word Tokenization With Normalization'\n\ntxt_tfms=[awt_tfm]\n\n\nFor Text Augmentation\n\n\n# apply_to_all means I will apply this augmentation to all the data \n# (including the original data and the augmented data/transformed data from previous augmentation/transformation)\nover_nonown_tfm = partial(sampling_with_condition,query='Source==\"non owned\"',frac=0.5,seed=42,apply_to_all=False)\nover_nonown_tfm.__name__ = 'Oversampling Non Owned'\n\nover_own_tfm = partial(sampling_with_condition,query='Source==\"owned\"',frac=2,seed=42,apply_to_all=False)\nover_own_tfm.__name__ = 'Oversampling Owned'\n\nover_hc_tfm = partial(sampling_with_condition,query='Source==\"hc search\"',frac=2.5,seed=42,apply_to_all=False)\nover_hc_tfm.__name__ = 'Oversampling HC search'\n\nremove_accent_tfm = partial(remove_vnmese_accent,frac=1,seed=42,apply_to_all=True)\nremove_accent_tfm.__name__ = 'Add No-Accent Text'\n\naug_tfms = [over_nonown_tfm,over_own_tfm,over_hc_tfm,remove_accent_tfm]\n\n\ntdm = TextDataMain(df,\n                    main_content='Content',\n                    metadatas='Source', # You can put a list of multiple metadatas\n                    label_names='L1', # You can put a list of multiple labels\n                    val_ratio=0.2,\n                    split_cols='L1', # You can even put a list of multiple columns to be used for validation splitting\n                    content_tfms = txt_tfms, # You can add multiple content transformation functions ...\n                    aug_tfms = aug_tfms, # ... as well as augmentation functions\n                    process_metadatas=True,\n                    seed=42,\n                    shuffle_trn=True)\n\n----- Input Validation Precheck -----\nDataFrame contains duplicated values!\n-----&gt; Number of duplications: 16 rows\n\n\nIf we want to directly create a TextDataMain object from our csv file, we can instead use this:\n\ntdm = TextDataMain.from_csv(DATA_PATH/'sample_large.csv',\n                            return_df=False,\n                            main_content='Content',\n                            metadatas='Source',\n                            label_names='L1',\n                            val_ratio=0.2,\n                            split_cols='L1',\n                            content_tfms = txt_tfms,\n                            aug_tfms = aug_tfms,\n                            process_metadatas=True,\n                            seed=42,\n                            shuffle_trn=True)\n\n----- Input Validation Precheck -----\nDataFrame contains duplicated values!\n-----&gt; Number of duplications: 16 rows\n\n\n\nsource\n\n\nTextDataMain.to_df\n\n TextDataMain.to_df ()\n\nTo execute all the defined processings and return a dataframe\nNote that all the previous constructor calls do not do any heavy processing yet.\nTo actually run all the processes, one can call TextDataMain.to_df()\n\ndf_processed = tdm.to_df()\n\n-------------------- Start Main Text Processing --------------------\n----- Metadata Simple Processing & Concatenating to Main Content -----\n----- Label Encoding -----\n-------------------- Text Transformation --------------------\n----- UTS Word Tokenization With Normalization -----\n-------------------- Train Test Split --------------------\nPrevious Validation Percentage: 20.009%\n- Before leak check\nSize: 454\n- After leak check\nSize: 447\n- Number of rows leaked: 7, or 1.54% of the original validation (or test) data\nCurrent Validation Percentage: 19.7%\n-------------------- Text Augmentation --------------------\nTrain data size before augmentation: 1822\n----- Oversampling Non Owned -----\nTrain data size after THIS augmentation: 2020\n----- Oversampling Owned -----\nTrain data size after THIS augmentation: 2248\n----- Oversampling HC search -----\nTrain data size after THIS augmentation: 2390\n----- Add No-Accent Text -----\nTrain data size after THIS augmentation: 4780\nTrain data size after ALL augmentation: 4780\n\n\n100%|██████████████████████████████████████████████████████████████████████████████| 2269/2269 [00:03&lt;00:00, 599.84it/s]\n100%|████████████████████████████████████████████████████████████████████████████| 2390/2390 [00:00&lt;00:00, 19530.97it/s]\n\n\nNotice this?\nPrevious Validation Percentage: 20.0%\n- Before leak check\nSize: 14\n- After leak check\nSize: 14\n- Number of rows leaked: 0, or 0.00% of the original validation (or test) data\nCurrent Validation Percentage: 20.0%\nAfter performing train/test split, the TextDataMain object also perform a “leak check”: After text_transformation is performed, it will compare the text from Content value in the validation set to the Content text in the train set. Any duplications (texts that belong to both set) will be removed from validation set.\n\ndf_processed.sample(5)\n\n\n\n\n\n\n\n\n\nContent\nSource\nL1\nis_valid\n\n\n\n\n5090\nnon owned - https://shopee.vn/maybeaty like và...\nnon owned\n5\nTrue\n\n\n2280\nowned -_Địa chị này dám giao ko a . 😂\nowned\n1\nFalse\n\n\n1258\ngoogle play - ble\ngoogle play\n5\nFalse\n\n\n3743\ngoogle play - Ko dừng tự_động cập_nhật được\ngoogle play\n3\nFalse\n\n\n4624\nowned - Như này là sao ạ\nowned\n5\nFalse\n\n\n\n\n\n\n\n\nNote that, since we have metadatas, the metadatas is concatenated to the front of the texture content\n\ndf_processed.Content.sample(5).values\n\narray(['owned - 🚀 *_* ĐÓN_CHÀO ƯU_ĐÃI VÀNG VÀO THỨ_BA VỚI PHIÊN CHỢ VOUCHER_*_*_🚀_*_🎁 Miễn_phí tối_đa 50K cho 1 giờ khi chọn dịch_vụ dọn_dẹp nhà từ bTaskee *_🤗_Giảm ngay 20 % gói học Toán 12 tháng từ VioEdu_*_🤑_ShopBack tặng ngay 25K tiền thưởng cho đơn hàng từ 150K_*_😍_Giảm 70K cho hóa đơn từ 300K , áp_dụng cho tất_cả dịch_vụ làm đẹp tại Lamia 🎉 ️ 🎉_Chỉ từ 1000 xu , SỐ_LƯỢNG CÓ_HẠN ➡_[_http://shopee.vn/ShopeeDoiXu ] ( http://shopee.vn/ShopeeDoiXu ) ___________________________💥_10.10_SIÊU_SALE CHÍNH HÃNG - VẪN CÒN SALE_🎊_▶_️ https://shopee.vn/1010-Sieu-Sale-Chinh-Hang Săn thêm ưu_đãi Xtra duy_nhất 11.10 : 🎁 Miễn_phí vận_chuyển 0 Đ_🎁_Thương_hiệu hoàn xu tới 50 % 🎁 Thu_thập voucher , nhận đến 1.2 triệu 🎁 4 Khung giờ săn sale đậm : 0H - 9H - 12H - 21H_#_Shopee1010SieuSaleChinhHang',\n       'ios - Dat_hang cu bi tu hoan hang ve rat la buc :)',\n       'owned - Các bạn từng mua gì đáng đồng_tiền nhất trên Sốp_Pi ?',\n       'google play - Được',\n       'google play - Chon thanh_toan ma cu the ghi no roi chuyen_khoan , bay di dau cai thanh_toan khi nhan hang roi'],\n      dtype=object)\n\n\nWe now have a new dataframe with only the necessary columns (the processed text column, metadatas, label, and is_valid which tells you which row belongs to the validation set). Notice that our class has also encode our label for us\nOur TextDataMain object also stores other useful attributes, such as:\n\n# The entire processed dataframe, similar to the df_processed above\ntdm.df.head()\n\n\n\n\n\n\n\n\n\nContent\nSource\nL1\nis_valid\n\n\n\n\n0\nhc search - làm_sao để hết lỗi m02\nhc search\n3\nFalse\n\n\n1\nhc search - xin chao\nhc search\n5\nFalse\n\n\n2\nowned - 8 NGAY DUA DON , TRUNG VOUCHER 8 THA...\nowned\n1\nFalse\n\n\n3\ngoogle play - 🤬_😡_🤬_😡\ngoogle play\n5\nFalse\n\n\n4\nhc search - mua hàng quốc_tế như thế_nào\nhc search\n4\nFalse\n\n\n\n\n\n\n\n\n\n# class names (This will be a list of list, as this class can handle multi-label classification)\ntdm.label_lists\n\n[['Buyer complained seller',\n  'Commercial',\n  'Delivery',\n  'Feature',\n  'Order/Item',\n  'Others',\n  'Payment',\n  'Return/Refund',\n  'Services',\n  'Shopee account']]\n\n\n\n# a dictionary storing unique value for each provided metadata\ntdm.metadata_dict\n\n{'Source': ['google play', 'hc search', 'ios', 'non owned', 'owned']}\n\n\nIf we want to see how a HuggingFace’s tokenizer work on our processed text:\n\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\n\n# this will pick a random text from train set to show\ntdm.tokenizer_explain_single(tokenizer)\n\n----- Tokenizer Explained -----\n--- Input ---\ngoogle play - Mang ro manh nhung lai dang nhap khong duoc : &lt;\n\n--- Tokenized results --- \n{'input_ids': [0, 38970, 14015, 31, 8283, 7135, 24136, 12088, 5135, 19058, 2008, 18679, 3014, 2662, 6190, 22899, 27, 8452, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n--- Results from tokenizer.convert_ids_to_tokens ---\n['&lt;s&gt;', 'google', 'play', '-', 'Mang', 'ro', 'manh', 'nhung', 'lai', 'dang', 'nh@@', 'ap', 'kh@@', 'ong', 'du@@', 'oc', ':', '&lt;', '&lt;/s&gt;']\n\n--- Results from tokenizer.decode --- \n&lt;s&gt; google play - Mang ro manh nhung lai dang nhap khong duoc : &lt; &lt;/s&gt;\n\n\n\nBy doing this, we can see how the tokenizer interact with our text.\n\nsource\n\n\nTextDataMain.to_datasetdict\n\n TextDataMain.to_datasetdict (tokenizer, is_split_into_words=False,\n                              max_length=None, trn_ratio=1.0, seed=42)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntokenizer\n\n\nTokenizer (preferably from HuggingFace)\n\n\nis_split_into_words\nbool\nFalse\nIs text split into list or not\n\n\nmax_length\nNoneType\nNone\npad to model’s allowed max length (default is max_sequence_length)\n\n\ntrn_ratio\nfloat\n1.0\nPortion of training data to be converted to datasetdict. Useful for sample experiments\n\n\nseed\nint\n42\nRandom seed\n\n\n\nSince we need to convert our data to HuggingFace’s DatasetDict format in order to utilize HuggingFace’s model well, we can directly export datasetdict using TextDataMain.to_datasetdict\n\nddict_sample = tdm.to_datasetdict(tokenizer)\n\n-------------------- Map Tokenize Function --------------------\n\n\n\n\n\n\n\n\n\nddict_sample\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 4780\n    })\n    validation: Dataset({\n        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 447\n    })\n})\n\n\n\nddict_sample['train']['text'][0]\n\n'hc search - làm_sao để hết lỗi m02'\n\n\n\nprint(ddict_sample['train']['input_ids'][0])\n\n[0, 1340, 1894, 51139, 31, 2407, 24, 351, 1210, 1387, 3974, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n\n\nNote that PhoBert will auto-pad our sentence to its model max_sequence_length, which is 256\n\nlen(ddict_sample['train']['input_ids'][0])\n\n256\n\n\n\nddict_sample['train']['label'][0]\n\n3\n\n\n\nsource\n\n\nTextDataMain.save_as_pickles\n\n TextDataMain.save_as_pickles (fname, parent='pickle_files',\n                               drop_data_attributes=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfname\n\n\nName of the pickle file\n\n\nparent\nstr\npickle_files\nParent folder\n\n\ndrop_data_attributes\nbool\nFalse\nWhether to drop all large-size data attributes\n\n\n\nAs the transformations/augmentations can take time for large dataset, we want to save our TextDataMain object. We can use TextDataMain.save_as_pickles to export a pickle file\n\ntdm.save_as_pickles('my_tdm')\n\nThen you can load it with\n\ntdm2 = TextDataMain.from_pickle('my_tdm')\n\n… and access all the attributes\n\ntdm2.df.head()\n\n\n\n\n\n\n\n\n\nContent\nSource\nL1\nis_valid\n\n\n\n\n0\nhc search - làm_sao để hết lỗi m02\nhc search\n3\nFalse\n\n\n1\nhc search - xin chao\nhc search\n5\nFalse\n\n\n2\nowned - 8 NGAY DUA DON , TRUNG VOUCHER 8 THA...\nowned\n1\nFalse\n\n\n3\ngoogle play - 🤬_😡_🤬_😡\ngoogle play\n5\nFalse\n\n\n4\nhc search - mua hàng quốc_tế như thế_nào\nhc search\n4\nFalse\n\n\n\n\n\n\n\n\n\ntdm2.label_lists[0]\n\n['Buyer complained seller',\n 'Commercial',\n 'Delivery',\n 'Feature',\n 'Order/Item',\n 'Others',\n 'Payment',\n 'Return/Refund',\n 'Services',\n 'Shopee account']\n\n\n\ntdm2.metadata_dict\n\n{'Source': ['google play', 'hc search', 'ios', 'non owned', 'owned']}\n\n\nLet’s check the file size\n\nfile_stats = os.stat(Path('pickle_files/my_tdm.pkl'))\nprint(f'File Size in MegaBytes is {file_stats.st_size / (1024 * 1024)}')\n\nFile Size in MegaBytes is 13.537714004516602\n\n\nAs it saves the entire processed dataframe (and datasetdict if you call to_datasetdict), the pickle size can be large. In some scenario you don’t need to store these data attributes (as inference time, or in production). Thus one can save a lighter pickle file by setting drop_data_attributes to True\n\ntdm.save_as_pickles('my_lightweight_tdm',drop_data_attributes=True)\n\n\nfile_stats = os.stat(Path('pickle_files/my_lightweight_tdm.pkl'))\nprint(f'File Size in MegaBytes is {file_stats.st_size / (1024 * 1024)}')\n\nFile Size in MegaBytes is 3.1605300903320312\n\n\nWe will see a bigger file size reduction when we work with much larger dataset\n\ntdm_light = TextDataMain.from_pickle('my_lightweight_tdm')\n\nYou can still access some important attributes (except for any data attributes, such as df or main_ddict\n\ntdm_light.label_lists[0]\n\n['Buyer complained seller',\n 'Commercial',\n 'Delivery',\n 'Feature',\n 'Order/Item',\n 'Others',\n 'Payment',\n 'Return/Refund',\n 'Services',\n 'Shopee account']\n\n\n\ntdm_light.metadata_dict\n\n{'Source': ['google play', 'hc search', 'ios', 'non owned', 'owned']}"
  },
  {
    "objectID": "deprecated/model_main_envibert_multihead-copy1.html",
    "href": "deprecated/model_main_envibert_multihead-copy1.html",
    "title": "Model Controller Tutorial: EnviBert model (MultiHead)",
    "section": "",
    "text": "We will walk through other cases of classification: multi-head and multi-label. Since we will showcase the capabiilty of this label in these cases, there won’t be as detailed as this tutorial"
  },
  {
    "objectID": "deprecated/model_main_envibert_multihead-copy1.html#load-data",
    "href": "deprecated/model_main_envibert_multihead-copy1.html#load-data",
    "title": "Model Controller Tutorial: EnviBert model (MultiHead)",
    "section": "Load data",
    "text": "Load data\n\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom that_nlp_library.text_main import *\n\n\nfrom underthesea import text_normalize\nfrom functools import partial\nfrom pathlib import Path\nfrom importlib.machinery import SourceFileLoader\nimport os\nfrom transformers import DataCollatorWithPadding\nimport pandas as pd\n\nDefine some necessary text augmentations and text transformations\n\nFor Text Transformation\n\n\ntxt_tfms=[text_normalize]\n\n\nFor Text Augmentation\n\n\nover_nonown_tfm = partial(sampling_with_condition,query='Source==\"non owned\"',frac=0.5,seed=42,apply_to_all=False)\nover_nonown_tfm.__name__ = 'Oversampling Non Owned'\n\nover_own_tfm = partial(sampling_with_condition,query='Source==\"owned\"',frac=2,seed=42,apply_to_all=False)\nover_own_tfm.__name__ = 'Oversampling Owned'\n\nover_hc_tfm = partial(sampling_with_condition,query='Source==\"hc search\"',frac=2.5,seed=42,apply_to_all=False)\nover_hc_tfm.__name__ = 'Oversampling HC search'\n\nremove_accent_tfm = partial(remove_vnmese_accent,frac=1,seed=42,apply_to_all=True)\nremove_accent_tfm.__name__ = 'Add No-Accent Text'\n\naug_tfms = [over_nonown_tfm,over_own_tfm,over_hc_tfm,remove_accent_tfm]\n\nLet’s load and preprocess our data\n\nDATA_PATH = Path('secret_data')\n\n\ndf = TextDataMain.from_csv(DATA_PATH/'buyer_listening_with_all_raw_data_w2223.csv',return_df=True)\n\n----- Input Validation Precheck -----\nDataFrame contains missing values!\n-----&gt; List of columns and the number of missing values for each\nis_valid    65804\ndtype: int64\nDataFrame contains duplicated values!\n-----&gt; Number of duplications: 7 rows\n\n\n\ndf.head(3)\n\n\n\n\n\n\n\n\n\nWeek\nGroup\nSource\nContent\nL1\nL2\nL3\nL4\nis_valid\niteration\n\n\n\n\n0\n1.0\nGoogle Play\nGoogle Play\nTại sao cứ hiện thông báo\nServices\nShopee communication channels\nAnnoying pop-up ads\nNon-tech\nNaN\n1\n\n\n1\n1.0\nGoogle Play\nGoogle Play\nMlem\nOthers\nCannot defined\n-\n-\nNaN\n1\n\n\n2\n1.0\nGoogle Play\nGoogle Play\n1 số sản phẩm trong giỏ hàng vừa đc cập nhật t...\nFeature\nCart & Order\nCart issues/suggestions\nTech\nNaN\n1\n\n\n\n\n\n\n\n\nQuick preprocess of data and train/validation split. Due to custom logic, we will sample our data here instead of using the train_ratio from the to_datasetdict function\n\ndf_rare = df[df.L2.isin(['Chatbot', 'Commercial Others'])].copy()\n\ndf_final = pd.concat([df.query('iteration==1').sample(500,random_state=42),\n                      df.query('iteration&gt;=7 & iteration&lt;13').sample(1200,random_state=42),\n                      df_rare,\n                      df.query('iteration&gt;=13'),\n                     ],axis=0).reset_index(drop=True)\n\nval_idxs = df_final[df_final.iteration&gt;=13].index.values # from week 9\n\n\ntdm = TextDataMain(df_final,\n                    main_content='Content',\n                    metadatas='Source',\n                    label_names=['L1','L2'],\n                    val_ratio=val_idxs,\n                    split_cols='L2',\n                    content_tfms = txt_tfms,\n                    aug_tfms = aug_tfms,\n                    process_metadatas=True,\n                    seed=42,\n                    cols_to_keep=['Content','Source','iteration','L1','L2'], \n                    shuffle_trn=True)\n\n----- Input Validation Precheck -----\nDataFrame contains missing values!\n-----&gt; List of columns and the number of missing values for each\nis_valid    498\ndtype: int64\n\n\nDefine our tokenizer for EnviBert\n\ncache_dir=Path('./envibert_tokenizer')\ntokenizer = SourceFileLoader(\"envibert.tokenizer\", \n                             str(cache_dir/'envibert_tokenizer.py')).load_module().RobertaTokenizer(cache_dir)\n\nCreate our DatasetDict from TextDataMain (as our ModelController class can also work with DatasetDict)\n\nmain_ddict= tdm.to_datasetdict(tokenizer,\n                               max_length=512)\n\n-------------------- Start Main Text Processing --------------------\n----- Metadata Simple Processing & Concatenating to Main Content -----\n----- Label Encoding -----\n-------------------- Text Transformation --------------------\n----- text_normalize -----\n-------------------- Train Test Split --------------------\nPrevious Validation Percentage: 74.101%\n- Before leak check\nSize: 4927\n- After leak check\nSize: 4885\n- Number of rows leaked: 42, or 0.85% of the original validation (or test) data\nCurrent Validation Percentage: 73.47%\n-------------------- Text Augmentation --------------------\nTrain data size before augmentation: 1764\n----- Oversampling Non Owned -----\nTrain data size after THIS augmentation: 2229\n----- Oversampling Owned -----\nTrain data size after THIS augmentation: 2789\n----- Oversampling HC search -----\nTrain data size after THIS augmentation: 2904\n----- Add No-Accent Text -----\nTrain data size after THIS augmentation: 5808\nTrain data size after ALL augmentation: 5808\n-------------------- Map Tokenize Function --------------------\n\n\n100%|█████████████████████████████████████| 6649/6649 [00:01&lt;00:00, 3804.73it/s]\n100%|████████████████████████████████████| 2904/2904 [00:00&lt;00:00, 10292.84it/s]\n\n\n\n\n\n\n\n\n\nmain_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'Source', 'iteration', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 5808\n    })\n    validation: Dataset({\n        features: ['text', 'label', 'Source', 'iteration', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 4885\n    })\n})"
  },
  {
    "objectID": "deprecated/model_main_envibert_multihead-copy1.html#train-envibert-with-hidden-layer-concatenation-using-tdm",
    "href": "deprecated/model_main_envibert_multihead-copy1.html#train-envibert-with-hidden-layer-concatenation-using-tdm",
    "title": "Model Controller Tutorial: EnviBert model (MultiHead)",
    "section": "Train EnviBert (with hidden layer concatenation), using TDM",
    "text": "Train EnviBert (with hidden layer concatenation), using TDM\n\nmodel_name='nguyenvulebinh/envibert'\nenvibert_body = RobertaModel.from_pretrained(model_name)\nnum_classes = [len(tdm.label_lists[0]),len(tdm.label_lists[1])]\n\nSome weights of the model checkpoint at nguyenvulebinh/envibert were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\nnum_classes\n\n[10, 66]\n\n\nLet’s create our model controller\n\n_model_kwargs={\n    # overall model hyperparams\n    'layer2concat':4,\n    'is_multilabel':tdm.is_multilabel, # False\n    'is_multihead':tdm.is_multihead, # False\n    'head_class_sizes':num_classes,\n    'head_class': ConcatHeadSimple,\n    'head_weights':[1,2], # weights for L1 and L2. This means L2's weight is twice as much as L1's\n    # classfication head hyperparams\n    'classifier_dropout':0.1 \n}\n\nmodel = model_init_classification(model_class = RobertaHiddenStateConcatForSequenceClassification,\n                                  cpoint_path = model_name, \n                                  output_hidden_states=True, # since we are not using 'hidden layer contatenation' technique\n                                  seed=42,\n                                  body_model=envibert_body,\n                                  model_kwargs = _model_kwargs)\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score] # we will use both f1_macro and accuracy score as metrics\ncontroller = ModelController(model,tdm,metric_funcs)\n\nLoading body weights. This assumes the body is the very first first-layer block of your custom architecture\n\n\nAnd we can start training our model\n\nlr = 8.2e-5\nbs=4\nwd=0.01\nepochs= 4\n\ncontroller.fit(epochs,lr,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics_classification,\n              )\n\n/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\n\n\n\n    \n      \n      \n      [2904/2904 03:18, Epoch 4/4]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1 Score L1\nAccuracy Score L1\nF1 Score L2\nAccuracy Score L2\n\n\n\n\n1\nNo log\n7.053230\n0.245424\n0.567861\n0.053983\n0.326305\n\n\n2\n4.094400\n6.307346\n0.431371\n0.635415\n0.100538\n0.436438\n\n\n3\n4.094400\n6.313769\n0.456480\n0.661822\n0.134447\n0.477175\n\n\n4\n1.088200\n6.553708\n0.479866\n0.673695\n0.137104\n0.477175\n\n\n\n\n\n\n\n\ncontroller.trainer.model.save_pretrained('./sample_weights/my_model')"
  },
  {
    "objectID": "deprecated/model_main_envibert_multihead-copy1.html#predict-using-trained-model-using-tdm",
    "href": "deprecated/model_main_envibert_multihead-copy1.html#predict-using-trained-model-using-tdm",
    "title": "Model Controller Tutorial: EnviBert model (MultiHead)",
    "section": "Predict using trained model, using TDM",
    "text": "Predict using trained model, using TDM\n\nLoad trained model\n\n_model_kwargs\n\n{'layer2concat': 4,\n 'is_multilabel': False,\n 'is_multihead': True,\n 'head_class_sizes': [10, 66],\n 'head_class': that_nlp_library.models.roberta.classifiers.ConcatHeadSimple,\n 'head_weights': [1, 2],\n 'classifier_dropout': 0.1}\n\n\n\ntrained_model = model_init_classification(model_class = RobertaHiddenStateConcatForSequenceClassification,\n                                          cpoint_path = Path('./sample_weights/my_model'), \n                                          output_hidden_states=True,\n                                          seed=42,\n                                          model_kwargs = _model_kwargs)\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score]\ncontroller = ModelController(trained_model,tdm,metric_funcs)\n\nSome weights of the model checkpoint at sample_weights/my_model were not used when initializing RobertaHiddenStateConcatForSequenceClassification: ['body_model.pooler.dense.weight', 'body_model.pooler.dense.bias']\n- This IS expected if you are initializing RobertaHiddenStateConcatForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaHiddenStateConcatForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\n\nPredict Train/Validation set\nMake prediction on all validation set\n\ndf_val = controller.predict_ddict(ds_type='validation',batch_size=8)\n\n-------------------- Start making predictions --------------------\n\n\n\n\n\n\ndf_val.head()\n\n\n\n\n\n\n\n\n\ntext\nlabel\nSource\niteration\npred_L1\npred_prob_L1\npred_L2\npred_prob_L2\n\n\n\n\n0\ngoogle play - lam phien\n[5, 12]\ngoogle play\n13\nFeature\n0.716796\nApp performance\n0.564550\n\n\n1\ngoogle play - .. t . À mà họ nữ ưu m\n[5, 12]\ngoogle play\n13\nOthers\n0.959613\nCannot defined\n0.986585\n\n\n2\ngoogle play - Cc lùa dao\n[5, 12]\ngoogle play\n13\nOthers\n0.989178\nCannot defined\n0.991321\n\n\n3\ngoogle play - Mặt hàng sp mình đều nhỡ với Gia...\n[2, 23]\ngoogle play\n13\nDelivery\n0.775884\nDelivery time\n0.585934\n\n\n4\ngoogle play - Chưa tối ưu tốt cho Android Oppo...\n[3, 5]\ngoogle play\n13\nFeature\n0.979507\nApp performance\n0.969157\n\n\n\n\n\n\n\n\nTo convert the label index to string, we can use the label_lists attribute of tdm\n\nimport pandas as pd\n\n\ndf_val[['label_L1','label_L2']] = pd.DataFrame(df_val.label.tolist(), index= df_val.index)\n\n\ndf_val.head()\n\n\n\n\n\n\n\n\n\ntext\nlabel\nSource\niteration\npred_L1\npred_prob_L1\npred_L2\npred_prob_L2\nlabel_L1\nlabel_L2\n\n\n\n\n0\ngoogle play - lam phien\n[5, 12]\ngoogle play\n13\nFeature\n0.716796\nApp performance\n0.564550\n5\n12\n\n\n1\ngoogle play - .. t . À mà họ nữ ưu m\n[5, 12]\ngoogle play\n13\nOthers\n0.959613\nCannot defined\n0.986585\n5\n12\n\n\n2\ngoogle play - Cc lùa dao\n[5, 12]\ngoogle play\n13\nOthers\n0.989178\nCannot defined\n0.991321\n5\n12\n\n\n3\ngoogle play - Mặt hàng sp mình đều nhỡ với Gia...\n[2, 23]\ngoogle play\n13\nDelivery\n0.775884\nDelivery time\n0.585934\n2\n23\n\n\n4\ngoogle play - Chưa tối ưu tốt cho Android Oppo...\n[3, 5]\ngoogle play\n13\nFeature\n0.979507\nApp performance\n0.969157\n3\n5\n\n\n\n\n\n\n\n\n\ndf_val['label_L1']= df_val['label_L1'].apply(lambda x: tdm.label_lists[0][x]).values\ndf_val['label_L2']= df_val['label_L2'].apply(lambda x: tdm.label_lists[1][x]).values\n\n\nf1_score(df_val.label_L1,df_val.pred_L1,average='macro'),f1_score(df_val.label_L2,df_val.pred_L2,average='macro')\n\n(0.47986578127681945, 0.13702228095693464)\n\n\n\n\nPredict Test set\nWe will go through details on how to make a prediction on a completely new and raw dataset using our trained model. For now, let’s reuse the sample csv and pretend it’s our test set\n\ndf_test = TextDataMain.from_csv(Path('sample_data')/'sample_large.csv',return_df=True)\n\n----- Input Validation Precheck -----\nDataFrame contains duplicated values!\n-----&gt; Number of duplications: 16 rows\n\n\nWe added the required columns as we defined in training process, and remove all the labels\n\ndf_test = df_test.drop(['L1','L2'],axis=1)\ndf_test['iteration']=df_val.iteration.max()+1\n\n\ndf_test.head()\n\n\n\n\n\n\n\n\n\nSource\nContent\niteration\n\n\n\n\n0\nGoogle Play\nApp ncc lúc nào cx lag đơ, phần tìm kiếm thì v...\n21\n\n\n1\nNon Owned\n..❗️ GÓC THANH LÝ Tính ra rẻ hơn cả mua #Shope...\n21\n\n\n2\nGoogle Play\nMắc gì người ta đặt hàng toàn lỗi 😃????\n21\n\n\n3\nOwned\n#GhienShopeePayawardT8 Khi bạn chơi shopee quá...\n21\n\n\n4\nGoogle Play\nRất bức xúc khi dùng . mã giảm giá người dùng ...\n21\n\n\n\n\n\n\n\n\nWe will create a DatasetDict for this test dataframe\n\ntest_ddict = tdm.get_test_datasetdict_from_df(df_test)\n\n-------------------- Getting Test Set --------------------\n----- Input Validation Precheck -----\nDataFrame contains duplicated values!\n-----&gt; Number of duplications: 19 rows\n-------------------- Start Test Set Transformation --------------------\n----- Metadata Simple Processing & Concatenating to Main Content -----\n-------------------- Text Transformation --------------------\n----- text_normalize -----\n-------------------- Test Leak Checking --------------------\n- Before leak check\nSize: 2269\n- After leak check\nSize: 2080\n- Number of rows leaked: 189, or 8.33% of the original validation (or test) data\n-------------------- Construct DatasetDict --------------------\n\n\n100%|█████████████████████████████████████| 2269/2269 [00:00&lt;00:00, 3963.29it/s]\n\n\n\n\n\n\ntest_ddict\n\nDatasetDict({\n    test: Dataset({\n        features: ['text', 'Source', 'iteration', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 2269\n    })\n})\n\n\nOur test data has been processed + transformed (but not augmented) the same way as the validation set. Now we can start making the prediction\n\ncontroller = ModelController(model,tdm)\ndf_result = controller.predict_ddict(ddict=test_ddict,ds_type='test')\n\n-------------------- Start making predictions --------------------\n\n\n\n\n\n\ndf_result.head()\n\n\n\n\n\n\n\n\n\ntext\nSource\niteration\npred_L1\npred_prob_L1\npred_L2\npred_prob_L2\n\n\n\n\n0\ngoogle play - App ncc lúc nào cx lag đơ , phần...\ngoogle play\n21\nFeature\n0.975637\nApp performance\n0.967774\n\n\n1\nnon owned - .. ❗ ️ GÓC THANH LÝ Tính ra rẻ hơn...\nnon owned\n21\nOthers\n0.998579\nCannot defined\n0.998430\n\n\n2\ngoogle play - Mắc gì người ta đặt hàng toàn lỗ...\ngoogle play\n21\nFeature\n0.800026\nApp performance\n0.596693\n\n\n3\nowned - # GhienShopeePayawardT8 Khi bạn chơi s...\nowned\n21\nCommercial\n0.984138\nShopee Programs\n0.992757\n\n\n4\ngoogle play - Rất bức xúc khi dùng . mã giảm g...\ngoogle play\n21\nFeature\n0.852134\nApply Voucher\n0.888675\n\n\n\n\n\n\n\n\nWe can even predict top k results\n\ndf_result = controller.predict_ddict(ddict=test_ddict,ds_type='test',topk=3)\ndf_result.head()\n\n-------------------- Start making predictions --------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\ntext\nSource\niteration\npred_L1\npred_prob_L1\npred_L2\npred_prob_L2\npred_L1_top1\npred_L1_top2\npred_L1_top3\npred_prob_L1_top1\npred_prob_L1_top2\npred_prob_L1_top3\npred_L2_top1\npred_L2_top2\npred_L2_top3\npred_prob_L2_top1\npred_prob_L2_top2\npred_prob_L2_top3\n\n\n\n\n0\ngoogle play - App ncc lúc nào cx lag đơ , phần...\ngoogle play\n21\n[3, 9, 5]\n[0.97563654, 0.010406992, 0.0027771755]\n[5, 13, 64]\n[0.96777445, 0.0056910617, 0.004303468]\nFeature\nShopee account\nOthers\n0.975637\n0.010407\n0.002777\nApp performance\nCart & Order\nSign up/Log in\n0.967774\n0.005691\n0.004303\n\n\n1\nnon owned - .. ❗ ️ GÓC THANH LÝ Tính ra rẻ hơn...\nnon owned\n21\n[5, 3, 1]\n[0.9985789, 0.00039970872, 0.00036777306]\n[12, 51, 5]\n[0.99842995, 0.0003309189, 0.00020129156]\nOthers\nFeature\nCommercial\n0.998579\n0.000400\n0.000368\nCannot defined\nSeller\nApp performance\n0.998430\n0.000331\n0.000201\n\n\n2\ngoogle play - Mắc gì người ta đặt hàng toàn lỗ...\ngoogle play\n21\n[3, 5, 4]\n[0.8000262, 0.16604221, 0.008356518]\n[5, 12, 13]\n[0.5966933, 0.30447525, 0.01972791]\nFeature\nOthers\nOrder/Item\n0.800026\n0.166042\n0.008357\nApp performance\nCannot defined\nCart & Order\n0.596693\n0.304475\n0.019728\n\n\n3\nowned - # GhienShopeePayawardT8 Khi bạn chơi s...\nowned\n21\n[1, 6, 8]\n[0.9841381, 0.012236756, 0.00065109425]\n[59, 63, 29]\n[0.99275726, 0.0051313634, 0.0004922476]\nCommercial\nPayment\nServices\n0.984138\n0.012237\n0.000651\nShopee Programs\nShopeePay\nGames/Minigames\n0.992757\n0.005131\n0.000492\n\n\n4\ngoogle play - Rất bức xúc khi dùng . mã giảm g...\ngoogle play\n21\n[3, 9, 6]\n[0.8521336, 0.103130266, 0.019738447]\n[6, 64, 7]\n[0.88867503, 0.022641763, 0.021038497]\nFeature\nShopee account\nPayment\n0.852134\n0.103130\n0.019738\nApply Voucher\nSign up/Log in\nBanned/blocked\n0.888675\n0.022642\n0.021038\n\n\n\n\n\n\n\n\nIf we just want to make a prediction on a small amount of data (single sentence, or a few sentences), we can use ModelController.predict_raw_text\n\n# Since we have some metadatas, we need to define a dictionary (to imitate a DatasetDict)\nraw_content={\n    'Source': 'Google play',\n    'iteration':21,\n    'Content':'Tôi không thích Shopee.Tại vì dùng app rất chậm,lag banh nhà lầu, thậm chí log in còn không đc'\n}\n\nIf we don’t use metadata, we can use something like this:\nraw_content='Tôi không thích Shopee.Tại vì dùng app rất chậm,lag banh nhà lầu, thậm chí log in còn không đc'\n\ndf_result = controller.predict_raw_text(raw_content,topk=1)\ndf_result\n\n100%|███████████████████████████████████████████| 1/1 [00:00&lt;00:00, 4438.42it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntext\nSource\niteration\npred_L1\npred_prob_L1\npred_L2\npred_prob_L2\n\n\n\n\n0\ngoogle play - Tôi không thích Shopee . Tại vì ...\ngoogle play\n21\nFeature\n0.983907\nApp performance\n0.982425"
  },
  {
    "objectID": "deprecated/model_main_envibert_conditional_prob-copy1.html",
    "href": "deprecated/model_main_envibert_conditional_prob-copy1.html",
    "title": "Model Controller Tutorial: EnviBert model with Conditional Probability",
    "section": "",
    "text": "We will walk through other cases of classification: multi-head and multi-label. Since we will showcase the capabiilty of this label in these cases, there won’t be as detailed as this tutorial"
  },
  {
    "objectID": "deprecated/model_main_envibert_conditional_prob-copy1.html#load-data",
    "href": "deprecated/model_main_envibert_conditional_prob-copy1.html#load-data",
    "title": "Model Controller Tutorial: EnviBert model with Conditional Probability",
    "section": "Load data",
    "text": "Load data\n\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom that_nlp_library.text_main import *\n\n\nfrom underthesea import text_normalize\nfrom functools import partial\nfrom pathlib import Path\nfrom importlib.machinery import SourceFileLoader\nfrom transformers import DataCollatorWithPadding\nimport pandas as pd\n\nDefine some necessary text augmentations and text transformations\n\nFor Text Transformation\n\n\ntxt_tfms=[text_normalize]\n\n\nFor Text Augmentation\n\n\nover_nonown_tfm = partial(sampling_with_condition,query='Source==\"non owned\"',frac=0.5,seed=42,apply_to_all=False)\nover_nonown_tfm.__name__ = 'Oversampling Non Owned'\n\nover_own_tfm = partial(sampling_with_condition,query='Source==\"owned\"',frac=2,seed=42,apply_to_all=False)\nover_own_tfm.__name__ = 'Oversampling Owned'\n\nover_hc_tfm = partial(sampling_with_condition,query='Source==\"hc search\"',frac=2.5,seed=42,apply_to_all=False)\nover_hc_tfm.__name__ = 'Oversampling HC search'\n\nremove_accent_tfm = partial(remove_vnmese_accent,frac=1,seed=42,apply_to_all=True)\nremove_accent_tfm.__name__ = 'Add No-Accent Text'\n\naug_tfms = [over_nonown_tfm,over_own_tfm,over_hc_tfm,remove_accent_tfm]\n\nLet’s load and preprocess our data\n\nDATA_PATH = Path('secret_data')\n\n\ndf = TextDataMain.from_csv(DATA_PATH/'buyer_listening_with_all_raw_data_w2223.csv',return_df=True)\n\n----- Input Validation Precheck -----\nDataFrame contains missing values!\n-----&gt; List of columns and the number of missing values for each\nis_valid    65804\ndtype: int64\nDataFrame contains duplicated values!\n-----&gt; Number of duplications: 7 rows\n\n\n\ndf.head(3)\n\n\n\n\n\n\n\n\n\nWeek\nGroup\nSource\nContent\nL1\nL2\nL3\nL4\nis_valid\niteration\n\n\n\n\n0\n1.0\nGoogle Play\nGoogle Play\nTại sao cứ hiện thông báo\nServices\nShopee communication channels\nAnnoying pop-up ads\nNon-tech\nNaN\n1\n\n\n1\n1.0\nGoogle Play\nGoogle Play\nMlem\nOthers\nCannot defined\n-\n-\nNaN\n1\n\n\n2\n1.0\nGoogle Play\nGoogle Play\n1 số sản phẩm trong giỏ hàng vừa đc cập nhật t...\nFeature\nCart & Order\nCart issues/suggestions\nTech\nNaN\n1\n\n\n\n\n\n\n\n\nQuick preprocess of data and train/validation split. Due to custom logic, we will sample our data here instead of using the train_ratio from the to_datasetdict function\n\ndf_rare = df[df.L2.isin(['Chatbot', 'Commercial Others'])].copy()\n\ndf_final = pd.concat([df.query('iteration==1').sample(500,random_state=42),\n                      df.query('iteration&gt;=7 & iteration&lt;13').sample(1200,random_state=42),\n                      df_rare,\n                      df.query('iteration&gt;=13'),\n                     ],axis=0).reset_index(drop=True)\n\nval_idxs = df_final[df_final.iteration&gt;=13].index.values # from week 9\n\n\ntdm = TextDataMain(df_final,\n                    main_content='Content',\n                    metadatas='Source',\n                    label_names=['L1','L2'],\n                    val_ratio=val_idxs,\n                    split_cols='L1',\n                    content_tfms = txt_tfms,\n                    aug_tfms = aug_tfms,\n                    process_metadatas=True,\n                    seed=42,\n                    shuffle_trn=True)\n\n----- Input Validation Precheck -----\nDataFrame contains missing values!\n-----&gt; List of columns and the number of missing values for each\nis_valid    498\ndtype: int64\n\n\nDefine our tokenizer for EnviBert\n\ncache_dir=Path('./envibert_tokenizer')\ntokenizer = SourceFileLoader(\"envibert.tokenizer\", \n                             str(cache_dir/'envibert_tokenizer.py')).load_module().RobertaTokenizer(cache_dir)\n\nCreate our DatasetDict from TextDataMain (as our ModelController class can also work with DatasetDict)\n\nmain_ddict= tdm.to_datasetdict(tokenizer,\n                               max_length=512,\n                               )\n\n-------------------- Start Main Text Processing --------------------\n----- Metadata Simple Processing & Concatenating to Main Content -----\n----- Label Encoding -----\n-------------------- Text Transformation --------------------\n----- text_normalize -----\n-------------------- Train Test Split --------------------\nPrevious Validation Percentage: 74.101%\n- Before leak check\nSize: 4927\n- After leak check\nSize: 4885\n- Number of rows leaked: 42, or 0.85% of the original validation (or test) data\nCurrent Validation Percentage: 73.47%\n-------------------- Text Augmentation --------------------\nTrain data size before augmentation: 1764\n----- Oversampling Non Owned -----\nTrain data size after THIS augmentation: 2229\n----- Oversampling Owned -----\nTrain data size after THIS augmentation: 2789\n----- Oversampling HC search -----\nTrain data size after THIS augmentation: 2904\n----- Add No-Accent Text -----\nTrain data size after THIS augmentation: 5808\nTrain data size after ALL augmentation: 5808\n-------------------- Map Tokenize Function --------------------\n\n\n100%|█████████████████████████████████████| 6649/6649 [00:01&lt;00:00, 3594.75it/s]\n100%|████████████████████████████████████| 2904/2904 [00:00&lt;00:00, 10563.57it/s]\n\n\n\n\n\n\n\n\n\nmain_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 5808\n    })\n    validation: Dataset({\n        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 4885\n    })\n})\n\n\n\nmain_ddict['validation']['label'][:5]\n\n[[5, 12], [5, 12], [5, 12], [2, 23], [3, 5]]"
  },
  {
    "objectID": "deprecated/model_main_envibert_conditional_prob-copy1.html#build-conditional-mask",
    "href": "deprecated/model_main_envibert_conditional_prob-copy1.html#build-conditional-mask",
    "title": "Model Controller Tutorial: EnviBert model with Conditional Probability",
    "section": "Build Conditional Mask",
    "text": "Build Conditional Mask\n\ndf_labels = tdm.df[tdm.label_names]\n\n\ndf_labels.head()\n\n\n\n\n\n\n\n\n\nL1\nL2\n\n\n\n\n0\n1\n59\n\n\n1\n5\n12\n\n\n2\n1\n59\n\n\n3\n1\n43\n\n\n4\n5\n12\n\n\n\n\n\n\n\n\n\nstandard_mask = build_standard_condition_mask(df_labels,*tdm.label_names)\n\n\nstandard_mask.shape\n\ntorch.Size([10, 76])\n\n\nExplain the first row of the mask\n\nstandard_mask[0]\n\ntensor([ True, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n         True, False, False, False, False, False, False, False, False, False,\n         True, False, False, False, False, False, False, False, False, False,\n        False,  True,  True, False, False, False, False, False, False, False,\n        False, False,  True,  True,  True, False, False, False, False, False,\n        False, False, False, False, False, False])\n\n\nSlicing the first portion for level 1, show string for True mask\n\nfor i in torch.where(standard_mask[0][:len(tdm.label_lists[0])]==True)[0]:\n    print(tdm.label_lists[0][i])\n\nBuyer complained seller\n\n\nSlicing the first portion for level 2, show string for True mask. The results are the sub-category of level 1 class Buyer complained seller\n\nfor i in torch.where(standard_mask[0][len(tdm.label_lists[0]):]==True)[0]:\n    print(tdm.label_lists[1][i])\n\nCustomer service (didn't respond/impolite)\nIllegal/counterfeit products\nProduct description\nProduct quality\nSellers cancelled order without any advanced notice/reason\nSellers cheated Buyers (Sellers tried to reach me outside of Shopee App)\nSellers packed fake orders"
  },
  {
    "objectID": "deprecated/model_main_envibert_conditional_prob-copy1.html#train-envibert-with-hidden-layer-concatenation-using-tdm",
    "href": "deprecated/model_main_envibert_conditional_prob-copy1.html#train-envibert-with-hidden-layer-concatenation-using-tdm",
    "title": "Model Controller Tutorial: EnviBert model with Conditional Probability",
    "section": "Train EnviBert (with hidden layer concatenation), using TDM",
    "text": "Train EnviBert (with hidden layer concatenation), using TDM\n\nmodel_name='nguyenvulebinh/envibert'\nenvibert_body = RobertaModel.from_pretrained(model_name)\n\nSome weights of the model checkpoint at nguyenvulebinh/envibert were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\nLet’s create our model controller\n\n_model_kwargs={\n    # overall model hyperparams\n    'size_l1':len(tdm.label_lists[0]),\n    'size_l2':len(tdm.label_lists[1]),\n    'standard_mask':standard_mask,\n    'layer2concat':4,\n    'head_class': ConcatHeadSimple,\n    # classfication head hyperparams\n    'classifier_dropout':0.1 \n}\n\nmodel = model_init_classification(model_class = RobertaHSCCProbSequenceClassification,\n                                  cpoint_path = model_name, \n                                  output_hidden_states=True, # since we are not using 'hidden layer contatenation' technique\n                                  seed=42,\n                                  body_model=envibert_body,\n                                  model_kwargs = _model_kwargs)\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score] # we will use both f1_macro and accuracy score as metrics\ncontroller = ModelController(model,tdm,metric_funcs)\n\nLoading body weights. This assumes the body is the very first first-layer block of your custom architecture\n\n\nAnd we can start training our model\n\nlr = 8.2e-5\nbs=4\nwd=0.01\nepochs= 4\n\ncontroller.fit(epochs,lr,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics_classification,\n              )\n\n/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\n\n\n\n    \n      \n      \n      [2904/2904 03:20, Epoch 4/4]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1 Score L1\nAccuracy Score L1\nF1 Score L2\nAccuracy Score L2\n\n\n\n\n1\nNo log\n0.262642\n0.085617\n0.394678\n0.024601\n0.195292\n\n\n2\n0.177300\n0.248724\n0.098848\n0.392835\n0.040627\n0.222723\n\n\n3\n0.177300\n0.260753\n0.103095\n0.393245\n0.043971\n0.242784\n\n\n4\n0.042900\n0.270646\n0.104099\n0.392631\n0.045365\n0.241760\n\n\n\n\n\n\n\n\ncontroller.trainer.model.save_pretrained('./sample_weights/my_model')"
  },
  {
    "objectID": "deprecated/model_main_envibert_conditional_prob-copy1.html#predict-using-trained-model-using-tdm",
    "href": "deprecated/model_main_envibert_conditional_prob-copy1.html#predict-using-trained-model-using-tdm",
    "title": "Model Controller Tutorial: EnviBert model with Conditional Probability",
    "section": "Predict using trained model, using TDM",
    "text": "Predict using trained model, using TDM\n\nLoad trained model\n\n_model_kwargs\n\n{'size_l1': 10,\n 'size_l2': 66,\n 'standard_mask': tensor([[ True, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False, False, False, False, False, False,\n           True, False, False, False, False, False, False, False, False, False,\n           True, False, False, False, False, False, False, False, False, False,\n          False,  True,  True, False, False, False, False, False, False, False,\n          False, False,  True,  True,  True, False, False, False, False, False,\n          False, False, False, False, False, False],\n         [False,  True, False, False, False, False, False, False, False, False,\n          False, False, False, False,  True, False, False, False, False, False,\n          False, False,  True, False, False, False,  True, False, False, False,\n          False, False, False, False, False, False, False, False,  True,  True,\n          False, False,  True, False, False, False, False, False, False, False,\n          False, False, False,  True, False, False, False, False, False, False,\n          False, False, False, False, False, False, False, False, False,  True,\n          False, False, False, False, False, False],\n         [False, False,  True, False, False, False, False, False, False, False,\n           True, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False, False, False, False, False, False,\n          False,  True,  True,  True, False, False,  True, False, False, False,\n          False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False, False, False,  True, False, False,\n          False, False, False, False, False, False, False,  True, False, False,\n          False, False, False, False, False, False],\n         [False, False, False,  True, False, False, False, False, False, False,\n          False, False, False, False, False,  True,  True, False, False, False,\n          False, False, False,  True,  True, False, False, False, False, False,\n          False, False, False, False,  True, False, False,  True, False, False,\n          False, False, False, False,  True, False, False, False, False, False,\n          False, False, False, False, False, False,  True, False, False, False,\n           True, False, False, False, False, False, False, False, False, False,\n          False,  True, False, False, False,  True],\n         [False, False, False, False,  True, False, False, False, False, False,\n          False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False, False, False, False, False,  True,\n          False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False,  True,  True, False, False, False,\n          False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False,  True, False, False, False, False,\n          False, False, False, False, False, False],\n         [False, False, False, False, False,  True, False, False, False, False,\n          False, False, False, False, False, False, False, False,  True,  True,\n          False, False,  True, False, False, False, False, False,  True, False,\n          False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False, False, False, False, False,  True,\n          False,  True, False, False, False, False, False, False,  True,  True,\n          False, False, False, False, False, False],\n         [False, False, False, False, False, False,  True, False, False, False,\n          False, False, False, False, False, False, False, False, False, False,\n           True,  True, False, False, False, False, False, False, False, False,\n          False, False, False, False, False, False, False, False, False, False,\n          False,  True, False, False, False, False, False,  True,  True,  True,\n          False, False, False, False, False, False, False, False,  True, False,\n          False, False, False, False, False, False, False, False, False, False,\n          False, False, False,  True, False, False],\n         [False, False, False, False, False, False, False,  True, False, False,\n          False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False,  True, False, False, False, False,\n          False, False, False, False, False, False, False, False, False, False,\n           True, False, False, False,  True,  True, False, False, False, False,\n          False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False, False],\n         [False, False, False, False, False, False, False, False,  True, False,\n          False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False,  True, False,  True, False, False,\n          False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False, False,  True, False, False, False,\n          False, False,  True, False, False, False],\n         [False, False, False, False, False, False, False, False, False,  True,\n          False,  True,  True,  True, False, False, False,  True, False, False,\n          False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False, False, False, False, False, False,\n          False, False, False,  True, False, False, False, False, False, False,\n          False, False, False, False, False, False, False, False, False, False,\n          False, False, False, False, False, False, False, False, False, False,\n           True, False, False, False,  True, False]]),\n 'layer2concat': 4,\n 'head_class': that_nlp_library.models.roberta.classifiers.ConcatHeadSimple,\n 'classifier_dropout': 0.1}\n\n\n\ntrained_model = model_init_classification(model_class = RobertaHSCCProbSequenceClassification,\n                                          cpoint_path = Path('./sample_weights/my_model'), \n                                          output_hidden_states=True,\n                                          seed=42,\n                                          model_kwargs = _model_kwargs)\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score]\ncontroller = ModelController(trained_model,tdm,metric_funcs)\n\nSome weights of the model checkpoint at sample_weights/my_model were not used when initializing RobertaHSCCProbSequenceClassification: ['body_model.pooler.dense.bias', 'body_model.pooler.dense.weight']\n- This IS expected if you are initializing RobertaHSCCProbSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaHSCCProbSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\n\nPredict Train/Validation set\nMake prediction on all validation set\n\ndf_val = controller.predict_ddict(ds_type='validation',batch_size=8)\n\n-------------------- Start making predictions --------------------\n\n\n\n\n\n\ndf_val.head()\n\n\n\n\n\n\n\n\n\ntext\nlabel\nSource\npred_L1\npred_prob_L1\npred_L2\npred_prob_L2\n\n\n\n\n0\ngoogle play - lam phien\n[5, 12]\ngoogle play\nOthers\n0.289139\nPromotions\n0.553118\n\n\n1\ngoogle play - .. t . À mà họ nữ ưu m\n[5, 12]\ngoogle play\nOthers\n0.547048\nCannot defined\n0.769387\n\n\n2\ngoogle play - Cc lùa dao\n[5, 12]\ngoogle play\nOthers\n0.480877\nCannot defined\n0.680652\n\n\n3\ngoogle play - Mặt hàng sp mình đều nhỡ với Gia...\n[2, 23]\ngoogle play\nOthers\n0.534984\nCannot defined\n0.597015\n\n\n4\ngoogle play - Chưa tối ưu tốt cho Android Oppo...\n[3, 5]\ngoogle play\nOthers\n0.564101\nCannot defined\n0.852026\n\n\n\n\n\n\n\n\nTo convert the label index to string, we can use the label_lists attribute of tdm\n\nimport pandas as pd\n\n\ndf_val[['label_L1','label_L2']] = pd.DataFrame(df_val.label.tolist(), index= df_val.index)\n\n\ndf_val.head()\n\n\n\n\n\n\n\n\n\ntext\nlabel\nSource\npred_L1\npred_prob_L1\npred_L2\npred_prob_L2\nlabel_L1\nlabel_L2\n\n\n\n\n0\ngoogle play - lam phien\n[5, 12]\ngoogle play\nOthers\n0.289139\nPromotions\n0.553118\n5\n12\n\n\n1\ngoogle play - .. t . À mà họ nữ ưu m\n[5, 12]\ngoogle play\nOthers\n0.547048\nCannot defined\n0.769387\n5\n12\n\n\n2\ngoogle play - Cc lùa dao\n[5, 12]\ngoogle play\nOthers\n0.480877\nCannot defined\n0.680652\n5\n12\n\n\n3\ngoogle play - Mặt hàng sp mình đều nhỡ với Gia...\n[2, 23]\ngoogle play\nOthers\n0.534984\nCannot defined\n0.597015\n2\n23\n\n\n4\ngoogle play - Chưa tối ưu tốt cho Android Oppo...\n[3, 5]\ngoogle play\nOthers\n0.564101\nCannot defined\n0.852026\n3\n5\n\n\n\n\n\n\n\n\n\ndf_val['label_L1']= df_val['label_L1'].apply(lambda x: tdm.label_lists[0][x]).values\ndf_val['label_L2']= df_val['label_L2'].apply(lambda x: tdm.label_lists[1][x]).values\n\n\nf1_score(df_val.label_L1,df_val.pred_L1,average='macro'),f1_score(df_val.label_L2,df_val.pred_L2,average='macro')\n\n(0.10407897411613716, 0.045350502179752714)\n\n\n\n\nPredict Test set\nWe will go through details on how to make a prediction on a completely new and raw dataset using our trained model. For now, let’s reuse the sample csv and pretend it’s our test set\n\ndf_test = TextDataMain.from_csv(Path('sample_data')/'sample_large.csv',return_df=True)\n\n----- Input Validation Precheck -----\nDataFrame contains duplicated values!\n-----&gt; Number of duplications: 16 rows\n\n\nWe will remove all the labels and unnecessary columns\n\ndf_test = df_test.drop(['L1','L2'],axis=1)\n\n\ndf_test.head()\n\n\n\n\n\n\n\n\n\nSource\nContent\n\n\n\n\n0\nGoogle Play\nApp ncc lúc nào cx lag đơ, phần tìm kiếm thì v...\n\n\n1\nNon Owned\n..❗️ GÓC THANH LÝ Tính ra rẻ hơn cả mua #Shope...\n\n\n2\nGoogle Play\nMắc gì người ta đặt hàng toàn lỗi 😃????\n\n\n3\nOwned\n#GhienShopeePayawardT8 Khi bạn chơi shopee quá...\n\n\n4\nGoogle Play\nRất bức xúc khi dùng . mã giảm giá người dùng ...\n\n\n\n\n\n\n\n\nWe will create a DatasetDict for this test dataframe\n\ntest_ddict = tdm.get_test_datasetdict_from_df(df_test)\n\n-------------------- Getting Test Set --------------------\n----- Input Validation Precheck -----\nDataFrame contains duplicated values!\n-----&gt; Number of duplications: 19 rows\n-------------------- Start Test Set Transformation --------------------\n----- Metadata Simple Processing & Concatenating to Main Content -----\n-------------------- Text Transformation --------------------\n----- text_normalize -----\n-------------------- Test Leak Checking --------------------\n- Before leak check\nSize: 2269\n- After leak check\nSize: 2080\n- Number of rows leaked: 189, or 8.33% of the original validation (or test) data\n-------------------- Construct DatasetDict --------------------\n\n\n100%|█████████████████████████████████████| 2269/2269 [00:00&lt;00:00, 3951.52it/s]\n\n\n\n\n\n\ntest_ddict\n\nDatasetDict({\n    test: Dataset({\n        features: ['text', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 2269\n    })\n})\n\n\nOur test data has been processed + transformed (but not augmented) the same way as the validation set. Now we can start making the prediction\n\ncontroller = ModelController(model,tdm)\ndf_result = controller.predict_ddict(ddict=test_ddict,ds_type='test')\n\n-------------------- Start making predictions --------------------\n\n\n\n\n\n\ndf_result.head()\n\n\n\n\n\n\n\n\n\ntext\nSource\npred_L1\npred_prob_L1\npred_L2\npred_prob_L2\n\n\n\n\n0\ngoogle play - App ncc lúc nào cx lag đơ , phần...\ngoogle play\nOthers\n0.537956\nSeller\n0.182006\n\n\n1\nnon owned - .. ❗ ️ GÓC THANH LÝ Tính ra rẻ hơn...\nnon owned\nOthers\n0.592825\nCannot defined\n0.822957\n\n\n2\ngoogle play - Mắc gì người ta đặt hàng toàn lỗ...\ngoogle play\nOthers\n0.435749\nDispute\n0.628866\n\n\n3\nowned - # GhienShopeePayawardT8 Khi bạn chơi s...\nowned\nCommercial\n0.906357\nShopee Programs\n0.785214\n\n\n4\ngoogle play - Rất bức xúc khi dùng . mã giảm g...\ngoogle play\nOthers\n0.266986\nPromotions\n0.322534\n\n\n\n\n\n\n\n\nWe can even predict top k results\n\ndf_result = controller.predict_ddict(ddict=test_ddict,ds_type='test',topk=3)\ndf_result.head()\n\n-------------------- Start making predictions --------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\ntext\nSource\npred_L1\npred_prob_L1\npred_L2\npred_prob_L2\npred_L1_top1\npred_L1_top2\npred_L1_top3\npred_prob_L1_top1\npred_prob_L1_top2\npred_prob_L1_top3\npred_L2_top1\npred_L2_top2\npred_L2_top3\npred_prob_L2_top1\npred_prob_L2_top2\npred_prob_L2_top3\n\n\n\n\n0\ngoogle play - App ncc lúc nào cx lag đơ , phần...\ngoogle play\n[5, 2, 1]\n[0.5379561, 0.1072576, 0.10138138]\n[51, 17, 35]\n[0.18200605, 0.16479525, 0.14931145]\nOthers\nDelivery\nCommercial\n0.537956\n0.107258\n0.101381\nSeller\nContact Agent\nOrder cancelled\n0.182006\n0.164795\n0.149311\n\n\n1\nnon owned - .. ❗ ️ GÓC THANH LÝ Tính ra rẻ hơn...\nnon owned\n[5, 3, 2]\n[0.59282476, 0.11958726, 0.093639985]\n[12, 5, 15]\n[0.82295734, 0.07322491, 0.020063285]\nOthers\nFeature\nDelivery\n0.592825\n0.119587\n0.093640\nCannot defined\nApp performance\nChatbot\n0.822957\n0.073225\n0.020063\n\n\n2\ngoogle play - Mắc gì người ta đặt hàng toàn lỗ...\ngoogle play\n[5, 3, 2]\n[0.43574947, 0.11776977, 0.11724175]\n[25, 43, 35]\n[0.62886584, 0.17175609, 0.044966247]\nOthers\nFeature\nDelivery\n0.435749\n0.117770\n0.117242\nDispute\nPromotions\nOrder cancelled\n0.628866\n0.171756\n0.044966\n\n\n3\nowned - # GhienShopeePayawardT8 Khi bạn chơi s...\nowned\n[1, 6, 5]\n[0.9063568, 0.035481054, 0.014191813]\n[59, 63, 45]\n[0.78521365, 0.13409148, 0.024677942]\nCommercial\nPayment\nOthers\n0.906357\n0.035481\n0.014192\nShopee Programs\nShopeePay\nReturn/Refund Others\n0.785214\n0.134091\n0.024678\n\n\n4\ngoogle play - Rất bức xúc khi dùng . mã giảm g...\ngoogle play\n[5, 1, 7]\n[0.26698554, 0.24040055, 0.12578185]\n[43, 35, 17]\n[0.3225337, 0.21654503, 0.12851809]\nOthers\nCommercial\nReturn/Refund\n0.266986\n0.240401\n0.125782\nPromotions\nOrder cancelled\nContact Agent\n0.322534\n0.216545\n0.128518\n\n\n\n\n\n\n\n\nIf we just want to make a prediction on a small amount of data (single sentence, or a few sentences), we can use ModelController.predict_raw_text\n\n# Since we have some metadatas, we need to define a dictionary (to imitate a DatasetDict)\nraw_content={\n    'Source': 'Google play',\n    'iteration':21,\n    'Content':'Tôi không thích Shopee.Tại vì dùng app rất chậm,lag banh nhà lầu, thậm chí log in còn không đc'\n}\n\nIf we don’t use metadata, we can use something like this:\nraw_content='Tôi không thích Shopee.Tại vì dùng app rất chậm,lag banh nhà lầu, thậm chí log in còn không đc'\n\ndf_result = controller.predict_raw_text(raw_content,topk=1)\ndf_result\n\n100%|███████████████████████████████████████████| 1/1 [00:00&lt;00:00, 4793.49it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntext\nSource\npred_L1\npred_prob_L1\npred_L2\npred_prob_L2\n\n\n\n\n0\ngoogle play - Tôi không thích Shopee . Tại vì ...\ngoogle play\nOthers\n0.544602\nCannot defined\n0.789728"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "utils",
    "section": "",
    "text": "source\n\nHiddenPrints\n\n HiddenPrints ()\n\nTo hide print command when called\n\nsource\n\n\ncreate_dir\n\n create_dir (path_dir)\n\nCreate directory if needed\n\nsource\n\n\nval2iterable\n\n val2iterable (val, lsize=1, t='list')\n\nConvert an element (nonlist value) to an iterable. Currently support list and nparray\n\nsource\n\n\nseed_everything\n\n seed_everything (seed=42)\n\n\nsource\n\n\nseed_notorch\n\n seed_notorch (seed=42)\n\n\nsource\n\n\nprint_msg\n\n print_msg (msg, dash_num=5, verbose=True)\n\n\nsource\n\n\ncallable_name\n\n callable_name (any_callable:Callable[...,Any])\n\nTo get name of any callable\n\nsource\n\n\ncheck_and_get_attribute\n\n check_and_get_attribute (obj, attr_name)\n\n\nsource\n\n\nload_pickle\n\n load_pickle (fname, parent='pickle_files')\n\n\nsource\n\n\nsave_to_pickle\n\n save_to_pickle (my_list, fname, parent='pickle_files')\n\n\nsource\n\n\ncheck_input_validation\n\n check_input_validation (df:pandas.core.frame.DataFrame, verbose=True)\n\n\nsource\n\n\ncheck_text_leaking\n\n check_text_leaking (trn_txt:list, test_txt:list, verbose=True)\n\n\nsource\n\n\nnone2emptystr\n\n none2emptystr (x)\n\n\nsource\n\n\nlambda_batch\n\n lambda_batch (inp, feature, func, is_batched)\n\n\n\n\n\nDetails\n\n\n\n\ninp\nHuggingFace Dataset\n\n\nfeature\nFeature name.\n\n\nfunc\nThe function to apply\n\n\nis_batched\nWhether batching is applied\n\n\n\n\nsource\n\n\nlambda_map_batch\n\n lambda_map_batch (inp, feature, func, is_batched, output_feature='same',\n                   is_func_batched=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninp\n\n\nHuggingFace Dataset\n\n\nfeature\n\n\nFeature name.\n\n\nfunc\n\n\nThe function to apply\n\n\nis_batched\n\n\nWhether batching is applied\n\n\noutput_feature\nstr\nsame\nNew feature output, if different from ‘feature’. If none, use function’s output feature(s)\n\n\nis_func_batched\nbool\nFalse\nWhether the func above only works with batch (mostly sklearn’s)\n\n\n\n\nsource\n\n\naugmentation_stream_generator\n\n augmentation_stream_generator (dset, text_name, func)\n\n\nsource\n\n\nfunc_all\n\n func_all (x, functions)\n\n\nsource\n\n\nget_dset_col_names\n\n get_dset_col_names (dset)\n\n\nsource\n\n\nhf_filter_dset\n\n hf_filter_dset (dset, func, is_batched=True, batch_size=1024, num_proc=1)\n\n\nsource\n\n\nhf_map_dset\n\n hf_map_dset (dset, func, is_batched=True, batch_size=1024, num_proc=1)\n\n\nsource\n\n\nresize_model_embeddings\n\n resize_model_embeddings (model, tokenizer)\n\n\nsource\n\n\nsigmoid\n\n sigmoid (x)\n\n*A numerically stable version of the logistic sigmoid function.\nSource: assignment3 of cs231n*",
    "crumbs": [
      "7. Utility",
      "utils"
    ]
  },
  {
    "objectID": "model_lm_main.html",
    "href": "model_lm_main.html",
    "title": "Language Model Main Functions and Controller",
    "section": "",
    "text": "from transformers import AutoModelForCausalLM, AutoModelForMaskedLM\n\n\nsource\n\nlanguage_model_init\n\n language_model_init (model_class, cpoint_path=None, config=None,\n                      device=None, seed=None)\n\nTo initialize a language model, either masked or casual\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel_class\n\n\nModel’s class object, e.g. AutoModelForMaskedLM\n\n\ncpoint_path\nNoneType\nNone\nEither model string name on HuggingFace, or the path to model checkpoint. Put None to train from scratch\n\n\nconfig\nNoneType\nNone\nModel config. If not provided, AutoConfig is used to load config from cpoint_path\n\n\ndevice\nNoneType\nNone\nDevice to train on\n\n\nseed\nNoneType\nNone\nRandom seed\n\n\n\n\n_model1 = language_model_init(AutoModelForMaskedLM,\n                              'roberta-base')\n_model1\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\nTotal parameters: 124697433\nTotal trainable parameters: 124697433\n\n\nRobertaForMaskedLM(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (lm_head): RobertaLMHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n  )\n)\n\n\n\n_model1 = language_model_init(AutoModelForMaskedLM,\n                              'nguyenvulebinh/envibert')\n_model1\n\nTotal parameters: 70764377\nTotal trainable parameters: 70764377\n\n\nRobertaForMaskedLM(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(59993, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-5): 6 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=1024, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=1024, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (lm_head): RobertaLMHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (decoder): Linear(in_features=768, out_features=59993, bias=True)\n  )\n)\n\n\n\n_model2 = language_model_init(AutoModelForCausalLM,\n                              'gpt2')\n_model2\n\nTotal parameters: 124439808\nTotal trainable parameters: 124439808\n\n\nGPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)\n\n\n\nsource\n\n\nfinetune_lm\n\n finetune_lm (lr, bs, wd, epochs, ddict, tokenizer, o_dir='./tmp_weights',\n              save_checkpoint=False, model=None, model_init=None,\n              data_collator=None, compute_metrics=None,\n              grad_accum_steps=2, lr_scheduler_type='cosine',\n              warmup_ratio=0.1, no_valid=False, val_bs=None, seed=None,\n              report_to='none', trainer_class=None, len_train=None)\n\nThe main model training/finetuning function\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlr\n\n\nLearning rate\n\n\nbs\n\n\nBatch size\n\n\nwd\n\n\nWeight decay\n\n\nepochs\n\n\nNumber of epochs\n\n\nddict\n\n\nThe HuggingFace datasetdict\n\n\ntokenizer\n\n\nHuggingFace tokenizer\n\n\no_dir\nstr\n./tmp_weights\nDirectory to save weights\n\n\nsave_checkpoint\nbool\nFalse\nWhether to save weights (checkpoints) to o_dir\n\n\nmodel\nNoneType\nNone\nNLP model\n\n\nmodel_init\nNoneType\nNone\nA function to initialize model\n\n\ndata_collator\nNoneType\nNone\nHuggingFace data collator\n\n\ncompute_metrics\nNoneType\nNone\nA function to compute metric, default to compute_lm_accuracy\n\n\ngrad_accum_steps\nint\n2\nThe batch at each step will be divided by this integer and gradient will be accumulated over gradient_accumulation_steps steps.\n\n\nlr_scheduler_type\nstr\ncosine\nThe scheduler type to use. Including: linear, cosine, cosine_with_restarts, polynomial, constant, constant_with_warmup\n\n\nwarmup_ratio\nfloat\n0.1\nThe warmup ratio for some lr scheduler\n\n\nno_valid\nbool\nFalse\nWhether there is a validation set or not\n\n\nval_bs\nNoneType\nNone\nValidation batch size\n\n\nseed\nNoneType\nNone\nRandom seed\n\n\nreport_to\nstr\nnone\nThe list of integrations to report the results and logs to. Supported platforms are “azure_ml”, “comet_ml”, “mlflow”, “neptune”, “tensorboard”,“clearml” and “wandb”. Use “all” to report to all integrations installed, “none” for no integrations.\n\n\ntrainer_class\nNoneType\nNone\nYou can include the class name of your custom trainer here\n\n\nlen_train\nNoneType\nNone\nestimated number of samples in the whole training set (for streaming dataset only)\n\n\n\n\nsource\n\n\nModelLMController\n\n ModelLMController (model, data_store=None, seed=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel\n\n\nNLP language model\n\n\ndata_store\nNoneType\nNone\na TextDataLMController/TextDataLMControllerStreaming object\n\n\nseed\nNoneType\nNone\nRandom seed\n\n\n\n\nsource\n\n\nModelLMController.fit\n\n ModelLMController.fit (epochs, learning_rate, ddict=None,\n                        compute_metrics=None, batch_size=16,\n                        val_batch_size=None, weight_decay=0.01,\n                        lr_scheduler_type='cosine', warmup_ratio=0.1,\n                        o_dir='./tmp_weights', save_checkpoint=False,\n                        hf_report_to='none', grad_accum_steps=2,\n                        tokenizer=None, data_collator=None, is_mlm=None,\n                        trainer_class=None, len_train=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nepochs\n\n\nNumber of epochs\n\n\nlearning_rate\n\n\nLearning rate\n\n\nddict\nNoneType\nNone\nDatasetDict to fit (will override data_store)\n\n\ncompute_metrics\nNoneType\nNone\nA function to compute metric, default to compute_lm_accuracy\n\n\nbatch_size\nint\n16\nBatch size\n\n\nval_batch_size\nNoneType\nNone\nValidation batch size. Set to batch_size if None\n\n\nweight_decay\nfloat\n0.01\nWeight decay\n\n\nlr_scheduler_type\nstr\ncosine\nThe scheduler type to use. Including: linear, cosine, cosine_with_restarts, polynomial, constant, constant_with_warmup\n\n\nwarmup_ratio\nfloat\n0.1\nThe warmup ratio for some lr scheduler\n\n\no_dir\nstr\n./tmp_weights\nDirectory to save weights\n\n\nsave_checkpoint\nbool\nFalse\nWhether to save weights (checkpoints) to o_dir\n\n\nhf_report_to\nstr\nnone\nThe list of HuggingFace-allowed integrations to report the results and logs to\n\n\ngrad_accum_steps\nint\n2\nGradient will be accumulated over gradient_accumulation_steps steps.\n\n\ntokenizer\nNoneType\nNone\nTokenizer (to override one in data_store)\n\n\ndata_collator\nNoneType\nNone\nData Collator (to override one in data_store)\n\n\nis_mlm\nNoneType\nNone\nWhether this is masked LM or casual LM\n\n\ntrainer_class\nNoneType\nNone\nYou can include the class name of your custom trainer here\n\n\nlen_train\nNoneType\nNone\nestimated number of samples in the whole training set (for streaming dataset only)\n\n\n\n\nsource\n\n\nModelLMController.predict_raw_text\n\n ModelLMController.predict_raw_text (content:dict|list|str,\n                                     print_result=True, **kwargs)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncontent\ndict | list | str\n\nEither a single sentence, list of sentence or a dictionary where keys are metadata, values are list\n\n\nprint_result\nbool\nTrue\nTo whether print the result in readable format, or get the result returned\n\n\nkwargs",
    "crumbs": [
      "6. Model Classes",
      "b. Model Controller for Language Modeling",
      "Language Model Main Functions and Controller"
    ]
  },
  {
    "objectID": "text_transformation.html",
    "href": "text_transformation.html",
    "title": "Text Transformation",
    "section": "",
    "text": "source\n\napply_vnmese_word_tokenize\n\n apply_vnmese_word_tokenize (sentence:str, normalize_text=False,\n                             fixed_words=[])\n\nApplying UnderTheSea Vietnamese word tokenization\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsentence\nstr\n\nInput sentence\n\n\nnormalize_text\nbool\nFalse\nTo ‘normalize’ the text before tokenization\n\n\nfixed_words\nlist\n[]\n\n\n\n\nFor non-Vietnamese word, it’s a hit-or-miss since UnderTheSea works best for Vietnamese sentences\n\ntext = 'This is a cat. New York city. San Francisco. New York and San Francisco Bay area. George Bush, Barrack Obama'\napply_vnmese_word_tokenize(text)\n\n'This is a_cat . New_York city . San_Francisco . New_York and_San_Francisco Bay area . George Bush , Barrack Obama'\n\n\nHere’s an example on a clean Vietnamese sentence\n\ntext = 'Chàng trai 9X Quảng Trị khởi nghiệp từ nấm sò'\napply_vnmese_word_tokenize(text)\n\n'Chàng trai 9X Quảng_Trị khởi_nghiệp từ nấm sò'\n\n\nWhat if the sentence is not cleaned?\n\ntext = \"Chàng trai 9X Quảng Trị khởi nghiệp từ nấm sò.Anh ấy không nuôi   nấm😊. nhưng anh này nuôi. Chị ấy lại không nuôi?(ai biết tại sao 😊😊? )Rồi? Rồi sao?rồi ?Rồi ủa...chứ chị ấy nuôi gì, #mộthaiba cũng không rõ =)) 😊. Haha :) 😊 hehe 😊.\"\n\n\napply_vnmese_word_tokenize(text)\n\n'Chàng trai 9X Quảng_Trị khởi_nghiệp từ nấm sò . Anh ấy không nuôi nấm 😊 . nhưng anh này nuôi . Chị ấy lại không nuôi ? ( ai biết tại_sao 😊_😊 ? ) Rồi ? Rồi sao ? rồi ? Rồi ủa ... chứ chị ấy nuôi gì , #_mộthaiba cũng không rõ =))_😊 . Haha :) 😊 hehe 😊 .'\n\n\nWe need to normalize the text\n\napply_vnmese_word_tokenize(text,normalize_text=True)\n\n'Chàng trai 9X Quảng_Trị khởi_nghiệp từ nấm sò . Anh ấy không nuôi nấm 😊 . nhưng anh này nuôi . Chị ấy lại không nuôi ? ( ai biết tại_sao 😊_😊 ? ) Rồi ? Rồi sao ? rồi ? Rồi ủa ... chứ chị ấy nuôi gì , #_mộthaiba cũng không rõ =))_😊 . Haha :) 😊 hehe 😊 .'\n\n\nWe can add a list of specific words to tokenize\n\ntext = \"Viện Nghiên Cứu chiến lược quốc gia về học máy\"\napply_vnmese_word_tokenize(text)\n\n'Viện Nghiên_Cứu chiến_lược quốc_gia về học máy'\n\n\n\napply_vnmese_word_tokenize(text,fixed_words=[\"Viện Nghiên Cứu\", \"học máy\"])\n\n'Viện_Nghiên_Cứu chiến_lược quốc_gia về học_máy'",
    "crumbs": [
      "5. Text Classes",
      "a. Text Processing",
      "Text Transformation"
    ]
  },
  {
    "objectID": "models.roberta.deep_hierarchical_classifiers.html",
    "href": "models.roberta.deep_hierarchical_classifiers.html",
    "title": "Deep Hierarchical Classifiers",
    "section": "",
    "text": "import pandas as pd\n\n\nsource\n\nbuild_DHC_conditional_mask\n\n build_DHC_conditional_mask (df_labels, label1, label2)\n\nThis is really similar to build_standard_condition_mask, but we don’t concatenate l1 1-hot matrix and l2 1-hot matrix\n\n_df_labels=pd.DataFrame({\n    'col_1':[0,0,0,1,1,2,2,2],\n    'col_2':[0,1,2,3,4,5,6,7]\n})\n_df_labels\n\n\n\n\n\n\n\n\n\ncol_1\ncol_2\n\n\n\n\n0\n0\n0\n\n\n1\n0\n1\n\n\n2\n0\n2\n\n\n3\n1\n3\n\n\n4\n1\n4\n\n\n5\n2\n5\n\n\n6\n2\n6\n\n\n7\n2\n7\n\n\n\n\n\n\n\n\n\nl1l2_tensor = build_DHC_conditional_mask(_df_labels,'col_1','col_2')\n\n\nl1l2_tensor\n\ntensor([[1., 1., 1., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 1., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 1., 1.]])\n\n\n\nsource\n\n\nloss_for_DHC\n\n loss_for_DHC (l1_repr_logits, l2_repr_logits, labels_l1, labels_l2,\n               dhc_mask, lloss_weight=1.0, dloss_weight=0.8)\n\nReference: https://github.com/Ugenteraan/Deep_Hierarchical_Classification/blob/main/model/hierarchical_loss.py\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nl1_repr_logits\n\n\nHead 1’s logit output\n\n\nl2_repr_logits\n\n\nHead 2’s logit output\n\n\nlabels_l1\n\n\nTrue label for head 1\n\n\nlabels_l2\n\n\nTrue label for head 2\n\n\ndhc_mask\n\n\nA one-hot matrix between classes of head 1 and 2\n\n\nlloss_weight\nfloat\n1.0\nWeight for Layer Loss (lloss)\n\n\ndloss_weight\nfloat\n0.8\nWeight for Dependence Loss (dloss)\n\n\n\n\nsource\n\n\nRobertaConcatHeadDHCRoot\n\n RobertaConcatHeadDHCRoot (config, classifier_dropout=0.1,\n                           last_hidden_size=768, layer2concat=4, **kwargs)\n\nConcatenated head for Roberta DHC Classification Model.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nconfig\n\n\nHuggingFace model configuration\n\n\nclassifier_dropout\nfloat\n0.1\nDropout ratio (for dropout layer right before the last nn.Linear)\n\n\nlast_hidden_size\nint\n768\nLast hidden size (before the last nn.Linear)\n\n\nlayer2concat\nint\n4\nnumber of hidden layer to concatenate (counting from top)\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nRobertaSimpleHSCDHCSequenceClassification\n\n RobertaSimpleHSCDHCSequenceClassification (config, dhc_mask,\n                                            lloss_weight=1.0,\n                                            dloss_weight=0.8,\n                                            layer2concat=4, device=None)\n\nRoberta Simple-DHC Architecture with Hidden-State-Concatenation for Sequence Classification task\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nconfig\n\n\nHuggingFace model configuration\n\n\ndhc_mask\n\n\nA one-hot matrix between classes of head 1 and 2\n\n\nlloss_weight\nfloat\n1.0\nWeight for Layer Loss (lloss)\n\n\ndloss_weight\nfloat\n0.8\nWeight for Dependence Loss (dloss)\n\n\nlayer2concat\nint\n4\nnumber of hidden layer to concatenate (counting from top)\n\n\ndevice\nNoneType\nNone\nCPU or GPU\n\n\n\n\nsource\n\n\nRobertaHSCDHCSequenceClassification\n\n RobertaHSCDHCSequenceClassification (config, dhc_mask,\n                                      classifier_dropout=0.1,\n                                      last_hidden_size=768,\n                                      linear_l1_size=None,\n                                      linear_l2_size=None,\n                                      lloss_weight=1.0, dloss_weight=0.8,\n                                      layer2concat=4, device=None)\n\nRoberta DHC Architecture with Hidden-State-Concatenation for Sequence Classification task\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nconfig\n\n\nHuggingFace model configuration\n\n\ndhc_mask\n\n\nA one-hot matrix between classes of head 1 and 2\n\n\nclassifier_dropout\nfloat\n0.1\nDropout ratio (for dropout layer right before the last nn.Linear)\n\n\nlast_hidden_size\nint\n768\nLast hidden size (before the last nn.Linear)\n\n\nlinear_l1_size\nNoneType\nNone\nlast hidden size for head 1\n\n\nlinear_l2_size\nNoneType\nNone\nlast hidden size for head 2\n\n\nlloss_weight\nfloat\n1.0\nWeight for Layer Loss (lloss)\n\n\ndloss_weight\nfloat\n0.8\nWeight for Dependence Loss (dloss)\n\n\nlayer2concat\nint\n4\nnumber of hidden layer to concatenate (counting from top)\n\n\ndevice\nNoneType\nNone\nCPU or GPU",
    "crumbs": [
      "6. Model Classes",
      "c. Roberta-based classification Model",
      "Deep Hierarchical Classifiers"
    ]
  },
  {
    "objectID": "text_main_lm_streaming.html",
    "href": "text_main_lm_streaming.html",
    "title": "Text Main For Language Model - Streaming",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom importlib.machinery import SourceFileLoader\nfrom datasets import load_dataset\nimport os",
    "crumbs": [
      "5. Text Classes",
      "c. Text Controller For Language Modeling",
      "Text Main For Language Model - Streaming"
    ]
  },
  {
    "objectID": "text_main_lm_streaming.html#class-textdatalmcontrollerstreaming",
    "href": "text_main_lm_streaming.html#class-textdatalmcontrollerstreaming",
    "title": "Text Main For Language Model - Streaming",
    "section": "Class TextDataLMControllerStreaming",
    "text": "Class TextDataLMControllerStreaming\n\nsource\n\nTextDataLMControllerStreaming\n\n TextDataLMControllerStreaming (inp, main_text:str, filter_dict={},\n                                metadatas=[], process_metas=True,\n                                metas_sep='.', content_transformations=[],\n                                seed=None, batch_size=1024, num_proc=1,\n                                cols_to_keep=None, verbose=True)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninp\n\n\nHuggingFainpce Dataset or DatasetDict\n\n\nmain_text\nstr\n\nName of the main text column\n\n\nfilter_dict\ndict\n{}\nA dictionary: {feature: filtering_function_for_that_feature}\n\n\nmetadatas\nlist\n[]\nNames of the metadata columns\n\n\nprocess_metas\nbool\nTrue\nWhether to do simple text processing on the chosen metadatas\n\n\nmetas_sep\nstr\n.\nSeparator, for multiple metadatas concatenation\n\n\ncontent_transformations\nlist\n[]\nA list of text transformations\n\n\nseed\nNoneType\nNone\nRandom seed\n\n\nbatch_size\nint\n1024\nTransformation + Tokenization batch size\n\n\nnum_proc\nint\n1\nNumber of process for multiprocessing\n\n\ncols_to_keep\nNoneType\nNone\nColumns to keep after all processings\n\n\nverbose\nbool\nTrue\nWhether to prdint processing information",
    "crumbs": [
      "5. Text Classes",
      "c. Text Controller For Language Modeling",
      "Text Main For Language Model - Streaming"
    ]
  },
  {
    "objectID": "text_main_lm_streaming.html#load-data-basic-use-case",
    "href": "text_main_lm_streaming.html#load-data-basic-use-case",
    "title": "Text Main For Language Model - Streaming",
    "section": "1. Load data + Basic use case",
    "text": "1. Load data + Basic use case\nDataset source: https://www.kaggle.com/datasets/kavita5/review_ecommerce\nWith line-by-line tokenization\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\nddict_with_val = dset.train_test_split(test_size=0.1)\nddict_with_val['validation'] = ddict_with_val['test']\nddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\ndel ddict_with_val['test']\n\n\nddict_with_val\n\nDatasetDict({\n    train: IterableDataset({\n        features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'],\n        n_shards: 1\n    })\n    validation: Dataset({\n        features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'],\n        num_rows: 2349\n    })\n})\n\n\n\ntdc = TextDataLMControllerStreaming(ddict_with_val,\n                                    filter_dict={'Review Text': lambda x: x is not None},\n                                    main_text='Review Text',\n                                   )\n\n\nfrom transformers import AutoTokenizer\n\n\ntokenizer = AutoTokenizer.from_pretrained('roberta-base')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\nsource\n\nTextDataLMControllerStreaming.process_and_tokenize\n\n TextDataLMControllerStreaming.process_and_tokenize (tokenizer,\n                                                     max_length=None,\n                                                     tok_num_proc=None,\n                                                     line_by_line=True,\n                                                     stride=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntokenizer\n\n\nTokenizer (preferably from HuggingFace)\n\n\nmax_length\nNoneType\nNone\npad to model’s allowed max length (default is max_sequence_length). Use -1 for no padding at all\n\n\ntok_num_proc\nNoneType\nNone\nNumber of processes for tokenization\n\n\nline_by_line\nbool\nTrue\nTo whether tokenize each sentence separately, or concatenate them\n\n\nstride\nNoneType\nNone\noption to do striding when line_by_line is False\n\n\n\n\ntdc.process_and_tokenize(tokenizer,line_by_line=True)\n\n-------------------- Data Filtering --------------------\nDone\n----- Metadata Simple Processing & Concatenating to Main Content -----\nDone\n-------------------- Dropping unused features --------------------\nDone\n----- Performing Content Transformation and Tokenization on Validation Set -----\nDone\n----- Creating a generator for content transformation and tokenization on Train set -----\nDone\n\n\n\n\n\n\n\n\n\ntdc.main_ddict\n\nDatasetDict({\n    train: IterableDataset({\n        features: Unknown,\n        n_shards: 1\n    })\n    validation: Dataset({\n        features: ['Review Text', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n        num_rows: 2260\n    })\n})\n\n\n\nfor i,v in enumerate(tdc.main_ddict['validation']):\n    if i==1:break\n    print(f\"Input ids: {v['input_ids']}\\nDecoded: {tokenizer.decode(v['input_ids'])}\\nAttention Mask: {v['attention_mask']}\")\n\nInput ids: [0, 17781, 129, 1381, 15, 53, 24, 16, 10, 182, 11962, 16576, 8, 939, 206, 40, 28, 3473, 7, 3568, 4, 939, 657, 5, 18632, 9, 5, 1468, 4, 939, 109, 2813, 24, 376, 11, 97, 8089, 4, 24, 16, 10, 410, 19351, 1468, 53, 45, 350, 203, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\nDecoded: &lt;s&gt;Have only tried on but it is a very cute skirt and i think will be comfortable to wear. i love the texture of the material. i do wish it came in other colors. it is a little heavier material but not too much.&lt;/s&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;\nAttention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n\n\nfor i,v in enumerate(tdc.main_ddict['train']):\n    if i==1:break\n    print(f\"Input ids: {v['input_ids']}\\n\\nDecoded: {tokenizer.decode(v['input_ids'])}\\n\\nAttention Mask: {v['attention_mask']}\")\n\n\n\n\nInput ids: [0, 713, 23204, 21, 11, 12, 8005, 11, 10, 55, 7974, 3195, 24943, 8, 21, 98, 1256, 4, 77, 939, 1381, 24, 15, 939, 5324, 10, 367, 383, 35, 5, 23193, 8, 1318, 32, 2299, 89, 111, 42, 16, 10, 2579, 2125, 190, 23, 455, 12, 17212, 4, 5, 13422, 16, 14, 24, 1237, 182, 739, 4, 939, 437, 3700, 10, 501, 73, 1549, 8, 10, 1836, 3023, 462, 11, 144, 6215, 1964, 6, 8, 190, 5, 739, 21, 10, 828, 929, 219, 4, 5, 97, 631, 21, 14, 5, 21764, 9, 42, 23204, 32, 1256, 251, 36, 463, 939, 33, 251, 3701, 322, 528, 7, 5, 10342, 847, 9, 5, 24150, 47, 1705, 75, 190, 269, 740, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n\nDecoded: &lt;s&gt;This sweater was in-store in a more neutral color palette and was so pretty. when i tried it on i noticed a few things: the versatility and quality are definitely there - this is a nice piece even at full-price. the downside is that it runs very large. i'm typically a 14/16 and a size xl in most retailer items, and even the large was a bit roomy. the other thing was that the sleeves of this sweater are pretty long (and i have long arms). due to the bell cut of the sleeve you couldn't even really c&lt;/s&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;\n\nAttention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]",
    "crumbs": [
      "5. Text Classes",
      "c. Text Controller For Language Modeling",
      "Text Main For Language Model - Streaming"
    ]
  },
  {
    "objectID": "text_main_lm_streaming.html#filtering-metadatas-content-transformation-tokenization",
    "href": "text_main_lm_streaming.html#filtering-metadatas-content-transformation-tokenization",
    "title": "Text Main For Language Model - Streaming",
    "section": "2. Filtering + Metadatas + Content Transformation + Tokenization",
    "text": "2. Filtering + Metadatas + Content Transformation + Tokenization\nDefine our tokenization\n\nfrom transformers import RobertaTokenizer\nfrom underthesea import text_normalize\n\n\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\n\nfrom that_nlp_library.text_main_lm import TextDataLMController\n\n\na) Option 1: Tokenize our corpus line-by-line\n\nWith no padding\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\nddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\nddict_with_val['validation'] = ddict_with_val['test']\nddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\ndel ddict_with_val['test']\n\n\ntdc = TextDataLMControllerStreaming(ddict_with_val,\n                                    main_text='Review Text',\n                                    filter_dict={'Review Text': lambda x: x is not None},\n                                    metadatas=['Title','Division Name'],\n                                    content_transformations=[text_normalize,str.lower],\n                                    cols_to_keep=['Clothing ID','Review Text'],\n                                    seed=42,\n                                    batch_size=1024,\n                                    verbose=False\n                                    )\n\n\ntdc.process_and_tokenize(tokenizer,line_by_line=True,max_length=-1)\n\n\nprint(tokenizer.decode(next(iter(tdc.main_ddict['train']))['input_ids']))\nprint()\nprint(tokenizer.decode(tdc.main_ddict['validation']['input_ids'][0]))\n\n&lt;s&gt;general petite. beautiful top, worth the necessary tailoring. the beautiful bold print drew me to this top and it did not disappoint upon receipt. however, the bottom ruffle belled so far out on each side that it was laughable! the actual fit is nothing like the picture ; clearly the model's arms are placed in front of all the extra fabric to hold the ruffle back. however, the fabric is beautiful, the fit was perfect ( size 2, 5'4 \", 106 lbs. ), the quality is great and i love the print so i decided to take it to my tailor to \" sew away \" the \" wings \" on both si&lt;/s&gt;\n\n&lt;s&gt;general. soft, feminine and fun pockets!. i love this tunic. purchased the dark orange in medium ( i am 5'9 and 140 lbs ). tried the small and almost kept it but i felt seams around my arm pits a tad, so went with the medium and glad i did - this top should be comfortable. feels very fall and perfect for casual get-togethers and running around town. only comment is that it is rayon... and for me anyway rayon doesn't wash too well - so we shall see how this one fairs.&lt;/s&gt;\n\n\n\nfor i,v in enumerate(tdc.main_ddict['train']):\n    if i%100==0:\n        print(i)\n    if i==1024-1:\n        break\n    pass\n\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nCPU times: user 884 ms, sys: 1.07 ms, total: 885 ms\nWall time: 877 ms\n\n\nCompare to non-streamed version\n\ndset2 = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\nddict_with_val2 = dset2.train_test_split(test_size=0.1,seed=42)\nddict_with_val2['validation'] = ddict_with_val2['test']\ndel ddict_with_val2['test']\n\ntdc2 = TextDataLMController(ddict_with_val2,\n                            main_text='Review Text',\n                            filter_dict={'Review Text': lambda x: x is not None},\n                            metadatas=['Title','Division Name'],\n                            content_transformations=[text_normalize,str.lower],\n                            cols_to_keep=['Clothing ID','Review Text'],\n                            seed=42,\n                            batch_size=1024,\n                            verbose=False\n                            )\ntdc2.process_and_tokenize(tokenizer,line_by_line=True,max_length=-1,shuffle_trn=False)\n\n\n# check whether train sets are the same\nassert len(list(tdc.main_ddict['train']))==len(tdc2.main_ddict['train'])\n\n\niter1 = iter(tdc.main_ddict['train'])\niter2 = iter(tdc2.main_ddict['train'])\nfor a,b in zip(iter1,iter2):\n    assert a['input_ids']==b['input_ids']\n\n\nprint(a)\nprint('-'*20)\nprint(b)\n\n{'Clothing ID': 1056, 'Review Text': 'general . perfect pant . I picked these up the other day looking for a good jeans alternative. i love them. they are the perfect fit of slim but not skinny. i went with my normal size (26) and so far after one wear, they are still in good shape. a little bit of stretch, but not too much. the moss color is so crisp and goes with a lot. they will be perfect for transitioning into fall.', 'input_ids': [0, 15841, 479, 1969, 16259, 479, 939, 2738, 209, 62, 5, 97, 183, 546, 13, 10, 205, 10844, 3626, 479, 939, 657, 106, 479, 51, 32, 5, 1969, 2564, 9, 11875, 53, 45, 22877, 479, 939, 439, 19, 127, 2340, 1836, 36, 973, 4839, 8, 98, 444, 71, 65, 3568, 2156, 51, 32, 202, 11, 205, 3989, 479, 10, 410, 828, 9, 4140, 2156, 53, 45, 350, 203, 479, 5, 40711, 3195, 16, 98, 17766, 8, 1411, 19, 10, 319, 479, 51, 40, 28, 1969, 13, 26135, 88, 1136, 479, 2], 'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n--------------------\n{'Clothing ID': 1056, 'Review Text': 'general . perfect pant . i picked these up the other day looking for a good jeans alternative . i love them . they are the perfect fit of slim but not skinny . i went with my normal size ( 26 ) and so far after one wear , they are still in good shape . a little bit of stretch , but not too much . the moss color is so crisp and goes with a lot . they will be perfect for transitioning into fall .', 'input_ids': [0, 15841, 479, 1969, 16259, 479, 939, 2738, 209, 62, 5, 97, 183, 546, 13, 10, 205, 10844, 3626, 479, 939, 657, 106, 479, 51, 32, 5, 1969, 2564, 9, 11875, 53, 45, 22877, 479, 939, 439, 19, 127, 2340, 1836, 36, 973, 4839, 8, 98, 444, 71, 65, 3568, 2156, 51, 32, 202, 11, 205, 3989, 479, 10, 410, 828, 9, 4140, 2156, 53, 45, 350, 203, 479, 5, 40711, 3195, 16, 98, 17766, 8, 1411, 19, 10, 319, 479, 51, 40, 28, 1969, 13, 26135, 88, 1136, 479, 2], 'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n\n# check whether validation set is the same\nassert len(list(tdc.main_ddict['validation']))==len(tdc2.main_ddict['validation'])\n\niter1 = iter(tdc.main_ddict['validation'])\niter2 = iter(tdc2.main_ddict['validation'])\nfor a,b in zip(iter1,iter2):\n    assert a==b\n\n\n\nWith padding\n(set max_length to None if you want to pad to model’s maximum sequence length)\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\nddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\nddict_with_val['validation'] = ddict_with_val['test']\nddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\ndel ddict_with_val['test']\n\ntdc = TextDataLMControllerStreaming(ddict_with_val,\n                                    main_text='Review Text',\n                                    filter_dict={'Review Text': lambda x: x is not None},\n                                    metadatas=['Title','Division Name'],\n                                    content_transformations=[text_normalize,str.lower],\n                                    cols_to_keep=['Clothing ID','Review Text'],\n                                    seed=42,\n                                    batch_size=1024,\n                                    verbose=True\n                                    )\ntdc.process_and_tokenize(tokenizer,line_by_line=True,max_length=256,tok_num_proc=1)\n\n-------------------- Data Filtering --------------------\nDone\n----- Metadata Simple Processing & Concatenating to Main Content -----\nDone\n-------------------- Dropping unused features --------------------\nDone\n----- Performing Content Transformation and Tokenization on Validation Set -----\nDone\n----- Creating a generator for content transformation and tokenization on Train set -----\nDone\n\n\n\n\n\n\nprint(tokenizer.decode(next(iter(tdc.main_ddict['train']))['input_ids']))\nprint()\nprint(tokenizer.decode(tdc.main_ddict['validation']['input_ids'][0]))\n\n\n\n\n\n\n\n\n\n\n&lt;s&gt;general petite. beautiful top, worth the necessary tailoring. the beautiful bold print drew me to this top and it did not disappoint upon receipt. however, the bottom ruffle belled so far out on each side that it was laughable! the actual fit is nothing like the picture ; clearly the model's arms are placed in front of all the extra fabric to hold the ruffle back. however, the fabric is beautiful, the fit was perfect ( size 2, 5'4 \", 106 lbs. ), the quality is great and i love the print so i decided to take it to my tailor to \" sew away \" the \" wings \" on both si&lt;/s&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;\n\n&lt;s&gt;general. soft, feminine and fun pockets!. i love this tunic. purchased the dark orange in medium ( i am 5'9 and 140 lbs ). tried the small and almost kept it but i felt seams around my arm pits a tad, so went with the medium and glad i did - this top should be comfortable. feels very fall and perfect for casual get-togethers and running around town. only comment is that it is rayon... and for me anyway rayon doesn't wash too well - so we shall see how this one fairs.&lt;/s&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;\n\n\nCompare to non-streamed version\n\ndset2 = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\nddict_with_val2 = dset2.train_test_split(test_size=0.1,seed=42)\nddict_with_val2['validation'] = ddict_with_val2['test']\ndel ddict_with_val2['test']\n\ntdc2 = TextDataLMController(ddict_with_val2,\n                            main_text='Review Text',\n                            filter_dict={'Review Text': lambda x: x is not None},\n                            metadatas=['Title','Division Name'],\n                            content_transformations=[text_normalize,str.lower],\n                            cols_to_keep=['Clothing ID','Review Text'],\n                            seed=42,\n                            batch_size=1024,\n                            verbose=False\n                            )\ntdc2.process_and_tokenize(tokenizer,line_by_line=True,max_length=256,shuffle_trn=False,tok_num_proc=1)\n\n\n# check whether train sets are the same\nassert len(list(tdc.main_ddict['train']))==len(tdc2.main_ddict['train'])\n\n\niter1 = iter(tdc.main_ddict['train'])\niter2 = iter(tdc2.main_ddict['train'])\nfor a,b in zip(iter1,iter2):\n    assert a==b\n\n\n# check whether validation set is the same\nassert len(list(tdc.main_ddict['validation']))==len(tdc2.main_ddict['validation'])\n\niter1 = iter(tdc.main_ddict['validation'])\niter2 = iter(tdc2.main_ddict['validation'])\nfor a,b in zip(iter1,iter2):\n    assert a==b\n\n\n\n\nb) Option 2: Tokenize every text, then concatenate them together before splitting them in smaller parts.\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\nddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\nddict_with_val['validation'] = ddict_with_val['test']\nddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\ndel ddict_with_val['test']\n\ntdc = TextDataLMControllerStreaming(ddict_with_val,\n                                    main_text='Review Text',\n                                    filter_dict={'Review Text': lambda x: x is not None},\n                                    metadatas=['Title','Division Name'],\n                                    content_transformations=[text_normalize,str.lower],\n                                    seed=42,\n                                    batch_size=1024,\n                                    verbose=False\n                                    )\ntdc.process_and_tokenize(tokenizer,line_by_line=False,max_length=256,tok_num_proc=1)\n\n\nprint(tokenizer.decode(next(iter(tdc.main_ddict['train']))['input_ids']))\nprint()\nprint(tokenizer.decode(tdc.main_ddict['validation']['input_ids'][0]))\n\n&lt;s&gt;general petite. beautiful top, worth the necessary tailoring. the beautiful bold print drew me to this top and it did not disappoint upon receipt. however, the bottom ruffle belled so far out on each side that it was laughable! the actual fit is nothing like the picture ; clearly the model's arms are placed in front of all the extra fabric to hold the ruffle back. however, the fabric is beautiful, the fit was perfect ( size 2, 5'4 \", 106 lbs. ), the quality is great and i love the print so i decided to take it to my tailor to \" sew away \" the \" wings \" on both si&lt;/s&gt;&lt;s&gt;general. not as short on me ( petite ). i ordered the xxs p as this dress is not a fitted dress, and that was the right size for me. only thing is the length is a bit linger still 9 lower on calf for me ), the straps are almost tight, so i would say the dress is a reversed taper shape. color is beautiful, i ordered green as the other color ( plum ) doesn't have petite available. green is rich, and classy, the fabric is surprisingly soft. i love the little details in the velvet. definitely need a\n\n&lt;s&gt;general. soft, feminine and fun pockets!. i love this tunic. purchased the dark orange in medium ( i am 5'9 and 140 lbs ). tried the small and almost kept it but i felt seams around my arm pits a tad, so went with the medium and glad i did - this top should be comfortable. feels very fall and perfect for casual get-togethers and running around town. only comment is that it is rayon... and for me anyway rayon doesn't wash too well - so we shall see how this one fairs.&lt;/s&gt;&lt;s&gt;general petite. a new staple!. tried these on out of sheer curiosity -- i've got a long torso & was pleasantly surprised how flattering they are! they manage to look flowing & sleek without shortening the legs. took a size 6 with my 27 \" waist, 37 \" hips. it's a bit of a generous fit, especially around the waist, but they're extremely comfortable & have room to tuck tops into. i have the cowled sweater tank in gray & it looks fantastic over these! couldn't resist getting both the rust and black. perfect for a dressy casual look&lt;/s&gt;&lt;s&gt;general. maybe swing is for me!. i love swing dresses but they never seem\n\n\n\nfor i,v in enumerate(tdc.main_ddict['train']):\n    if i%100==0:\n        print(i)\n    if i==1024-1:\n        break\n    pass\n\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nCPU times: user 10.5 s, sys: 28.4 ms, total: 10.5 s\nWall time: 10.5 s\n\n\nCompare to non-streamed version\n\ndset2 = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\nddict_with_val2 = dset2.train_test_split(test_size=0.1,seed=42)\nddict_with_val2['validation'] = ddict_with_val2['test']\ndel ddict_with_val2['test']\n\ntdc2 = TextDataLMController(ddict_with_val2,\n                            main_text='Review Text',\n                            filter_dict={'Review Text': lambda x: x is not None},\n                            metadatas=['Title','Division Name'],\n                            content_transformations=[text_normalize,str.lower],\n                            seed=42,\n                            batch_size=1024,\n                            verbose=False\n                            )\ntdc2.process_and_tokenize(tokenizer,line_by_line=False,max_length=256,shuffle_trn=False,tok_num_proc=1)\n\n\n# check whether train sets are the same\nassert len(list(tdc.main_ddict['train']))==len(tdc2.main_ddict['train'])\n\niter1 = iter(tdc.main_ddict['train'])\niter2 = iter(tdc2.main_ddict['train'])\nfor a,b in zip(iter1,iter2):\n    assert a==b\n\n\n# check whether validation set is the same\nassert len(list(tdc.main_ddict['validation']))==len(tdc2.main_ddict['validation'])\n\niter1 = iter(tdc.main_ddict['validation'])\niter2 = iter(tdc2.main_ddict['validation'])\nfor a,b in zip(iter1,iter2):\n    assert a==b\n\n\n\nc) Striding (For Concatenation of tokens)\nIf your sentences (or paragraphs) are larger than max_length, after concatenation, they will be broken apart; your long paragraph will be incompleted in terms of meaning. Striding is a way to somewhat preserve the sentence’s meaning, by getting part of the sentence back. We will demonstrate it with an example, and you can compare it with the previous one (without striding) to see the differences\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\nddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\nddict_with_val['validation'] = ddict_with_val['test']\nddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\ndel ddict_with_val['test']\n\ntdc = TextDataLMControllerStreaming(ddict_with_val,\n                                    main_text='Review Text',\n                                    filter_dict={'Review Text': lambda x: x is not None},\n                                    metadatas=['Title','Division Name'],\n                                    content_transformations=[text_normalize,str.lower],\n                                    seed=42,\n                                    batch_size=1024,\n                                    verbose=False\n                                    )\n\n\ntdc.process_and_tokenize(tokenizer,line_by_line=False,max_length=100,stride=20,tok_num_proc=1)\n# Stride is 20, meaning for the next entry, we go back 20 tokens\n\n\nfor i,v in enumerate(tdc.main_ddict['train']):\n    if i==2: break\n    print(tokenizer.decode(v['input_ids']))\n    print('-'*20)\n\n&lt;s&gt;general petite. beautiful top, worth the necessary tailoring. the beautiful bold print drew me to this top and it did not disappoint upon receipt. however, the bottom ruffle belled so far out on each side that it was laughable! the actual fit is nothing like the picture ; clearly the model's arms are placed in front of all the extra fabric to hold the ruffle back. however, the fabric is beautiful, the fit was perfect ( size 2, 5'4 \",\n--------------------\n however, the fabric is beautiful, the fit was perfect ( size 2, 5'4 \", 106 lbs. ), the quality is great and i love the print so i decided to take it to my tailor to \" sew away \" the \" wings \" on both si&lt;/s&gt;&lt;s&gt;general. not as short on me ( petite ). i ordered the xxs p as this dress is not a fitted dress, and that was the right size for me. only thing is the length is a\n--------------------\n\n\n\nprint(tokenizer.decode(tdc.main_ddict['validation']['input_ids'][0]))\nprint(tokenizer.decode(tdc.main_ddict['validation']['input_ids'][1]))\n\n&lt;s&gt;general. soft, feminine and fun pockets!. i love this tunic. purchased the dark orange in medium ( i am 5'9 and 140 lbs ). tried the small and almost kept it but i felt seams around my arm pits a tad, so went with the medium and glad i did - this top should be comfortable. feels very fall and perfect for casual get-togethers and running around town. only comment is that it is rayon... and for me anyway rayon doesn\n running around town. only comment is that it is rayon... and for me anyway rayon doesn't wash too well - so we shall see how this one fairs.&lt;/s&gt;&lt;s&gt;general petite. a new staple!. tried these on out of sheer curiosity -- i've got a long torso & was pleasantly surprised how flattering they are! they manage to look flowing & sleek without shortening the legs. took a size 6 with my 27 \" waist, 37 \" hips. it's a bit\n\n\nFor the second entry, we can see it starts with the last 20 tokens of the previous entry\nCompare to non-streamed version\n\ndset2 = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\nddict_with_val2 = dset2.train_test_split(test_size=0.1,seed=42)\nddict_with_val2['validation'] = ddict_with_val2['test']\ndel ddict_with_val2['test']\n\ntdc2 = TextDataLMController(ddict_with_val2,\n                            main_text='Review Text',\n                            filter_dict={'Review Text': lambda x: x is not None},\n                            metadatas=['Title','Division Name'],\n                            content_transformations=[text_normalize,str.lower],\n                            seed=42,\n                            batch_size=1024,\n                            verbose=False\n                            )\ntdc2.process_and_tokenize(tokenizer,line_by_line=False,max_length=100,shuffle_trn=False,\n                          stride=20,tok_num_proc=1)\n\n\n# check whether train sets are the same\nassert len(list(tdc.main_ddict['train']))==len(tdc2.main_ddict['train'])\n\niter1 = iter(tdc.main_ddict['train'])\niter2 = iter(tdc2.main_ddict['train'])\nfor a,b in zip(iter1,iter2):\n    assert a==b\n\n\n# check whether validation set is the same\nassert len(list(tdc.main_ddict['validation']))==len(tdc2.main_ddict['validation'])\n\niter1 = iter(tdc.main_ddict['validation'])\niter2 = iter(tdc2.main_ddict['validation'])\nfor a,b in zip(iter1,iter2):\n    assert a==b",
    "crumbs": [
      "5. Text Classes",
      "c. Text Controller For Language Modeling",
      "Text Main For Language Model - Streaming"
    ]
  },
  {
    "objectID": "text_main_lm_streaming.html#data-collator",
    "href": "text_main_lm_streaming.html#data-collator",
    "title": "Text Main For Language Model - Streaming",
    "section": "3. Data Collator",
    "text": "3. Data Collator\n\nfrom underthesea import text_normalize\nfrom transformers import AutoTokenizer\n\n\nFor masked language model\n\ntokenizer = AutoTokenizer.from_pretrained('roberta-base')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\nLet’s define our text controller first\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\nddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\nddict_with_val['validation'] = ddict_with_val['test']\nddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\ndel ddict_with_val['test']\n\ntdc = TextDataLMControllerStreaming(ddict_with_val,\n                                    main_text='Review Text',\n                                    filter_dict={'Review Text': lambda x: x is not None},\n                                    metadatas=['Title','Division Name'],\n                                    content_transformations=[text_normalize,str.lower],\n                                    cols_to_keep=['Clothing ID','Review Text'],\n                                    seed=42,\n                                    batch_size=1024,\n                                    verbose=False\n                                    )\n\nWe will tokenize our corpus line-by-line\n\ntdc.process_and_tokenize(tokenizer,line_by_line=True,max_length=-1)\n\n\ntdc.set_data_collator()\n\n\ntdc.set_data_collator(is_mlm=True,mlm_prob=0.15)\n\n\ntdc.data_collator\n\nDataCollatorForLanguageModeling(tokenizer=RobertaTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '&lt;s&gt;', 'eos_token': '&lt;/s&gt;', 'unk_token': '&lt;unk&gt;', 'sep_token': '&lt;/s&gt;', 'pad_token': '&lt;pad&gt;', 'cls_token': '&lt;s&gt;', 'mask_token': '&lt;mask&gt;'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n    0: AddedToken(\"&lt;s&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n    1: AddedToken(\"&lt;pad&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n    2: AddedToken(\"&lt;/s&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n    3: AddedToken(\"&lt;unk&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n    50264: AddedToken(\"&lt;mask&gt;\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n}, mlm=True, mlm_probability=0.15, pad_to_multiple_of=8, tf_experimental_compile=False, return_tensors='pt')\n\n\nBefore applying the collator…\n\nfor i,v in enumerate(tdc.main_ddict['train']):\n    if i==2: break\n    print(v)\n    print(f\"Length of input_ids: {len(v['input_ids'])}\")\n    print('-'*20)\n\n{'Clothing ID': 841, 'Review Text': 'general petite . beautiful top, worth the necessary tailoring . The beautiful bold print drew me to this top and it did not disappoint upon receipt. however, the bottom ruffle belled so far out on each side that it was laughable! the actual fit is nothing like the picture; clearly the model\\'s arms are placed in front of all the extra fabric to hold the ruffle back.\\r\\nhowever, the fabric is beautiful, the fit was perfect (size 2, 5\\'4\", 106 lbs.), the quality is great and i love the print so i decided to take it to my tailor to \"sew away\" the \"wings\" on both si', 'input_ids': [0, 15841, 4716, 1459, 479, 2721, 299, 2156, 966, 5, 2139, 7886, 5137, 479, 5, 2721, 7457, 5780, 4855, 162, 7, 42, 299, 8, 24, 222, 45, 17534, 2115, 18245, 479, 959, 2156, 5, 2576, 910, 15315, 28, 9970, 98, 444, 66, 15, 349, 526, 14, 24, 21, 38677, 27785, 5, 3031, 2564, 16, 1085, 101, 5, 2170, 25606, 2563, 5, 1421, 18, 3701, 32, 2325, 11, 760, 9, 70, 5, 1823, 10199, 7, 946, 5, 910, 15315, 124, 479, 959, 2156, 5, 10199, 16, 2721, 2156, 5, 2564, 21, 1969, 36, 1836, 132, 2156, 195, 128, 204, 22, 2156, 13442, 23246, 479, 4839, 2156, 5, 1318, 16, 372, 8, 939, 657, 5, 5780, 98, 939, 1276, 7, 185, 24, 7, 127, 26090, 7, 22, 35043, 409, 22, 5, 22, 11954, 22, 15, 258, 3391, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}\nLength of input_ids: 136\n--------------------\n{'Clothing ID': 1110, 'Review Text': \"general . not as short on me (petite) . I ordered the xxs p as this dress is not a fitted dress, and that was the right size for me. only thing is the length is a bit linger still 9lower on calf for me), the straps are almost tight, so i would say the dress is a reversed taper shape. color is beautiful, i ordered green as the other color (plum) doesn't have petite available. green is rich, and classy, the fabric is surprisingly soft. i love the little details in the velvet. definitely need a strapless bra for this one.\\r\\n\\r\\n115 lbsm 30d\", 'input_ids': [0, 15841, 479, 45, 25, 765, 15, 162, 36, 4716, 1459, 4839, 479, 939, 2740, 5, 37863, 29, 181, 25, 42, 3588, 16, 45, 10, 15898, 3588, 2156, 8, 14, 21, 5, 235, 1836, 13, 162, 479, 129, 631, 16, 5, 5933, 16, 10, 828, 18277, 202, 361, 795, 15, 16701, 13, 162, 4839, 2156, 5, 31622, 32, 818, 3229, 2156, 98, 939, 74, 224, 5, 3588, 16, 10, 13173, 326, 15888, 3989, 479, 3195, 16, 2721, 2156, 939, 2740, 2272, 25, 5, 97, 3195, 36, 36838, 4839, 630, 75, 33, 4716, 1459, 577, 479, 2272, 16, 4066, 2156, 8, 30228, 2156, 5, 10199, 16, 10262, 3793, 479, 939, 657, 5, 410, 1254, 11, 5, 29986, 479, 2299, 240, 10, 18052, 16979, 11689, 13, 42, 65, 479, 12312, 23246, 119, 389, 385, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}\nLength of input_ids: 133\n--------------------\n\n\nWe can see that the length of each token list is different from each other\nLet’s apply the collator\n\n# extract only the required keys\ninp_keys = tokenizer.model_input_names\niter1 = iter(tdc.main_ddict['train'])\n_result=[]\nfor i in range(5):\n    _inp = next(iter1)\n    _result.append({k:_inp[k] for k in inp_keys})\n\n\nout = tdc.data_collator(_result)\n\n\nout.keys()\n\ndict_keys(['input_ids', 'attention_mask', 'labels'])\n\n\nNow all token lists have the same length, which is a multiple of 8\n\nout['input_ids'].shape\n\ntorch.Size([5, 136])\n\n\n\nout['input_ids'][:3,:]\n\ntensor([[    0, 15841,  4716,  1459,   479,  2721,   299,  2156,   966, 50264,\n          2139,  7886,  5137,   479,     5,  2721,  7457,  5780,  4855,   162,\n             7,    42,   299,     8, 50264,   222,    45, 17534,  2115, 50264,\n           479, 50264,  2156,     5,  2576,   910, 15315,    28,  9970,    98,\n           444,    66,    15,   349,   526,    14,    24,    21, 38677, 27785,\n         17138,  3031, 50264,    16,  1085,   101,     5,  2170, 25606,  2563,\n             5,  1421,    18,  3701,    32,  2325,    11,   760,     9,    70,\n             5,  1823, 10199, 50264, 29261, 50264,   910, 15315,   124,   479,\n           959,  2156,     5, 10199,    16,  2721,  2156, 50264,  2564,    21,\n          1969,    36,  1836,   132, 50264,   195,   128,   204,    22, 50264,\n         13442, 23246,   479, 50264,  2156,     5, 50264,    16, 50264,     8,\n           939,   657,     5, 50264,    98,   939,  1276, 50264,   185,    24,\n             7,   127, 26090,     7,    22, 35043,   409, 50264,     5,    22,\n         11954,    22, 50264,   258,  3391,     2],\n        [    0, 15841,   479,    45,    25, 13055,    15, 50264,    36,  4716,\n          1459,  4839,   479, 50264,  2740,     5, 37863, 50264,   181, 50264,\n            42,  3588,    16,    45,    10, 15898,  3588, 50264,     8,    14,\n            21,     5,   235,  1836,    13,   162,   479,   129,   631,    16,\n             5,  5933,    16, 38152,   828, 18277,   202,   361,   795,    15,\n         16701,    13,   162,  4839,  2156,     5, 31622,    32,   818,  3229,\n          2156,    98,   939,    74, 50264,     5,  3588,    16,    10, 13173,\n           326, 50264,  3989,   479,  3195,    16, 50264,  2156,   939,  2740,\n          2272,    25,     5,    97,  3195,    36, 36838,  4839, 50264,    75,\n            33,  4716,  1459, 50264, 50264,  2272,    16,  4066,  2156,     8,\n         30228, 50264, 50264, 50264,    16, 10262,  3793,   479,   939, 50264,\n             5,   410,  1254,    11,     5, 50264,   479,  2299,   240, 50264,\n         18052, 16979, 11689,    13,    42, 50264,   479, 12312, 23246,   119,\n           389,   385,     2,     1,     1,     1],\n        [    0, 15841,   479,  1969, 50264,    13,    80, 50264,   479, 15983,\n         10717, 39574,    47,   240, 50264,   216,    14,    42,  1907,     9,\n         50264, 50264,     5,    65,    14,    40,   120,  6538,    36,   939,\n          2162,     5, 50264,    65,  4839,   479,    24,    16,  2422,  7174,\n         50264,  9869,  2156,    53,   939,    21,   129,   441,     7,   120,\n            80, 15033,    66, 50264,    24, 50264, 50264,   222, 10397,    24,\n         50264, 50264,  4925, 50264,    18,  1836,   142,   939,  1079, 47904,\n            24, 50264, 50264,   172, 10601,     7,  3841,   479,   939,    21,\n          2422,  5779, 50264,     5,  3568,    53, 10874,   145,   441,     7,\n         50264,    24,   396,   864,    23, 50264, 50264,  6215,   479,     2,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1]])\n\n\nThe labels have also been constructed, which shows the “mask” tokens (non -100) in which the model has to predict. To increase the amount of masked tokens, increase the mlm_prob\n\nout['labels'][:3,:]\n\ntensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,     5,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,    24,  -100,  -100,  -100,  -100, 18245,\n          -100,   959,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n             5,  -100,  2564,  -100,  -100,  -100,  -100,  -100,  -100,  2563,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,     7,   946,     5,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,     5,  -100,  -100,\n          -100,  -100,  -100,  -100,  2156,  -100,  -100,  -100,  -100,  2156,\n          -100,  -100,  -100,  4839,  -100,  -100,  1318,  -100,   372,  -100,\n          -100,  -100,  -100,  5780,  -100,  -100,  -100,     7,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,    22,  -100,  -100,\n          -100,  -100,    15,  -100,  -100,  -100],\n        [ -100, 15841,  -100,  -100,  -100,   765,  -100,   162,  -100,  -100,\n          -100,  -100,  -100,   939,  -100,  -100,  -100,    29,  -100,    25,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  2156,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n             5,  -100,  -100,    10,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,   224,  -100,  -100,  -100,  -100,  -100,\n          -100, 15888,  -100,  -100,  -100,  -100,  2721,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   630,  -100,\n          -100,  -100,  -100,   577,   479,  -100,  -100,  -100,  -100,  -100,\n          -100,  2156,     5, 10199,  -100,  -100,  -100,  -100,  -100,   657,\n          -100,  -100,  -100,  -100,  -100, 29986,  -100,  -100,  -100,    10,\n          -100,  -100,  -100,  -100,  -100,    65,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100],\n        [ -100,  -100,  -100,  -100, 39574,  -100,  -100, 15033,  -100,  -100,\n          -100,  -100,  -100,  -100,     7,  -100,  -100,  -100,  -100,  -100,\n         10199,    16,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  1104,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n             8,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,     9,  -100,   479,   939,  -100,  -100,  -100,\n             8,    24,  -100,    24,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,   150,  7727,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,    59,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           671,  -100,  -100,  -100,  -100,   127,   400,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100]])\n\n\nIf you apply padding in the tokenization step (by adjusting the max_length argument), no matter whether it’s line-by-line tokenization or not, the data collator will skip the padding step\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\nddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\nddict_with_val['validation'] = ddict_with_val['test']\nddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\ndel ddict_with_val['test']\n\ntdc = TextDataLMControllerStreaming(ddict_with_val,\n                                    main_text='Review Text',\n                                    filter_dict={'Review Text': lambda x: x is not None},\n                                    metadatas=['Title','Division Name'],\n                                    content_transformations=[text_normalize,str.lower],\n                                    cols_to_keep=['Clothing ID','Review Text'],\n                                    seed=42,\n                                    batch_size=1024,\n                                    verbose=False\n                                    )\n\n\ntdc.process_and_tokenize(tokenizer,line_by_line=False,max_length=100)\n\n\ntdc.set_data_collator(is_mlm=True,mlm_prob=0.15)\n\nLet’s apply the collator\n\n# extract only the required keys\ninp_keys = tokenizer.model_input_names\niter1 = iter(tdc.main_ddict['train'])\n_result=[]\nfor i in range(5):\n    _inp = next(iter1)\n    _result.append({k:_inp[k] for k in inp_keys})\n        \n\nout = tdc.data_collator(_result)\n\n\nout['input_ids'].shape\n\ntorch.Size([5, 100])\n\n\n\nout['input_ids'][:2,:]\n\ntensor([[    0, 15841,  4716,  1459,   479,  2721,   299,  2156,   966, 50264,\n          2139,  7886,  5137,   479,     5,  2721,  7457,  5780,  4855,   162,\n             7,    42,   299,     8,    24,   222,    45, 17534,  2115, 50264,\n           479, 50264,  2156,     5,  2576,   910, 15315,    28,  9970,    98,\n           444,    66,    15,   349,   526,    14,    24,    21, 38677, 27785,\n         50264,  3031, 50264,    16,  1085,   101,     5,  2170, 25606, 41316,\n             5,  1421,    18,  3701,    32,  2325,    11,   760,     9,    70,\n             5,  1823, 10199, 50264, 17204, 50264,   910, 15315,   124,   479,\n           959,  2156,     5, 10199,    16,  2721,  2156, 50264,  2564,    21,\n          1969,    36,  1836,   132, 50264,   195,   128,   204,    22, 50264],\n        [13442, 23246,   479, 50264,  2156,     5, 50264,    16, 23781,     8,\n           939,   657,     5,  5780,    98,   939,  1276, 50264,   185,    24,\n             7,   127, 26090,     7,    22, 35043,   409, 50264,     5,    22,\n         11954,    22, 50264,   258,  3391,     2,     0, 50264,   479,    45,\n            25, 50264,    15, 44224,    36,  4716,  1459,  4839,   479, 50264,\n          2740,     5, 37863, 50264,   181,    25,    42,  3588,    16,    45,\n            10, 15898,  3588, 50264,     8,    14,    21,     5,   235,  1836,\n            13,   162,   479,   129,   631,    16, 50264,  5933,    16, 50264,\n           828, 18277,   202,   361,   795,    15, 16701,    13,   162,  4839,\n          2156,     5, 31622,    32,   818,  3229,  2156,    98,   939,    74]])\n\n\n\nout['labels'][:2,:]\n\ntensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,     5,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,    24,  -100,  -100,  -100,  -100, 18245,\n          -100,   959,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n             5,  -100,  2564,  -100,  -100,  -100,  -100,  -100,  -100,  2563,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,     7,   946,     5,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,     5,  -100,  -100,\n          -100,  -100,  -100,  -100,  2156,  -100,  -100,  -100,  -100,  2156],\n        [ -100,  -100,  -100,  4839,  -100,  -100,  1318,  -100,   372,  -100,\n          -100,  -100,  -100,  5780,  -100,  -100,  -100,     7,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,    22,  -100,  -100,\n          -100,  -100,    15,  -100,  -100,  -100,  -100, 15841,  -100,  -100,\n          -100,   765,  -100,   162,  -100,  -100,  -100,  -100,  -100,   939,\n          -100,  -100,  -100,    29,  -100,    25,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  2156,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,     5,  -100,  -100,    10,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])\n\n\nSince we are using the concatenation-of-tokenization technique, one smart thing that the HuggingFace’s DataCollatorForLanguageModeling (which is the data collator we use) does is to allow maskings at every position, at opposed to to the previous cases (with line-by-line tokenization), there’s no masking near the end tokens of each list, because those end tokens are padding tokens\n\n\nFor causal language model\n\nfrom transformers import AutoTokenizer\nfrom tokenizers import processors\n\nLet’s define our GPT2 tokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('gpt2')\n\n\ntokenizer\n\nGPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '&lt;|endoftext|&gt;', 'eos_token': '&lt;|endoftext|&gt;', 'unk_token': '&lt;|endoftext|&gt;'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n    50256: AddedToken(\"&lt;|endoftext|&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n\n\nGPT2 does not use start/end-of-sentence token:\n\nprint(tokenizer.convert_ids_to_tokens(tokenizer(\"this is a text. That is a second text.But there's a third one\")['input_ids']))\n\n['this', 'Ġis', 'Ġa', 'Ġtext', '.', 'ĠThat', 'Ġis', 'Ġa', 'Ġsecond', 'Ġtext', '.', 'But', 'Ġthere', \"'s\", 'Ġa', 'Ġthird', 'Ġone']\n\n\nIf you want to perform concatenation-of-token, and you want your causal LM to differentiate between sentences, you can add a special token to separate sentences, as follow:\n\ntokenizer._tokenizer.post_processor = processors.TemplateProcessing(\n    single=\"$A \" + tokenizer.eos_token,\n    special_tokens=[(tokenizer.eos_token, tokenizer.eos_token_id)],\n)\ntokenizer.pad_token = tokenizer.eos_token\n\n\nprint(tokenizer.convert_ids_to_tokens(tokenizer(\"this is a text. That is a second text.But there's a third one\")['input_ids']))\n\n['this', 'Ġis', 'Ġa', 'Ġtext', '.', 'ĠThat', 'Ġis', 'Ġa', 'Ġsecond', 'Ġtext', '.', 'But', 'Ġthere', \"'s\", 'Ġa', 'Ġthird', 'Ġone', '&lt;|endoftext|&gt;']\n\n\nWith this modified tokenizer, let’s perform concatenation-of-tokenization using GPT2\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\nddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\nddict_with_val['validation'] = ddict_with_val['test']\nddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\ndel ddict_with_val['test']\n\ntdc = TextDataLMControllerStreaming(ddict_with_val,\n                                    main_text='Review Text',\n                                    filter_dict={'Review Text': lambda x: x is not None},\n                                    metadatas=['Title','Division Name'],\n                                    content_transformations=[text_normalize,str.lower],\n                                    seed=42,\n                                    batch_size=1024,\n                                    verbose=False\n                                    )\n\n\ntdc.process_and_tokenize(tokenizer,line_by_line=False,max_length=100)\n\nSince it’s casual language modeling, let’s turn off is_mlm\n\ntdc.set_data_collator(is_mlm=False)\n\nLet’s apply the collator\n\niter1 = iter(tdc.main_ddict['train'])\nout = tdc.data_collator([next(iter1) for i in range(5)]) # simulation with batch size 5\n\n\nout['input_ids'].shape\n\ntorch.Size([5, 100])\n\n\n\nout['input_ids'][:2,:]\n\ntensor([[24622,  4273,   578,   764,  4950,  1353,   837,  2861,   262,  3306,\n          7894,  3255,   764,   262,  4950, 10758,  3601,  9859,   502,   284,\n           428,  1353,   290,   340,   750,   407,  6703,  2402, 14507,   764,\n          2158,   837,   262,  4220,   374, 18137,   307,  3353,   523,  1290,\n           503,   319,  1123,  1735,   326,   340,   373, 47623,  5145,   262,\n          4036,  4197,   318,  2147,   588,   262,  4286,  2162,  4084,   262,\n          2746,   338,  5101,   389,  4624,   287,  2166,   286,   477,   262,\n          3131,  9664,   284,  1745,   262,   374, 18137,   736,   764,  2158,\n           837,   262,  9664,   318,  4950,   837,   262,  4197,   373,  2818,\n           357,  2546,   362,   837,   642,   705,   604,   366,   837, 15696],\n        [15785,   764,  1267,   837,   262,  3081,   318,  1049,   290,  1312,\n          1842,   262,  3601,   523,  1312,  3066,   284,  1011,   340,   284,\n           616, 35280,   284,   366, 34249,  1497,   366,   262,   366, 12098,\n           366,   319,  1111, 33721, 50256, 24622,   764,   407,   355,  1790,\n           319,   502,   357,  4273,   578,  1267,   764,  1312,  6149,   262,\n         31383,    82,   279,   355,   428,  6576,   318,   407,   257, 18235,\n          6576,   837,   290,   326,   373,   262,   826,  2546,   329,   502,\n           764,   691,  1517,   318,   262,  4129,   318,   257,  1643, 31402,\n           991,   860,  2793,   319, 31134,   329,   502,  1267,   837,   262,\n         29552,   389,  2048,  5381,   837,   523,  1312,   561,   910,   262]])\n\n\n\nout['labels'][:2,:]\n\ntensor([[24622,  4273,   578,   764,  4950,  1353,   837,  2861,   262,  3306,\n          7894,  3255,   764,   262,  4950, 10758,  3601,  9859,   502,   284,\n           428,  1353,   290,   340,   750,   407,  6703,  2402, 14507,   764,\n          2158,   837,   262,  4220,   374, 18137,   307,  3353,   523,  1290,\n           503,   319,  1123,  1735,   326,   340,   373, 47623,  5145,   262,\n          4036,  4197,   318,  2147,   588,   262,  4286,  2162,  4084,   262,\n          2746,   338,  5101,   389,  4624,   287,  2166,   286,   477,   262,\n          3131,  9664,   284,  1745,   262,   374, 18137,   736,   764,  2158,\n           837,   262,  9664,   318,  4950,   837,   262,  4197,   373,  2818,\n           357,  2546,   362,   837,   642,   705,   604,   366,   837, 15696],\n        [15785,   764,  1267,   837,   262,  3081,   318,  1049,   290,  1312,\n          1842,   262,  3601,   523,  1312,  3066,   284,  1011,   340,   284,\n           616, 35280,   284,   366, 34249,  1497,   366,   262,   366, 12098,\n           366,   319,  1111, 33721,  -100, 24622,   764,   407,   355,  1790,\n           319,   502,   357,  4273,   578,  1267,   764,  1312,  6149,   262,\n         31383,    82,   279,   355,   428,  6576,   318,   407,   257, 18235,\n          6576,   837,   290,   326,   373,   262,   826,  2546,   329,   502,\n           764,   691,  1517,   318,   262,  4129,   318,   257,  1643, 31402,\n           991,   860,  2793,   319, 31134,   329,   502,  1267,   837,   262,\n         29552,   389,  2048,  5381,   837,   523,  1312,   561,   910,   262]])\n\n\nFor CLM, the labels are essentially the same as input_ids. From HuggingFace documentation:\n`DataCollatorForLanguageModeling` will take care of creating the language model labels — in causal language modeling the inputs serve as labels too (just shifted by one element), and this data collator creates them on the fly during training.",
    "crumbs": [
      "5. Text Classes",
      "c. Text Controller For Language Modeling",
      "Text Main For Language Model - Streaming"
    ]
  },
  {
    "objectID": "text_main_lm_streaming.html#save-and-load-textdatacontroller",
    "href": "text_main_lm_streaming.html#save-and-load-textdatacontroller",
    "title": "Text Main For Language Model - Streaming",
    "section": "4. Save and Load TextDataController",
    "text": "4. Save and Load TextDataController\n\nsource\n\nTextDataLMControllerStreaming.save_as_pickles\n\n TextDataLMControllerStreaming.save_as_pickles (fname,\n                                                parent='pickle_files')\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfname\n\n\nName of the pickle file\n\n\nparent\nstr\npickle_files\nParent folder\n\n\n\n\nsource\n\n\nTextDataControllerStreaming.from_pickle\n\n TextDataControllerStreaming.from_pickle (fname, parent='pickle_files')\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfname\n\n\nName of the pickle file\n\n\nparent\nstr\npickle_files\nParent folder\n\n\n\nTextDataLMControllerStreaming object can be saved and loaded with ease. This is especially useful after text processing and/or tokenization have been done\n\nfrom datasets import disable_caching\n\n\ndisable_caching()\n\n\ntokenizer = AutoTokenizer.from_pretrained('roberta-base')\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\nddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\nddict_with_val['validation'] = ddict_with_val['test']\nddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\ndel ddict_with_val['test']\n\ntdc = TextDataLMControllerStreaming(ddict_with_val,\n                                    main_text='Review Text',\n                                    filter_dict={'Review Text': lambda x: x is not None},\n                                    metadatas=['Title','Division Name'],\n                                    content_transformations=[text_normalize,str.lower],\n                                    seed=42,\n                                    batch_size=1024,\n                                    verbose=False\n                                    )\n\n\ntdc.process_and_tokenize(tokenizer,line_by_line=True,max_length=-1)\n\ntdc.set_data_collator(is_mlm=True,mlm_prob=0.15)\n\n\ntdc.save_as_pickles('my_lm_tdc')\n\nLoad back our object\n\ntdc2 = TextDataLMController.from_pickle('my_lm_tdc')\n\nYou can still access all its attributes, data, preprocessings, transformations …\n\ntdc2.main_ddict\n\nDatasetDict({\n    train: IterableDataset({\n        features: Unknown,\n        n_shards: 1\n    })\n    validation: Dataset({\n        features: ['Title', 'Review Text', 'Division Name', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n        num_rows: 2253\n    })\n})\n\n\n\ntdc2.filter_dict,tdc2.content_tfms\n\n({'Review Text': &lt;function __main__.&lt;lambda&gt;(x)&gt;},\n [&lt;function underthesea.pipeline.text_normalize.text_normalize(text, tokenizer='underthesea')&gt;,\n  &lt;method 'lower' of 'str' objects&gt;])",
    "crumbs": [
      "5. Text Classes",
      "c. Text Controller For Language Modeling",
      "Text Main For Language Model - Streaming"
    ]
  },
  {
    "objectID": "roberta_singlehead_for_streaming.html",
    "href": "roberta_singlehead_for_streaming.html",
    "title": "Roberta model with a streamed dataset (Custom Single Head)",
    "section": "",
    "text": "In this series, we walk through some of the capability of this library: single-head classification, multi-head classification, multi-label classification, and regression. If you want a more detailed tutorial, check this out\nimport os\n#This will specify a (or a list) of GPUs for training\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom that_nlp_library.text_main_streaming import *\nfrom that_nlp_library.utils import seed_everything\nfrom underthesea import text_normalize\nfrom functools import partial\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport nlpaug.augmenter.char as nac\nfrom datasets import load_dataset\nimport random\nfrom transformers import RobertaTokenizer\nfrom datasets import Dataset",
    "crumbs": [
      "2. All Use Cases",
      "e. Training A Streamed Dataset",
      "Roberta model with a streamed dataset (Custom Single Head)"
    ]
  },
  {
    "objectID": "roberta_singlehead_for_streaming.html#define-and-train-a-custom-roberta-model",
    "href": "roberta_singlehead_for_streaming.html#define-and-train-a-custom-roberta-model",
    "title": "Roberta model with a streamed dataset (Custom Single Head)",
    "section": "Define and train a custom Roberta model",
    "text": "Define and train a custom Roberta model\n\nfrom transformers.models.roberta.modeling_roberta import RobertaModel\n\n\nfrom that_nlp_library.models.roberta.classifiers import *\nfrom that_nlp_library.model_main import *\nfrom sklearn.metrics import f1_score, accuracy_score\n\n\nnum_classes = len(tdc.label_lists[0])\n\n\nroberta_body = RobertaModel.from_pretrained('roberta-base')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nThen we can define a classification head. One trick we can use to boost the performance of our entire model is to concatenate the outputs of [CLS] from the four last layers of the pre-trained Roberta model (source: https://ieeexplore.ieee.org/document/9335912). We already define such custom head (ConcatHeadSimple), and the necessary architecture to make it work (RobertaHiddenStateConcatForSequenceClassification)\n\n# our model is more complex, so it's best to define some of its arguments\n_model_kwargs={\n    # overall model hyperparams\n    'head_class_sizes':num_classes,\n    'head_class': ConcatHeadSimple,\n    # classfication head hyperparams\n    'layer2concat':2, # you can change the number of layers to concat (default is 4, based on the paper)\n    'classifier_dropout':0.1 \n}\n\n\nmodel = model_init_classification(model_class = RobertaHiddenStateConcatForSequenceClassification,\n                                  cpoint_path = 'roberta-base', \n                                  output_hidden_states=True, # since we are using 'hidden layer contatenation' technique\n                                  seed=42,\n                                  body_model=roberta_body,\n                                  model_kwargs = _model_kwargs)\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score]\ncontroller = ModelController(model,tdc,seed=42)\n\nLoading body weights. This assumes the body is the very first block of your custom architecture\nTotal parameters: 124654854\nTotal trainable parameters: 124654854\n\n\nAnd we can start training our model\n\nseed_everything(42)\n\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 3\n\ncontroller.fit(epochs,lr,\n               metric_funcs=metric_funcs,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics,\n               len_train=20000 # estimation of number of samples in train set\n              )\n\nmax_steps is given, it will override any value given in num_train_epochs\n\n\n\n\n    \n      \n      \n      [936/936 05:59, Epoch 2/9223372036854775807]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1 Score Department name\nAccuracy Score Department name\n\n\n\n\n0\nNo log\n0.312615\n0.750225\n0.921438\n\n\n1\n0.413400\n0.274220\n0.754716\n0.923657\n\n\n2\n0.413400\n0.253840\n0.762595\n0.932534\n\n\n\n\n\n\n\n\ncontroller.trainer.model.save_pretrained('./sample_weights/my_model')",
    "crumbs": [
      "2. All Use Cases",
      "e. Training A Streamed Dataset",
      "Roberta model with a streamed dataset (Custom Single Head)"
    ]
  },
  {
    "objectID": "roberta_singlehead_for_streaming.html#make-predictions",
    "href": "roberta_singlehead_for_streaming.html#make-predictions",
    "title": "Roberta model with a streamed dataset (Custom Single Head)",
    "section": "Make predictions",
    "text": "Make predictions\n\nLoad trained model\n\n_model_kwargs\n\n{'head_class_sizes': 6,\n 'head_class': that_nlp_library.models.roberta.classifiers.ConcatHeadSimple,\n 'layer2concat': 2,\n 'classifier_dropout': 0.1}\n\n\n\ntrained_model = model_init_classification(model_class = RobertaHiddenStateConcatForSequenceClassification,\n                                          cpoint_path = Path('./sample_weights/my_model'), \n                                          output_hidden_states=True,\n                                          seed=42,\n                                          model_kwargs = _model_kwargs)\n\ncontroller = ModelController(trained_model,tdc,seed=42)\n\nSome weights of the model checkpoint at sample_weights/my_model were not used when initializing RobertaHiddenStateConcatForSequenceClassification: ['body_model.pooler.dense.bias', 'body_model.pooler.dense.weight']\n- This IS expected if you are initializing RobertaHiddenStateConcatForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaHiddenStateConcatForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\nTotal parameters: 124064262\nTotal trainable parameters: 124064262\n\n\n\n\nPredict Train/Validation set\n\ndf_val = controller.predict_ddict(ds_type='validation')\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_val = df_val.to_pandas()\ndf_val.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nDivision Name\nDepartment Name\nlabel\ninput_ids\nattention_mask\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\nsoft, feminine and fun pockets!\ngeneral . soft , feminine and fun pockets ! . ...\ngeneral\nTops\n4\n[0, 15841, 479, 3793, 2156, 27360, 8, 1531, 12...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.996728\n\n\n1\na new staple!\ngeneral petite . a new staple ! . tried these ...\ngeneral petite\nBottoms\n0\n[0, 15841, 4716, 1459, 479, 10, 92, 17771, 277...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nBottoms\n0.960517\n\n\n2\nmaybe swing is for me!\ngeneral . maybe swing is for me ! . i love swi...\ngeneral\nTops\n4\n[0, 15841, 479, 2085, 7021, 16, 13, 162, 27785...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.983545\n\n\n3\ntoo flare\ngeneral . too flare . too small ... too flare ...\ngeneral\nBottoms\n0\n[0, 15841, 479, 350, 24186, 479, 350, 650, 166...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nBottoms\n0.986469\n\n\n4\nlove\ngeneral . love . i love this top it is easy to...\ngeneral\nTops\n4\n[0, 15841, 479, 657, 479, 939, 657, 42, 299, 2...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.997049\n\n\n\n\n\n\n\n\nYou can try to get your metric to see if it matches your last traing epoch’s above\n\nf1_score(df_val['Department Name'],df_val['pred_Department Name'],average='macro')\n\n0.7625951075732446\n\n\n\n\nPredict Test set\nWe will go through details on how to make a prediction on a completely new and raw dataset using our trained model. For now, let’s reuse the sample csv and pretend it’s our test set\n\ndf_test = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig').sample(frac=0.2,random_state=1)\n# drop NaN values in the label column\ndf_test = df_test[~df_test['Department Name'].isna()].reset_index(drop=True)\n\n# save the label, as we will calculate some metrics later. We also filter out labels with NaN Review Text,\n# as there will be a filtering processing on the test set\ntrue_labels = df_test.loc[~df_test['Review Text'].isna(),'Department Name'].values \n\n# drop the label (you don't need to, but this is necessary to simulate an actual test set)\ndf_test.drop('Department Name',axis=1,inplace=True)\n\n\n_test_dset = Dataset.from_pandas(df_test)\n_test_dset_predicted = controller.predict_raw_dset(_test_dset,\n                                                   do_filtering=True, # since we have some text filtering in the processing\n                                                  )\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_test_predicted = _test_dset_predicted.to_pandas()\n\n\ndf_test_predicted.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nDivision Name\ninput_ids\nattention_mask\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\nperfect for work and play\ngeneral . perfect for work and play . this shi...\ngeneral\n[0, 15841, 479, 1969, 13, 173, 8, 310, 479, 42...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.997284\n\n\n1\n\ngeneral petite . . i don't know why i had the ...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 479, 939, 218, 75,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nBottoms\n0.989114\n\n\n2\ngreat pants\ngeneral petite . great pants . thes e cords ar...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 372, 9304, 479, 5,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nBottoms\n0.986304\n\n\n3\nsurprisingly comfy for a button down\ngeneral petite . surprisingly comfy for a butt...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 10262, 3137, 24382...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.990987\n\n\n4\nshort and small\ngeneral petite . short and small . the shirt i...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 765, 8, 650, 479, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.996322\n\n\n\n\n\n\n\n\nLet’s quickly check the f1 score to make sure everything works correctly\n\nf1_score(true_labels,df_test_predicted['pred_Department Name'],average='macro')\n\n0.7615999294223502\n\n\nPredict top k results\n\n_test_dset = Dataset.from_pandas(df_test)\n_test_dset_predicted = controller.predict_raw_dset(_test_dset,\n                                                   do_filtering=True,\n                                                   topk=3\n                                                  )\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_test_predicted = _test_dset_predicted.to_pandas()\n\ndf_test_predicted.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nDivision Name\ninput_ids\nattention_mask\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\nperfect for work and play\ngeneral . perfect for work and play . this shi...\ngeneral\n[0, 15841, 479, 1969, 13, 173, 8, 310, 479, 42...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[Tops, Intimate, Trending]\n[0.9972837, 0.0011419549, 0.0010542183]\n\n\n1\n\ngeneral petite . . i don't know why i had the ...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 479, 939, 218, 75,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[Bottoms, Intimate, Trending]\n[0.9891139, 0.006692194, 0.0033946035]\n\n\n2\ngreat pants\ngeneral petite . great pants . thes e cords ar...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 372, 9304, 479, 5,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[Bottoms, Intimate, Trending]\n[0.98630387, 0.009437396, 0.0035212967]\n\n\n3\nsurprisingly comfy for a button down\ngeneral petite . surprisingly comfy for a butt...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 10262, 3137, 24382...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[Tops, Intimate, Jackets]\n[0.9909869, 0.003928944, 0.0020938655]\n\n\n4\nshort and small\ngeneral petite . short and small . the shirt i...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 765, 8, 650, 479, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[Tops, Intimate, Trending]\n[0.99632156, 0.0016891895, 0.0012567489]\n\n\n\n\n\n\n\n\n\n# Since we have some metadatas (Title and Division Name), we need to define a dictionary containing those values\nraw_content={'Review Text': 'This shirt is so comfortable I love it!',\n             'Title': 'Great shirt',\n             'Division Name': 'general'}\n\n\ndf_result = controller.predict_raw_text(raw_content,topk=3)\n\nnum_proc must be &lt;= 1. Reducing num_proc to 1 for dataset of size 1.\n\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_result\n\n{'Review Text': ['general . great shirt . this shirt is so comfortable i love it !'],\n 'Title': ['great shirt'],\n 'Division Name': ['general'],\n 'input_ids': [[0,\n   15841,\n   479,\n   372,\n   6399,\n   479,\n   42,\n   6399,\n   16,\n   98,\n   3473,\n   939,\n   657,\n   24,\n   27785,\n   2]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n 'pred_Department Name': [['Tops', 'Intimate', 'Trending']],\n 'pred_prob_Department Name': [[0.9973528385162354,\n   0.0012270379811525345,\n   0.0009467267664149404]]}\n\n\n\n\nPredict a Streamed Test set\nLet’s try to make predictions on a streamed dataset\n\ndf_test = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig').sample(frac=0.2,random_state=1)\n# drop NaN values in the label column\ndf_test = df_test[~df_test['Department Name'].isna()].reset_index(drop=True)\n\n# In this example we will keep all rows of the test set\ntrue_labels = df_test['Department Name'].values \n\ndf_test.drop('Department Name',axis=1,inplace=True)\n\n\ndf_test.shape\n\n(4692, 9)\n\n\n\n_test_dset_stream = Dataset.from_pandas(df_test).to_iterable_dataset()\n\nCurrently the ModelController does not support prediction on a streamied dataset yet, so we will have to manually create and predict each batch\nWe are going to predict each batch of 1000 items\n\nfrom collections import defaultdict\n\nWe only keep these attributes in the results, as in streamied dataset, storing all attributes can be costly\n\ncols_to_keep = ['Title'] + [f'{i}_Department Name' for i in ['pred','pred_prob']]\ncols_to_keep\n\n['Title', 'pred_Department Name', 'pred_prob_Department Name']\n\n\n\npred_bs = 1000\nresults=[]\nbatch_dic=defaultdict(list)\ncount=0\nbatch_count=0\nfor d in _test_dset_stream:\n    # forming a batch\n    for k,v in d.items():\n        batch_dic[k].append(v)\n    count+=1\n    \n    if count==pred_bs:\n        # make predictions on complete batch\n        # you can increase gpu batch size here, since inference is less costly than training\n        _pred_dset = controller.predict_raw_dset(Dataset.from_dict(batch_dic),do_filtering=False,batch_size=64)\n        _pred_dset = _pred_dset.remove_columns([c for c in _pred_dset.column_names if c not in cols_to_keep])\n        results.append(_pred_dset)\n        print(f'Finish prediction for batch {batch_count+1}')\n        \n        batch_count+=1\n        count=0\n        batch_dic=defaultdict(list)\n\n# last batch of &lt;1000 values\nif count!=0:\n    _pred_dset = controller.predict_raw_dset(Dataset.from_dict(batch_dic),do_filtering=False,batch_size=64)\n    _pred_dset = _pred_dset.remove_columns([c for c in _pred_dset.column_names if c not in cols_to_keep])\n    results.append(_pred_dset)\n    print(f'Finish prediction for batch {batch_count+1}')\n\n-------------------- Start making predictions --------------------\nFinish prediction for batch 1\n-------------------- Start making predictions --------------------\nFinish prediction for batch 2\n-------------------- Start making predictions --------------------\nFinish prediction for batch 3\n-------------------- Start making predictions --------------------\nFinish prediction for batch 4\n-------------------- Start making predictions --------------------\nFinish prediction for batch 5\n\n\nExample of first batch’s prediction\n\nresults[0].to_pandas()\n\n\n\n\n\n\n\n\n\nTitle\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\nperfect for work and play\nTops\n0.997284\n\n\n1\n\nBottoms\n0.989114\n\n\n2\ngreat pants\nBottoms\n0.986304\n\n\n3\nsurprisingly comfy for a button down\nTops\n0.990987\n\n\n4\nshort and small\nTops\n0.996322\n\n\n...\n...\n...\n...\n\n\n995\ngreat design\nBottoms\n0.976045\n\n\n996\ni'm wearing the hadley tunic for my birthday!!!!\nTops\n0.996853\n\n\n997\ntunic has a beautiful print, sparkle detail\nDresses\n0.489722\n\n\n998\nlove the ruffle detail\nTops\n0.996255\n\n\n999\nsimply amazing\nTops\n0.893297\n\n\n\n\n1000 rows × 3 columns\n\n\n\n\nLast batch\n\nresults[-1].to_pandas()\n\n\n\n\n\n\n\n\n\nTitle\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\ncomfortable meets cute\nDresses\n0.988495\n\n\n1\ngorgeous!!\nTops\n0.994889\n\n\n2\n\nTops\n0.996483\n\n\n3\n\nBottoms\n0.985292\n\n\n4\nnice summer dress\nDresses\n0.988174\n\n\n...\n...\n...\n...\n\n\n687\ncute but...\nTops\n0.989280\n\n\n688\nsize down one or two sizes\nTops\n0.829144\n\n\n689\nnot worth it for the price\nBottoms\n0.986075\n\n\n690\nbeautiful maxi!\nDresses\n0.988027\n\n\n691\nbasic with a twist\nTops\n0.996946\n\n\n\n\n692 rows × 3 columns\n\n\n\n\nChecking f1 score for all 5 batches\n\nall_preds=[]\nfor r in results:\n    all_preds += r['pred_Department Name'].tolist()\n\n\nf1_score(true_labels,all_preds,average='macro')\n\n0.7541583296621743",
    "crumbs": [
      "2. All Use Cases",
      "e. Training A Streamed Dataset",
      "Roberta model with a streamed dataset (Custom Single Head)"
    ]
  },
  {
    "objectID": "gpt2_multihead_regression.html",
    "href": "gpt2_multihead_regression.html",
    "title": "GPT2 model (Regression)",
    "section": "",
    "text": "In this series, we walk through some of the capability of this library: single-head classification, multi-head classification, multi-label classification, and regression. If you want a more detailed tutorial, check this out\nimport os\n#This will specify a (or a list) of GPUs for training\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom that_nlp_library.text_main import *\nfrom that_nlp_library.utils import seed_everything\nfrom underthesea import text_normalize\nfrom functools import partial\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport nlpaug.augmenter.char as nac\nfrom datasets import load_dataset\nimport random\nfrom transformers import AutoTokenizer\nfrom datasets import Dataset\nimport torch\nfrom transformers.models.gpt2.modeling_gpt2 import GPT2Model",
    "crumbs": [
      "2. All Use Cases",
      "b. GPT2-based models for Supervised Learning",
      "GPT2 model (Regression)"
    ]
  },
  {
    "objectID": "gpt2_multihead_regression.html#using-huggingface-model-initialization",
    "href": "gpt2_multihead_regression.html#using-huggingface-model-initialization",
    "title": "GPT2 model (Regression)",
    "section": "Using HuggingFace model initialization",
    "text": "Using HuggingFace model initialization\n\nfrom transformers.models.gpt2.modeling_gpt2 import GPT2ForSequenceClassification\n\n\nnum_classes=1\n\n\nseed_everything(42)\nmodel = GPT2ForSequenceClassification.from_pretrained('gpt2',num_labels=num_classes)\nmodel = model.to('cuda:0')\n\nSome weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\nmodel.config.pad_token_id = model.config.eos_token_id\nmodel.resize_token_embeddings(len(_tokenizer))\n\nEmbedding(50257, 768)\n\n\n\nmetric_funcs = [mean_absolute_error,mean_squared_log_error]\ncontroller = ModelController(model,tdc,seed=42)\n\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 3\n\ncontroller.fit(epochs,lr,\n               metric_funcs=metric_funcs,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics,\n              )\n\n\n\n    \n      \n      \n      [849/849 07:25, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nMean Absolute Error Rating\nMean Squared Log Error Rating\n\n\n\n\n1\nNo log\n0.401165\n0.452420\n0.026643\n\n\n2\n0.837500\n0.346818\n0.407272\n0.022015\n\n\n3\n0.837500\n0.335389\n0.400436\n0.021186",
    "crumbs": [
      "2. All Use Cases",
      "b. GPT2-based models for Supervised Learning",
      "GPT2 model (Regression)"
    ]
  },
  {
    "objectID": "gpt2_multihead_regression.html#using-gpt2-based-model-no-concatenation-but-with-a-custom-head-to-limit-the-output-range",
    "href": "gpt2_multihead_regression.html#using-gpt2-based-model-no-concatenation-but-with-a-custom-head-to-limit-the-output-range",
    "title": "GPT2 model (Regression)",
    "section": "Using GPT2-based model (no concatenation), but with a custom head to limit the output range",
    "text": "Using GPT2-based model (no concatenation), but with a custom head to limit the output range\n\nnum_classes=1\n\n\ngpt2_body = GPT2Model.from_pretrained('gpt2')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\nclass GPT2SigmoidRange(torch.nn.Module):\n    def __init__(self,\n                 config,\n                 high,\n                 low,\n                 **kwargs\n                ):\n        super().__init__()\n        self.high=high\n        self.low=low\n        self.score = torch.nn.Linear(config.n_embd, config.num_labels, bias=False)\n    def forward(self, inp, **kwargs):\n        logits = self.score(inp)\n        return torch.sigmoid(logits)*(self.high-self.low)+self.low\n\n\n_model_kwargs={\n    # overall model hyperparams\n    'head_class_sizes':num_classes,\n    'head_class': GPT2SigmoidRange,\n    # classfication head hyperparams\n    'high':5, # the maximum rating\n    'low': 1, # the minimum rating\n}\n\nmodel = model_init_classification(model_class = GPT2BaseForSequenceClassification,\n                                  cpoint_path = 'gpt2', \n                                  output_hidden_states=False,\n                                  seed=42,\n                                  body_model=gpt2_body,\n                                  model_kwargs = _model_kwargs)\n\nLoading body weights. This assumes the body is the very first block of your custom architecture\nTotal parameters: 124440576\nTotal trainable parameters: 124440576\n\n\n\nmodel.body_model.resize_token_embeddings(len(_tokenizer))\n\nEmbedding(50257, 768)\n\n\n\nmetric_funcs = [mean_absolute_error,mean_squared_log_error]\ncontroller = ModelController(model,tdc,seed=42)\n\n\nseed_everything(42)\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 3\n\ncontroller.fit(epochs,lr,\n               metric_funcs=metric_funcs,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics,\n              )\n\n\n\n    \n      \n      \n      [849/849 07:49, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nMean Absolute Error Rating\nMean Squared Log Error Rating\n\n\n\n\n1\nNo log\n0.392251\n0.473784\n0.024639\n\n\n2\n0.591400\n0.348463\n0.412898\n0.021849\n\n\n3\n0.591400\n0.326785\n0.391602\n0.020824",
    "crumbs": [
      "2. All Use Cases",
      "b. GPT2-based models for Supervised Learning",
      "GPT2 model (Regression)"
    ]
  },
  {
    "objectID": "gpt2_multihead_regression.html#using-the-gpt2-custom-model-concatenation",
    "href": "gpt2_multihead_regression.html#using-the-gpt2-custom-model-concatenation",
    "title": "GPT2 model (Regression)",
    "section": "Using the GPT2 custom model (concatenation)",
    "text": "Using the GPT2 custom model (concatenation)\n\nnum_classes=1\n\n\ngpt2_body = GPT2Model.from_pretrained('gpt2')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\n_model_kwargs={\n    # overall model hyperparams\n    'head_class_sizes':num_classes,\n    'head_class': ConcatHeadSimple,\n    # classfication head hyperparams\n    'layer2concat':4,\n    'classifier_dropout':0.1 \n}\n\n\nmodel = model_init_classification(model_class = GPT2HiddenStateConcatForSequenceClassification,\n                                  cpoint_path = 'gpt2', \n                                  output_hidden_states=True, # since we are using 'hidden layer contatenation' technique\n                                  seed=42,\n                                  body_model=gpt2_body,\n                                  model_kwargs = _model_kwargs)\nmodel.body_model.resize_token_embeddings(len(_tokenizer))\n\nLoading body weights. This assumes the body is the very first block of your custom architecture\nTotal parameters: 124442881\nTotal trainable parameters: 124442881\n\n\nEmbedding(50257, 768)\n\n\n\nmetric_funcs = [mean_absolute_error,mean_squared_log_error]\ncontroller = ModelController(model,tdc,seed=42)\n\n\nseed_everything(42)\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 3\n\ncontroller.fit(epochs,lr,\n               metric_funcs=metric_funcs,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics,\n              )\n\n\n\n    \n      \n      \n      [849/849 06:02, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nMean Absolute Error Rating\nMean Squared Log Error Rating\n\n\n\n\n1\nNo log\n0.590046\n0.550116\n0.037543\n\n\n2\n2.443300\n0.409290\n0.443936\n0.025740\n\n\n3\n2.443300\n0.379117\n0.441241\n0.023792\n\n\n\n\n\n\n\n\ncontroller.trainer.model.save_pretrained('./sample_weights/my_model')",
    "crumbs": [
      "2. All Use Cases",
      "b. GPT2-based models for Supervised Learning",
      "GPT2 model (Regression)"
    ]
  },
  {
    "objectID": "gpt2_multihead_regression.html#predict-validation",
    "href": "gpt2_multihead_regression.html#predict-validation",
    "title": "GPT2 model (Regression)",
    "section": "Predict Validation",
    "text": "Predict Validation\n\n_model_kwargs\n\n{'head_class_sizes': 1,\n 'head_class': that_nlp_library.models.roberta.classifiers.ConcatHeadSimple,\n 'layer2concat': 4,\n 'classifier_dropout': 0.1}\n\n\n\ntrained_model = model_init_classification(model_class = GPT2HiddenStateConcatForSequenceClassification,\n                                          cpoint_path = Path('./sample_weights/my_model'), \n                                          output_hidden_states=True,\n                                          seed=42,\n                                          model_kwargs = _model_kwargs)\n\ncontroller = ModelController(trained_model,tdc,seed=42)\n\nTotal parameters: 124442881\nTotal trainable parameters: 124442881\n\n\n\ndf_val = controller.predict_ddict(ds_type='validation')\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_val = df_val.to_pandas()\ndf_val.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nRating\nDivision Name\nlabel\ninput_ids\nattention_mask\npred_Rating\n\n\n\n\n0\n\ngeneral . . this picture doesn't do the skirt ...\n5.0\ngeneral\n5.0\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n4.741378\n\n\n1\n\ngeneral . . easy to wear ! cute , comfy ... wi...\n4.0\ngeneral\n4.0\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n4.796369\n\n\n2\n\ngeneral . . nice sweater , just did not look g...\n3.0\ngeneral\n3.0\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n2.616789\n\n\n3\nnice cropped jacket\ngeneral . nice cropped jacket . this jacket wa...\n5.0\ngeneral\n5.0\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n4.419146\n\n\n4\ngreat dress!\ngeneral petite . great dress ! . i wasn't plan...\n5.0\ngeneral petite\n5.0\n[24622, 4273, 578, 764, 1049, 6576, 5145, 764,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n5.079652\n\n\n\n\n\n\n\n\nYou can try to get your metric to see if it matches your last traing epoch’s above\n\nmean_absolute_error(df_val['label'],df_val['pred_Rating'])\n\n0.441279638942753\n\n\n\nmean_squared_log_error(df_val['label'],df_val['pred_Rating'])\n\n0.023789442640954332",
    "crumbs": [
      "2. All Use Cases",
      "b. GPT2-based models for Supervised Learning",
      "GPT2 model (Regression)"
    ]
  },
  {
    "objectID": "gpt2_multihead_regression.html#predict-test-set",
    "href": "gpt2_multihead_regression.html#predict-test-set",
    "title": "GPT2 model (Regression)",
    "section": "Predict Test set",
    "text": "Predict Test set\nWe will go through details on how to make a prediction on a completely new and raw dataset using our trained model. For now, let’s reuse the sample csv and pretend it’s our test set\n\ndf_test = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig').sample(frac=0.2,random_state=1)\n\n\n# save the label, as we will calculate some metrics later\ntrue_labels = df_test[~df_test['Review Text'].isna()].Rating.values \n\n# drop the label (you don't need to, but this is necessary to simulate an actual test set)\ndf_test.drop('Rating',axis=1,inplace=True)\n\n\n_test_dset = Dataset.from_pandas(df_test)\n_test_dset_predicted = controller.predict_raw_dset(_test_dset,\n                                                   do_filtering=True, # since we have some text filtering in the processing\n                                                  )\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_test_predicted = _test_dset_predicted.to_pandas()\n\n\ndf_test_predicted.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nDivision Name\ninput_ids\nattention_mask\npred_Rating\n\n\n\n\n0\nperfect for work and play\ngeneral . perfect for work and play . this shi...\ngeneral\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n4.962859\n\n\n1\n\ngeneral petite . . i don't know why i had the ...\ngeneral petite\n[24622, 4273, 578, 764, 764, 1312, 836, 470, 7...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n4.025272\n\n\n2\ngreat pants\ngeneral petite . great pants . thes e cords ar...\ngeneral petite\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n4.888588\n\n\n3\nsurprisingly comfy for a button down\ngeneral petite . surprisingly comfy for a butt...\ngeneral petite\n[24622, 4273, 578, 764, 12362, 401, 24928, 329...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n4.510598\n\n\n4\nshort and small\ngeneral petite . short and small . the shirt i...\ngeneral petite\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n2.427316\n\n\n\n\n\n\n\n\nLet’s quickly check the score to make sure everything works correctly\n\nmean_absolute_error(true_labels,df_test_predicted['pred_Rating'])\n\n0.41902880738461595\n\n\n\nmean_squared_log_error(true_labels,df_test_predicted['pred_Rating'])\n\n0.022995927657672072\n\n\n\nraw_content={'Review Text': 'This shirt is so comfortable I love it!',\n             'Title': 'Great shirt',\n             'Division Name': 'general'}\nraw_content\n\n{'Review Text': 'This shirt is so comfortable I love it!',\n 'Title': 'Great shirt',\n 'Division Name': 'general'}\n\n\n\ndf_result = controller.predict_raw_text(raw_content)\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_result\n\n{'Review Text': ['general . great shirt . this shirt is so comfortable i love it !'],\n 'Title': ['great shirt'],\n 'Division Name': ['general'],\n 'input_ids': [[24622,\n   764,\n   1049,\n   10147,\n   764,\n   428,\n   10147,\n   318,\n   523,\n   6792,\n   1312,\n   1842,\n   340,\n   5145]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n 'pred_Rating': [4.925951957702637]}",
    "crumbs": [
      "2. All Use Cases",
      "b. GPT2-based models for Supervised Learning",
      "GPT2 model (Regression)"
    ]
  },
  {
    "objectID": "gpt2_multihead_regression.html#re-define-the-textdatacontroller",
    "href": "gpt2_multihead_regression.html#re-define-the-textdatacontroller",
    "title": "GPT2 model (Regression)",
    "section": "Re-define the TextDataController",
    "text": "Re-define the TextDataController\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names=['Rating','Department Name'],\n                         sup_types=['regression','classification'],\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None},\n                         metadatas=['Title'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         val_ratio=0.2,\n                         batch_size=1000,\n                         seed=42,\n                         num_proc=20,\n                         verbose=False\n                        )\n\n\n_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n\n\n_tokenizer.pad_token = _tokenizer.eos_token\n_tokenizer.padding_side = 'left'\n\nProcess and tokenize our dataset\n\ntdc.process_and_tokenize(_tokenizer,max_length=100,shuffle_trn=True)\n\n\ntdc.main_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Title', 'Review Text', 'Rating', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 18101\n    })\n    validation: Dataset({\n        features: ['Title', 'Review Text', 'Rating', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 4526\n    })\n})\n\n\n\ntdc.label_lists\n\n[[], ['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']]",
    "crumbs": [
      "2. All Use Cases",
      "b. GPT2-based models for Supervised Learning",
      "GPT2 model (Regression)"
    ]
  },
  {
    "objectID": "gpt2_multihead_regression.html#using-the-gpt2-custom-model-concatenation-1",
    "href": "gpt2_multihead_regression.html#using-the-gpt2-custom-model-concatenation-1",
    "title": "GPT2 model (Regression)",
    "section": "Using the GPT2 custom model (concatenation)",
    "text": "Using the GPT2 custom model (concatenation)\n\nnum_classes=[1,len(tdc.label_lists[1])] # 1 head size 1 for regression, 1 head size 6 for classification\nnum_classes\n\n[1, 6]\n\n\n\ngpt2_body = GPT2Model.from_pretrained('gpt2')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\n_model_kwargs={\n    # overall model hyperparams\n    'head_class_sizes':num_classes,\n    'head_class': ConcatHeadSimple,\n    # classfication head hyperparams\n    'layer2concat':3,\n    'classifier_dropout':0.1 \n}\n\n\nmodel = model_init_classification(model_class = GPT2HiddenStateConcatForSequenceClassification,\n                                  cpoint_path = 'gpt2', \n                                  output_hidden_states=True,\n                                  seed=42,\n                                  body_model=gpt2_body,\n                                  model_kwargs = _model_kwargs)\n\nLoading body weights. This assumes the body is the very first block of your custom architecture\nTotal parameters: 124455943\nTotal trainable parameters: 124455943\n\n\n\nmodel.body_model.resize_token_embeddings(len(_tokenizer))\n\nEmbedding(50257, 768)\n\n\nIf you use multihead and each head does a different supervised learning type (as in this case when you have both classification and regression head), and you have separate metric for each type, you must define a metric_types to let the controller knows what metric functions to apply for each head\n\nmetric_funcs = [mean_absolute_error,partial(f1_score,average='macro'),accuracy_score]\nmetric_types = ['regression','classification','classification']\n\n\ncontroller = ModelController(model,tdc,seed=42)\n\n\nseed_everything(42)\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 3\n\ncontroller.fit(epochs,lr,\n               metric_funcs=metric_funcs,\n               metric_types=metric_types,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics,\n              )\n\n\n\n    \n      \n      \n      [849/849 06:06, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nMean Absolute Error Rating\nF1 Score Department name\nAccuracy Score Department name\n\n\n\n\n1\nNo log\n1.206080\n0.571639\n0.544659\n0.821034\n\n\n2\n3.874900\n1.046785\n0.537101\n0.618333\n0.849757\n\n\n3\n3.874900\n0.865819\n0.460715\n0.639875\n0.865665\n\n\n\n\n\n\n\n\ncontroller.trainer.model.save_pretrained('./sample_weights/my_model')",
    "crumbs": [
      "2. All Use Cases",
      "b. GPT2-based models for Supervised Learning",
      "GPT2 model (Regression)"
    ]
  },
  {
    "objectID": "gpt2_multihead_regression.html#predict-validation-1",
    "href": "gpt2_multihead_regression.html#predict-validation-1",
    "title": "GPT2 model (Regression)",
    "section": "Predict Validation",
    "text": "Predict Validation\n\n_model_kwargs\n\n{'head_class_sizes': [1, 6],\n 'head_class': that_nlp_library.models.roberta.classifiers.ConcatHeadSimple,\n 'layer2concat': 3,\n 'classifier_dropout': 0.1}\n\n\n\ntrained_model = model_init_classification(model_class = GPT2HiddenStateConcatForSequenceClassification,\n                                          cpoint_path = Path('./sample_weights/my_model'), \n                                          output_hidden_states=True,\n                                          seed=42,\n                                          model_kwargs = _model_kwargs)\n\ncontroller = ModelController(trained_model,tdc,seed=42)\n\nTotal parameters: 124455943\nTotal trainable parameters: 124455943\n\n\n\ndf_val = controller.predict_ddict(ds_type='validation')\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_val = df_val.to_pandas()\ndf_val.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nRating\nDepartment Name\nlabel\ninput_ids\nattention_mask\npred_Rating\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\n\n. such a fun jacket ! great to wear in the spr...\n5.0\nIntimate\n[5.0, 2.0]\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n5.052881\nJackets\n0.806607\n\n\n1\nsimple and elegant\nsimple and elegant . i thought this shirt was ...\n5.0\nTops\n[5.0, 4.0]\n[36439, 290, 19992, 764, 1312, 1807, 428, 1014...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n3.821376\nTops\n0.996092\n\n\n2\nretro and pretty\nretro and pretty . this top has a bit of a ret...\n5.0\nTops\n[5.0, 4.0]\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n4.635709\nTops\n0.988367\n\n\n3\nsummer/fall wear\nsummer / fall wear . i first spotted this on a...\n5.0\nDresses\n[5.0, 1.0]\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, ...\n4.859474\nDresses\n0.931358\n\n\n4\nperfect except slip\nperfect except slip . this is my new favorite ...\n4.0\nDresses\n[4.0, 1.0]\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n4.317022\nDresses\n0.987220\n\n\n\n\n\n\n\n\nYou can try to get your metric to see if it matches your last traing epoch’s above\n\nmean_absolute_error(df_val['Rating'],df_val['pred_Rating'])\n\n0.46070288352823024\n\n\n\nf1_score(df_val['Department Name'],df_val['pred_Department Name'],average='macro')\n\n0.6396845952183047",
    "crumbs": [
      "2. All Use Cases",
      "b. GPT2-based models for Supervised Learning",
      "GPT2 model (Regression)"
    ]
  },
  {
    "objectID": "gpt2_multihead_regression.html#predict-test-set-1",
    "href": "gpt2_multihead_regression.html#predict-test-set-1",
    "title": "GPT2 model (Regression)",
    "section": "Predict Test set",
    "text": "Predict Test set\nWe will go through details on how to make a prediction on a completely new and raw dataset using our trained model. For now, let’s reuse the sample csv and pretend it’s our test set\n\ndf_test = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig').sample(frac=0.2,random_state=1)\n\n# drop the label (you don't need to, but this is necessary to simulate an actual test set)\ndf_test.drop(['Rating','Department Name'],axis=1,inplace=True)\n\n\n_test_dset = Dataset.from_pandas(df_test)\n_test_dset_predicted = controller.predict_raw_dset(_test_dset,\n                                                   do_filtering=True, # since we have some text filtering in the processing\n                                                   topk=3\n                                                  )\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_test_predicted = _test_dset_predicted.to_pandas()\n\n\ndf_test_predicted.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\ninput_ids\nattention_mask\npred_Rating\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\nperfect for work and play\nperfect for work and play . this shirt works f...\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n5.155851\n[Tops, Intimate, Jackets]\n[0.9964365, 0.0024720305, 0.0007216654]\n\n\n1\n\n. i don't know why i had the opposite problem ...\n[13, 1312, 836, 470, 760, 1521, 1312, 550, 262...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n4.188741\n[Bottoms, Intimate, Dresses]\n[0.97982305, 0.01562881, 0.001401943]\n\n\n2\ngreat pants\ngreat pants . thes e cords are great--lightwei...\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n4.878179\n[Bottoms, Intimate, Jackets]\n[0.97345406, 0.022959404, 0.0018552544]\n\n\n3\nsurprisingly comfy for a button down\nsurprisingly comfy for a button down . i am a ...\n[41199, 401, 24928, 329, 257, 4936, 866, 764, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n4.531747\n[Tops, Intimate, Jackets]\n[0.952523, 0.022002747, 0.015237918]\n\n\n4\nshort and small\nshort and small . the shirt is mostly a thick ...\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n2.355370\n[Tops, Intimate, Jackets]\n[0.98523074, 0.010804329, 0.0018044778]\n\n\n\n\n\n\n\n\n\nraw_content={'Review Text': 'This shirt is so comfortable I love it!',\n             'Title': 'Great shirt'}\nraw_content\n\n{'Review Text': 'This shirt is so comfortable I love it!',\n 'Title': 'Great shirt'}\n\n\n\ndf_result = controller.predict_raw_text(raw_content)\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_result\n\n{'Review Text': ['great shirt . this shirt is so comfortable i love it !'],\n 'Title': ['great shirt'],\n 'input_ids': [[18223,\n   10147,\n   764,\n   428,\n   10147,\n   318,\n   523,\n   6792,\n   1312,\n   1842,\n   340,\n   5145]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n 'pred_Rating': [5.187490940093994],\n 'pred_Department Name': ['Tops'],\n 'pred_prob_Department Name': [0.9844864010810852]}",
    "crumbs": [
      "2. All Use Cases",
      "b. GPT2-based models for Supervised Learning",
      "GPT2 model (Regression)"
    ]
  },
  {
    "objectID": "roberta_multihead_regression.html",
    "href": "roberta_multihead_regression.html",
    "title": "Roberta model (Regression)",
    "section": "",
    "text": "In this series, we walk through some of the capability of this library: single-head classification, multi-head classification, multi-label classification, and regression. If you want a more detailed tutorial, check this out\nimport os\n#This will specify a (or a list) of GPUs for training\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom that_nlp_library.text_main import *\nfrom that_nlp_library.utils import seed_everything\nfrom underthesea import text_normalize\nfrom functools import partial\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport nlpaug.augmenter.char as nac\nfrom datasets import load_dataset\nimport random\nfrom transformers import RobertaTokenizer\nfrom datasets import Dataset\nfrom transformers.models.roberta.modeling_roberta import RobertaModel\nimport torch",
    "crumbs": [
      "2. All Use Cases",
      "a. Roberta-based models for Supervised Learning",
      "Roberta model (Regression)"
    ]
  },
  {
    "objectID": "roberta_multihead_regression.html#using-huggingface-model-initialization",
    "href": "roberta_multihead_regression.html#using-huggingface-model-initialization",
    "title": "Roberta model (Regression)",
    "section": "Using HuggingFace model initialization",
    "text": "Using HuggingFace model initialization\n\nfrom transformers.models.roberta.modeling_roberta import RobertaForSequenceClassification\n\n\nnum_classes=1\n\n\nseed_everything(42)\nmodel = RobertaForSequenceClassification.from_pretrained('roberta-base',num_labels=num_classes)\nmodel = model.to('cuda:0')\n\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\nmetric_funcs = [mean_absolute_error,mean_squared_log_error]\ncontroller = ModelController(model,tdc,seed=42)\n\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 3\n\ncontroller.fit(epochs,lr,\n               metric_funcs=metric_funcs,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics,\n              )\n\n/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\n\n\n\n    \n      \n      \n      [849/849 02:04, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nMean Absolute Error Rating\nMean Squared Log Error Rating\n\n\n\n\n1\nNo log\n0.390571\n0.395417\n0.024300\n\n\n2\n1.053300\n0.416924\n0.460436\n0.023507\n\n\n3\n1.053300\n0.342913\n0.397407\n0.020786",
    "crumbs": [
      "2. All Use Cases",
      "a. Roberta-based models for Supervised Learning",
      "Roberta model (Regression)"
    ]
  },
  {
    "objectID": "roberta_multihead_regression.html#using-roberta-base-model-no-concatenation-but-with-a-custom-head-to-limit-the-output-range",
    "href": "roberta_multihead_regression.html#using-roberta-base-model-no-concatenation-but-with-a-custom-head-to-limit-the-output-range",
    "title": "Roberta model (Regression)",
    "section": "Using Roberta-base model (no concatenation), but with a custom head to limit the output range",
    "text": "Using Roberta-base model (no concatenation), but with a custom head to limit the output range\n\nnum_classes=1\n\n\nroberta_body = RobertaModel.from_pretrained('roberta-base')\n\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\nclass RobertaSigmoidRange(torch.nn.Module):\n    def __init__(self,\n                 config,\n                 high,\n                 low,\n                 **kwargs\n                ):\n        super().__init__()\n        self.high=high\n        self.low=low\n        self.score = torch.nn.Linear(config.hidden_size, config.num_labels, bias=False)\n    def forward(self, inp, **kwargs):\n        logits = self.score(inp)\n        return torch.sigmoid(logits)*(self.high-self.low)+self.low\n\n\n_model_kwargs={\n    # overall model hyperparams\n    'head_class_sizes':num_classes,\n    'head_class': RobertaSigmoidRange,\n    # classfication head hyperparams\n    'high':5, # the maximum rating\n    'low': 1, # the minimum rating\n}\n\nmodel = model_init_classification(model_class = RobertaBaseForSequenceClassification,\n                                  cpoint_path = 'roberta-base', \n                                  output_hidden_states=False,\n                                  seed=42,\n                                  body_model=roberta_body,\n                                  model_kwargs = _model_kwargs)\n\nLoading body weights. This assumes the body is the very first block of your custom architecture\n\n\n\nmetric_funcs = [mean_absolute_error,mean_squared_log_error]\ncontroller = ModelController(model,tdc,seed=42)\n\n\nseed_everything(42)\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 3\n\ncontroller.fit(epochs,lr,\n               metric_funcs=metric_funcs,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics,\n              )\n\n/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\n\n\n\n    \n      \n      \n      [849/849 02:11, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nMean Absolute Error Rating\nMean Squared Log Error Rating\n\n\n\n\n1\nNo log\n0.366913\n0.412213\n0.023336\n\n\n2\n0.494600\n0.320408\n0.386471\n0.020400\n\n\n3\n0.494600\n0.313202\n0.371211\n0.019664",
    "crumbs": [
      "2. All Use Cases",
      "a. Roberta-based models for Supervised Learning",
      "Roberta model (Regression)"
    ]
  },
  {
    "objectID": "roberta_multihead_regression.html#using-the-roberta-custom-model-concatenation",
    "href": "roberta_multihead_regression.html#using-the-roberta-custom-model-concatenation",
    "title": "Roberta model (Regression)",
    "section": "Using the Roberta custom model (concatenation)",
    "text": "Using the Roberta custom model (concatenation)\n\nfrom transformers.models.roberta.modeling_roberta import RobertaModel\n\n\nnum_classes=1\n\n\nroberta_body = RobertaModel.from_pretrained('roberta-base')\n\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n_model_kwargs={\n    # overall model hyperparams\n    'head_class_sizes':num_classes,\n    'head_class': ConcatHeadSimple,\n    # classfication head hyperparams\n    'layer2concat':4,\n    'classifier_dropout':0.1 \n}\n\n\nmodel = model_init_classification(model_class = RobertaHiddenStateConcatForSequenceClassification,\n                                  cpoint_path = 'roberta-base', \n                                  output_hidden_states=True, # since we are using 'hidden layer contatenation' technique\n                                  seed=42,\n                                  body_model=roberta_body,\n                                  model_kwargs = _model_kwargs)\n\nLoading body weights. This assumes the body is the very first block of your custom architecture\nTotal parameters: 124648705\nTotal trainable parameters: 124648705\n\n\n\nmetric_funcs = [mean_absolute_error,mean_squared_log_error]\ncontroller = ModelController(model,tdc,seed=42)\n\n\nseed_everything(42)\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 3\n\ncontroller.fit(epochs,lr,\n               metric_funcs=metric_funcs,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics,\n              )\n\n\n\n    \n      \n      \n      [849/849 02:29, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nMean Absolute Error Rating\nMean Squared Log Error Rating\n\n\n\n\n1\nNo log\n0.526983\n0.560607\n0.029044\n\n\n2\n1.226800\n0.338014\n0.397377\n0.021910\n\n\n3\n1.226800\n0.332928\n0.389946\n0.020401\n\n\n\n\n\n\n\n\ncontroller.trainer.model.save_pretrained('./sample_weights/my_model')",
    "crumbs": [
      "2. All Use Cases",
      "a. Roberta-based models for Supervised Learning",
      "Roberta model (Regression)"
    ]
  },
  {
    "objectID": "roberta_multihead_regression.html#predict-validation",
    "href": "roberta_multihead_regression.html#predict-validation",
    "title": "Roberta model (Regression)",
    "section": "Predict Validation",
    "text": "Predict Validation\n\n_model_kwargs\n\n{'head_class_sizes': 1,\n 'head_class': that_nlp_library.models.roberta.classifiers.ConcatHeadSimple,\n 'layer2concat': 4,\n 'classifier_dropout': 0.1}\n\n\n\ntrained_model = model_init_classification(model_class = RobertaHiddenStateConcatForSequenceClassification,\n                                          cpoint_path = Path('./sample_weights/my_model'), \n                                          output_hidden_states=True,\n                                          seed=42,\n                                          model_kwargs = _model_kwargs)\n\ncontroller = ModelController(trained_model,tdc,seed=42)\n\nSome weights of the model checkpoint at sample_weights/my_model were not used when initializing RobertaHiddenStateConcatForSequenceClassification: ['body_model.pooler.dense.weight', 'body_model.pooler.dense.bias']\n- This IS expected if you are initializing RobertaHiddenStateConcatForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaHiddenStateConcatForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\nTotal parameters: 124058113\nTotal trainable parameters: 124058113\n\n\n\ndf_val = controller.predict_ddict(ds_type='validation')\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_val = df_val.to_pandas()\ndf_val.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nRating\nDivision Name\nlabel\ninput_ids\nattention_mask\npred_Rating\n\n\n\n\n0\n\ngeneral . . this picture doesn't do the skirt ...\n5.0\ngeneral\n5.0\n[0, 15841, 479, 479, 42, 2170, 630, 75, 109, 5...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n5.009285\n\n\n1\n\ngeneral . . easy to wear ! cute , comfy ... wi...\n4.0\ngeneral\n4.0\n[0, 15841, 479, 479, 1365, 7, 3568, 27785, 119...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n4.959775\n\n\n2\n\ngeneral . . nice sweater , just did not look g...\n3.0\ngeneral\n3.0\n[0, 15841, 479, 479, 2579, 23204, 2156, 95, 22...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n2.813448\n\n\n3\nnice cropped jacket\ngeneral . nice cropped jacket . this jacket wa...\n5.0\ngeneral\n5.0\n[0, 15841, 479, 2579, 30197, 8443, 479, 42, 84...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n4.425314\n\n\n4\ngreat dress!\ngeneral petite . great dress ! . i wasn't plan...\n5.0\ngeneral petite\n5.0\n[0, 15841, 4716, 1459, 479, 372, 3588, 27785, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n5.152634\n\n\n\n\n\n\n\n\nYou can try to get your metric to see if it matches your last traing epoch’s above\n\nmean_absolute_error(df_val['label'],df_val['pred_Rating'])\n# 0.3844665757181577\n\n0.38998359958971274\n\n\n\nmean_squared_log_error(df_val['label'],df_val['pred_Rating'])\n# 0.020327507632071154\n\n0.020402761232161202",
    "crumbs": [
      "2. All Use Cases",
      "a. Roberta-based models for Supervised Learning",
      "Roberta model (Regression)"
    ]
  },
  {
    "objectID": "roberta_multihead_regression.html#predict-test-set",
    "href": "roberta_multihead_regression.html#predict-test-set",
    "title": "Roberta model (Regression)",
    "section": "Predict Test set",
    "text": "Predict Test set\nWe will go through details on how to make a prediction on a completely new and raw dataset using our trained model. For now, let’s reuse the sample csv and pretend it’s our test set\n\ndf_test = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig').sample(frac=0.2,random_state=1)\n\n\n# save the label, as we will calculate some metrics later\ntrue_labels = df_test[~df_test['Review Text'].isna()].Rating.values \n\n# drop the label (you don't need to, but this is necessary to simulate an actual test set)\ndf_test.drop('Rating',axis=1,inplace=True)\n\n\n_test_dset = Dataset.from_pandas(df_test)\n_test_dset_predicted = controller.predict_raw_dset(_test_dset,\n                                                   do_filtering=True, # since we have some text filtering in the processing\n                                                  )\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_test_predicted = _test_dset_predicted.to_pandas()\n\n\ndf_test_predicted.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nDivision Name\ninput_ids\nattention_mask\npred_Rating\n\n\n\n\n0\nperfect for work and play\ngeneral . perfect for work and play . this shi...\ngeneral\n[0, 15841, 479, 1969, 13, 173, 8, 310, 479, 42...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n5.153669\n\n\n1\n\ngeneral petite . . i don't know why i had the ...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 479, 939, 218, 75,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n4.160178\n\n\n2\ngreat pants\ngeneral petite . great pants . thes e cords ar...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 372, 9304, 479, 5,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n5.120962\n\n\n3\nsurprisingly comfy for a button down\ngeneral petite . surprisingly comfy for a butt...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 10262, 3137, 24382...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n4.653287\n\n\n4\nshort and small\ngeneral petite . short and small . the shirt i...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 765, 8, 650, 479, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n2.688593\n\n\n\n\n\n\n\n\nLet’s quickly check the score to make sure everything works correctly\n\nmean_absolute_error(true_labels,df_test_predicted['pred_Rating'])\n\n0.3419881553779289\n\n\n\nmean_squared_log_error(true_labels,df_test_predicted['pred_Rating'])\n\n0.016165478401855043\n\n\n\nraw_content={'Review Text': 'This shirt is so comfortable I love it!',\n             'Title': 'Great shirt',\n             'Division Name': 'general'}\nraw_content\n\n{'Review Text': 'This shirt is so comfortable I love it!',\n 'Title': 'Great shirt',\n 'Division Name': 'general'}\n\n\n\ndf_result = controller.predict_raw_text(raw_content)\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_result\n\n{'Review Text': ['general . great shirt . this shirt is so comfortable i love it !'],\n 'Title': ['great shirt'],\n 'Division Name': ['general'],\n 'input_ids': [[0,\n   15841,\n   479,\n   372,\n   6399,\n   479,\n   42,\n   6399,\n   16,\n   98,\n   3473,\n   939,\n   657,\n   24,\n   27785,\n   2]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n 'pred_Rating': [5.104530334472656]}",
    "crumbs": [
      "2. All Use Cases",
      "a. Roberta-based models for Supervised Learning",
      "Roberta model (Regression)"
    ]
  },
  {
    "objectID": "roberta_multihead_regression.html#re-define-the-textdatacontroller",
    "href": "roberta_multihead_regression.html#re-define-the-textdatacontroller",
    "title": "Roberta model (Regression)",
    "section": "Re-define the TextDataController",
    "text": "Re-define the TextDataController\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names=['Rating','Department Name'],\n                         sup_types=['regression','classification'],\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None},\n                         metadatas=['Title'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         # add \"str.lower\" here because nearby_aug might return uppercase character\n                         val_ratio=0.2,\n                         batch_size=1000,\n                         seed=42,\n                         num_proc=20,\n                         verbose=False\n                        )\n\nDefine our tokenizer for Roberta\n\n_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\nProcess and tokenize our dataset\n\ntdc.process_and_tokenize(_tokenizer,max_length=100,shuffle_trn=True)\n\n\ntdc.main_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Title', 'Review Text', 'Rating', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 18101\n    })\n    validation: Dataset({\n        features: ['Title', 'Review Text', 'Rating', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 4526\n    })\n})\n\n\n\ntdc.label_lists\n\n[[], ['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']]",
    "crumbs": [
      "2. All Use Cases",
      "a. Roberta-based models for Supervised Learning",
      "Roberta model (Regression)"
    ]
  },
  {
    "objectID": "roberta_multihead_regression.html#using-the-roberta-custom-model-concatenation-1",
    "href": "roberta_multihead_regression.html#using-the-roberta-custom-model-concatenation-1",
    "title": "Roberta model (Regression)",
    "section": "Using the Roberta custom model (concatenation)",
    "text": "Using the Roberta custom model (concatenation)\n\nfrom transformers.models.roberta.modeling_roberta import RobertaModel\n\n\nnum_classes=[1,len(tdc.label_lists[1])] # 1 head size 1 for regression, 1 head size 6 for classification\nnum_classes\n\n[1, 6]\n\n\n\nroberta_body = RobertaModel.from_pretrained('roberta-base')\n\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n_model_kwargs={\n    # overall model hyperparams\n    'head_class_sizes':num_classes,\n    'head_class': ConcatHeadSimple,\n    # classfication head hyperparams\n    'layer2concat':3,\n    'classifier_dropout':0.1 \n}\n\n\nmodel = model_init_classification(model_class = RobertaHiddenStateConcatForSequenceClassification,\n                                  cpoint_path = 'roberta-base', \n                                  output_hidden_states=True, # since we are using 'hidden layer contatenation' technique\n                                  seed=42,\n                                  body_model=roberta_body,\n                                  model_kwargs = _model_kwargs)\n\nLoading body weights. This assumes the body is the very first block of your custom architecture\nTotal parameters: 124661767\nTotal trainable parameters: 124661767\n\n\nIf you use multihead and each head does a different supervised learning type (as in this case when you have both classification and regression head), and you have separate metric for each type, you must define a metric_types to let the controller knows what metric functions to apply for each head\n\nmetric_funcs = [mean_absolute_error,mean_squared_log_error,partial(f1_score,average='macro'),accuracy_score]\nmetric_types = ['regression','regression','classification','classification']\n\n\ncontroller = ModelController(model,tdc,seed=42)\n\n\nseed_everything(42)\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 3\n\ncontroller.fit(epochs,lr,\n               metric_funcs=metric_funcs,\n               metric_types=metric_types,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics,\n              )\n\n\n\n    \n      \n      \n      [849/849 02:29, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nMean Absolute Error Rating\nMean Squared Log Error Rating\nF1 Score Department name\nAccuracy Score Department name\n\n\n\n\n1\nNo log\n0.895920\n0.443452\n0.025369\n0.656077\n0.869200\n\n\n2\n1.819800\n0.737174\n0.423700\n0.021711\n0.674119\n0.881794\n\n\n3\n1.819800\n0.730880\n0.397481\n0.021648\n0.684661\n0.884887\n\n\n\n\n\n\n\n\ncontroller.trainer.model.save_pretrained('./sample_weights/my_model')",
    "crumbs": [
      "2. All Use Cases",
      "a. Roberta-based models for Supervised Learning",
      "Roberta model (Regression)"
    ]
  },
  {
    "objectID": "roberta_multihead_regression.html#predict-validation-1",
    "href": "roberta_multihead_regression.html#predict-validation-1",
    "title": "Roberta model (Regression)",
    "section": "Predict Validation",
    "text": "Predict Validation\n\n_model_kwargs\n\n{'head_class_sizes': [1, 6],\n 'head_class': that_nlp_library.models.roberta.classifiers.ConcatHeadSimple,\n 'layer2concat': 3,\n 'classifier_dropout': 0.1}\n\n\n\ntrained_model = model_init_classification(model_class = RobertaHiddenStateConcatForSequenceClassification,\n                                          cpoint_path = Path('./sample_weights/my_model'), \n                                          output_hidden_states=True,\n                                          seed=42,\n                                          model_kwargs = _model_kwargs)\n\ncontroller = ModelController(trained_model,tdc,seed=42)\n\nSome weights of the model checkpoint at sample_weights/my_model were not used when initializing RobertaHiddenStateConcatForSequenceClassification: ['body_model.pooler.dense.weight', 'body_model.pooler.dense.bias']\n- This IS expected if you are initializing RobertaHiddenStateConcatForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaHiddenStateConcatForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\nTotal parameters: 124071175\nTotal trainable parameters: 124071175\n\n\n\ndf_val = controller.predict_ddict(ds_type='validation')\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_val = df_val.to_pandas()\ndf_val.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nRating\nDepartment Name\nlabel\ninput_ids\nattention_mask\npred_Rating\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\n\n. such a fun jacket ! great to wear in the spr...\n5.0\nIntimate\n[5.0, 2.0]\n[0, 4, 215, 10, 1531, 8443, 27785, 372, 7, 356...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n5.087322\nJackets\n0.884425\n\n\n1\nsimple and elegant\nsimple and elegant . i thought this shirt was ...\n5.0\nTops\n[5.0, 4.0]\n[0, 41918, 8, 14878, 479, 939, 802, 42, 6399, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n3.857383\nTops\n0.989074\n\n\n2\nretro and pretty\nretro and pretty . this top has a bit of a ret...\n5.0\nTops\n[5.0, 4.0]\n[0, 4903, 1001, 8, 1256, 479, 42, 299, 34, 10,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n4.827574\nTops\n0.985488\n\n\n3\nsummer/fall wear\nsummer / fall wear . i first spotted this on a...\n5.0\nDresses\n[5.0, 1.0]\n[0, 18581, 2089, 1589, 1136, 3568, 479, 939, 7...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n5.098561\nDresses\n0.984738\n\n\n4\nperfect except slip\nperfect except slip . this is my new favorite ...\n4.0\nDresses\n[4.0, 1.0]\n[0, 20473, 4682, 9215, 479, 42, 16, 127, 92, 2...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n3.854942\nDresses\n0.974034\n\n\n\n\n\n\n\n\nYou can try to get your metric to see if it matches your last traing epoch’s above\n\nmean_absolute_error(df_val['Rating'],df_val['pred_Rating'])\n\n0.39745882958160583\n\n\n\nmean_squared_log_error(df_val['Rating'],df_val['pred_Rating'])\n\n0.021647425684093793\n\n\n\nf1_score(df_val['Department Name'],df_val['pred_Department Name'],average='macro')\n\n0.6845558685013334",
    "crumbs": [
      "2. All Use Cases",
      "a. Roberta-based models for Supervised Learning",
      "Roberta model (Regression)"
    ]
  },
  {
    "objectID": "roberta_multihead_regression.html#predict-test-set-1",
    "href": "roberta_multihead_regression.html#predict-test-set-1",
    "title": "Roberta model (Regression)",
    "section": "Predict Test set",
    "text": "Predict Test set\nWe will go through details on how to make a prediction on a completely new and raw dataset using our trained model. For now, let’s reuse the sample csv and pretend it’s our test set\n\ndf_test = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig').sample(frac=0.2,random_state=1)\n\n# drop the label (you don't need to, but this is necessary to simulate an actual test set)\ndf_test.drop(['Rating','Department Name'],axis=1,inplace=True)\n\n\n_test_dset = Dataset.from_pandas(df_test)\n_test_dset_predicted = controller.predict_raw_dset(_test_dset,\n                                                   do_filtering=True, # since we have some text filtering in the processing\n                                                   topk=3\n                                                  )\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_test_predicted = _test_dset_predicted.to_pandas()\n\n\ndf_test_predicted.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\ninput_ids\nattention_mask\npred_Rating\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\nperfect for work and play\nperfect for work and play . this shirt works f...\n[0, 20473, 13, 173, 8, 310, 479, 42, 6399, 136...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n5.025642\n[Tops, Intimate, Dresses]\n[0.9891802, 0.00918336, 0.0005342441]\n\n\n1\n\n. i don't know why i had the opposite problem ...\n[0, 4, 939, 218, 75, 216, 596, 939, 56, 5, 548...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n4.258958\n[Bottoms, Intimate, Trend]\n[0.9956339, 0.0024540052, 0.0011451989]\n\n\n2\ngreat pants\ngreat pants . thes e cords are great--lightwei...\n[0, 12338, 9304, 479, 5, 29, 364, 37687, 32, 3...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n5.143690\n[Bottoms, Intimate, Trend]\n[0.9899479, 0.008132041, 0.0010822185]\n\n\n3\nsurprisingly comfy for a button down\nsurprisingly comfy for a button down . i am a ...\n[0, 33258, 3137, 24382, 13, 10, 6148, 159, 479...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n4.399732\n[Tops, Intimate, Dresses]\n[0.9533946, 0.03952966, 0.004206666]\n\n\n4\nshort and small\nshort and small . the shirt is mostly a thick ...\n[0, 20263, 8, 650, 479, 5, 6399, 16, 2260, 10,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n2.788084\n[Tops, Intimate, Jackets]\n[0.91881263, 0.076821946, 0.0015537328]\n\n\n\n\n\n\n\n\n\nraw_content={'Review Text': 'This shirt is so comfortable I love it!',\n             'Title': 'Great shirt'}\nraw_content\n\n{'Review Text': 'This shirt is so comfortable I love it!',\n 'Title': 'Great shirt'}\n\n\n\ndf_result = controller.predict_raw_text(raw_content)\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_result\n\n{'Review Text': ['great shirt . this shirt is so comfortable i love it !'],\n 'Title': ['great shirt'],\n 'input_ids': [[0,\n   12338,\n   6399,\n   479,\n   42,\n   6399,\n   16,\n   98,\n   3473,\n   939,\n   657,\n   24,\n   27785,\n   2]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n 'pred_Rating': [5.002801418304443],\n 'pred_Department Name': ['Tops'],\n 'pred_prob_Department Name': [0.9914304614067078]}",
    "crumbs": [
      "2. All Use Cases",
      "a. Roberta-based models for Supervised Learning",
      "Roberta model (Regression)"
    ]
  },
  {
    "objectID": "models.gpt2.classifiers.html",
    "href": "models.gpt2.classifiers.html",
    "title": "GPT2 Classifiers",
    "section": "",
    "text": "source\n\n\n\n GPT2BaseForSequenceClassification (config, is_multilabel=False,\n                                    is_multihead=False,\n                                    head_class_sizes=[], head_weights=[],\n                                    head_class=None, **head_class_kwargs)\n\nGPT2 Architecture for Sequence Classification task Based on: https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L1376\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nconfig\n\n\nHuggingFace model configuration\n\n\nis_multilabel\nbool\nFalse\nWhether this is a multilabel classification\n\n\nis_multihead\nbool\nFalse\nWhether this is a multihead (multi-level) classification\n\n\nhead_class_sizes\nlist\n[]\nClass size for each head\n\n\nhead_weights\nlist\n[]\nloss weight for each head. This will be multiplied to the loss of each head’s output\n\n\nhead_class\nNoneType\nNone\nThe class object of the head.\n\n\nhead_class_kwargs\n\n\n\n\n\n\n\nsource\n\n\n\n\n GPT2HiddenStateConcatForSequenceClassification (config, layer2concat=4,\n                                                 is_multilabel=False,\n                                                 is_multihead=False,\n                                                 head_class_sizes=[],\n                                                 head_weights=[],\n                                                 head_class=None,\n                                                 **head_class_kwargs)\n\nGPT2 Architecture for Sequence Classification task Based on: https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L1376\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nconfig\n\n\nHuggingFace model configuration\n\n\nlayer2concat\nint\n4\nnumber of hidden layer to concatenate (counting from top)\n\n\nis_multilabel\nbool\nFalse\nWhether this is a multilabel classification\n\n\nis_multihead\nbool\nFalse\nWhether this is a multihead (multi-level) classification\n\n\nhead_class_sizes\nlist\n[]\nClass size for each head\n\n\nhead_weights\nlist\n[]\nloss weight for each head. This will be multiplied to the loss of each head’s output\n\n\nhead_class\nNoneType\nNone\nThe class object of the head. You can use ConcatHeadSimple or ConcatHeadExtended\n\n\nhead_class_kwargs",
    "crumbs": [
      "6. Model Classes",
      "d. GPT2-based classification Model",
      "GPT2 Classifiers"
    ]
  },
  {
    "objectID": "models.gpt2.classifiers.html#main-classification-architecture",
    "href": "models.gpt2.classifiers.html#main-classification-architecture",
    "title": "GPT2 Classifiers",
    "section": "",
    "text": "source\n\n\n\n GPT2BaseForSequenceClassification (config, is_multilabel=False,\n                                    is_multihead=False,\n                                    head_class_sizes=[], head_weights=[],\n                                    head_class=None, **head_class_kwargs)\n\nGPT2 Architecture for Sequence Classification task Based on: https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L1376\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nconfig\n\n\nHuggingFace model configuration\n\n\nis_multilabel\nbool\nFalse\nWhether this is a multilabel classification\n\n\nis_multihead\nbool\nFalse\nWhether this is a multihead (multi-level) classification\n\n\nhead_class_sizes\nlist\n[]\nClass size for each head\n\n\nhead_weights\nlist\n[]\nloss weight for each head. This will be multiplied to the loss of each head’s output\n\n\nhead_class\nNoneType\nNone\nThe class object of the head.\n\n\nhead_class_kwargs\n\n\n\n\n\n\n\nsource\n\n\n\n\n GPT2HiddenStateConcatForSequenceClassification (config, layer2concat=4,\n                                                 is_multilabel=False,\n                                                 is_multihead=False,\n                                                 head_class_sizes=[],\n                                                 head_weights=[],\n                                                 head_class=None,\n                                                 **head_class_kwargs)\n\nGPT2 Architecture for Sequence Classification task Based on: https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L1376\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nconfig\n\n\nHuggingFace model configuration\n\n\nlayer2concat\nint\n4\nnumber of hidden layer to concatenate (counting from top)\n\n\nis_multilabel\nbool\nFalse\nWhether this is a multilabel classification\n\n\nis_multihead\nbool\nFalse\nWhether this is a multihead (multi-level) classification\n\n\nhead_class_sizes\nlist\n[]\nClass size for each head\n\n\nhead_weights\nlist\n[]\nloss weight for each head. This will be multiplied to the loss of each head’s output\n\n\nhead_class\nNoneType\nNone\nThe class object of the head. You can use ConcatHeadSimple or ConcatHeadExtended\n\n\nhead_class_kwargs",
    "crumbs": [
      "6. Model Classes",
      "d. GPT2-based classification Model",
      "GPT2 Classifiers"
    ]
  },
  {
    "objectID": "roberta_conditional_prob.html",
    "href": "roberta_conditional_prob.html",
    "title": "Roberta model with Conditional Probability",
    "section": "",
    "text": "In this tutorial, we walk through a special case of classification with multiple heads. This is inspired by this paper: https://arxiv.org/pdf/1911.06475.pdf\nimport os\n#This will specify a (or a list) of GPUs for training\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom that_nlp_library.text_main import *\nfrom that_nlp_library.utils import seed_everything\nfrom underthesea import text_normalize\nfrom functools import partial\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport nlpaug.augmenter.char as nac\nfrom datasets import load_dataset\nimport random\nfrom transformers import RobertaTokenizer\nfrom datasets import Dataset",
    "crumbs": [
      "2. All Use Cases",
      "a. Roberta-based models for Supervised Learning",
      "Roberta model with Conditional Probability"
    ]
  },
  {
    "objectID": "roberta_conditional_prob.html#build-conditional-mask",
    "href": "roberta_conditional_prob.html#build-conditional-mask",
    "title": "Roberta model with Conditional Probability",
    "section": "Build Conditional Mask",
    "text": "Build Conditional Mask\n\ntdc.label_names\n\n['Division Name', 'Department Name']\n\n\n\ntdc.label_lists\n\n[['General', 'General Petite', 'Initmates'],\n ['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']]\n\n\n\ndf_trn = tdc.main_ddict['train'].to_pandas()\n\n\ndf_labels = pd.DataFrame(df_trn['label'].tolist())\ndf_labels.columns=tdc.label_names\n\n\ndf_labels.head()\n\n\n\n\n\n\n\n\n\nDivision Name\nDepartment Name\n\n\n\n\n0\n0\n4\n\n\n1\n1\n1\n\n\n2\n1\n1\n\n\n3\n1\n3\n\n\n4\n0\n1\n\n\n\n\n\n\n\n\n\nstandard_mask = build_standard_condition_mask(df_labels,*tdc.label_names)\n\n\nstandard_mask\n\ntensor([[ True, False, False,  True,  True, False,  True,  True,  True],\n        [False,  True, False,  True,  True,  True,  True,  True,  True],\n        [False, False,  True, False, False,  True, False, False, False]])\n\n\nExplain the first row of the mask\n\nstandard_mask[0]\n\ntensor([ True, False, False,  True,  True, False,  True,  True,  True])\n\n\nSlicing the first portion for Division Name (the first 3 values), show string for True mask\n\nfor i in torch.where(standard_mask[0][:len(tdc.label_lists[0])]==True)[0]:\n    print(tdc.label_lists[0][i])\n\nGeneral\n\n\nSlicing the first portion for Department Name, show string for True mask. The results are the sub-category of Division Name\n\nfor i in torch.where(standard_mask[0][len(tdc.label_lists[0]):]==True)[0]:\n    print(tdc.label_lists[1][i])\n\nBottoms\nDresses\nJackets\nTops\nTrend\n\n\n\n# let's double check with the original data\nnp.sort(df_trn[df_trn['Division Name']=='General']['Department Name'].unique())\n\narray(['Bottoms', 'Dresses', 'Jackets', 'Tops', 'Trend'], dtype=object)",
    "crumbs": [
      "2. All Use Cases",
      "a. Roberta-based models for Supervised Learning",
      "Roberta model with Conditional Probability"
    ]
  },
  {
    "objectID": "roberta_conditional_prob.html#define-and-train-a-custom-roberta-model",
    "href": "roberta_conditional_prob.html#define-and-train-a-custom-roberta-model",
    "title": "Roberta model with Conditional Probability",
    "section": "Define and train a custom Roberta model",
    "text": "Define and train a custom Roberta model\n\nroberta_body = RobertaModel.from_pretrained('roberta-base')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n_model_kwargs={\n    # overall model hyperparams\n    'size_l1':len(tdc.label_lists[0]),\n    'size_l2':len(tdc.label_lists[1]),\n    'standard_mask':standard_mask,\n    'layer2concat':2,\n    'head_class': ConcatHeadSimple,\n    # classfication head hyperparams\n    'classifier_dropout':0.1 \n}\n\nmodel = model_init_classification(model_class = RobertaHSCCProbSequenceClassification,\n                                  cpoint_path = 'roberta-base', \n                                  output_hidden_states=True,\n                                  seed=42,\n                                  body_model=roberta_body,\n                                  model_kwargs = _model_kwargs)\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score]\ncontroller = ModelController(model,tdc,seed=42)\n\nLoading body weights. This assumes the body is the very first block of your custom architecture\nTotal parameters: 124659465\nTotal trainable parameters: 124659465\n\n\nAnd we can start training our model\n\nseed_everything(42)\n\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 3\n\ncontroller.fit(epochs,lr,\n               metric_funcs=metric_funcs,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics,\n              )\n\n\n\n    \n      \n      \n      [849/849 05:25, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1 Score Division name\nAccuracy Score Division name\nF1 Score Department name\nAccuracy Score Department name\n\n\n\n\n1\nNo log\n0.109697\n0.419514\n0.615113\n0.650357\n0.868979\n\n\n2\n0.141400\n0.096176\n0.451226\n0.613566\n0.682490\n0.881131\n\n\n3\n0.141400\n0.094754\n0.447835\n0.614229\n0.682274\n0.883120\n\n\n\n\n\n\n\n\ncontroller.trainer.model.save_pretrained('./sample_weights/my_model')",
    "crumbs": [
      "2. All Use Cases",
      "a. Roberta-based models for Supervised Learning",
      "Roberta model with Conditional Probability"
    ]
  },
  {
    "objectID": "roberta_conditional_prob.html#make-predictions",
    "href": "roberta_conditional_prob.html#make-predictions",
    "title": "Roberta model with Conditional Probability",
    "section": "Make predictions",
    "text": "Make predictions\n\nLoad trained model\n\n_model_kwargs\n\n{'size_l1': 3,\n 'size_l2': 6,\n 'standard_mask': tensor([[ True, False, False,  True,  True, False,  True,  True,  True],\n         [False,  True, False,  True,  True,  True,  True,  True,  True],\n         [False, False,  True, False, False,  True, False, False, False]]),\n 'layer2concat': 2,\n 'head_class': that_nlp_library.models.roberta.classifiers.ConcatHeadSimple,\n 'classifier_dropout': 0.1}\n\n\n\ntrained_model = model_init_classification(model_class = RobertaHSCCProbSequenceClassification,\n                                          cpoint_path = Path('./sample_weights/my_model'), \n                                          output_hidden_states=True,\n                                          seed=42,\n                                          model_kwargs = _model_kwargs)\n\ncontroller = ModelController(trained_model,tdc,seed=42)\n\nSome weights of the model checkpoint at sample_weights/my_model were not used when initializing RobertaHSCCProbSequenceClassification: ['body_model.pooler.dense.bias', 'body_model.pooler.dense.weight']\n- This IS expected if you are initializing RobertaHSCCProbSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaHSCCProbSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\nTotal parameters: 124068873\nTotal trainable parameters: 124068873\n\n\n\n\nPredict Train/Validation set\n\ndf_val = controller.predict_ddict(ds_type='validation')\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_val = df_val.to_pandas()\ndf_val.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nDivision Name\nDepartment Name\nlabel\ninput_ids\nattention_mask\npred_Division Name\npred_prob_Division Name\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\n\n. such a fun jacket ! great to wear in the spr...\nGeneral Petite\nIntimate\n[1, 2]\n[0, 4, 215, 10, 1531, 8443, 27785, 372, 7, 356...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.591014\nJackets\n0.898804\n\n\n1\nsimple and elegant\nsimple and elegant . i thought this shirt was ...\nGeneral Petite\nTops\n[1, 4]\n[0, 41918, 8, 14878, 479, 939, 802, 42, 6399, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.533907\nTops\n0.999752\n\n\n2\nretro and pretty\nretro and pretty . this top has a bit of a ret...\nGeneral\nTops\n[0, 4]\n[0, 4903, 1001, 8, 1256, 479, 42, 299, 34, 10,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.564118\nTops\n0.999757\n\n\n3\nsummer/fall wear\nsummer / fall wear . i first spotted this on a...\nGeneral Petite\nDresses\n[1, 1]\n[0, 18581, 2089, 1589, 1136, 3568, 479, 939, 7...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.520808\nDresses\n0.999089\n\n\n4\nperfect except slip\nperfect except slip . this is my new favorite ...\nGeneral Petite\nDresses\n[1, 1]\n[0, 20473, 4682, 9215, 479, 42, 16, 127, 92, 2...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.559546\nDresses\n0.999006\n\n\n\n\n\n\n\n\nYou can try to get your metric to see if it matches your last traing epoch’s above\n\nf1_score(df_val['Division Name'],df_val['pred_Division Name'],average='macro')\n# 0.4486166839108015\n\n0.4479421567807432\n\n\n\nf1_score(df_val['Department Name'],df_val['pred_Department Name'],average='macro')\n# 0.6818255247330748\n\n0.6822742871946978\n\n\n\n\nPredict Test set\nWe will go through details on how to make a prediction on a completely new and raw dataset using our trained model. For now, let’s reuse the sample csv and pretend it’s our test set\n\ndf_test = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig').sample(frac=0.2,random_state=1)\n# drop NaN values in the label column\ndf_test = df_test[~df_test['Department Name'].isna()].reset_index(drop=True)\n\n# save the label, as we will calculate some metrics later. We also filter out labels with NaN Review Text,\n# as there will be a filtering processing on the test set\ntrue_labels = df_test.loc[~df_test['Review Text'].isna(),'Department Name'].values \n\n# drop the label (you don't need to, but this is necessary to simulate an actual test set)\ndf_test.drop(['Division Name','Department Name'],axis=1,inplace=True)\n\n\n_test_dset = Dataset.from_pandas(df_test)\n_test_dset_predicted = controller.predict_raw_dset(_test_dset,\n                                                   do_filtering=True, # since we have some text filtering in the processing\n                                                  )\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_test_predicted = _test_dset_predicted.to_pandas()\n\n\ndf_test_predicted.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\ninput_ids\nattention_mask\npred_Division Name\npred_prob_Division Name\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\nperfect for work and play\nperfect for work and play . this shirt works f...\n[0, 20473, 13, 173, 8, 310, 479, 42, 6399, 136...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.549556\nTops\n0.999717\n\n\n1\n\n. i don't know why i had the opposite problem ...\n[0, 4, 939, 218, 75, 216, 596, 939, 56, 5, 548...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.668210\nBottoms\n0.999585\n\n\n2\ngreat pants\ngreat pants . thes e cords are great--lightwei...\n[0, 12338, 9304, 479, 5, 29, 364, 37687, 32, 3...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.629028\nBottoms\n0.999531\n\n\n3\nsurprisingly comfy for a button down\nsurprisingly comfy for a button down . i am a ...\n[0, 33258, 3137, 24382, 13, 10, 6148, 159, 479...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.530994\nTops\n0.996693\n\n\n4\nshort and small\nshort and small . the shirt is mostly a thick ...\n[0, 20263, 8, 650, 479, 5, 6399, 16, 2260, 10,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.499403\nTops\n0.997771\n\n\n\n\n\n\n\n\nLet’s quickly check the f1 score to make sure everything works correctly\n\nf1_score(true_labels,df_test_predicted['pred_Department Name'],average='macro')\n\n0.7058640842784157\n\n\nPredict top k results\n\n_test_dset = Dataset.from_pandas(df_test)\n_test_dset_predicted = controller.predict_raw_dset(_test_dset,\n                                                   do_filtering=True,\n                                                   topk=3\n                                                  )\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_test_predicted = _test_dset_predicted.to_pandas()\n\ndf_test_predicted.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\ninput_ids\nattention_mask\npred_Division Name\npred_prob_Division Name\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\nperfect for work and play\nperfect for work and play . this shirt works f...\n[0, 20473, 13, 173, 8, 310, 479, 42, 6399, 136...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[General, General Petite, Initmates]\n[0.5495558, 0.38857713, 0.061867107]\n[Tops, Intimate, Trend]\n[0.9997173, 0.00027123763, 5.448324e-06]\n\n\n1\n\n. i don't know why i had the opposite problem ...\n[0, 4, 939, 218, 75, 216, 596, 939, 56, 5, 548...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[General, General Petite, Initmates]\n[0.6682097, 0.26135367, 0.0704367]\n[Bottoms, Intimate, Trend]\n[0.99958473, 0.00031657313, 6.8090965e-05]\n\n\n2\ngreat pants\ngreat pants . thes e cords are great--lightwei...\n[0, 12338, 9304, 479, 5, 29, 364, 37687, 32, 3...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[General, General Petite, Initmates]\n[0.62902796, 0.29412356, 0.07684846]\n[Bottoms, Intimate, Trend]\n[0.9995307, 0.00037790896, 5.9614045e-05]\n\n\n3\nsurprisingly comfy for a button down\nsurprisingly comfy for a button down . i am a ...\n[0, 33258, 3137, 24382, 13, 10, 6148, 159, 479...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[General, General Petite, Initmates]\n[0.530994, 0.384637, 0.084368974]\n[Tops, Intimate, Dresses]\n[0.99669266, 0.003254413, 2.1810652e-05]\n\n\n4\nshort and small\nshort and small . the shirt is mostly a thick ...\n[0, 20263, 8, 650, 479, 5, 6399, 16, 2260, 10,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[General, General Petite, Initmates]\n[0.49940288, 0.3735293, 0.12706791]\n[Tops, Intimate, Trend]\n[0.9977709, 0.0022051241, 1.0216948e-05]\n\n\n\n\n\n\n\n\n\n# Since we have some metadatas (Title and Division Name), we need to define a dictionary containing those values\nraw_content={'Review Text': 'This shirt is so comfortable I love it!',\n             'Title': 'Great shirt'}\n\n\ncontroller.data_store.num_proc=1\n\n\ndf_result = controller.predict_raw_text(raw_content,topk=3)\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_result\n\n{'Review Text': ['great shirt . this shirt is so comfortable i love it !'],\n 'Title': ['great shirt'],\n 'input_ids': [[0,\n   12338,\n   6399,\n   479,\n   42,\n   6399,\n   16,\n   98,\n   3473,\n   939,\n   657,\n   24,\n   27785,\n   2]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n 'pred_Division Name': [['General', 'General Petite', 'Initmates']],\n 'pred_prob_Division Name': [[0.564812958240509,\n   0.3748474419116974,\n   0.06033959984779358]],\n 'pred_Department Name': [['Tops', 'Intimate', 'Trend']],\n 'pred_prob_Department Name': [[0.9997146725654602,\n   0.00027399769169278443,\n   5.55193309992319e-06]]}",
    "crumbs": [
      "2. All Use Cases",
      "a. Roberta-based models for Supervised Learning",
      "Roberta model with Conditional Probability"
    ]
  },
  {
    "objectID": "gpt2_singlehead.html",
    "href": "gpt2_singlehead.html",
    "title": "GPT2 model (Custom Single Head)",
    "section": "",
    "text": "In this series, we walk through some of the capability of this library: single-head classification, multi-head classification, multi-label classification, and regression. If you want a more detailed tutorial, check this out\nimport os\n#This will specify a (or a list) of GPUs for training\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom that_nlp_library.text_main import *\nfrom that_nlp_library.utils import seed_everything\nfrom underthesea import text_normalize\nfrom functools import partial\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport nlpaug.augmenter.char as nac\nfrom datasets import load_dataset\nimport random\nfrom transformers import AutoTokenizer\nfrom datasets import Dataset",
    "crumbs": [
      "2. All Use Cases",
      "b. GPT2-based models for Supervised Learning",
      "GPT2 model (Custom Single Head)"
    ]
  },
  {
    "objectID": "gpt2_singlehead.html#define-and-train-a-vanilla-gpt2-model",
    "href": "gpt2_singlehead.html#define-and-train-a-vanilla-gpt2-model",
    "title": "GPT2 model (Custom Single Head)",
    "section": "Define and train a vanilla GPT2 model",
    "text": "Define and train a vanilla GPT2 model\n\nfrom transformers.models.gpt2.modeling_gpt2 import GPT2Model\n\n\nfrom that_nlp_library.models.roberta.classifiers import ConcatHeadSimple\nfrom that_nlp_library.model_main import *\nfrom that_nlp_library.models.gpt2.classifiers import *\nfrom sklearn.metrics import f1_score, accuracy_score\n\n\nUsing HuggingFace model initialization\n\nfrom transformers.models.gpt2.modeling_gpt2 import GPT2ForSequenceClassification\n\n\nnum_classes = len(tdc.label_lists[0])\nnum_classes\n\n6\n\n\n\nseed_everything(42)\nmodel = GPT2ForSequenceClassification.from_pretrained('gpt2',num_labels=num_classes)\nmodel = model.to('cuda:0')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\nmodel.config.pad_token_id = model.config.eos_token_id\n\n\nmodel.resize_token_embeddings(len(_tokenizer))\n\nEmbedding(50257, 768)\n\n\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score]\ncontroller = ModelController(model,tdc,seed=42)\n\nAnd we can start training our model\n\nlr = 8e-5\nbs=32\nwd=0.01\nepochs= 3\n\ncontroller.fit(epochs,lr,\n               metric_funcs=metric_funcs,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics,\n              )\n\n\n\n    \n      \n      \n      [849/849 02:55, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1 Score Department name\nAccuracy Score Department name\n\n\n\n\n1\nNo log\n0.283675\n0.739092\n0.910075\n\n\n2\n0.656600\n0.261791\n0.749196\n0.920901\n\n\n3\n0.656600\n0.263783\n0.751478\n0.922448\n\n\n\n\n\n\n\n\n\nUsing the GPT2Base model (designed for not only single-head but multi-head, multi-label …)\n\ngpt2body = GPT2Model.from_pretrained('gpt2')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\n# our model is more complex, so it's best to define some of its arguments\n_model_kwargs={\n    # overall model hyperparams\n    'head_class_sizes':num_classes,\n    # classfication head hyperparams\n    'classifier_dropout':0.1 \n}\n\nmodel = model_init_classification(model_class = GPT2BaseForSequenceClassification,\n                                  cpoint_path = 'gpt2', \n                                  output_hidden_states=False, # since we are not using 'hidden layer contatenation' technique\n                                  seed=42,\n                                  body_model=gpt2body,\n                                  model_kwargs = _model_kwargs)\n\nLoading body weights. This assumes the body is the very first block of your custom architecture\nTotal parameters: 124444416\nTotal trainable parameters: 124444416\n\n\n\n# resize token embedding\nmodel.body_model.resize_token_embeddings(len(_tokenizer))\n\nEmbedding(50257, 768)\n\n\nCreate ModelController and start training\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score]\ncontroller = ModelController(model,tdc,seed=42)\n\nAnd we can start training our model\n\nlr = 8e-5\nbs=32\nwd=0.01\nepochs= 3\n\ncontroller.fit(epochs,lr,\n               metric_funcs=metric_funcs,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics,\n              )\n\n\n\n    \n      \n      \n      [849/849 03:08, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1 Score Department name\nAccuracy Score Department name\n\n\n\n\n1\nNo log\n0.293438\n0.736128\n0.910296\n\n\n2\n0.743200\n0.263558\n0.748740\n0.918692\n\n\n3\n0.743200\n0.264788\n0.746244\n0.917587",
    "crumbs": [
      "2. All Use Cases",
      "b. GPT2-based models for Supervised Learning",
      "GPT2 model (Custom Single Head)"
    ]
  },
  {
    "objectID": "gpt2_singlehead.html#make-predictions",
    "href": "gpt2_singlehead.html#make-predictions",
    "title": "GPT2 model (Custom Single Head)",
    "section": "Make predictions",
    "text": "Make predictions\n\ndf_val = controller.predict_ddict(ds_type='validation')\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_val = df_val.to_pandas()\ndf_val.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nDivision Name\nDepartment Name\nlabel\ninput_ids\nattention_mask\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\n\ngeneral petite . . such a fun jacket ! great t...\ngeneral petite\nIntimate\n2\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\nJackets\n0.879402\n\n\n1\nsimple and elegant\ngeneral petite . simple and elegant . i though...\ngeneral petite\nTops\n4\n[24622, 4273, 578, 764, 2829, 290, 19992, 764,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.998374\n\n\n2\nretro and pretty\ngeneral . retro and pretty . this top has a bi...\ngeneral\nTops\n4\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\nTops\n0.999834\n\n\n3\nsummer/fall wear\ngeneral petite . summer / fall wear . i first ...\ngeneral petite\nDresses\n1\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, ...\nDresses\n0.949195\n\n\n4\nperfect except slip\ngeneral petite . perfect except slip . this is...\ngeneral petite\nDresses\n1\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\nDresses\n0.993209\n\n\n\n\n\n\n\n\nYou can try to get your metric to see if it matches your last traing epoch’s above\n\nf1_score(df_val['Department Name'],df_val['pred_Department Name'],average='macro')\n\n0.7462441580902758",
    "crumbs": [
      "2. All Use Cases",
      "b. GPT2-based models for Supervised Learning",
      "GPT2 model (Custom Single Head)"
    ]
  },
  {
    "objectID": "gpt2_singlehead.html#define-and-train-a-custom-gpt2-model",
    "href": "gpt2_singlehead.html#define-and-train-a-custom-gpt2-model",
    "title": "GPT2 model (Custom Single Head)",
    "section": "Define and train a custom GPT2 model",
    "text": "Define and train a custom GPT2 model\n\nfrom transformers.models.gpt2.modeling_gpt2 import GPT2Model\n\n\nfrom that_nlp_library.models.roberta.classifiers import ConcatHeadSimple\nfrom that_nlp_library.model_main import *\nfrom that_nlp_library.models.gpt2.classifiers import *\nfrom sklearn.metrics import f1_score, accuracy_score\n\n\nnum_classes = len(tdc.label_lists[0])\nnum_classes\n\n6\n\n\n\ngpt2body = GPT2Model.from_pretrained('gpt2')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\nThen we can define a classification head. One trick we can use to boost the performance of our entire model is to concatenate the outputs of the last tokens from the four last layers of the pre-trained Roberta model (an improvised approach from this source: https://ieeexplore.ieee.org/document/9335912). We already define such custom head (ConcatHeadSimple), and the necessary architecture to make it work (GPT2HiddenStateConcatForSequenceClassification)\n\n# our model is more complex, so it's best to define some of its arguments\n_model_kwargs={\n    # overall model hyperparams\n    'head_class_sizes':num_classes,\n    'head_class': ConcatHeadSimple,\n    # classfication head hyperparams\n    'layer2concat':2, # you can change the number of layers to concat (default is 4, based on the paper)\n    'classifier_dropout':0.1 \n}\n\nmodel = model_init_classification(model_class = GPT2HiddenStateConcatForSequenceClassification,\n                                  cpoint_path = 'gpt2', \n                                  output_hidden_states=True, # since we are using 'hidden layer contatenation' technique\n                                  seed=42,\n                                  body_model=gpt2body,\n                                  model_kwargs = _model_kwargs)\n\nLoading body weights. This assumes the body is the very first block of your custom architecture\nTotal parameters: 124449030\nTotal trainable parameters: 124449030\n\n\n\n# resize token embedding\nmodel.body_model.resize_token_embeddings(len(_tokenizer))\n\nEmbedding(50257, 768)\n\n\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score]\ncontroller = ModelController(model,tdc,seed=42)\n\nAnd we can start training our model\n\nseed_everything(42)\n\n\nlr = 8e-5\nbs=32\nwd=0.01\nepochs= 3\n\ncontroller.fit(epochs,lr,\n               metric_funcs=metric_funcs,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics,\n              )\n\n# Epoch Training Loss   Validation Loss F1 Score Department name    Accuracy Score Department name\n# 1 No log  0.301476    0.746599    0.914494\n# 2 0.400300    0.263080    0.749670    0.920901\n\n\n\n    \n      \n      \n      [849/849 03:15, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1 Score Department name\nAccuracy Score Department name\n\n\n\n\n1\nNo log\n0.316337\n0.733385\n0.907866\n\n\n2\n0.719600\n0.271895\n0.746649\n0.915820\n\n\n3\n0.719600\n0.269325\n0.745756\n0.916924\n\n\n\n\n\n\n\n\ncontroller.trainer.model.save_pretrained('./sample_weights/my_model')",
    "crumbs": [
      "2. All Use Cases",
      "b. GPT2-based models for Supervised Learning",
      "GPT2 model (Custom Single Head)"
    ]
  },
  {
    "objectID": "gpt2_singlehead.html#make-predictions-1",
    "href": "gpt2_singlehead.html#make-predictions-1",
    "title": "GPT2 model (Custom Single Head)",
    "section": "Make predictions",
    "text": "Make predictions\n\nLoad trained model\n\n_model_kwargs\n\n{'head_class_sizes': 6,\n 'head_class': that_nlp_library.models.roberta.classifiers.ConcatHeadSimple,\n 'layer2concat': 2,\n 'classifier_dropout': 0.1}\n\n\n\ntrained_model = model_init_classification(model_class = GPT2HiddenStateConcatForSequenceClassification,\n                                          cpoint_path = Path('./sample_weights/my_model'), \n                                          output_hidden_states=True,\n                                          seed=42,\n                                          model_kwargs = _model_kwargs)\n\ncontroller = ModelController(trained_model,tdc,seed=42)\n\nTotal parameters: 124449030\nTotal trainable parameters: 124449030\n\n\n\n\nPredict Train/Validation set\n\ndf_val = controller.predict_ddict(ds_type='validation')\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_val = df_val.to_pandas()\ndf_val.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nDivision Name\nDepartment Name\nlabel\ninput_ids\nattention_mask\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\n\ngeneral petite . . such a fun jacket ! great t...\ngeneral petite\nIntimate\n2\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\nJackets\n0.937680\n\n\n1\nsimple and elegant\ngeneral petite . simple and elegant . i though...\ngeneral petite\nTops\n4\n[24622, 4273, 578, 764, 2829, 290, 19992, 764,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.997647\n\n\n2\nretro and pretty\ngeneral . retro and pretty . this top has a bi...\ngeneral\nTops\n4\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\nTops\n0.998581\n\n\n3\nsummer/fall wear\ngeneral petite . summer / fall wear . i first ...\ngeneral petite\nDresses\n1\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, ...\nDresses\n0.973116\n\n\n4\nperfect except slip\ngeneral petite . perfect except slip . this is...\ngeneral petite\nDresses\n1\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\nDresses\n0.996431\n\n\n\n\n\n\n\n\nYou can try to get your metric to see if it matches your last traing epoch’s above\n\nf1_score(df_val['Department Name'],df_val['pred_Department Name'],average='macro')\n\n0.746104914178913\n\n\n\n\nPredict Test set\nWe will go through details on how to make a prediction on a completely new and raw dataset using our trained model. For now, let’s reuse the sample csv and pretend it’s our test set\n\ndf_test = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig').sample(frac=0.2,random_state=1)\n# drop NaN values in the label column\ndf_test = df_test[~df_test['Department Name'].isna()].reset_index(drop=True)\n\n# save the label, as we will calculate some metrics later. We also filter out labels with NaN Review Text,\n# as there will be a filtering processing on the test set\ntrue_labels = df_test.loc[~df_test['Review Text'].isna(),'Department Name'].values \n\n# drop the label (you don't need to, but this is necessary to simulate an actual test set)\ndf_test.drop('Department Name',axis=1,inplace=True)\n\n\n_test_dset = Dataset.from_pandas(df_test)\n_test_dset_predicted = controller.predict_raw_dset(_test_dset,\n                                                   do_filtering=True, # since we have some text filtering in the processing\n                                                  )\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_test_predicted = _test_dset_predicted.to_pandas()\n\n\ndf_test_predicted.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nDivision Name\ninput_ids\nattention_mask\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\nperfect for work and play\ngeneral . perfect for work and play . this shi...\ngeneral\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\nTops\n0.999322\n\n\n1\n\ngeneral petite . . i don't know why i had the ...\ngeneral petite\n[24622, 4273, 578, 764, 764, 1312, 836, 470, 7...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nBottoms\n0.988174\n\n\n2\ngreat pants\ngeneral petite . great pants . thes e cords ar...\ngeneral petite\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\nBottoms\n0.995248\n\n\n3\nsurprisingly comfy for a button down\ngeneral petite . surprisingly comfy for a butt...\ngeneral petite\n[24622, 4273, 578, 764, 12362, 401, 24928, 329...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.914309\n\n\n4\nshort and small\ngeneral petite . short and small . the shirt i...\ngeneral petite\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\nTops\n0.997992\n\n\n\n\n\n\n\n\nLet’s quickly check the f1 score to make sure everything works correctly\n\nf1_score(true_labels,df_test_predicted['pred_Department Name'],average='macro')\n\n0.757361738460204\n\n\nPredict top k results\n\n_test_dset = Dataset.from_pandas(df_test)\n_test_dset_predicted = controller.predict_raw_dset(_test_dset,\n                                                   do_filtering=True,\n                                                   topk=3\n                                                  )\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_test_predicted = _test_dset_predicted.to_pandas()\n\ndf_test_predicted.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nDivision Name\ninput_ids\nattention_mask\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\nperfect for work and play\ngeneral . perfect for work and play . this shi...\ngeneral\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n[Tops, Dresses, Jackets]\n[0.9993216, 0.00019925594, 0.00019223447]\n\n\n1\n\ngeneral petite . . i don't know why i had the ...\ngeneral petite\n[24622, 4273, 578, 764, 764, 1312, 836, 470, 7...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[Bottoms, Intimate, Jackets]\n[0.98817396, 0.0063540265, 0.004216876]\n\n\n2\ngreat pants\ngeneral petite . great pants . thes e cords ar...\ngeneral petite\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n[Bottoms, Intimate, Trend]\n[0.995248, 0.004601938, 0.00012541868]\n\n\n3\nsurprisingly comfy for a button down\ngeneral petite . surprisingly comfy for a butt...\ngeneral petite\n[24622, 4273, 578, 764, 12362, 401, 24928, 329...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[Tops, Dresses, Bottoms]\n[0.914309, 0.06820013, 0.00940009]\n\n\n4\nshort and small\ngeneral petite . short and small . the shirt i...\ngeneral petite\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n[Tops, Intimate, Jackets]\n[0.9979918, 0.00080933404, 0.0005731517]\n\n\n\n\n\n\n\n\n\n# Since we have some metadatas (Title and Division Name), we need to define a dictionary containing those values\nraw_content={'Review Text': 'This shirt is so comfortable I love it!',\n             'Title': 'Great shirt',\n             'Division Name': 'general'}\n\n\ncontroller.data_store.num_proc=1\n\n\ndf_result = controller.predict_raw_text(raw_content,topk=3)\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_result\n\n{'Review Text': ['general . great shirt . this shirt is so comfortable i love it !'],\n 'Title': ['great shirt'],\n 'Division Name': ['general'],\n 'input_ids': [[24622,\n   764,\n   1049,\n   10147,\n   764,\n   428,\n   10147,\n   318,\n   523,\n   6792,\n   1312,\n   1842,\n   340,\n   5145]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n 'pred_Department Name': [['Tops', 'Trend', 'Dresses']],\n 'pred_prob_Department Name': [[0.9987654685974121,\n   0.0003842698351945728,\n   0.0003074762353207916]]}",
    "crumbs": [
      "2. All Use Cases",
      "b. GPT2-based models for Supervised Learning",
      "GPT2 model (Custom Single Head)"
    ]
  },
  {
    "objectID": "model_lm_roberta_tutorial.html",
    "href": "model_lm_roberta_tutorial.html",
    "title": "Model Controller Tutorial: Training a Roberta Language Model",
    "section": "",
    "text": "import os\n#This will specify a (or a list) of GPUs for training\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom that_nlp_library.text_main_lm import *\nfrom that_nlp_library.utils import seed_everything\nfrom that_nlp_library.model_lm_main import *\nfrom underthesea import text_normalize\nfrom functools import partial\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForMaskedLM\nfrom datasets import load_dataset\nimport pandas as pd\nimport numpy as np\nfrom transformers import DataCollatorForLanguageModeling",
    "crumbs": [
      "1. Quick Starts",
      "Model Controller Tutorial: Training a Roberta Language Model"
    ]
  },
  {
    "objectID": "model_lm_roberta_tutorial.html#a-create-a-textdatalmcontroller-object",
    "href": "model_lm_roberta_tutorial.html#a-create-a-textdatalmcontroller-object",
    "title": "Model Controller Tutorial: Training a Roberta Language Model",
    "section": "a) Create a TextDataLMController object",
    "text": "a) Create a TextDataLMController object\nWe will reuse the data and the preprocessings in this tutorial\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataLMController(dset,\n                         main_text='Review Text',\n                         filter_dict={'Review Text': lambda x: x is not None},\n                         metadatas='Title',\n                         content_transformations=[text_normalize,str.lower],\n                         cols_to_keep=['Clothing ID','Review Text'],\n                         seed=42,\n                         verbose=False\n                        )\n\nDefine our tokenizer for Roberta\n\n_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\nProcess and tokenize our dataset (using line-by-line tokenization)\n\nblock_size=112\ntdc.process_and_tokenize(_tokenizer,line_by_line=True,max_length=block_size) \n# set max_length=-1 if you want the data collator (instead of the tokenizer) to pad\n\n\ntdc.main_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Clothing ID', 'Review Text', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n        num_rows: 18112\n    })\n    validation: Dataset({\n        features: ['Clothing ID', 'Review Text', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n        num_rows: 4529\n    })\n})\n\n\nAnd set the data collator\n\ntdc.set_data_collator(is_mlm=True,mlm_prob=0.15)",
    "crumbs": [
      "1. Quick Starts",
      "Model Controller Tutorial: Training a Roberta Language Model"
    ]
  },
  {
    "objectID": "model_lm_roberta_tutorial.html#b-initialize-and-train-roberta-language-model-from-scratch",
    "href": "model_lm_roberta_tutorial.html#b-initialize-and-train-roberta-language-model-from-scratch",
    "title": "Model Controller Tutorial: Training a Roberta Language Model",
    "section": "b) Initialize and train Roberta Language Model from scratch",
    "text": "b) Initialize and train Roberta Language Model from scratch\n\n_config = AutoConfig.from_pretrained('roberta-base',\n                                     # just in case...\n                                     vocab_size=len(_tokenizer),\n                                     bos_token_id=_tokenizer.bos_token_id,\n                                     eos_token_id=_tokenizer.eos_token_id,\n                                     )\n_config\n\nRobertaConfig {\n  \"_name_or_path\": \"roberta-base\",\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.40.1\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n\n\n# _config = AutoConfig.from_pretrained('roberta-base',\n#                                      # just in case...\n#                                      vocab_size=len(_tokenizer),\n#                                      bos_token_id=_tokenizer.bos_token_id,\n#                                      eos_token_id=_tokenizer.eos_token_id\n#                                      )\n# _config\n\n\n_model = language_model_init(AutoModelForMaskedLM,\n                             config=_config,\n                             cpoint_path=None, # leave this as None to get a non-pretrained model\n                             seed=42\n                            )\n\nInitiate a new language model from scratch\nTotal parameters: 124697433\nTotal trainable parameters: 124697433\n\n\nCreate a model controller\n\ncontroller = ModelLMController(_model,data_store=tdc,seed=42)\n\nAnd we can start training our model\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 4\nwarmup_ratio=0.25\ncontroller.fit(epochs,lr,\n               batch_size=bs,\n               weight_decay=wd,\n               warmup_ratio=warmup_ratio,\n               save_checkpoint=False,\n              )\n\n\n\n    \n      \n      \n      [1132/1132 05:16, Epoch 4/4]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\nNo log\n5.603508\n0.132908\n\n\n2\n6.342900\n5.404186\n0.153836\n\n\n3\n6.342900\n5.259489\n0.169787\n\n\n4\n5.286800\n5.232296\n0.178328\n\n\n\n\n\n\n\n\n    \n      \n      \n      [142/142 00:07]\n    \n    \n\n\nPerplexity on validation set: 187.860\n\n\n\ncontroller.trainer.model.save_pretrained('./sample_weights/lm_model')",
    "crumbs": [
      "1. Quick Starts",
      "Model Controller Tutorial: Training a Roberta Language Model"
    ]
  },
  {
    "objectID": "model_lm_roberta_tutorial.html#c-fill-mask-using-model",
    "href": "model_lm_roberta_tutorial.html#c-fill-mask-using-model",
    "title": "Model Controller Tutorial: Training a Roberta Language Model",
    "section": "c) Fill mask using model",
    "text": "c) Fill mask using model\n\ntrained_model = language_model_init(AutoModelForMaskedLM,\n                                    cpoint_path='./sample_weights/lm_model',\n                                   )\n\nTotal parameters: 124697433\nTotal trainable parameters: 124697433\n\n\n\ncontroller2 = ModelLMController(trained_model,data_store=tdc,seed=42)\n\n\ncontroller2.data_store.tokenizer.mask_token\n\n'&lt;mask&gt;'\n\n\n\ninp1 = {'Clothing ID':1,\n        'Title':'Flattering',\n        'Review Text': \"Love this &lt;mask&gt;. The detail is amazing. Runs small I ordered a 12 I'm usually a 10, but still a little snug\"\n       }\n\n\ncontroller2.predict_raw_text(inp1,print_result=True)\n\nScore: 0.160 &gt;&gt;&gt; flattering. love this top. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.113 &gt;&gt;&gt; flattering. love this dress. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.050 &gt;&gt;&gt; flattering. love this!. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.040 &gt;&gt;&gt; flattering. love this is. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.027 &gt;&gt;&gt; flattering. love this sweater. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\n--------------------\n\n\nYou can input several raw texts\n\ninp2 = {'Clothing ID':[1,2],\n        'Title':['Flattering','Lovely, but small'],\n        'Review Text': [\"Love this &lt;mask&gt;. The detail is amazing. Runs small I ordered a 12 I'm usually a 10, but still a little snug\",\n                        \"Love this skirt. The detail is amazing. Runs &lt;mask&gt;, I ordered a 12 I'm usually a 10, but still a little snug\"]\n       }\n\n\ncontroller2.predict_raw_text(inp2,print_result=True)\n\nScore: 0.160 &gt;&gt;&gt; flattering. love this top. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.113 &gt;&gt;&gt; flattering. love this dress. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.050 &gt;&gt;&gt; flattering. love this!. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.040 &gt;&gt;&gt; flattering. love this is. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.027 &gt;&gt;&gt; flattering. love this sweater. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\n--------------------\nScore: 0.071 &gt;&gt;&gt; lovely, but small. love this skirt. the detail is amazing. runs it, i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.052 &gt;&gt;&gt; lovely, but small. love this skirt. the detail is amazing. runs the, i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.050 &gt;&gt;&gt; lovely, but small. love this skirt. the detail is amazing. runs and, i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.048 &gt;&gt;&gt; lovely, but small. love this skirt. the detail is amazing. runs., i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.046 &gt;&gt;&gt; lovely, but small. love this skirt. the detail is amazing. runs,, i ordered a 12 i'm usually a 10, but still a little snug\n--------------------",
    "crumbs": [
      "1. Quick Starts",
      "Model Controller Tutorial: Training a Roberta Language Model"
    ]
  },
  {
    "objectID": "model_lm_roberta_tutorial.html#a-create-a-textdatalmcontroller-object-1",
    "href": "model_lm_roberta_tutorial.html#a-create-a-textdatalmcontroller-object-1",
    "title": "Model Controller Tutorial: Training a Roberta Language Model",
    "section": "a) Create a TextDataLMController object",
    "text": "a) Create a TextDataLMController object\nWe will reuse the data and the preprocessings in this tutorial\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataLMController(dset,\n                         main_text='Review Text',\n                         filter_dict={'Review Text': lambda x: x is not None},\n                         metadatas='Title',\n                         content_transformations=[text_normalize,str.lower],\n                         cols_to_keep=['Clothing ID','Review Text'],\n                         seed=42,\n                         verbose=False\n                        )\n\nDefine our tokenizer for Roberta\n\n_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\nProcess and tokenize our dataset (using line-by-line tokenization)\n\nblock_size=112\ntdc.process_and_tokenize(_tokenizer,line_by_line=True,max_length=block_size) \n# set max_length=-1 if you want the data collator to pad\n\n\ntdc.main_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Clothing ID', 'Review Text', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n        num_rows: 18112\n    })\n    validation: Dataset({\n        features: ['Clothing ID', 'Review Text', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n        num_rows: 4529\n    })\n})\n\n\nAnd set the data collator\n\ntdc.set_data_collator(is_mlm=True,mlm_prob=0.15)",
    "crumbs": [
      "1. Quick Starts",
      "Model Controller Tutorial: Training a Roberta Language Model"
    ]
  },
  {
    "objectID": "model_lm_roberta_tutorial.html#b-initialize-and-train-roberta-language-model",
    "href": "model_lm_roberta_tutorial.html#b-initialize-and-train-roberta-language-model",
    "title": "Model Controller Tutorial: Training a Roberta Language Model",
    "section": "b) Initialize and train Roberta Language Model",
    "text": "b) Initialize and train Roberta Language Model\n\n_config = AutoConfig.from_pretrained('roberta-base',\n                                    vocab_size=len(_tokenizer))\n_config\n\nRobertaConfig {\n  \"_name_or_path\": \"roberta-base\",\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.40.1\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n\n\n_model = language_model_init(AutoModelForMaskedLM,\n                             config=_config,\n                             cpoint_path='roberta-base',\n                             seed=42\n                            )\n\nTotal parameters: 124697433\nTotal trainable parameters: 124697433\n\n\nCreate a model controller\n\ncontroller = ModelLMController(_model,data_store=tdc,seed=42)\n\nAnd we can start training our model\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 4\nwarmup_ratio=0.25\ncontroller.fit(epochs,lr,\n               batch_size=bs,\n               weight_decay=wd,\n               warmup_ratio=warmup_ratio,\n               save_checkpoint=False,\n              )\n\n\n\n    \n      \n      \n      [1132/1132 05:36, Epoch 4/4]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\nNo log\n1.559132\n0.650172\n\n\n2\n1.682400\n1.451852\n0.667497\n\n\n3\n1.682400\n1.360187\n0.684915\n\n\n4\n1.405600\n1.331839\n0.688361\n\n\n\n\n\n\n\n\n    \n      \n      \n      [142/142 00:07]\n    \n    \n\n\nPerplexity on validation set: 3.779\n\n\nFinetuning from a pretrained model results in a massive improvement in terms of metrics\n\ncontroller.trainer.model.save_pretrained('./sample_weights/lm_model')",
    "crumbs": [
      "1. Quick Starts",
      "Model Controller Tutorial: Training a Roberta Language Model"
    ]
  },
  {
    "objectID": "model_lm_roberta_tutorial.html#c-fill-mask-using-model-1",
    "href": "model_lm_roberta_tutorial.html#c-fill-mask-using-model-1",
    "title": "Model Controller Tutorial: Training a Roberta Language Model",
    "section": "c) Fill mask using model",
    "text": "c) Fill mask using model\n\ntrained_model = language_model_init(AutoModelForMaskedLM,\n                                    cpoint_path='./sample_weights/lm_model',\n                                   )\n\nTotal parameters: 124697433\nTotal trainable parameters: 124697433\n\n\n\ncontroller2 = ModelLMController(trained_model,data_store=tdc,seed=42)\n\n\ncontroller2.data_store.tokenizer.mask_token\n\n'&lt;mask&gt;'\n\n\n\ninp1 = {'Clothing ID':1,\n        'Title':'Flattering',\n        'Review Text': \"Love this &lt;mask&gt;. The detail is amazing. Runs small I ordered a 12 I'm usually a 10, but still a little snug\"\n       }\n\n\ncontroller2.predict_raw_text(inp1,print_result=True)\n\nScore: 0.285 &gt;&gt;&gt; flattering. love this dress. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.244 &gt;&gt;&gt; flattering. love this top. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.187 &gt;&gt;&gt; flattering. love this shirt. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.070 &gt;&gt;&gt; flattering. love this sweater. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.068 &gt;&gt;&gt; flattering. love this skirt. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\n--------------------\n\n\nYou can input several raw texts\n\ninp2 = {'Clothing ID':[1,2],\n        'Title':['Flattering','Lovely, but small'],\n        'Review Text': [\"Love this &lt;mask&gt;. The detail is amazing. Runs small I ordered a 12 I'm usually a 10, but still a little snug\",\n                        \"Love this skirt. The detail is amazing. Runs &lt;mask&gt;, I ordered a 12 I'm usually a 10, but still a little snug\"]\n       }\n\n\ncontroller2.predict_raw_text(inp2,print_result=True)\n\nScore: 0.285 &gt;&gt;&gt; flattering. love this dress. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.244 &gt;&gt;&gt; flattering. love this top. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.187 &gt;&gt;&gt; flattering. love this shirt. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.070 &gt;&gt;&gt; flattering. love this sweater. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.068 &gt;&gt;&gt; flattering. love this skirt. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\n--------------------\nScore: 0.893 &gt;&gt;&gt; lovely, but small. love this skirt. the detail is amazing. runs small, i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.051 &gt;&gt;&gt; lovely, but small. love this skirt. the detail is amazing. runs large, i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.022 &gt;&gt;&gt; lovely, but small. love this skirt. the detail is amazing. runs big, i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.006 &gt;&gt;&gt; lovely, but small. love this skirt. the detail is amazing. runs short, i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.005 &gt;&gt;&gt; lovely, but small. love this skirt. the detail is amazing. runs tiny, i ordered a 12 i'm usually a 10, but still a little snug\n--------------------\n\n\n\ncontroller2.predict_raw_text(inp2,print_result=False)\n\n[[{'score': 0.28502416610717773,\n   'token': 3588,\n   'token_str': ' dress',\n   'sequence': \"flattering. love this dress. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\"},\n  {'score': 0.24447907507419586,\n   'token': 299,\n   'token_str': ' top',\n   'sequence': \"flattering. love this top. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\"},\n  {'score': 0.18709176778793335,\n   'token': 6399,\n   'token_str': ' shirt',\n   'sequence': \"flattering. love this shirt. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\"},\n  {'score': 0.06980422139167786,\n   'token': 23204,\n   'token_str': ' sweater',\n   'sequence': \"flattering. love this sweater. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\"},\n  {'score': 0.06781341880559921,\n   'token': 16576,\n   'token_str': ' skirt',\n   'sequence': \"flattering. love this skirt. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\"}],\n [{'score': 0.8933400511741638,\n   'token': 650,\n   'token_str': ' small',\n   'sequence': \"lovely, but small. love this skirt. the detail is amazing. runs small, i ordered a 12 i'm usually a 10, but still a little snug\"},\n  {'score': 0.05062047392129898,\n   'token': 739,\n   'token_str': ' large',\n   'sequence': \"lovely, but small. love this skirt. the detail is amazing. runs large, i ordered a 12 i'm usually a 10, but still a little snug\"},\n  {'score': 0.0221096184104681,\n   'token': 380,\n   'token_str': ' big',\n   'sequence': \"lovely, but small. love this skirt. the detail is amazing. runs big, i ordered a 12 i'm usually a 10, but still a little snug\"},\n  {'score': 0.006218481808900833,\n   'token': 765,\n   'token_str': ' short',\n   'sequence': \"lovely, but small. love this skirt. the detail is amazing. runs short, i ordered a 12 i'm usually a 10, but still a little snug\"},\n  {'score': 0.0046571786515414715,\n   'token': 5262,\n   'token_str': ' tiny',\n   'sequence': \"lovely, but small. love this skirt. the detail is amazing. runs tiny, i ordered a 12 i'm usually a 10, but still a little snug\"}]]",
    "crumbs": [
      "1. Quick Starts",
      "Model Controller Tutorial: Training a Roberta Language Model"
    ]
  },
  {
    "objectID": "model_lm_roberta_tutorial.html#d-extract-hidden-states-from-model",
    "href": "model_lm_roberta_tutorial.html#d-extract-hidden-states-from-model",
    "title": "Model Controller Tutorial: Training a Roberta Language Model",
    "section": "d) Extract hidden states from model",
    "text": "d) Extract hidden states from model\n\nFrom raw texts\n\ninp1 = {'Clothing ID':1,\n        'Title':'Flattering',\n        'Review Text': \"Love this skirt. The detail is amazing. Runs small I ordered a 12 I'm usually a 10, but still a little snug\"\n       }\n\n\n_config = AutoConfig.from_pretrained('./sample_weights/lm_model',output_hidden_states=True)\n\n\ntrained_model = language_model_init(AutoModelForMaskedLM,\n                                    cpoint_path='./sample_weights/lm_model',\n                                    config=_config\n                                   )\n\ncontroller2 = ModelLMController(trained_model,data_store=tdc,seed=42)\n\nTotal parameters: 124697433\nTotal trainable parameters: 124697433\n\n\n\nhidden_from_ip1 = controller2.get_hidden_states_from_raw_text(inp1,\n                                                              state_name='hidden_states',\n                                                              state_idx=[-1,0]\n                                                             )\n\n\nhidden_from_ip1\n\nDataset({\n    features: ['Clothing ID', 'Review Text', 'input_ids', 'attention_mask', 'special_tokens_mask', 'hidden_states'],\n    num_rows: 1\n})\n\n\n\nhidden_from_ip1['hidden_states'].shape\n\n(1, 768)\n\n\n\n\nFrom validation (or even train) set\n\nhidden_from_vals = controller2.get_hidden_states(ds_type='validation',\n                                                 state_name='hidden_states',\n                                                 state_idx=[-1,0]\n                                                )\n\n\nhidden_from_vals\n\nDataset({\n    features: ['Clothing ID', 'Review Text', 'input_ids', 'attention_mask', 'special_tokens_mask', 'hidden_states'],\n    num_rows: 4529\n})\n\n\n\nhidden_from_vals['hidden_states'].shape\n\n(4529, 768)",
    "crumbs": [
      "1. Quick Starts",
      "Model Controller Tutorial: Training a Roberta Language Model"
    ]
  },
  {
    "objectID": "model_lm_roberta_tutorial.html#a-create-a-textdatalmcontroller-object-2",
    "href": "model_lm_roberta_tutorial.html#a-create-a-textdatalmcontroller-object-2",
    "title": "Model Controller Tutorial: Training a Roberta Language Model",
    "section": "a) Create a TextDataLMController object",
    "text": "a) Create a TextDataLMController object\nWe will reuse the data and the preprocessings in this tutorial\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataLMController(dset,\n                         main_text='Review Text',\n                         filter_dict={'Review Text': lambda x: x is not None},\n                         metadatas='Title',\n                         content_transformations=[text_normalize,str.lower],\n                         seed=42,\n                         verbose=False\n                        )\n\nDefine our tokenizer for Roberta\n\n_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\nProcess and tokenize our dataset (using token concatenation technique)\n\nblock_size=112\ntdc.process_and_tokenize(_tokenizer,line_by_line=False,max_length=block_size)\n\n\ntdc.main_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'special_tokens_mask'],\n        num_rows: 12901\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask', 'special_tokens_mask'],\n        num_rows: 3276\n    })\n})\n\n\nAnd set the data collator\n\ntdc.set_data_collator(is_mlm=True,mlm_prob=0.15)",
    "crumbs": [
      "1. Quick Starts",
      "Model Controller Tutorial: Training a Roberta Language Model"
    ]
  },
  {
    "objectID": "model_lm_roberta_tutorial.html#b-initialize-and-train-roberta-language-model-1",
    "href": "model_lm_roberta_tutorial.html#b-initialize-and-train-roberta-language-model-1",
    "title": "Model Controller Tutorial: Training a Roberta Language Model",
    "section": "b) Initialize and train Roberta Language Model",
    "text": "b) Initialize and train Roberta Language Model\n\n_config = AutoConfig.from_pretrained('roberta-base',\n                                    vocab_size=len(_tokenizer))\n_config\n\nRobertaConfig {\n  \"_name_or_path\": \"roberta-base\",\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.40.1\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n\n\n_model = language_model_init(AutoModelForMaskedLM,\n                             config=_config,\n                             cpoint_path='roberta-base',\n                             seed=42\n                            )\n\nTotal parameters: 124697433\nTotal trainable parameters: 124697433\n\n\nCreate a model controller\n\ncontroller = ModelLMController(_model,data_store=tdc,seed=42)\n\nAnd we can start training our model\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 4\nwarmup_ratio=0.25\ncontroller.fit(epochs,lr,\n               batch_size=bs,\n               weight_decay=wd,\n               warmup_ratio=warmup_ratio,\n               save_checkpoint=False,\n              )\n\n\n\n    \n      \n      \n      [808/808 03:58, Epoch 4/4]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\nNo log\n1.694216\n0.628713\n\n\n2\n1.860100\n1.601513\n0.642077\n\n\n3\n1.860100\n1.515734\n0.656354\n\n\n4\n1.561200\n1.477700\n0.662074\n\n\n\n\n\n\n\n\n    \n      \n      \n      [103/103 00:04]\n    \n    \n\n\nPerplexity on validation set: 4.413\n\n\nSlightly less perplexity than the previous model\n\ncontroller.trainer.model.save_pretrained('./sample_weights/lm_model')",
    "crumbs": [
      "1. Quick Starts",
      "Model Controller Tutorial: Training a Roberta Language Model"
    ]
  },
  {
    "objectID": "model_lm_roberta_tutorial.html#c-fill-mask-using-model-2",
    "href": "model_lm_roberta_tutorial.html#c-fill-mask-using-model-2",
    "title": "Model Controller Tutorial: Training a Roberta Language Model",
    "section": "c) Fill mask using model",
    "text": "c) Fill mask using model\n\ntrained_model = language_model_init(AutoModelForMaskedLM,\n                                    cpoint_path='./sample_weights/lm_model',\n                                   )\n\nTotal parameters: 124697433\nTotal trainable parameters: 124697433\n\n\n\ncontroller2 = ModelLMController(trained_model,data_store=tdc,seed=42)\n\n\ninp1 = {'Title':'Flattering',\n        'Review Text': \"Love this &lt;mask&gt;. The detail is amazing. Runs small I ordered a 12 I'm usually a 10, but still a little snug\"\n       }\n\n\ncontroller2.predict_raw_text(inp1,print_result=True)\n\nScore: 0.328 &gt;&gt;&gt; flattering. love this top. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.304 &gt;&gt;&gt; flattering. love this dress. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.128 &gt;&gt;&gt; flattering. love this shirt. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.076 &gt;&gt;&gt; flattering. love this sweater. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.045 &gt;&gt;&gt; flattering. love this skirt. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\n--------------------\n\n\nYou can input several raw texts\n\ninp2 = {'Title':['Flattering','Lovely, but small'],\n        'Review Text': [\"Love this &lt;mask&gt;. The detail is amazing. Runs small I ordered a 12 I'm usually a 10, but still a little snug\",\n                        \"Love this skirt. The detail is amazing. Runs &lt;mask&gt;, I ordered a 12 I'm usually a 10, but still a little snug\"]\n       }\n\n\ncontroller2.predict_raw_text(inp2,print_result=True)\n\nScore: 0.328 &gt;&gt;&gt; flattering. love this top. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.304 &gt;&gt;&gt; flattering. love this dress. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.128 &gt;&gt;&gt; flattering. love this shirt. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.076 &gt;&gt;&gt; flattering. love this sweater. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.045 &gt;&gt;&gt; flattering. love this skirt. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\n--------------------\nScore: 0.893 &gt;&gt;&gt; lovely, but small. love this skirt. the detail is amazing. runs small, i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.062 &gt;&gt;&gt; lovely, but small. love this skirt. the detail is amazing. runs large, i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.020 &gt;&gt;&gt; lovely, but small. love this skirt. the detail is amazing. runs big, i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.004 &gt;&gt;&gt; lovely, but small. love this skirt. the detail is amazing. runs short, i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.003 &gt;&gt;&gt; lovely, but small. love this skirt. the detail is amazing. runs tiny, i ordered a 12 i'm usually a 10, but still a little snug\n--------------------",
    "crumbs": [
      "1. Quick Starts",
      "Model Controller Tutorial: Training a Roberta Language Model"
    ]
  },
  {
    "objectID": "gpt2_multihead.html",
    "href": "gpt2_multihead.html",
    "title": "GPT2 model (Multi Head)",
    "section": "",
    "text": "In this series, we walk through some of the capability of this library: single-head classification, multi-head classification, multi-label classification, and regression. If you want a more detailed tutorial, check this out\nimport os\n#This will specify a (or a list) of GPUs for training\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom that_nlp_library.text_main import *\nfrom that_nlp_library.utils import seed_everything\nfrom underthesea import text_normalize\nfrom functools import partial\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport nlpaug.augmenter.char as nac\nfrom datasets import load_dataset\nimport random\nfrom transformers import AutoTokenizer\nfrom datasets import Dataset",
    "crumbs": [
      "2. All Use Cases",
      "b. GPT2-based models for Supervised Learning",
      "GPT2 model (Multi Head)"
    ]
  },
  {
    "objectID": "gpt2_multihead.html#define-and-train-a-vanilla-gpt2base-model",
    "href": "gpt2_multihead.html#define-and-train-a-vanilla-gpt2base-model",
    "title": "GPT2 model (Multi Head)",
    "section": "Define and train a vanilla GPT2Base model",
    "text": "Define and train a vanilla GPT2Base model\n\nfrom transformers.models.gpt2.modeling_gpt2 import GPT2Model\n\n\nfrom that_nlp_library.models.roberta.classifiers import ConcatHeadSimple\nfrom that_nlp_library.model_main import *\nfrom that_nlp_library.models.gpt2.classifiers import *\nfrom sklearn.metrics import f1_score, accuracy_score\n\n\ngpt2body = GPT2Model.from_pretrained('gpt2')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\nnum_classes = [len(tdc.label_lists[0]),len(tdc.label_lists[1])] \nnum_classes\n\n[3, 6]\n\n\n\n# our model is more complex, so it's best to define some of its arguments\n_model_kwargs={\n    # overall model hyperparams\n    'head_class_sizes':num_classes,\n    'is_multilabel':tdc.is_multilabel, # False\n    'is_multihead':tdc.is_multihead, # True\n    'head_weights':[1,1],\n    # classfication head hyperparams\n    'classifier_dropout':0.1 \n}\n\n\nmodel = model_init_classification(model_class = GPT2BaseForSequenceClassification,\n                                  cpoint_path = 'gpt2', \n                                  output_hidden_states=False, # since we are using 'hidden layer contatenation' technique\n                                  seed=42,\n                                  body_model=gpt2body,\n                                  model_kwargs = _model_kwargs)\n\nLoading body weights. This assumes the body is the very first block of your custom architecture\nTotal parameters: 124446720\nTotal trainable parameters: 124446720\n\n\n\n# resize token embedding\nmodel.body_model.resize_token_embeddings(len(_tokenizer))\n\nEmbedding(50257, 768)\n\n\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score]\ncontroller = ModelController(model,tdc,seed=42)\n\nAnd we can start training our model\n\nlr = 8e-5\nbs=32\nwd=0.01\nepochs= 3\n\ncontroller.fit(epochs,lr,\n               metric_funcs=metric_funcs,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics,\n              )\n\n\n\n    \n      \n      \n      [849/849 06:52, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1 Score Division name\nAccuracy Score Division name\nF1 Score Department name\nAccuracy Score Department name\n\n\n\n\n1\nNo log\n1.232870\n0.408721\n0.610252\n0.619346\n0.854839\n\n\n2\n1.711600\n1.140891\n0.453457\n0.612240\n0.676615\n0.879585\n\n\n3\n1.711600\n1.139938\n0.465679\n0.612019\n0.680045\n0.881794\n\n\n\n\n\n\n\n\ncontroller.trainer.model.save_pretrained('./sample_weights/my_model1')",
    "crumbs": [
      "2. All Use Cases",
      "b. GPT2-based models for Supervised Learning",
      "GPT2 model (Multi Head)"
    ]
  },
  {
    "objectID": "gpt2_multihead.html#make-predictions",
    "href": "gpt2_multihead.html#make-predictions",
    "title": "GPT2 model (Multi Head)",
    "section": "Make predictions",
    "text": "Make predictions\n\ntrained_model = model_init_classification(model_class = GPT2BaseForSequenceClassification,\n                                          cpoint_path = Path('./sample_weights/my_model1'), \n                                          output_hidden_states=True,\n                                          seed=42,\n                                          model_kwargs = _model_kwargs)\n\ncontroller = ModelController(trained_model,tdc,seed=42)\n\nTotal parameters: 124446720\nTotal trainable parameters: 124446720\n\n\n\ndf_val = controller.predict_ddict(ds_type='validation')\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_val = df_val.to_pandas()\ndf_val.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nDivision Name\nDepartment Name\nlabel\ninput_ids\nattention_mask\npred_Division Name\npred_prob_Division Name\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\n\n. such a fun jacket ! great to wear in the spr...\nGeneral Petite\nIntimate\n[1, 2]\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\nGeneral\n0.529604\nJackets\n0.734662\n\n\n1\nsimple and elegant\nsimple and elegant . i thought this shirt was ...\nGeneral Petite\nTops\n[1, 4]\n[36439, 290, 19992, 764, 1312, 1807, 428, 1014...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.551158\nTops\n0.982016\n\n\n2\nretro and pretty\nretro and pretty . this top has a bit of a ret...\nGeneral\nTops\n[0, 4]\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\nGeneral\n0.656122\nTops\n0.990658\n\n\n3\nsummer/fall wear\nsummer / fall wear . i first spotted this on a...\nGeneral Petite\nDresses\n[1, 1]\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, ...\nGeneral\n0.550641\nDresses\n0.932515\n\n\n4\nperfect except slip\nperfect except slip . this is my new favorite ...\nGeneral Petite\nDresses\n[1, 1]\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\nGeneral\n0.512942\nDresses\n0.988941\n\n\n\n\n\n\n\n\nYou can try to get your metric to see if it matches your last traing epoch’s above\n\nf1_score(df_val['Division Name'],df_val['pred_Division Name'],average='macro')\n# 0.45506833397695967\n\n0.4656789717699823\n\n\n\nf1_score(df_val['Department Name'],df_val['pred_Department Name'],average='macro')\n# 0.6795641996672526\n\n0.6800451854614634",
    "crumbs": [
      "2. All Use Cases",
      "b. GPT2-based models for Supervised Learning",
      "GPT2 model (Multi Head)"
    ]
  },
  {
    "objectID": "gpt2_multihead.html#define-and-train-a-custom-gpt2-model",
    "href": "gpt2_multihead.html#define-and-train-a-custom-gpt2-model",
    "title": "GPT2 model (Multi Head)",
    "section": "Define and train a custom GPT2 model",
    "text": "Define and train a custom GPT2 model\n\nnum_classes = [len(tdc.label_lists[0]),len(tdc.label_lists[1])] \nnum_classes\n\n[3, 6]\n\n\n\ngpt2body = GPT2Model.from_pretrained('gpt2')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\n# our model is more complex, so it's best to define some of its arguments\n_model_kwargs={\n    # overall model hyperparams\n    'head_class_sizes':num_classes,\n    'head_class': ConcatHeadSimple,\n    'is_multilabel':tdc.is_multilabel, # False\n    'is_multihead':tdc.is_multihead, # True\n    'head_weights':[1,1], # weights for label 1 and label 2 This means L2's weight is twice as much as L1's\n    # classfication head hyperparams\n    'layer2concat':2, # you can change the number of layers to concat (default is 4, based on the paper)\n    'classifier_dropout':0.1 \n}\n\nmodel = model_init_classification(model_class = GPT2HiddenStateConcatForSequenceClassification,\n                                  cpoint_path = 'gpt2', \n                                  output_hidden_states=True, # since we are using 'hidden layer contatenation' technique\n                                  seed=42,\n                                  body_model=gpt2body,\n                                  model_kwargs = _model_kwargs)\n\nLoading body weights. This assumes the body is the very first block of your custom architecture\nTotal parameters: 124453641\nTotal trainable parameters: 124453641\n\n\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score]\ncontroller = ModelController(model,tdc,seed=42)\n\nAnd we can start training our model\n\nseed_everything(42)\n\n\nlr = 8e-5\nbs=32\nwd=0.01\nepochs= 3\n\ncontroller.fit(epochs,lr,\n               metric_funcs=metric_funcs,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics,\n              )\n\n\n\n    \n      \n      \n      [849/849 06:50, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1 Score Division name\nAccuracy Score Division name\nF1 Score Department name\nAccuracy Score Department name\n\n\n\n\n1\nNo log\n1.443740\n0.355963\n0.602298\n0.597171\n0.846001\n\n\n2\n2.155500\n1.200898\n0.446875\n0.610252\n0.657054\n0.867212\n\n\n3\n2.155500\n1.193677\n0.459581\n0.604507\n0.659238\n0.869863\n\n\n\n\n\n\n\n\ncontroller.trainer.model.save_pretrained('./sample_weights/my_model1')",
    "crumbs": [
      "2. All Use Cases",
      "b. GPT2-based models for Supervised Learning",
      "GPT2 model (Multi Head)"
    ]
  },
  {
    "objectID": "gpt2_multihead.html#make-predictions-1",
    "href": "gpt2_multihead.html#make-predictions-1",
    "title": "GPT2 model (Multi Head)",
    "section": "Make predictions",
    "text": "Make predictions\n\nLoad trained model\n\n_model_kwargs\n\n{'head_class_sizes': [3, 6],\n 'head_class': that_nlp_library.models.roberta.classifiers.ConcatHeadSimple,\n 'is_multilabel': False,\n 'is_multihead': True,\n 'head_weights': [1, 1],\n 'layer2concat': 2,\n 'classifier_dropout': 0.1}\n\n\n\ntrained_model = model_init_classification(model_class = GPT2HiddenStateConcatForSequenceClassification,\n                                          cpoint_path = Path('./sample_weights/my_model1'), \n                                          output_hidden_states=True,\n                                          seed=42,\n                                          model_kwargs = _model_kwargs)\n\ncontroller = ModelController(trained_model,tdc,seed=42)\n\nTotal parameters: 124453641\nTotal trainable parameters: 124453641\n\n\n\n\nPredict Train/Validation set\n\ndf_val = controller.predict_ddict(ds_type='validation')\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_val = df_val.to_pandas()\ndf_val.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nDivision Name\nDepartment Name\nlabel\ninput_ids\nattention_mask\npred_Division Name\npred_prob_Division Name\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\n\n. such a fun jacket ! great to wear in the spr...\nGeneral Petite\nIntimate\n[1, 2]\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\nGeneral\n0.481536\nJackets\n0.862960\n\n\n1\nsimple and elegant\nsimple and elegant . i thought this shirt was ...\nGeneral Petite\nTops\n[1, 4]\n[36439, 290, 19992, 764, 1312, 1807, 428, 1014...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.632173\nTops\n0.988462\n\n\n2\nretro and pretty\nretro and pretty . this top has a bit of a ret...\nGeneral\nTops\n[0, 4]\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\nGeneral\n0.620987\nTops\n0.988526\n\n\n3\nsummer/fall wear\nsummer / fall wear . i first spotted this on a...\nGeneral Petite\nDresses\n[1, 1]\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, ...\nGeneral\n0.670211\nDresses\n0.925290\n\n\n4\nperfect except slip\nperfect except slip . this is my new favorite ...\nGeneral Petite\nDresses\n[1, 1]\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\nGeneral\n0.562400\nDresses\n0.983849\n\n\n\n\n\n\n\n\nYou can try to get your metric to see if it matches your last traing epoch’s above\n\nf1_score(df_val['Division Name'],df_val['pred_Division Name'],average='macro')\n\n0.45352717018986105\n\n\n\nf1_score(df_val['Department Name'],df_val['pred_Department Name'],average='macro')\n\n0.6628016843655465\n\n\n\n\nPredict Test set\nWe will go through details on how to make a prediction on a completely new and raw dataset using our trained model. For now, let’s reuse the sample csv and pretend it’s our test set\n\ndf_test = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig').sample(frac=0.2,random_state=1)\n# drop NaN values in the label column\ndf_test = df_test[~df_test['Department Name'].isna()].reset_index(drop=True)\n\n# save the label, as we will calculate some metrics later. We also filter out labels with NaN Review Text,\n# as there will be a filtering processing on the test set\ntrue_labels = df_test.loc[~df_test['Review Text'].isna(),'Department Name'].values \n\n# drop the label (you don't need to, but this is necessary to simulate an actual test set)\ndf_test.drop(['Division Name','Department Name'],axis=1,inplace=True)\n\n\n_test_dset = Dataset.from_pandas(df_test)\n_test_dset_predicted = controller.predict_raw_dset(_test_dset,\n                                                   do_filtering=True, # since we have some text filtering in the processing\n                                                  )\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_test_predicted = _test_dset_predicted.to_pandas()\n\n\ndf_test_predicted.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\ninput_ids\nattention_mask\npred_Division Name\npred_prob_Division Name\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\nperfect for work and play\nperfect for work and play . this shirt works f...\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\nGeneral\n0.609993\nTops\n0.990637\n\n\n1\n\n. i don't know why i had the opposite problem ...\n[13, 1312, 836, 470, 760, 1521, 1312, 550, 262...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.814815\nBottoms\n0.995138\n\n\n2\ngreat pants\ngreat pants . thes e cords are great--lightwei...\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\nGeneral\n0.658985\nBottoms\n0.994834\n\n\n3\nsurprisingly comfy for a button down\nsurprisingly comfy for a button down . i am a ...\n[41199, 401, 24928, 329, 257, 4936, 866, 764, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.692814\nTops\n0.975600\n\n\n4\nshort and small\nshort and small . the shirt is mostly a thick ...\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\nGeneral\n0.621851\nTops\n0.947042\n\n\n\n\n\n\n\n\nLet’s quickly check the f1 score to make sure everything works correctly\n\nf1_score(true_labels,df_test_predicted['pred_Department Name'],average='macro')\n\n0.6731872399763867\n\n\nPredict top k results\n\n_test_dset = Dataset.from_pandas(df_test)\n_test_dset_predicted = controller.predict_raw_dset(_test_dset,\n                                                   do_filtering=True,\n                                                   topk=3\n                                                  )\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_test_predicted = _test_dset_predicted.to_pandas()\n\ndf_test_predicted.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\ninput_ids\nattention_mask\npred_Division Name\npred_prob_Division Name\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\nperfect for work and play\nperfect for work and play . this shirt works f...\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n[General, General Petite, Initmates]\n[0.6099926, 0.38029346, 0.009713977]\n[Tops, Intimate, Jackets]\n[0.99063677, 0.0068505756, 0.0011757102]\n\n\n1\n\n. i don't know why i had the opposite problem ...\n[13, 1312, 836, 470, 760, 1521, 1312, 550, 262...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[General, General Petite, Initmates]\n[0.81481504, 0.17968053, 0.005504485]\n[Bottoms, Intimate, Trend]\n[0.9951379, 0.0033080056, 0.00085609016]\n\n\n2\ngreat pants\ngreat pants . thes e cords are great--lightwei...\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n[General, General Petite, Initmates]\n[0.65898484, 0.31146494, 0.029550197]\n[Bottoms, Intimate, Trend]\n[0.99483407, 0.0045331665, 0.00034278215]\n\n\n3\nsurprisingly comfy for a button down\nsurprisingly comfy for a button down . i am a ...\n[41199, 401, 24928, 329, 257, 4936, 866, 764, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[General, General Petite, Initmates]\n[0.6928137, 0.29272404, 0.014462292]\n[Tops, Jackets, Intimate]\n[0.9755996, 0.011239706, 0.0076327748]\n\n\n4\nshort and small\nshort and small . the shirt is mostly a thick ...\n[50256, 50256, 50256, 50256, 50256, 50256, 502...\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n[General, General Petite, Initmates]\n[0.6218508, 0.32296118, 0.055188037]\n[Tops, Intimate, Jackets]\n[0.94704163, 0.0429352, 0.0054707266]\n\n\n\n\n\n\n\n\n\n# Since we have some metadatas (Title), we need to define a dictionary containing those values\nraw_content={'Review Text': 'This shirt is so comfortable I love it!',\n             'Title': 'Great shirt'}\n\n\ncontroller.data_store.num_proc=1\n\n\ndf_result = controller.predict_raw_text(raw_content,topk=3)\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_result\n\n{'Review Text': ['great shirt . this shirt is so comfortable i love it !'],\n 'Title': ['great shirt'],\n 'input_ids': [[18223,\n   10147,\n   764,\n   428,\n   10147,\n   318,\n   523,\n   6792,\n   1312,\n   1842,\n   340,\n   5145]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n 'pred_Division Name': [['General', 'General Petite', 'Initmates']],\n 'pred_prob_Division Name': [[0.6347099542617798,\n   0.347072035074234,\n   0.018217984586954117]],\n 'pred_Department Name': [['Tops', 'Intimate', 'Jackets']],\n 'pred_prob_Department Name': [[0.9934250116348267,\n   0.005981422495096922,\n   0.00047667595208622515]]}",
    "crumbs": [
      "2. All Use Cases",
      "b. GPT2-based models for Supervised Learning",
      "GPT2 model (Multi Head)"
    ]
  },
  {
    "objectID": "model_classification_tutorial.html",
    "href": "model_classification_tutorial.html",
    "title": "Model Controller Tutorial: Classification",
    "section": "",
    "text": "import os\n#This will specify a (or a list) of GPUs for training\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom that_nlp_library.text_main import *\nfrom that_nlp_library.utils import seed_everything\nfrom underthesea import text_normalize\nfrom functools import partial\nfrom pathlib import Path\nfrom transformers import DataCollatorWithPadding,RobertaTokenizer\nfrom transformers.models.roberta.modeling_roberta import RobertaForSequenceClassification\nimport nlpaug.augmenter.char as nac\nfrom datasets import load_dataset\nimport random\nimport pandas as pd\nimport numpy as np",
    "crumbs": [
      "1. Quick Starts",
      "Model Controller Tutorial: Classification"
    ]
  },
  {
    "objectID": "model_classification_tutorial.html#a-train-roberta-model-using-the-model-controller",
    "href": "model_classification_tutorial.html#a-train-roberta-model-using-the-model-controller",
    "title": "Model Controller Tutorial: Classification",
    "section": "a) Train Roberta model using the Model Controller",
    "text": "a) Train Roberta model using the Model Controller\nHere are the unique values in our label\n\ntdc.label_lists[0]\n\n['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']\n\n\n\nnum_classes = len(tdc.label_lists[0])\n\nLet’s define our model\n\nseed_everything(42)\n\n\nmodel_name='roberta-base'\n_model = RobertaForSequenceClassification.from_pretrained(model_name,num_labels=num_classes)\n_model = _model.to('cuda:0')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nThen we can define the metrics to used, and the Model Controller object\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score] \n# we will use both f1_macro and accuracy score as metrics\n\n\ncontroller = ModelController(_model,\n                             data_store=tdc,\n                             seed=42)\n\nAnd we can start training our model\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 3\n\ncontroller.fit(epochs,lr,\n               metric_funcs=metric_funcs,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics,\n              )\n\n\n\n    \n      \n      \n      [849/849 02:09, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1 Score Department name\nAccuracy Score Department name\n\n\n\n\n1\nNo log\n0.304115\n0.743430\n0.914494\n\n\n2\n0.417500\n0.264885\n0.749442\n0.919797\n\n\n3\n0.417500\n0.281572\n0.747713\n0.918471\n\n\n\n\n\n\n\n\nLogging your training\nYou can log your training using HuggingFace:\n\nSupported platforms are “azure_ml”, “comet_ml”, “mlflow”, “neptune”, “tensorboard”, “clearml” and “wandb”\nReferences:\n\nhttps://huggingface.co/docs/transformers/v4.40.2/en/main_classes/trainer#transformers.TrainingArguments\nhttps://docs.wandb.ai/guides/integrations/huggingface\n\n\ncontroller.fit(epochs,lr,\n               metric_funcs=metric_funcs,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics,\n               hf_report_to='wandb'\n              )\nYou can save your model weights at the end of your training\ncontroller.trainer.model.save_pretrained('./sample_weights/model_progress')\nOr you can save your weights at every epochs during your training\ncontroller.fit(epochs,lr,\n               metric_funcs=metric_funcs,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=True,\n               o_dir='my_saved_weights',\n               compute_metrics=compute_metrics,",
    "crumbs": [
      "1. Quick Starts",
      "Model Controller Tutorial: Classification"
    ]
  },
  {
    "objectID": "model_classification_tutorial.html#b-train-model-with-only-a-tokenized-datasetdict-no-textdatacontroller",
    "href": "model_classification_tutorial.html#b-train-model-with-only-a-tokenized-datasetdict-no-textdatacontroller",
    "title": "Model Controller Tutorial: Classification",
    "section": "b) Train model with only a Tokenized DatasetDict (no TextDataController)",
    "text": "b) Train model with only a Tokenized DatasetDict (no TextDataController)\nThis part assumes you already have your tokenized datasetdict (you don’t even need to pad your tokens, as demonstrated below). We will ‘borrow’ TextDataController to create such datasetdict for us\n\n# import copy\n\n# main_ddict = copy.deepcopy(tdc.main_ddict)\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         sup_types='classification',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         # add \"str.lower\" here because nearby_aug might return uppercase character\n                         val_ratio=0.2,\n                         batch_size=1000,\n                         seed=42,\n                         num_proc=20,\n                         verbose=True\n                        )\n\n_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\n# set max_length to -1 to skip the padding\ntdc.process_and_tokenize(_tokenizer,max_length=-1,shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\n----- Do &lt;lambda&gt; on Department Name -----\nDone\n----- Metadata Simple Processing & Concatenating to Main Content -----\nDone\n----- Label Encoding -----\nDone\n-------------------- Text Transformation --------------------\n----- text_normalize -----\n----- lower -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 0, which is 0.00% of training set\n-------------------- Text Augmentation --------------------\n----- nlp_aug_stochastic -----\n----- lower -----\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\nmain_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Title', 'Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 18102\n    })\n    validation: Dataset({\n        features: ['Title', 'Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 4526\n    })\n})\n\n\nNote that your DatasetDict must contain tokens besides raw text (which typically includes ‘input_ids’, ‘token_type_ids’, ‘attention_mask’)\n\nnum_classes = 6 # the number of classes\n\n\nseed_everything(42)\n\n\nmodel_name='roberta-base'\n_model = RobertaForSequenceClassification.from_pretrained(model_name,num_labels=num_classes)\n_model = _model.to('cuda:0')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score] \n\n# note that you omit the `data_store` argument\ncontroller = ModelController(_model,seed=42)\n\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 3\n\ncontroller.fit(epochs,lr,\n               ddict=main_ddict, # Put in your tokenized datasetdict here\n               metric_funcs=metric_funcs,\n               label_names='Department Name',\n               head_sizes=num_classes,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics,\n               tokenizer=_tokenizer,\n              )\n\n\n\n    \n      \n      \n      [849/849 02:12, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1 Score Department name\nAccuracy Score Department name\n\n\n\n\n1\nNo log\n0.299611\n0.733846\n0.910738\n\n\n2\n0.414500\n0.257584\n0.748776\n0.920018\n\n\n3\n0.414500\n0.263301\n0.747707\n0.921564\n\n\n\n\n\n\n\n\ncontroller.trainer.model.save_pretrained('./sample_weights/model_progress')",
    "crumbs": [
      "1. Quick Starts",
      "Model Controller Tutorial: Classification"
    ]
  },
  {
    "objectID": "model_classification_tutorial.html#c-make-predictions-using-textdatacontroller",
    "href": "model_classification_tutorial.html#c-make-predictions-using-textdatacontroller",
    "title": "Model Controller Tutorial: Classification",
    "section": "c) Make predictions, using TextDataController",
    "text": "c) Make predictions, using TextDataController\n\nLoad trained model\n\ntrained_model = RobertaForSequenceClassification.from_pretrained('./sample_weights/model_progress',num_labels=6).to('cuda:0')\n\n\ncontroller = ModelController(trained_model,tdc,seed=42)\n\n\n\nPredict Train/Validation set\nMake prediction on all validation set\n\ndf_val = controller.predict_ddict(ds_type='validation')\n\n-------------------- Start making predictions --------------------\n\n\n\n\n\n\n\n\n\ndf_val = df_val.to_pandas()\ndf_val.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nDivision Name\nDepartment Name\nlabel\ninput_ids\nattention_mask\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\n\ngeneral petite . . such a fun jacket ! great t...\ngeneral petite\nIntimate\n2\n[0, 15841, 4716, 1459, 479, 479, 215, 10, 1531...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nJackets\n0.823920\n\n\n1\nsimple and elegant\ngeneral petite . simple and elegant . i though...\ngeneral petite\nTops\n4\n[0, 15841, 4716, 1459, 479, 2007, 8, 14878, 47...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.995652\n\n\n2\nretro and pretty\ngeneral . retro and pretty . this top has a bi...\ngeneral\nTops\n4\n[0, 15841, 479, 11299, 8, 1256, 479, 42, 299, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.995805\n\n\n3\nsummer/fall wear\ngeneral petite . summer / fall wear . i first ...\ngeneral petite\nDresses\n1\n[0, 15841, 4716, 1459, 479, 1035, 1589, 1136, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nDresses\n0.985551\n\n\n4\nperfect except slip\ngeneral petite . perfect except slip . this is...\ngeneral petite\nDresses\n1\n[0, 15841, 4716, 1459, 479, 1969, 4682, 9215, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nDresses\n0.985531\n\n\n\n\n\n\n\n\nYou can try to get your metric to see if it matches your last traing epoch’s above\n\nf1_score(df_val['Department Name'],df_val['pred_Department Name'],average='macro')\n\n0.7485565717033943\n\n\nYou can also make predictions on all training set, by changing argument ds_type to “train”\n\n\nPredict Test set\nWe will go through details on how to make a prediction on a completely new and raw dataset using our trained model. For now, let’s reuse the sample csv and pretend it’s our test set\n\ndf_test = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig').sample(frac=0.2,random_state=1)\n# drop NaN values in the label column\ndf_test = df_test[~df_test['Department Name'].isna()].reset_index(drop=True)\n\n# save the label, as we will calculate some metrics later. We also filter out labels with NaN Review Text,\n# as there will be a filtering processing on the test set\ntrue_labels = df_test.loc[~df_test['Review Text'].isna(),'Department Name'].values \n\n# drop the label (you don't need to, but this is necessary to simulate an actual test set)\ndf_test.drop('Department Name',axis=1,inplace=True)\n\n\ndf_test.shape\n\n(4692, 9)\n\n\n\ndf_test.head(5)\n\n\n\n\n\n\n\n\n\nClothing ID\nAge\nTitle\nReview Text\nRating\nRecommended IND\nPositive Feedback Count\nDivision Name\nClass Name\n\n\n\n\n0\n872\n42\nPerfect for work and play\nThis shirt works for both going out and going ...\n5\n1\n0\nGeneral\nKnits\n\n\n1\n1033\n40\nNaN\nI don't know why i had the opposite problem mo...\n4\n1\n0\nGeneral Petite\nJeans\n\n\n2\n1037\n45\nGreat pants\nThese cords are great--lightweight for fl wint...\n5\n1\n1\nGeneral Petite\nJeans\n\n\n3\n829\n35\nSurprisingly comfy for a button down\nI am a 10 m and got the 10. it fits perfectly ...\n5\n1\n1\nGeneral Petite\nBlouses\n\n\n4\n872\n29\nShort and small\nThe shirt is mostly a thick sweatshirt materia...\n3\n0\n15\nGeneral Petite\nKnits\n\n\n\n\n\n\n\n\nFrom here, you have 2 options\n\nUse TextDataController to process your data, then ModelController’s job is to perform prediction\nConvert your dataframe to a HuggingFace Dataset, and let the ModelController take care of the preprocessing and the prediction\n\nOption 1:\n\n_test_dset_processed = tdc.prepare_test_dataset_from_df(df_test,validate=True,do_filtering=True)\n\n- Input Validation Precheck -\nData contains missing values!\n-----&gt; List of columns and the number of missing values for each\nTitle          758\nReview Text    164\ndtype: int64\nData contains duplicated values!\n-----&gt; Number of duplications: 2 rows\n-------------------- Start Test Set Transformation --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\nDone\n----- Metadata Simple Processing & Concatenating to Main Content -----\nDone\n-------------------- Text Transformation --------------------\n----- text_normalize -----\n----- lower -----\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_test_dset_processed\n\nDataset({\n    features: ['Title', 'Review Text', 'Division Name', 'input_ids', 'attention_mask'],\n    num_rows: 4528\n})\n\n\n\n_test_dset_predicted = controller.predict_ddict(_test_dset_processed)\n\n-------------------- Start making predictions --------------------\n\n\n\n\n\n\n\n\n\ndf_test_predicted = _test_dset_predicted.to_pandas()\ndf_test_predicted.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nDivision Name\ninput_ids\nattention_mask\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\nperfect for work and play\ngeneral . perfect for work and play . this shi...\ngeneral\n[0, 15841, 479, 1969, 13, 173, 8, 310, 479, 42...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.996438\n\n\n1\n\ngeneral petite . . i don't know why i had the ...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 479, 939, 218, 75,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nBottoms\n0.976738\n\n\n2\ngreat pants\ngeneral petite . great pants . thes e cords ar...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 372, 9304, 479, 5,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nBottoms\n0.958788\n\n\n3\nsurprisingly comfy for a button down\ngeneral petite . surprisingly comfy for a butt...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 10262, 3137, 24382...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.994487\n\n\n4\nshort and small\ngeneral petite . short and small . the shirt i...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 765, 8, 650, 479, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.995179\n\n\n\n\n\n\n\n\nOption 2:\n\nfrom datasets import Dataset\n\nIf you want to turn off the info printing, you can do it to the TextDataController (stored as data_store) in the ModelController class\n\ncontroller.data_store.set_verbose(False)\n\n\n_test_dset = Dataset.from_pandas(df_test)\n_test_dset_predicted = controller.predict_raw_dset(_test_dset,\n                                                   do_filtering=True, # since we have some text filtering in the processing\n                                                  )\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_test_predicted = _test_dset_predicted.to_pandas()\n\n\ndf_test_predicted.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nDivision Name\ninput_ids\nattention_mask\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\nperfect for work and play\ngeneral . perfect for work and play . this shi...\ngeneral\n[0, 15841, 479, 1969, 13, 173, 8, 310, 479, 42...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.996438\n\n\n1\n\ngeneral petite . . i don't know why i had the ...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 479, 939, 218, 75,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nBottoms\n0.976738\n\n\n2\ngreat pants\ngeneral petite . great pants . thes e cords ar...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 372, 9304, 479, 5,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nBottoms\n0.958788\n\n\n3\nsurprisingly comfy for a button down\ngeneral petite . surprisingly comfy for a butt...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 10262, 3137, 24382...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.994487\n\n\n4\nshort and small\ngeneral petite . short and small . the shirt i...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 765, 8, 650, 479, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.995179\n\n\n\n\n\n\n\n\nLet’s quickly check the f1 score to make sure everything works correctly\n\nf1_score(true_labels,df_test_predicted['pred_Department Name'],average='macro')\n\n0.759160993145196\n\n\nThis is not too far off from the validation F1 score. Notice that the ‘test set’ is just a sample from the original dataset, not the entire new set\nWe can even predict top k results\n\n_test_dset = Dataset.from_pandas(df_test)\n_test_dset_predicted = controller.predict_raw_dset(_test_dset,\n                                                   do_filtering=True,\n                                                   topk=3\n                                                  )\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_test_predicted = _test_dset_predicted.to_pandas()\n\ndf_test_predicted.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nDivision Name\ninput_ids\nattention_mask\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\nperfect for work and play\ngeneral . perfect for work and play . this shi...\ngeneral\n[0, 15841, 479, 1969, 13, 173, 8, 310, 479, 42...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[Tops, Intimate, Trend]\n[0.9964378, 0.0014704004, 0.00085006363]\n\n\n1\n\ngeneral petite . . i don't know why i had the ...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 479, 939, 218, 75,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[Bottoms, Intimate, Trend]\n[0.97673845, 0.017872315, 0.0033529706]\n\n\n2\ngreat pants\ngeneral petite . great pants . thes e cords ar...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 372, 9304, 479, 5,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[Bottoms, Intimate, Trend]\n[0.95878834, 0.033563487, 0.004869911]\n\n\n3\nsurprisingly comfy for a button down\ngeneral petite . surprisingly comfy for a butt...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 10262, 3137, 24382...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[Tops, Intimate, Jackets]\n[0.994487, 0.0027335314, 0.0009791912]\n\n\n4\nshort and small\ngeneral petite . short and small . the shirt i...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 765, 8, 650, 479, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[Tops, Intimate, Trend]\n[0.9951786, 0.002501535, 0.00096233515]\n\n\n\n\n\n\n\n\nIf we just want to make a prediction on a small amount of data (single sentence, or a few sentences), we can use ModelController.predict_raw_text\n\n# Since we have some metadatas (Title and Division Name), we need to define a dictionary containing those values\nraw_content={'Review Text': 'This shirt is so comfortable I love it!',\n             'Title': 'Great shirt',\n             'Division Name': 'general'}\n\nIf you don’t use metadata, just create a string instead, e.g.\nraw_content='This shirt is so comfortable I love it!'\n\ndf_result = controller.predict_raw_text(raw_content)\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_result\n\n{'Review Text': ['general . great shirt . this shirt is so comfortable i love it !'],\n 'Title': ['great shirt'],\n 'Division Name': ['general'],\n 'input_ids': [[0,\n   15841,\n   479,\n   372,\n   6399,\n   479,\n   42,\n   6399,\n   16,\n   98,\n   3473,\n   939,\n   657,\n   24,\n   27785,\n   2]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n 'pred_Department Name': ['Tops'],\n 'pred_prob_Department Name': [0.996221661567688]}\n\n\n\ndf_result = controller.predict_raw_text(raw_content,topk=3)\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_result\n\n{'Review Text': ['general . great shirt . this shirt is so comfortable i love it !'],\n 'Title': ['great shirt'],\n 'Division Name': ['general'],\n 'input_ids': [[0,\n   15841,\n   479,\n   372,\n   6399,\n   479,\n   42,\n   6399,\n   16,\n   98,\n   3473,\n   939,\n   657,\n   24,\n   27785,\n   2]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n 'pred_Department Name': [['Tops', 'Intimate', 'Trend']],\n 'pred_prob_Department Name': [[0.996221661567688,\n   0.0016704618465155363,\n   0.0008719302131794393]]}",
    "crumbs": [
      "1. Quick Starts",
      "Model Controller Tutorial: Classification"
    ]
  },
  {
    "objectID": "model_classification_tutorial.html#d-make-predictions-using-only-tokenized-datasetdict",
    "href": "model_classification_tutorial.html#d-make-predictions-using-only-tokenized-datasetdict",
    "title": "Model Controller Tutorial: Classification",
    "section": "d) Make predictions, using only Tokenized DatasetDict",
    "text": "d) Make predictions, using only Tokenized DatasetDict\n\nLoad trained model\n\n# Load trained model from section 4.2\ntrained_model = RobertaForSequenceClassification.from_pretrained('./sample_weights/model_progress',num_labels=6).to('cuda:0')\ncontroller = ModelController(trained_model,seed=42)\n\n\n\nPredict Train/Validation set\n\nmain_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Title', 'Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 18102\n    })\n    validation: Dataset({\n        features: ['Title', 'Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 4526\n    })\n})\n\n\nSince we don’t use a TextDataController, we have to define a few arguments to make it work\n\nlabel_names='Department Name'\nnum_classes=6\nclass_predefined = ['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']\n\n\ndf_val = controller.predict_ddict(main_ddict,\n                                 ds_type='validation',\n                                 is_multilabel=False,\n                                 tokenizer=_tokenizer,\n                                 label_names=label_names,\n                                 class_names_predefined=class_predefined\n                                )\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_val = df_val.to_pandas()\ndf_val.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nDivision Name\nDepartment Name\nlabel\ninput_ids\nattention_mask\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\n\ngeneral petite . . such a fun jacket ! great t...\ngeneral petite\nIntimate\n2\n[0, 15841, 4716, 1459, 479, 479, 215, 10, 1531...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nJackets\n0.823920\n\n\n1\nsimple and elegant\ngeneral petite . simple and elegant . i though...\ngeneral petite\nTops\n4\n[0, 15841, 4716, 1459, 479, 2007, 8, 14878, 47...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.995665\n\n\n2\nretro and pretty\ngeneral . retro and pretty . this top has a bi...\ngeneral\nTops\n4\n[0, 15841, 479, 11299, 8, 1256, 479, 42, 299, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.995805\n\n\n3\nsummer/fall wear\ngeneral petite . summer / fall wear . i first ...\ngeneral petite\nDresses\n1\n[0, 15841, 4716, 1459, 479, 1035, 1589, 1136, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nDresses\n0.985551\n\n\n4\nperfect except slip\ngeneral petite . perfect except slip . this is...\ngeneral petite\nDresses\n1\n[0, 15841, 4716, 1459, 479, 1969, 4682, 9215, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nDresses\n0.985531\n\n\n\n\n\n\n\n\n\n\nPredict Test set\n\ndf_test = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig').sample(frac=0.2,random_state=1)\n# drop NaN values in the label column\ndf_test = df_test[~df_test['Department Name'].isna()].reset_index(drop=True)\n\n# save the label, as we will calculate some metrics later. We also filter out labels with NaN Review Text,\n# as there will be a filtering processing on the test set\ntrue_labels = df_test.loc[~df_test['Review Text'].isna(),'Department Name'].values \n\n# drop the label (you don't need to, but this is necessary to simulate an actual test set)\ndf_test.drop('Department Name',axis=1,inplace=True)\n\nSimilarly, you have to have your test dataset that has been preprocessed and tokenized, so that the final dataset should have some or all of these fields: input_ids, token_type_ids, attention_mask. For now we will borrow the previous tdc to do the preprocessing for us.\n\n_test_dset_processed = tdc.prepare_test_dataset_from_df(df_test,validate=True,do_filtering=True)\n\n- Input Validation Precheck -\nData contains missing values!\n-----&gt; List of columns and the number of missing values for each\nTitle          758\nReview Text    164\ndtype: int64\nData contains duplicated values!\n-----&gt; Number of duplications: 2 rows\n\n\n\n_test_dset_processed\n\nDataset({\n    features: ['Title', 'Review Text', 'Division Name', 'input_ids', 'attention_mask'],\n    num_rows: 4528\n})\n\n\nAgain, we are using TextDataController to make this process easier to handle. If you have your own pipeline, feel free to use it to produce the processed test dataset. Also, as this point, all you need in your dataset is either (or all) of these features: input_ids, token_type_ids, attention_mask. You can drop other features if you want, though it’s not required\n\n_test_dset_processed = _test_dset_processed.remove_columns(['Title','Review Text','Division Name'])\n\n\ndf_test_predicted = controller.predict_ddict(_test_dset_processed,\n                                             is_multilabel=False,\n                                             tokenizer=_tokenizer,\n                                             label_names=label_names,\n                                             class_names_predefined=class_predefined\n                                            )\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_test_predicted = df_test_predicted.to_pandas()\ndf_test_predicted.head()\n\n\n\n\n\n\n\n\n\ninput_ids\nattention_mask\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\n[0, 15841, 479, 1969, 13, 173, 8, 310, 479, 42...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.996438\n\n\n1\n[0, 15841, 4716, 1459, 479, 479, 939, 218, 75,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nBottoms\n0.976738\n\n\n2\n[0, 15841, 4716, 1459, 479, 372, 9304, 479, 5,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nBottoms\n0.958788\n\n\n3\n[0, 15841, 4716, 1459, 479, 10262, 3137, 24382...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.994487\n\n\n4\n[0, 15841, 4716, 1459, 479, 765, 8, 650, 479, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.995179",
    "crumbs": [
      "1. Quick Starts",
      "Model Controller Tutorial: Classification"
    ]
  },
  {
    "objectID": "model_classification_tutorial.html#a-define-and-train-a-custom-roberta-model",
    "href": "model_classification_tutorial.html#a-define-and-train-a-custom-roberta-model",
    "title": "Model Controller Tutorial: Classification",
    "section": "a) Define and train a custom Roberta model",
    "text": "a) Define and train a custom Roberta model\n\nnum_classes = len(tdc.label_lists[0])\nnum_classes\n\n6\n\n\nLet’s define a Roberta model (without a head), because we will create our custom classification head\n\nfrom transformers.models.roberta.modeling_roberta import RobertaModel\n\n\nroberta_body = RobertaModel.from_pretrained('roberta-base')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nThen we can define a classification head. One trick we can use to boost the performance of our entire model is to concatenate the outputs of [CLS] from the four last layers of the pre-trained Roberta model (source: https://ieeexplore.ieee.org/document/9335912). We already define such custom head (ConcatHeadSimple), and the necessary architecture to make it work (RobertaHiddenStateConcatForSequenceClassification)\n\n# our model is more complex, so it's best to define some of its arguments\n_model_kwargs={\n    # overall model hyperparams\n    'head_class_sizes':num_classes,\n    'head_class': ConcatHeadSimple,\n    # classfication head hyperparams\n    'layer2concat':2, # you can change the number of layers to concat (default is 4, based on the paper)\n    'classifier_dropout':0.1 \n}\n\nmodel = model_init_classification(model_class = RobertaHiddenStateConcatForSequenceClassification,\n                                  cpoint_path = 'roberta-base', \n                                  output_hidden_states=True, # since we are using 'hidden layer contatenation' technique\n                                  seed=42,\n                                  body_model=roberta_body,\n                                  model_kwargs = _model_kwargs)\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score]\ncontroller = ModelController(model,tdc,seed=42)\n\nLoading body weights. This assumes the body is the very first block of your custom architecture\nTotal parameters: 124654854\nTotal trainable parameters: 124654854\n\n\nAnd we can start training our model\n\nseed_everything(42)\n\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 3\n\ncontroller.fit(epochs,lr,\n               metric_funcs=metric_funcs,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics,\n              )\n\n\n\n    \n      \n      \n      [849/849 02:15, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1 Score Department name\nAccuracy Score Department name\n\n\n\n\n1\nNo log\n0.296447\n0.744318\n0.914936\n\n\n2\n0.428200\n0.258439\n0.752792\n0.922669\n\n\n3\n0.428200\n0.272308\n0.747529\n0.920018",
    "crumbs": [
      "1. Quick Starts",
      "Model Controller Tutorial: Classification"
    ]
  },
  {
    "objectID": "model_classification_tutorial.html#b-make-predictions",
    "href": "model_classification_tutorial.html#b-make-predictions",
    "title": "Model Controller Tutorial: Classification",
    "section": "b) Make predictions",
    "text": "b) Make predictions\nMake prediction on all validation set\n\ndf_val = controller.predict_ddict(ds_type='validation')\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_val = df_val.to_pandas()\ndf_val.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nDivision Name\nDepartment Name\nlabel\ninput_ids\nattention_mask\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\n\ngeneral petite . . such a fun jacket ! great t...\ngeneral petite\nIntimate\n2\n[0, 15841, 4716, 1459, 479, 479, 215, 10, 1531...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nJackets\n0.818531\n\n\n1\nsimple and elegant\ngeneral petite . simple and elegant . i though...\ngeneral petite\nTops\n4\n[0, 15841, 4716, 1459, 479, 2007, 8, 14878, 47...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.997387\n\n\n2\nretro and pretty\ngeneral . retro and pretty . this top has a bi...\ngeneral\nTops\n4\n[0, 15841, 479, 11299, 8, 1256, 479, 42, 299, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.997603\n\n\n3\nsummer/fall wear\ngeneral petite . summer / fall wear . i first ...\ngeneral petite\nDresses\n1\n[0, 15841, 4716, 1459, 479, 1035, 1589, 1136, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nDresses\n0.988438\n\n\n4\nperfect except slip\ngeneral petite . perfect except slip . this is...\ngeneral petite\nDresses\n1\n[0, 15841, 4716, 1459, 479, 1969, 4682, 9215, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nDresses\n0.989150\n\n\n\n\n\n\n\n\nYou can try to get your metric to see if it matches your last traing epoch’s above\n\nf1_score(df_val['Department Name'],df_val['pred_Department Name'],average='macro')\n\n0.7475294947215362\n\n\n\ndf_val = controller.predict_ddict(ds_type='validation',topk=2)\ndf_val = df_val.to_pandas()\ndf_val.head()\n\n-------------------- Start making predictions --------------------\n\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nDivision Name\nDepartment Name\nlabel\ninput_ids\nattention_mask\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\n\ngeneral petite . . such a fun jacket ! great t...\ngeneral petite\nIntimate\n2\n[0, 15841, 4716, 1459, 479, 479, 215, 10, 1531...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[Jackets, Tops]\n[0.8185308, 0.14819466]\n\n\n1\nsimple and elegant\ngeneral petite . simple and elegant . i though...\ngeneral petite\nTops\n4\n[0, 15841, 4716, 1459, 479, 2007, 8, 14878, 47...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[Tops, Intimate]\n[0.9973871, 0.0010666925]\n\n\n2\nretro and pretty\ngeneral . retro and pretty . this top has a bi...\ngeneral\nTops\n4\n[0, 15841, 479, 11299, 8, 1256, 479, 42, 299, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[Tops, Intimate]\n[0.997603, 0.001102674]\n\n\n3\nsummer/fall wear\ngeneral petite . summer / fall wear . i first ...\ngeneral petite\nDresses\n1\n[0, 15841, 4716, 1459, 479, 1035, 1589, 1136, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[Dresses, Trend]\n[0.98843807, 0.005955467]\n\n\n4\nperfect except slip\ngeneral petite . perfect except slip . this is...\ngeneral petite\nDresses\n1\n[0, 15841, 4716, 1459, 479, 1969, 4682, 9215, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[Dresses, Trend]\n[0.9891495, 0.0058816983]",
    "crumbs": [
      "1. Quick Starts",
      "Model Controller Tutorial: Classification"
    ]
  },
  {
    "objectID": "trainers.html",
    "href": "trainers.html",
    "title": "Trainer",
    "section": "",
    "text": "source\n\nget_cosine_restart_class\n\n get_cosine_restart_class (warmup_ratio=0.1, num_cycles=2)\n\n*Class getter for a Trainer that consists of Cosine Restart with Muptiple Cycles LR Scheduler\nSource: https://discuss.huggingface.co/t/how-do-use-lr-scheduler/4046/8*\nThis can be used as a Trainer for any tasks, either supervised (classification/regression/multilabel …) or semi-supervised (language model) on any model, either vanilla or customed\nExample: Training a RobertaHiddenStateConcatForSequenceClassification model with Cosine Restart LR scheduler for 2 cycles with warmup ratio of 0.01\nmodel_name='roberta-base'\nroberta_body = RobertaModel.from_pretrained(model_name)\n\nmodel = model_init_classification(model_class = RobertaHiddenStateConcatForSequenceClassification,\n                                  cpoint_path = model_name, \n                                  output_hidden_states=True,\n                                  seed=seed,\n                                  body_model=roberta_body,\n                                  model_kwargs = _model_kwargs)\ncontroller = ModelController(model,tdc,seed=seed)\n\nmetric_funcs = partial(f1_score,average='macro')\ntrainer_class = get_cosine_restart_class(warmup_ratio=0.01,num_cycles=2)\n\ncontroller.fit(epochs=8,\n               learning_rate=1e-4,\n               batch_size=64,\n               save_checkpoint=False,\n               metric_funcs=metric_funcs,\n               trainer_class=trainer_class,\n               compute_metrics=compute_metrics,\n              )"
  },
  {
    "objectID": "roberta_singlehead.html",
    "href": "roberta_singlehead.html",
    "title": "Roberta model (Custom Single Head)",
    "section": "",
    "text": "In this series, we walk through some of the capability of this library: single-head classification, multi-head classification, multi-label classification, and regression. If you want a more detailed tutorial, check this out\nimport os\n#This will specify a (or a list) of GPUs for training\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom that_nlp_library.text_main import *\nfrom that_nlp_library.utils import seed_everything\nfrom underthesea import text_normalize\nfrom functools import partial\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport nlpaug.augmenter.char as nac\nfrom datasets import load_dataset\nimport random\nfrom transformers import RobertaTokenizer\nfrom datasets import Dataset",
    "crumbs": [
      "2. All Use Cases",
      "a. Roberta-based models for Supervised Learning",
      "Roberta model (Custom Single Head)"
    ]
  },
  {
    "objectID": "roberta_singlehead.html#define-and-train-a-custom-roberta-model",
    "href": "roberta_singlehead.html#define-and-train-a-custom-roberta-model",
    "title": "Roberta model (Custom Single Head)",
    "section": "Define and train a custom Roberta model",
    "text": "Define and train a custom Roberta model\n\nfrom transformers.models.roberta.modeling_roberta import RobertaModel\n\n\nfrom that_nlp_library.models.roberta.classifiers import *\nfrom that_nlp_library.model_main import *\nfrom sklearn.metrics import f1_score, accuracy_score\n\n\nnum_classes = len(tdc.label_lists[0])\n\n\nroberta_body = RobertaModel.from_pretrained('roberta-base')\n\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nThen we can define a classification head. One trick we can use to boost the performance of our entire model is to concatenate the outputs of [CLS] from the four last layers of the pre-trained Roberta model (source: https://ieeexplore.ieee.org/document/9335912). We already define such custom head (ConcatHeadSimple), and the necessary architecture to make it work (RobertaHiddenStateConcatForSequenceClassification)\n\n# our model is more complex, so it's best to define some of its arguments\n_model_kwargs={\n    # overall model hyperparams\n    'head_class_sizes':num_classes,\n    'head_class': ConcatHeadSimple,\n    # classfication head hyperparams\n    'layer2concat':2, # you can change the number of layers to concat (default is 4, based on the paper)\n    'classifier_dropout':0.1 \n}\n\nmodel = model_init_classification(model_class = RobertaHiddenStateConcatForSequenceClassification,\n                                  cpoint_path = 'roberta-base', \n                                  output_hidden_states=True, # since we are using 'hidden layer contatenation' technique\n                                  seed=42,\n                                  body_model=roberta_body,\n                                  model_kwargs = _model_kwargs)\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score]\ncontroller = ModelController(model,tdc,seed=42)\n\nLoading body weights. This assumes the body is the very first block of your custom architecture\nTotal parameters: 124654854\nTotal trainable parameters: 124654854\n\n\nAnd we can start training our model\n\nseed_everything(42)\n\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 3\n\ncontroller.fit(epochs,lr,\n               metric_funcs=metric_funcs,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics,\n              )\n\n\n\n    \n      \n      \n      [849/849 06:17, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1 Score Department name\nAccuracy Score Department name\n\n\n\n\n1\nNo log\n0.305263\n0.744996\n0.916041\n\n\n2\n0.431600\n0.266121\n0.752087\n0.922448\n\n\n3\n0.431600\n0.270825\n0.752354\n0.923111\n\n\n\n\n\n\n\n\ncontroller.trainer.model.save_pretrained('./sample_weights/my_model')",
    "crumbs": [
      "2. All Use Cases",
      "a. Roberta-based models for Supervised Learning",
      "Roberta model (Custom Single Head)"
    ]
  },
  {
    "objectID": "roberta_singlehead.html#make-predictions",
    "href": "roberta_singlehead.html#make-predictions",
    "title": "Roberta model (Custom Single Head)",
    "section": "Make predictions",
    "text": "Make predictions\n\nLoad trained model\n\n_model_kwargs\n\n{'head_class_sizes': 6,\n 'head_class': that_nlp_library.models.roberta.classifiers.ConcatHeadSimple,\n 'layer2concat': 2,\n 'classifier_dropout': 0.1}\n\n\n\ntrained_model = model_init_classification(model_class = RobertaHiddenStateConcatForSequenceClassification,\n                                          cpoint_path = Path('./sample_weights/my_model'), \n                                          output_hidden_states=True,\n                                          seed=42,\n                                          model_kwargs = _model_kwargs)\n\ncontroller = ModelController(trained_model,tdc,seed=42)\n\nSome weights of the model checkpoint at sample_weights/my_model were not used when initializing RobertaHiddenStateConcatForSequenceClassification: ['body_model.pooler.dense.bias', 'body_model.pooler.dense.weight']\n- This IS expected if you are initializing RobertaHiddenStateConcatForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaHiddenStateConcatForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\nTotal parameters: 124064262\nTotal trainable parameters: 124064262\n\n\n\n\nPredict Train/Validation set\n\ndf_val = controller.predict_ddict(ds_type='validation')\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_val = df_val.to_pandas()\ndf_val.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nDivision Name\nDepartment Name\nlabel\ninput_ids\nattention_mask\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\n\ngeneral petite . . such a fun jacket ! great t...\ngeneral petite\nIntimate\n2\n[0, 15841, 4716, 1459, 479, 479, 215, 10, 1531...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nJackets\n0.776409\n\n\n1\nsimple and elegant\ngeneral petite . simple and elegant . i though...\ngeneral petite\nTops\n4\n[0, 15841, 4716, 1459, 479, 2007, 8, 14878, 47...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.997269\n\n\n2\nretro and pretty\ngeneral . retro and pretty . this top has a bi...\ngeneral\nTops\n4\n[0, 15841, 479, 11299, 8, 1256, 479, 42, 299, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.997735\n\n\n3\nsummer/fall wear\ngeneral petite . summer / fall wear . i first ...\ngeneral petite\nDresses\n1\n[0, 15841, 4716, 1459, 479, 1035, 1589, 1136, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nDresses\n0.980090\n\n\n4\nperfect except slip\ngeneral petite . perfect except slip . this is...\ngeneral petite\nDresses\n1\n[0, 15841, 4716, 1459, 479, 1969, 4682, 9215, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nDresses\n0.990936\n\n\n\n\n\n\n\n\nYou can try to get your metric to see if it matches your last traing epoch’s above\n\nf1_score(df_val['Department Name'],df_val['pred_Department Name'],average='macro')\n\n0.7523539516254371\n\n\n\n\nPredict Test set\nWe will go through details on how to make a prediction on a completely new and raw dataset using our trained model. For now, let’s reuse the sample csv and pretend it’s our test set\n\ndf_test = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig').sample(frac=0.2,random_state=1)\n# drop NaN values in the label column\ndf_test = df_test[~df_test['Department Name'].isna()].reset_index(drop=True)\n\n# save the label, as we will calculate some metrics later. We also filter out labels with NaN Review Text,\n# as there will be a filtering processing on the test set\ntrue_labels = df_test.loc[~df_test['Review Text'].isna(),'Department Name'].values \n\n# drop the label (you don't need to, but this is necessary to simulate an actual test set)\ndf_test.drop('Department Name',axis=1,inplace=True)\n\n\n_test_dset = Dataset.from_pandas(df_test)\n_test_dset_predicted = controller.predict_raw_dset(_test_dset,\n                                                   do_filtering=True, # since we have some text filtering in the processing\n                                                  )\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_test_predicted = _test_dset_predicted.to_pandas()\n\n\ndf_test_predicted.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nDivision Name\ninput_ids\nattention_mask\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\nperfect for work and play\ngeneral . perfect for work and play . this shi...\ngeneral\n[0, 15841, 479, 1969, 13, 173, 8, 310, 479, 42...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.997632\n\n\n1\n\ngeneral petite . . i don't know why i had the ...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 479, 939, 218, 75,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nBottoms\n0.993063\n\n\n2\ngreat pants\ngeneral petite . great pants . thes e cords ar...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 372, 9304, 479, 5,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nBottoms\n0.980067\n\n\n3\nsurprisingly comfy for a button down\ngeneral petite . surprisingly comfy for a butt...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 10262, 3137, 24382...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.995013\n\n\n4\nshort and small\ngeneral petite . short and small . the shirt i...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 765, 8, 650, 479, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.997465\n\n\n\n\n\n\n\n\nLet’s quickly check the f1 score to make sure everything works correctly\n\nf1_score(true_labels,df_test_predicted['pred_Department Name'],average='macro')\n# 0.7551497694535967\n\n0.7554695549351907\n\n\nPredict top k results\n\n_test_dset = Dataset.from_pandas(df_test)\n_test_dset_predicted = controller.predict_raw_dset(_test_dset,\n                                                   do_filtering=True,\n                                                   topk=3\n                                                  )\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_test_predicted = _test_dset_predicted.to_pandas()\n\ndf_test_predicted.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nDivision Name\ninput_ids\nattention_mask\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\nperfect for work and play\ngeneral . perfect for work and play . this shi...\ngeneral\n[0, 15841, 479, 1969, 13, 173, 8, 310, 479, 42...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[Tops, Trend, Intimate]\n[0.99763227, 0.0011167374, 0.000746253]\n\n\n1\n\ngeneral petite . . i don't know why i had the ...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 479, 939, 218, 75,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[Bottoms, Intimate, Trend]\n[0.9930628, 0.0033172437, 0.0027576974]\n\n\n2\ngreat pants\ngeneral petite . great pants . thes e cords ar...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 372, 9304, 479, 5,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[Bottoms, Intimate, Trend]\n[0.980067, 0.01673956, 0.0024985557]\n\n\n3\nsurprisingly comfy for a button down\ngeneral petite . surprisingly comfy for a butt...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 10262, 3137, 24382...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[Tops, Intimate, Trend]\n[0.9950134, 0.001822388, 0.00145723]\n\n\n4\nshort and small\ngeneral petite . short and small . the shirt i...\ngeneral petite\n[0, 15841, 4716, 1459, 479, 765, 8, 650, 479, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[Tops, Trend, Intimate]\n[0.997465, 0.001083337, 0.00081098813]\n\n\n\n\n\n\n\n\n\n# Since we have some metadatas (Title and Division Name), we need to define a dictionary containing those values\nraw_content={'Review Text': 'This shirt is so comfortable I love it!',\n             'Title': 'Great shirt',\n             'Division Name': 'general'}\n\n\ndf_result = controller.predict_raw_text(raw_content,topk=3)\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_result\n\n{'Review Text': ['general . great shirt . this shirt is so comfortable i love it !'],\n 'Title': ['great shirt'],\n 'Division Name': ['general'],\n 'input_ids': [[0,\n   15841,\n   479,\n   372,\n   6399,\n   479,\n   42,\n   6399,\n   16,\n   98,\n   3473,\n   939,\n   657,\n   24,\n   27785,\n   2]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n 'pred_Department Name': [['Tops', 'Trend', 'Intimate']],\n 'pred_prob_Department Name': [[0.9976713061332703,\n   0.0011040765093639493,\n   0.0007168611045926809]]}",
    "crumbs": [
      "2. All Use Cases",
      "a. Roberta-based models for Supervised Learning",
      "Roberta model (Custom Single Head)"
    ]
  },
  {
    "objectID": "roberta_multilabel.html",
    "href": "roberta_multilabel.html",
    "title": "Roberta model (Multi-Label)",
    "section": "",
    "text": "In this series, we walk through some of the capability of this library: single-head classification, multi-head classification, multi-label classification, and regression. If you want a more detailed tutorial, check this out\nimport os\n#This will specify a (or a list) of GPUs for training\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom that_nlp_library.text_main import *\nfrom that_nlp_library.utils import seed_everything\nfrom underthesea import text_normalize\nfrom functools import partial\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport nlpaug.augmenter.char as nac\nfrom datasets import load_dataset\nimport random\nfrom transformers import RobertaTokenizer\nfrom datasets import Dataset",
    "crumbs": [
      "2. All Use Cases",
      "a. Roberta-based models for Supervised Learning",
      "Roberta model (Multi-Label)"
    ]
  },
  {
    "objectID": "roberta_multilabel.html#define-and-train-a-custom-roberta-model",
    "href": "roberta_multilabel.html#define-and-train-a-custom-roberta-model",
    "title": "Roberta model (Multi-Label)",
    "section": "Define and train a custom Roberta model",
    "text": "Define and train a custom Roberta model\n\nfrom transformers.models.roberta.modeling_roberta import RobertaModel\n\n\ntdc.label_lists[0]\n\n['Bottoms',\n 'Dresses',\n 'General',\n 'General Petite',\n 'Initmates',\n 'Intimate',\n 'Jackets',\n 'Tops',\n 'Trend']\n\n\n\nnum_classes = len(tdc.label_lists[0])\n\n\nroberta_body = RobertaModel.from_pretrained('roberta-base')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n# our model is more complex, so it's best to define some of its arguments\n_model_kwargs={\n    # overall model hyperparams\n    'head_class_sizes':num_classes,\n    'head_class': ConcatHeadSimple,\n    'is_multilabel':tdc.is_multilabel, # True\n    'is_multihead':tdc.is_multihead, # False\n    # classfication head hyperparams\n    'layer2concat':2, # you can change the number of layers to concat (default is 4, based on the paper)\n    'classifier_dropout':0.1 \n}\n\nmodel = model_init_classification(model_class = RobertaHiddenStateConcatForSequenceClassification,\n                                  cpoint_path = 'roberta-base', \n                                  output_hidden_states=True, # since we are using 'hidden layer contatenation' technique\n                                  seed=42,\n                                  body_model=roberta_body,\n                                  model_kwargs = _model_kwargs)\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score]\ncontroller = ModelController(model,tdc,seed=42)\n\nLoading body weights. This assumes the body is the very first block of your custom architecture\nTotal parameters: 124659465\nTotal trainable parameters: 124659465\n\n\nAnd we can start training our model\n\nseed_everything(42)\n\n\n# you can adjust the `compute_metrics` to perform multi-label with a threshold\n_cmc = partial(compute_metrics,\n               is_multilabel=tdc.is_multilabel,\n               multilabel_threshold=0.55)\n\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 3\n\ncontroller.fit(epochs,lr,\n               metric_funcs=metric_funcs,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=_cmc,\n              )\n\n\n\n    \n      \n      \n      [849/849 06:44, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1 Score Multi Label\nAccuracy Score Multi Label\n\n\n\n\n1\nNo log\n0.242643\n0.574833\n0.402121\n\n\n2\n0.268500\n0.230503\n0.594430\n0.509280\n\n\n3\n0.268500\n0.230640\n0.598910\n0.518118\n\n\n\n\n\n\n\n\ncontroller.trainer.model.save_pretrained('./sample_weights/my_model')",
    "crumbs": [
      "2. All Use Cases",
      "a. Roberta-based models for Supervised Learning",
      "Roberta model (Multi-Label)"
    ]
  },
  {
    "objectID": "roberta_multilabel.html#make-predictions",
    "href": "roberta_multilabel.html#make-predictions",
    "title": "Roberta model (Multi-Label)",
    "section": "Make predictions",
    "text": "Make predictions\n\nLoad trained model\n\n_model_kwargs\n\n{'head_class_sizes': 9,\n 'head_class': that_nlp_library.models.roberta.classifiers.ConcatHeadSimple,\n 'is_multilabel': True,\n 'is_multihead': False,\n 'layer2concat': 2,\n 'classifier_dropout': 0.1}\n\n\n\ntrained_model = model_init_classification(model_class = RobertaHiddenStateConcatForSequenceClassification,\n                                          cpoint_path = Path('./sample_weights/my_model'), \n                                          output_hidden_states=True,\n                                          seed=42,\n                                          model_kwargs = _model_kwargs)\n\ncontroller = ModelController(trained_model,tdc,seed=42)\n\nSome weights of the model checkpoint at sample_weights/my_model were not used when initializing RobertaHiddenStateConcatForSequenceClassification: ['body_model.pooler.dense.bias', 'body_model.pooler.dense.weight']\n- This IS expected if you are initializing RobertaHiddenStateConcatForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaHiddenStateConcatForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\nTotal parameters: 124068873\nTotal trainable parameters: 124068873\n\n\n\n\nPredict Train/Validation set\n\ndf_val = controller.predict_ddict(ds_type='validation')\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_val = df_val.to_pandas()\ndf_val.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nMulti_Label\nlabel\ninput_ids\nattention_mask\npred_Multi_Label\npred_prob_Multi_Label\n\n\n\n\n0\n\n. such a fun jacket ! great to wear in the spr...\n[Intimate, General Petite]\n[0, 0, 0, 1, 0, 1, 0, 0, 0]\n[0, 4, 215, 10, 1531, 8443, 27785, 372, 7, 356...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[General, Jackets]\n[0.0016225622, 0.0026141205, 0.52690035, 0.390...\n\n\n1\nsimple and elegant\nsimple and elegant . i thought this shirt was ...\n[Tops, General Petite]\n[0, 0, 0, 1, 0, 0, 0, 1, 0]\n[0, 41918, 8, 14878, 479, 939, 802, 42, 6399, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[General, Tops]\n[0.00071811187, 0.0011346012, 0.63563234, 0.36...\n\n\n2\nretro and pretty\nretro and pretty . this top has a bit of a ret...\n[Tops, General]\n[0, 0, 1, 0, 0, 0, 0, 1, 0]\n[0, 4903, 1001, 8, 1256, 479, 42, 299, 34, 10,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[General, Tops]\n[0.0008183706, 0.00083204935, 0.6578452, 0.357...\n\n\n3\nsummer/fall wear\nsummer / fall wear . i first spotted this on a...\n[Dresses, General Petite]\n[0, 1, 0, 1, 0, 0, 0, 0, 0]\n[0, 18581, 2089, 1589, 1136, 3568, 479, 939, 7...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[Dresses, General]\n[0.001032995, 0.97645515, 0.55536234, 0.428432...\n\n\n4\nperfect except slip\nperfect except slip . this is my new favorite ...\n[Dresses, General Petite]\n[0, 1, 0, 1, 0, 0, 0, 0, 0]\n[0, 20473, 4682, 9215, 479, 42, 16, 127, 92, 2...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[Dresses, General]\n[0.00093759556, 0.9759572, 0.54164094, 0.42817...\n\n\n\n\n\n\n\n\n\ncontroller.data_store.label_lists\n\n[['Bottoms',\n  'Dresses',\n  'General',\n  'General Petite',\n  'Initmates',\n  'Intimate',\n  'Jackets',\n  'Tops',\n  'Trend']]\n\n\nYou can try to get your metric to see if it matches your last traing epoch’s above\n\ndf_val['pred_prob_Multi_Label'].apply(lambda x: (np.array(x)&gt;0.55).astype(int))\n\n0       [0, 0, 0, 0, 0, 0, 1, 0, 0]\n1       [0, 0, 1, 0, 0, 0, 0, 1, 0]\n2       [0, 0, 1, 0, 0, 0, 0, 1, 0]\n3       [0, 1, 1, 0, 0, 0, 0, 0, 0]\n4       [0, 1, 0, 0, 0, 0, 0, 0, 0]\n                   ...             \n4521    [0, 0, 1, 0, 0, 0, 0, 1, 0]\n4522    [0, 0, 1, 0, 0, 0, 0, 1, 0]\n4523    [0, 1, 1, 0, 0, 0, 0, 0, 0]\n4524    [1, 0, 1, 0, 0, 0, 0, 0, 0]\n4525    [0, 0, 1, 0, 0, 0, 0, 1, 0]\nName: pred_prob_Multi_Label, Length: 4526, dtype: object\n\n\n\nf1_score(df_val['label'].tolist(),\n         df_val['pred_prob_Multi_Label'].apply(lambda x: (np.array(x)&gt;0.55).astype(int)).tolist(),\n         average='macro')\n# 0.6073332283258327\n\n0.5989434710627796\n\n\n\n\nPredict Test set\nWe will go through details on how to make a prediction on a completely new and raw dataset using our trained model. For now, let’s reuse the sample csv and pretend it’s our test set\n\ndf_test = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig').sample(frac=0.2,random_state=1)\n# drop NaN values in the label column\ndf_test = df_test[~df_test['Department Name'].isna()].reset_index(drop=True)\n \n# drop the label (you don't need to, but this is necessary to simulate an actual test set)\ndf_test.drop('Department Name',axis=1,inplace=True)\n\n\n_test_dset = Dataset.from_pandas(df_test)\n_test_dset_predicted = controller.predict_raw_dset(_test_dset,\n                                                   do_filtering=True, # since we have some text filtering in the processing\n                                                   multilabel_threshold=0.55\n                                                  )\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_test_predicted = _test_dset_predicted.to_pandas()\n\n\ndf_test_predicted.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\ninput_ids\nattention_mask\npred_Multi_Label\npred_prob_Multi_Label\n\n\n\n\n0\nperfect for work and play\nperfect for work and play . this shirt works f...\n[0, 20473, 13, 173, 8, 310, 479, 42, 6399, 136...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[General, Tops]\n[0.0008787949, 0.0011617275, 0.6554981, 0.3466...\n\n\n1\n\n. i don't know why i had the opposite problem ...\n[0, 4, 939, 218, 75, 216, 596, 939, 56, 5, 548...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[Bottoms, General]\n[0.981327, 0.0017199836, 0.72738814, 0.2887234...\n\n\n2\ngreat pants\ngreat pants . thes e cords are great--lightwei...\n[0, 12338, 9304, 479, 5, 29, 364, 37687, 32, 3...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[Bottoms, General]\n[0.98825514, 0.0013365657, 0.72356224, 0.28617...\n\n\n3\nsurprisingly comfy for a button down\nsurprisingly comfy for a button down . i am a ...\n[0, 33258, 3137, 24382, 13, 10, 6148, 159, 479...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[General, Tops]\n[0.00110166, 0.0023981999, 0.6438682, 0.361307...\n\n\n4\nshort and small\nshort and small . the shirt is mostly a thick ...\n[0, 20263, 8, 650, 479, 5, 6399, 16, 2260, 10,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[General, Tops]\n[0.00052336225, 0.0006901586, 0.5660622, 0.351...\n\n\n\n\n\n\n\n\n\n# Since we have some metadatas (Title), we need to define a dictionary containing those values\nraw_content={'Review Text': 'This shirt is so comfortable I love it!',\n             'Title': 'Great shirt'}\n\n\ncontroller.data_store.num_proc=1\n\n\ndf_result = controller.predict_raw_text(raw_content,multilabel_threshold=0.55)\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_result\n\n{'Review Text': ['great shirt . this shirt is so comfortable i love it !'],\n 'Title': ['great shirt'],\n 'input_ids': [[0,\n   12338,\n   6399,\n   479,\n   42,\n   6399,\n   16,\n   98,\n   3473,\n   939,\n   657,\n   24,\n   27785,\n   2]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n 'pred_Multi_Label': [['General', 'Tops']],\n 'pred_prob_Multi_Label': [[0.0008708593086339533,\n   0.0008345667738467455,\n   0.63884437084198,\n   0.36591553688049316,\n   0.011160656809806824,\n   0.012419871985912323,\n   0.0008258746238425374,\n   0.9864105582237244,\n   0.002458298346027732]]}",
    "crumbs": [
      "2. All Use Cases",
      "a. Roberta-based models for Supervised Learning",
      "Roberta model (Multi-Label)"
    ]
  },
  {
    "objectID": "text_main_lm.html",
    "href": "text_main_lm.html",
    "title": "Text Main For Language Model",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom importlib.machinery import SourceFileLoader\nfrom datasets import load_dataset\nimport os",
    "crumbs": [
      "5. Text Classes",
      "c. Text Controller For Language Modeling",
      "Text Main For Language Model"
    ]
  },
  {
    "objectID": "text_main_lm.html#class-textdatalmcontroller",
    "href": "text_main_lm.html#class-textdatalmcontroller",
    "title": "Text Main For Language Model",
    "section": "Class TextDataLMController",
    "text": "Class TextDataLMController\n\nsource\n\nTextDataLMController\n\n TextDataLMController (inp, main_text:str, filter_dict={}, metadatas=[],\n                       process_metas=True, metas_sep='.',\n                       content_transformations=[],\n                       val_ratio:int|float|None=0.2, stratify_cols=[],\n                       seed=None, batch_size=1024, num_proc=4,\n                       cols_to_keep=None, verbose=True)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninp\n\n\nHuggingFainpce Dataset or DatasetDict\n\n\nmain_text\nstr\n\nName of the main text column\n\n\nfilter_dict\ndict\n{}\nA dictionary: {feature: filtering_function_for_that_feature}\n\n\nmetadatas\nlist\n[]\nNames of the metadata columns\n\n\nprocess_metas\nbool\nTrue\nWhether to do simple text processing on the chosen metadatas\n\n\nmetas_sep\nstr\n.\nSeparator, for multiple metadatas concatenation\n\n\ncontent_transformations\nlist\n[]\nA list of text transformations\n\n\nval_ratio\nint | float | None\n0.2\nRatio of data for validation set\n\n\nstratify_cols\nlist\n[]\nColumn(s) needed to do stratified shuffle split\n\n\nseed\nNoneType\nNone\nRandom seed\n\n\nbatch_size\nint\n1024\nCPU batch size\n\n\nnum_proc\nint\n4\nNumber of process for multiprocessing\n\n\ncols_to_keep\nNoneType\nNone\nColumns to keep after all processings\n\n\nverbose\nbool\nTrue\nWhether to prdint processing information",
    "crumbs": [
      "5. Text Classes",
      "c. Text Controller For Language Modeling",
      "Text Main For Language Model"
    ]
  },
  {
    "objectID": "text_main_lm.html#load-data-basic-use-case",
    "href": "text_main_lm.html#load-data-basic-use-case",
    "title": "Text Main For Language Model",
    "section": "1. Load data + Basic use case",
    "text": "1. Load data + Basic use case\n\nsource\n\nTextDataController.from_csv\n\n TextDataController.from_csv (file_path, **kwargs)\n\n\nsource\n\n\nTextDataController.from_df\n\n TextDataController.from_df (df, validate=True, **kwargs)\n\nYou can create a TextDataLMController from a csv, pandas DataFrame, or directly from a HuggingFace dataset object. Currently, TextDataLMController is designed for processing text in order to train a language model\nDataset source: https://www.kaggle.com/datasets/kavita5/review_ecommerce\n\nimport pandas as pd\n\n\ndf = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\n\n\ndf.shape\n\n(23486, 10)\n\n\n\ndf.sample(5)\n\n\n\n\n\n\n\n\n\nClothing ID\nAge\nTitle\nReview Text\nRating\nRecommended IND\nPositive Feedback Count\nDivision Name\nDepartment Name\nClass Name\n\n\n\n\n18374\n1077\n43\nNaN\nI love the color, which is eye popping without...\n4\n1\n1\nGeneral\nDresses\nDresses\n\n\n9201\n862\n47\nNaN\nI love this top. so much so that i bought it i...\n5\n1\n9\nGeneral\nTops\nKnits\n\n\n10964\n1083\n36\nGor-geous\nThis dress is absolutely fantastic. beautiful,...\n5\n1\n0\nGeneral\nDresses\nDresses\n\n\n4108\n829\n44\nGreat quality, unique design\nVery unique shirt-- you will get a compliment!...\n5\n1\n1\nGeneral\nTops\nBlouses\n\n\n9892\n860\n70\nNot a wow\nI bought the bronze color which was nice but t...\n1\n0\n0\nGeneral Petite\nTops\nKnits\n\n\n\n\n\n\n\n\nYou can create a TextDataLMController from a dataframe. This also provides a quick input validation check (NaN check and Duplication check)\n\ntdc = TextDataLMController.from_df(df,main_text='Review Text')\n\n- Input Validation Precheck -\nData contains missing values!\n-----&gt; List of columns and the number of missing values for each\nTitle              3810\nReview Text         845\nDivision Name        14\nDepartment Name      14\nClass Name           14\ndtype: int64\nData contains duplicated values!\n-----&gt; Number of duplications: 21 rows\n\n\nYou can also create a TextDataLMController directly from the csv file. The good thing about using HuggingFace Dataset as the main backend is that you can utilize lots of its useful functionality, such as caching\n\ntdc = TextDataLMController.from_csv('sample_data/Womens_Clothing_Reviews.csv',main_text='Review Text')\n\nYou can also create a TextDataLMController from a HuggingFace Dataset\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ndset\n\nDataset({\n    features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'],\n    num_rows: 23486\n})\n\n\n\ntdc = TextDataLMController(dset,main_text='Review Text')\n\nIn the “Input Validation Precheck” above, we notice that our dataset has missing values in the text field and the label field. For now, let’s load the data as a Pandas’ DataFrame, perform some cleaning, and create our TextDataLMController\n\ndf = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\n\n\ndf = df[(~df['Review Text'].isna()) & (~df['Department Name'].isna())].reset_index(drop=True)\n\n\ntdc = TextDataLMController.from_df(df,main_text='Review Text')\n\n- Input Validation Precheck -\nData contains missing values!\n-----&gt; List of columns and the number of missing values for each\nTitle    2966\ndtype: int64\nData contains duplicated values!\n-----&gt; Number of duplications: 1 rows\n\n\nAt this point you can start perform 2 important steps on your data\n\nText preprocessings + Train/Validation Split\nTokenization\n\n\nddict = tdc.do_all_preprocessing(shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 1, which is 0.01% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n\n\n\n\n\n\n\n\n\nddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Review Text'],\n        num_rows: 18100\n    })\n    validation: Dataset({\n        features: ['Review Text'],\n        num_rows: 4526\n    })\n})\n\n\nOur DatasetDict now has two split: train and validation. Note that train split is now IterableDataset, for processing efficiency\n\nddict['train'][:3]\n\n{'Review Text': ['A lovely skirt and i\\'m so glad i found it before the medium sold out! having said that, i expected the medium to run small and that i\\'d have to squeeze into it but having tried it on this evening it\\'s not the case at all. i nice fit. i might even have fitted into a small, which i think is the only size remaining. the skirt is very spain inspired. very flamenco! i love it! i will say that you\\'d need a bit of height to wear this skirt due to the length at the back. i\\'m 5\\'6\" which is tall enough fo',\n  \"The velvet isn't as soft or plush as i thought it would be but these are comfy pants. i won't wear them until next winter, which is fine.\",\n  \"So i almost returned this top without trying it on because i've been binging on tops with thin blue lines but so glad i didn't!! i'm busty like ddd36 and i weigh 170, but i got the 8 and it fits like a glove! perfection!! plus i got it on sale!! so fab!\"]}\n\n\n\nddict['validation'][:3]\n\n{'Review Text': [\"I love these jeans! i really like the way they fit and haven't had problems with them stretching out like other reviewers have.\",\n  'This shirt is so cute alone with jeans or dressed up with nice jewelry, a scarf or cardi. its just the right weight, true to size, drapes nicely and its very flattering. i\"m sorry i didn\\'t order more when i had the chance. its already sold out in the colors and sizes i wanted. excellent quality as usual -- thanks again retailer!',\n  'The colors on these leggings are very nice and the fit was fabulous. the waist is high enough to hold in a slight \"muffin\" top and the control in the fabric is just right. i received several compliments on them and hubby really liked them.']}",
    "crumbs": [
      "5. Text Classes",
      "c. Text Controller For Language Modeling",
      "Text Main For Language Model"
    ]
  },
  {
    "objectID": "text_main_lm.html#filtering",
    "href": "text_main_lm.html#filtering",
    "title": "Text Main For Language Model",
    "section": "2. Filtering",
    "text": "2. Filtering\nThis preprocessing step allow you to filter out certain values of a certain column in your dataset. Let’s say I want to filter out any None value in the column ‘Review Text’\n\ndf = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\ndf[(~df['Review Text'].isna())].isna().sum()\n\nClothing ID                   0\nAge                           0\nTitle                      2966\nReview Text                   0\nRating                        0\nRecommended IND               0\nPositive Feedback Count       0\nDivision Name                13\nDepartment Name              13\nClass Name                   13\ndtype: int64\n\n\nWe will provide a dictionary containing the name of the column and the filtering function to apply on that column. Note that the filtering function will receive an item from the column, and the function should return a boolean\n\ntdc = TextDataLMController.from_df(df,\n                                 main_text='Review Text',\n                                 filter_dict={'Review Text': lambda x: x is not None},\n                                 seed=42\n                                )\n\n- Input Validation Precheck -\nData contains missing values!\n-----&gt; List of columns and the number of missing values for each\nTitle              3810\nReview Text         845\nDivision Name        14\nDepartment Name      14\nClass Name           14\ndtype: int64\nData contains duplicated values!\n-----&gt; Number of duplications: 21 rows\n\n\n\nddict = tdc.do_all_preprocessing(shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 1, which is 0.01% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\nddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Review Text'],\n        num_rows: 18111\n    })\n    validation: Dataset({\n        features: ['Review Text'],\n        num_rows: 4529\n    })\n})\n\n\nLet’s check if we have filtered out all NaN/None value\n\nfor i in ddict['train']['Review Text']:\n    assert i is not None\nfor i in ddict['validation']['Review Text']:\n    assert i is not None\n\nWe can even add multiple filtering functions. Remember from our precheck, there are also None values in ‘Department Name’. While we are at it, let’s filter out any rating that is less than 3 (just to showcase what our filtering can do)\n\ndf.Rating.value_counts()\n\nRating\n5    13131\n4     5077\n3     2871\n2     1565\n1      842\nName: count, dtype: int64\n\n\nNote that TextDataLMController will only keep the text and the metadatas columns; any other column will be dropped. To double-check our result, we need to define the cols_to_keep argument\n\ndf = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\ntdc = TextDataLMController.from_df(df,\n                                   main_text='Review Text',\n                                   filter_dict={'Review Text': lambda x: x is not None,\n                                                'Department Name': lambda x: x is not None,\n                                                'Rating': lambda x: x&gt;=3\n                                               },\n                                   cols_to_keep=['Review Text','Rating','Department Name'],\n                                   seed=42\n                                  )\n\n- Input Validation Precheck -\nData contains missing values!\n-----&gt; List of columns and the number of missing values for each\nTitle              3810\nReview Text         845\nDivision Name        14\nDepartment Name      14\nClass Name           14\ndtype: int64\nData contains duplicated values!\n-----&gt; Number of duplications: 21 rows\n\n\n\nddict = tdc.do_all_preprocessing(shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\n----- Do &lt;lambda&gt; on Department Name -----\n----- Do &lt;lambda&gt; on Rating -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 1, which is 0.01% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor i in ddict['train']['Department Name']:\n    assert i is not None\nfor i in ddict['validation']['Department Name']:\n    assert i is not None\n\nfor i in ddict['train']['Rating']:\n    assert i is not None\nfor i in ddict['validation']['Rating']:\n    assert i &gt;= 3",
    "crumbs": [
      "5. Text Classes",
      "c. Text Controller For Language Modeling",
      "Text Main For Language Model"
    ]
  },
  {
    "objectID": "text_main_lm.html#metadatas-concatenation",
    "href": "text_main_lm.html#metadatas-concatenation",
    "title": "Text Main For Language Model",
    "section": "3. Metadatas concatenation",
    "text": "3. Metadatas concatenation\nIf we think metadatas can be helpful, we can concatenate them into the front of your text, so that our text classification model is aware of it.\nIn this example, Let’s add ‘Title’ as our metadata\n\ndf = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\ntdc = TextDataLMController.from_df(df,\n                                   main_text='Review Text',\n                                   filter_dict={'Review Text': lambda x: x is not None},\n                                   metadatas='Title',\n                                   process_metas=True, # to preprocess the metadata (currently it's just empty space stripping and lowercasing),\n                                   seed=42\n                                  )\n\n- Input Validation Precheck -\nData contains missing values!\n-----&gt; List of columns and the number of missing values for each\nTitle              3810\nReview Text         845\nDivision Name        14\nDepartment Name      14\nClass Name           14\ndtype: int64\nData contains duplicated values!\n-----&gt; Number of duplications: 21 rows\n\n\n\nddict = tdc.do_all_preprocessing(shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\nDone\n----- Metadata Simple Processing & Concatenating to Main Content -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 0, which is 0.00% of training set\n-------------------- Shuffling and flattening train set --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\nddict['train'][:3]\n\n{'Title': ['not flattering on me', '', ''],\n 'Review Text': ['not flattering on me . I ordered this online and was disappointed with the fit when it arrived. i ordered the xs and it was still oversize to the point of being unflattering. i am tall 5\\'9\" about 130 pounds and have a fairly thin torso and look best in cloths that have some shape. if you like a loose fit this might be for you. the material is thicker and warm and comfortable. i would suggest ordering down a size.',\n  \" . So unflattering! really disappointed. made me look 6 month pregnant and i'm a petite size 2.\",\n  ' . This t-shirt does a great job of elevating the basic t-shirt in to one with a touch of flair. i typically wear a medium but luckily read earlier reviews and went with the small.']}\n\n\n\nddict['validation'][:3]\n\n{'Title': ['', '', ''],\n 'Review Text': [\" . This picture doesn't do the skirt justice. i paired it with a creme colored cashmere cowlneck sweater and a silver jeweled belt. it is really pretty and flattering on.\",\n  ' . Easy to wear! cute, comfy...will be a go to for summer.',\n  ' . Nice sweater, just did not look good on me. sorry, going back.']}",
    "crumbs": [
      "5. Text Classes",
      "c. Text Controller For Language Modeling",
      "Text Main For Language Model"
    ]
  },
  {
    "objectID": "text_main_lm.html#content-transformation",
    "href": "text_main_lm.html#content-transformation",
    "title": "Text Main For Language Model",
    "section": "4. Content Transformation",
    "text": "4. Content Transformation\nThis processing allows you to alter the text content in your dataset. You need to define a function that accepts a single string and returns a new, processed string. Note that this transformation will be applied to ALL of your dataset (both train and validation)\nLet’s say we want to normalize our text, because the text might contain some extra spaces between words, or not follow the “single space after a period” rule\n\n_tmp = \"This is a      sentence,which doesn't follow any rule!No single space is provided after period or punctuation marks.    Maybe there are too many spaces!?!   \"\n\n\nfrom underthesea import text_normalize\n\n\ntext_normalize(_tmp)\n\n\"This is a sentence , which doesn't follow any rule ! No single space is provided after period or punctuation marks . Maybe there are too many spaces ! ? !\"\n\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataLMController(dset,\n                         main_text='Review Text',\n                         filter_dict={'Review Text': lambda x: x is not None},\n                         content_transformations=text_normalize,\n                         seed=42\n                        )\n\n\nddict = tdc.do_all_preprocessing(shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\nDone\n-------------------- Text Transformation --------------------\n----- text_normalize -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 1, which is 0.01% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n\n\n\n\n\n\n\n\n\nddict['train']['Review Text'][0]\n\n'I ordered this online and was disappointed with the fit when it arrived . i ordered the xs and it was still oversize to the point of being unflattering . i am tall 5 \\' 9 \" about 130 pounds and have a fairly thin torso and look best in cloths that have some shape . if you like a loose fit this might be for you . the material is thicker and warm and comfortable . i would suggest ordering down a size .'\n\n\n\nddict['validation']['Review Text'][0]\n\n\"This picture doesn't do the skirt justice . i paired it with a creme colored cashmere cowlneck sweater and a silver jeweled belt . it is really pretty and flattering on .\"\n\n\nYou can chain multiple functions. Let’s say after text normalizing, I want to lowercase the text\n\nstr.lower('tHis IS NoT lowerCASE')\n\n'this is not lowercase'\n\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataLMController(dset,\n                         main_text='Review Text',\n                         filter_dict={'Review Text': lambda x: x is not None},\n                         content_transformations=[text_normalize,str.lower],\n                         seed=42\n                        )\n\n\nddict = tdc.do_all_preprocessing(shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\nDone\n-------------------- Text Transformation --------------------\n----- text_normalize -----\n----- lower -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 1, which is 0.01% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n\n\n\n\n\n\n\n\n\nddict['train']['Review Text'][0]\n\n'i ordered this online and was disappointed with the fit when it arrived . i ordered the xs and it was still oversize to the point of being unflattering . i am tall 5 \\' 9 \" about 130 pounds and have a fairly thin torso and look best in cloths that have some shape . if you like a loose fit this might be for you . the material is thicker and warm and comfortable . i would suggest ordering down a size .'\n\n\n\nddict['validation']['Review Text'][0]\n\n\"this picture doesn't do the skirt justice . i paired it with a creme colored cashmere cowlneck sweater and a silver jeweled belt . it is really pretty and flattering on .\"",
    "crumbs": [
      "5. Text Classes",
      "c. Text Controller For Language Modeling",
      "Text Main For Language Model"
    ]
  },
  {
    "objectID": "text_main_lm.html#trainvalidation-split",
    "href": "text_main_lm.html#trainvalidation-split",
    "title": "Text Main For Language Model",
    "section": "5. Train/Validation Split",
    "text": "5. Train/Validation Split\nThere are several ways to perform a train/validation split with TextDataLMController\nThe first way is when you already have a validation split in your HuggingFace’s Dataset. Let’s use the Dataset built-in function train_test_split to simulate this\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\nddict_with_val = dset.train_test_split(test_size=0.1)\n# This will create a 'test' split instead of 'validation', so we will process a bit to have a validation split\nddict_with_val['validation']=ddict_with_val['test']\ndel ddict_with_val['test']\n\n\nddict_with_val\n\nDatasetDict({\n    train: Dataset({\n        features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'],\n        num_rows: 21137\n    })\n    validation: Dataset({\n        features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'],\n        num_rows: 2349\n    })\n})\n\n\n\ntdc = TextDataLMController(ddict_with_val,\n                         main_text='Review Text',\n                         filter_dict={'Review Text': lambda x: x is not None},\n                         seed=42\n                        )\nddict = tdc.do_all_preprocessing(shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\nDone\n-------------------- Train Test Split --------------------\nValidation split already exists\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 0, which is 0.00% of training set\n-------------------- Shuffling and flattening train set --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\nddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Review Text'],\n        num_rows: 20368\n    })\n    validation: Dataset({\n        features: ['Review Text'],\n        num_rows: 2273\n    })\n})\n\n\nA second way is to split randomly based on a ratio (a float between 0 and 1), or based on the number of data in your validation set\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataLMController(dset,\n                         main_text='Review Text',\n                         filter_dict={'Review Text': lambda x: x is not None},\n                         val_ratio=0.15,\n                         seed=42,\n                         verbose=False\n                        )\nddict = tdc.do_all_preprocessing(shuffle_trn=True)\nddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Review Text'],\n        num_rows: 19243\n    })\n    validation: Dataset({\n        features: ['Review Text'],\n        num_rows: 3397\n    })\n})\n\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataLMController(dset,\n                         main_text='Review Text',\n                         filter_dict={'Review Text': lambda x: x is not None},\n                         val_ratio=5000,\n                         seed=42,\n                         verbose=False\n                        )\nddict = tdc.do_all_preprocessing(shuffle_trn=True)\nddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Review Text'],\n        num_rows: 17640\n    })\n    validation: Dataset({\n        features: ['Review Text'],\n        num_rows: 5000\n    })\n})\n\n\nA third way is to do a random stratified split (inspired by sklearn’s). Let’s do a stratified split based on our label ‘Department Name’\n\ndf = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\n\n\ndf['Department Name'].value_counts(normalize=True)\n\nDepartment Name\nTops        0.445978\nDresses     0.269214\nBottoms     0.161852\nIntimate    0.073918\nJackets     0.043967\nTrend       0.005070\nName: proportion, dtype: float64\n\n\n\ntdc = TextDataLMController.from_df(df,\n                                 main_text='Review Text',\n                                 filter_dict={'Review Text': lambda x: x is not None,\n                                              'Department Name': lambda x: x is not None,\n                                             },\n                                 val_ratio=0.2,\n                                 stratify_cols='Department Name',\n                                 cols_to_keep=['Review Text','Department Name'],\n                                 seed=42\n                                )\nddict = tdc.do_all_preprocessing(shuffle_trn=True)\nddict\n\n- Input Validation Precheck -\nData contains missing values!\n-----&gt; List of columns and the number of missing values for each\nTitle              3810\nReview Text         845\nDivision Name        14\nDepartment Name      14\nClass Name           14\ndtype: int64\nData contains duplicated values!\n-----&gt; Number of duplications: 21 rows\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\n----- Do &lt;lambda&gt; on Department Name -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio, with stratifying\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 2, which is 0.01% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['Review Text', 'Department Name'],\n        num_rows: 18100\n    })\n    validation: Dataset({\n        features: ['Review Text', 'Department Name'],\n        num_rows: 4526\n    })\n})\n\n\n\npd.Series(ddict['train']['Department Name']).value_counts(normalize=True)\n\nTops        0.444033\nDresses     0.271602\nBottoms     0.161878\nIntimate    0.072983\nJackets     0.044309\nTrend       0.005193\nName: proportion, dtype: float64\n\n\n\npd.Series(ddict['validation']['Department Name']).value_counts(normalize=True)\n\nTops        0.444101\nDresses     0.271542\nBottoms     0.161732\nIntimate    0.073133\nJackets     0.044189\nTrend       0.005303\nName: proportion, dtype: float64\n\n\nYou can also use multiple columns for your stratification\n\ntdc = TextDataLMController.from_df(df,\n                                 main_text='Review Text',\n                                 filter_dict={'Review Text': lambda x: x is not None,\n                                              'Department Name': lambda x: x is not None,\n                                             },\n                                 val_ratio=0.2,\n                                 stratify_cols=['Department Name','Rating'],\n                                 cols_to_keep=['Review Text','Department Name','Rating'],\n                                 seed=42,\n                                 verbose=False\n                                )\nddict = tdc.do_all_preprocessing(shuffle_trn=True)\nddict\n\n- Input Validation Precheck -\nData contains missing values!\n-----&gt; List of columns and the number of missing values for each\nTitle              3810\nReview Text         845\nDivision Name        14\nDepartment Name      14\nClass Name           14\ndtype: int64\nData contains duplicated values!\n-----&gt; Number of duplications: 21 rows\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['Review Text', 'Rating', 'Department Name'],\n        num_rows: 18100\n    })\n    validation: Dataset({\n        features: ['Review Text', 'Rating', 'Department Name'],\n        num_rows: 4526\n    })\n})\n\n\nAnd finally, you can omit any validation split if you specify val_ratio as None\n\ntdc = TextDataLMController.from_df(df,\n                                 main_text='Review Text',\n                                 filter_dict={'Review Text': lambda x: x is not None},\n                                 val_ratio=None,\n                                 seed=42\n                                )\nddict = tdc.do_all_preprocessing(shuffle_trn=True)\nddict\n\n- Input Validation Precheck -\nData contains missing values!\n-----&gt; List of columns and the number of missing values for each\nTitle              3810\nReview Text         845\nDivision Name        14\nDepartment Name      14\nClass Name           14\ndtype: int64\nData contains duplicated values!\n-----&gt; Number of duplications: 21 rows\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\nDone\n-------------------- Train Test Split --------------------\nNo validation split defined\nDone\n-------------------- Dropping unused features --------------------\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n\n\n\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['Review Text'],\n        num_rows: 22641\n    })\n})",
    "crumbs": [
      "5. Text Classes",
      "c. Text Controller For Language Modeling",
      "Text Main For Language Model"
    ]
  },
  {
    "objectID": "text_main_lm.html#tokenization",
    "href": "text_main_lm.html#tokenization",
    "title": "Text Main For Language Model",
    "section": "6. Tokenization",
    "text": "6. Tokenization\nDefine our tokenization\n\nfrom transformers import RobertaTokenizer\nfrom underthesea import text_normalize\n\n\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\nsource\n\nTextDataLMController.process_and_tokenize\n\n TextDataLMController.process_and_tokenize (tokenizer, max_length=None,\n                                            line_by_line=True,\n                                            stride=None, trn_size=None,\n                                            tok_num_proc=None,\n                                            shuffle_trn=True,\n                                            check_val_leak=True)\n\nThis will perform do_all_processing then do_tokenization\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntokenizer\n\n\nTokenizer (preferably from HuggingFace)\n\n\nmax_length\nNoneType\nNone\npad to model’s allowed max length (default is max_sequence_length)\n\n\nline_by_line\nbool\nTrue\nTo whether tokenize each sentence separately, or concatenate them and then tokenize\n\n\nstride\nNoneType\nNone\noption to do striding when line_by_line is False\n\n\ntrn_size\nNoneType\nNone\nThe number of training data to be tokenized\n\n\ntok_num_proc\nNoneType\nNone\nNumber of processes for tokenization\n\n\nshuffle_trn\nbool\nTrue\nTo shuffle the train set before tokenization\n\n\ncheck_val_leak\nbool\nTrue\nTo check (and remove) training data which is leaked to validation set\n\n\n\n\n\na) Option 1: Tokenize our corpus line-by-line\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataLMController(dset,\n                         main_text='Review Text',\n                         filter_dict={'Review Text': lambda x: x is not None},\n                         content_transformations=[text_normalize,str.lower],\n                         cols_to_keep=['Clothing ID','Review Text'],\n                         seed=42,\n                         verbose=False\n                        )\n\nWith no padding\n\ntdc.process_and_tokenize(tokenizer,line_by_line=True,max_length=-1)\n\n\ntdc.main_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Clothing ID', 'Review Text', 'input_ids', 'special_tokens_mask', 'attention_mask'],\n        num_rows: 18111\n    })\n    validation: Dataset({\n        features: ['Clothing ID', 'Review Text', 'input_ids', 'special_tokens_mask', 'attention_mask'],\n        num_rows: 4529\n    })\n})\n\n\n\nprint(tdc.main_ddict['train']['Review Text'][0])\nprint(tdc.main_ddict['validation']['Review Text'][0])\n\ni ordered this online and was disappointed with the fit when it arrived . i ordered the xs and it was still oversize to the point of being unflattering . i am tall 5 ' 9 \" about 130 pounds and have a fairly thin torso and look best in cloths that have some shape . if you like a loose fit this might be for you . the material is thicker and warm and comfortable . i would suggest ordering down a size .\nthis picture doesn't do the skirt justice . i paired it with a creme colored cashmere cowlneck sweater and a silver jeweled belt . it is really pretty and flattering on .\n\n\n\nprint(tokenizer.decode(tdc.main_ddict['train']['input_ids'][0]))\nprint(tokenizer.decode(tdc.main_ddict['validation']['input_ids'][0]))\n\n&lt;s&gt;i ordered this online and was disappointed with the fit when it arrived. i ordered the xs and it was still oversize to the point of being unflattering. i am tall 5'9 \" about 130 pounds and have a fairly thin torso and look best in cloths that have some shape. if you like a loose fit this might be for you. the material is thicker and warm and comfortable. i would suggest ordering down a size.&lt;/s&gt;\n&lt;s&gt;this picture doesn't do the skirt justice. i paired it with a creme colored cashmere cowlneck sweater and a silver jeweled belt. it is really pretty and flattering on.&lt;/s&gt;\n\n\nWith padding (set max_length to None if you want to pad to model’s maximum sequence length)\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataLMController(dset,\n                         main_text='Review Text',\n                         filter_dict={'Review Text': lambda x: x is not None},\n                         content_transformations=[text_normalize,str.lower],\n                         cols_to_keep=['Clothing ID','Review Text'],\n                         seed=42,\n                         verbose=False\n                        )\n\n\ntdc.process_and_tokenize(tokenizer,line_by_line=True,max_length=100)\n\n\nprint(tokenizer.decode(tdc.main_ddict['train']['input_ids'][0]))\nprint(tokenizer.decode(tdc.main_ddict['validation']['input_ids'][0]))\n\n&lt;s&gt;i ordered this online and was disappointed with the fit when it arrived. i ordered the xs and it was still oversize to the point of being unflattering. i am tall 5'9 \" about 130 pounds and have a fairly thin torso and look best in cloths that have some shape. if you like a loose fit this might be for you. the material is thicker and warm and comfortable. i would suggest ordering down a size.&lt;/s&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;\n&lt;s&gt;this picture doesn't do the skirt justice. i paired it with a creme colored cashmere cowlneck sweater and a silver jeweled belt. it is really pretty and flattering on.&lt;/s&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;\n\n\n\n\nb) Option 2: Tokenize every text, then concatenate them together before splitting them in smaller parts.\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataLMController(dset,\n                         main_text='Review Text',\n                         filter_dict={'Review Text': lambda x: x is not None},\n                         content_transformations=[text_normalize,str.lower],\n                         cols_to_keep=['Clothing ID','Review Text'],\n                         seed=42,\n                         verbose=False,\n                        )\n\n\ntdc.process_and_tokenize(tokenizer,line_by_line=False,max_length=100)\n\n\ntdc.main_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'special_tokens_mask', 'attention_mask'],\n        num_rows: 13573\n    })\n    validation: Dataset({\n        features: ['input_ids', 'special_tokens_mask', 'attention_mask'],\n        num_rows: 3446\n    })\n})\n\n\nNotice that even when I put in the cols_to_keep parameters, the returned DatasetDict still does not keep them, because it wouldn’t make sense to retain them when the tokens are concatenated. Normally, you will leave cols_to_keep as None, which is the default, when line_by_line is False\n\nfor i in tdc.main_ddict['train']['input_ids'][:3]:\n    print(tokenizer.decode(i))\n    print('-'*100)\n\n&lt;s&gt;i ordered this online and was disappointed with the fit when it arrived. i ordered the xs and it was still oversize to the point of being unflattering. i am tall 5'9 \" about 130 pounds and have a fairly thin torso and look best in cloths that have some shape. if you like a loose fit this might be for you. the material is thicker and warm and comfortable. i would suggest ordering down a size.&lt;/s&gt;&lt;s&gt;so unflattering! really disappointed. made\n----------------------------------------------------------------------------------------------------\n me look 6 month pregnant and i'm a petite size 2.&lt;/s&gt;&lt;s&gt;i love rompers and this one is really cute. i usually wear size 12 but should have got a 10, it runs big. it seems too long, and i'm 5'9 \". the prints cute but a little blah. i paid $ 158 which is too much, since i haven't worn it yet, i should have waited for it to go on sale.&lt;/s&gt;&lt;s&gt;... the print is so\n----------------------------------------------------------------------------------------------------\n sharking, and i love the way it looks on the model -- but i'm a more curvy figure, and the boxy-ish cut plus rather stuff fabric in front is incredibly unflattering. ordinarily i love everything made by maeve, but this one sadly must be returned... on a thinner / straighter-shaped person i expect it would be great.&lt;/s&gt;&lt;s&gt;i've had my eye on this poncho for weeks and finally scored the olive green one over thanksgiving /\n----------------------------------------------------------------------------------------------------\n\n\n\nfor i in tdc.main_ddict['validation']['input_ids'][:3]:\n    print(tokenizer.decode(i))\n    print('-'*100)\n\n&lt;s&gt;this picture doesn't do the skirt justice. i paired it with a creme colored cashmere cowlneck sweater and a silver jeweled belt. it is really pretty and flattering on.&lt;/s&gt;&lt;s&gt;easy to wear! cute, comfy... will be a go to for summer.&lt;/s&gt;&lt;s&gt;nice sweater, just did not look good on me. sorry, going back.&lt;/s&gt;&lt;s&gt;this jacket was a little shorter than i had expected, but i still really enjoy the cut and fit of it\n----------------------------------------------------------------------------------------------------\n.&lt;/s&gt;&lt;s&gt;i wasn't planning on loving this dress when i tried it on. i loved the the color which is what prompted me to buy it. this dress fit perfectly. it hugs my body without feeling tight. the ruching is perfect. i didn't want to take it off! it's also very comfortable. i'm 5'1 \", 107 lbs and the xs petite fit perfectly. the dress hits me at the same length that is pictured. i think it would\n----------------------------------------------------------------------------------------------------\n be easy to hem if you wanted it to be shorter. i have a short torso and saw no issues with that as some reviewer&lt;/s&gt;&lt;s&gt;i like flowy tops because i have a bit of a belly and i like to camouflage it but this top was really flowy. the fabric is great and the embroidery is beautiful, i was hoping for this to be a holiday staple this year. it has to go back though, just too large. i don't love it quite enough to order\n----------------------------------------------------------------------------------------------------\n\n\n\n\nc) Striding (For Concatenation of tokens)\nIf your sentences (or paragraphs) are larger than max_length, after concatenation, they will be broken apart; your long paragraph will be incompleted in terms of meaning. Striding is a way to somewhat preserve the sentence’s meaning, by getting part of the sentence back. We will demonstrate it with an example, and you can compare it with the previous one (without striding) to see the differences\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataLMController(dset,\n                         main_text='Review Text',\n                         filter_dict={'Review Text': lambda x: x is not None},\n                         content_transformations=[text_normalize,str.lower],\n                         seed=42,\n                         verbose=False,\n                        )\n\n\ntdc.process_and_tokenize(tokenizer,line_by_line=False,max_length=100,stride=20)\n# Stride is 20, meaning for the next entry, we go back 20 tokens\n\n\nfor i in tdc.main_ddict['train']['input_ids'][:3]:\n    print(tokenizer.decode(i))\n    print('-'*100)\n\n&lt;s&gt;i ordered this online and was disappointed with the fit when it arrived. i ordered the xs and it was still oversize to the point of being unflattering. i am tall 5'9 \" about 130 pounds and have a fairly thin torso and look best in cloths that have some shape. if you like a loose fit this might be for you. the material is thicker and warm and comfortable. i would suggest ordering down a size.&lt;/s&gt;&lt;s&gt;so unflattering! really disappointed. made\n----------------------------------------------------------------------------------------------------\n comfortable. i would suggest ordering down a size.&lt;/s&gt;&lt;s&gt;so unflattering! really disappointed. made me look 6 month pregnant and i'm a petite size 2.&lt;/s&gt;&lt;s&gt;i love rompers and this one is really cute. i usually wear size 12 but should have got a 10, it runs big. it seems too long, and i'm 5'9 \". the prints cute but a little blah. i paid $ 158 which is too much, since i haven't worn it\n----------------------------------------------------------------------------------------------------\n but a little blah. i paid $ 158 which is too much, since i haven't worn it yet, i should have waited for it to go on sale.&lt;/s&gt;&lt;s&gt;... the print is so sharking, and i love the way it looks on the model -- but i'm a more curvy figure, and the boxy-ish cut plus rather stuff fabric in front is incredibly unflattering. ordinarily i love everything made by maeve, but this one sadly must be returned... on\n----------------------------------------------------------------------------------------------------\n\n\nFor the second entry, we can see it starts with the last 20 tokens of the previous entry: comfortable. i would suggest ordering down a size.&lt;/s&gt;&lt;s&gt;so unflattering! really disappointed. made)\n\nfor i in tdc.main_ddict['validation']['input_ids'][:3]:\n    print(tokenizer.decode(i))\n    print('-'*100)\n\n&lt;s&gt;this picture doesn't do the skirt justice. i paired it with a creme colored cashmere cowlneck sweater and a silver jeweled belt. it is really pretty and flattering on.&lt;/s&gt;&lt;s&gt;easy to wear! cute, comfy... will be a go to for summer.&lt;/s&gt;&lt;s&gt;nice sweater, just did not look good on me. sorry, going back.&lt;/s&gt;&lt;s&gt;this jacket was a little shorter than i had expected, but i still really enjoy the cut and fit of it\n----------------------------------------------------------------------------------------------------\n was a little shorter than i had expected, but i still really enjoy the cut and fit of it.&lt;/s&gt;&lt;s&gt;i wasn't planning on loving this dress when i tried it on. i loved the the color which is what prompted me to buy it. this dress fit perfectly. it hugs my body without feeling tight. the ruching is perfect. i didn't want to take it off! it's also very comfortable. i'm 5'1 \", 107 lbs and the xs pet\n----------------------------------------------------------------------------------------------------\n it's also very comfortable. i'm 5'1 \", 107 lbs and the xs petite fit perfectly. the dress hits me at the same length that is pictured. i think it would be easy to hem if you wanted it to be shorter. i have a short torso and saw no issues with that as some reviewer&lt;/s&gt;&lt;s&gt;i like flowy tops because i have a bit of a belly and i like to camouflage it but this top was really flowy. the fabric is great and\n----------------------------------------------------------------------------------------------------",
    "crumbs": [
      "5. Text Classes",
      "c. Text Controller For Language Modeling",
      "Text Main For Language Model"
    ]
  },
  {
    "objectID": "text_main_lm.html#data-collator",
    "href": "text_main_lm.html#data-collator",
    "title": "Text Main For Language Model",
    "section": "7. Data Collator",
    "text": "7. Data Collator\n\nfrom underthesea import text_normalize\nfrom transformers import AutoTokenizer\n\n\na) For masked language model\n\ntokenizer = AutoTokenizer.from_pretrained('roberta-base')\n\nLet’s define our text controller first\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataLMController(dset,\n                         main_text='Review Text',\n                         filter_dict={'Review Text': lambda x: x is not None},\n                         content_transformations=[text_normalize,str.lower],\n                         cols_to_keep=['Clothing ID','Review Text'],\n                         seed=42,\n                         verbose=False\n                        )\n\nWe will tokenize our corpus line-by-line\n\ntdc.process_and_tokenize(tokenizer,line_by_line=True,max_length=-1)\n\n\ntdc.set_data_collator(is_mlm=True,mlm_prob=0.15)\n\n\ntdc.data_collator\n\nDataCollatorForLanguageModeling(tokenizer=RobertaTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '&lt;s&gt;', 'eos_token': '&lt;/s&gt;', 'unk_token': '&lt;unk&gt;', 'sep_token': '&lt;/s&gt;', 'pad_token': '&lt;pad&gt;', 'cls_token': '&lt;s&gt;', 'mask_token': '&lt;mask&gt;'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n    0: AddedToken(\"&lt;s&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n    1: AddedToken(\"&lt;pad&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n    2: AddedToken(\"&lt;/s&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n    3: AddedToken(\"&lt;unk&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n    50264: AddedToken(\"&lt;mask&gt;\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n}, mlm=True, mlm_probability=0.15, pad_to_multiple_of=8, tf_experimental_compile=False, return_tensors='pt')\n\n\nBefore applying the collator…\n\nprint([tdc.main_ddict['train'][i] for i in range(2)])\n\n[{'Clothing ID': 937, 'Review Text': 'i ordered this online and was disappointed with the fit when it arrived . i ordered the xs and it was still oversize to the point of being unflattering . i am tall 5 \\' 9 \" about 130 pounds and have a fairly thin torso and look best in cloths that have some shape . if you like a loose fit this might be for you . the material is thicker and warm and comfortable . i would suggest ordering down a size .', 'input_ids': [0, 118, 2740, 42, 804, 8, 21, 5779, 19, 5, 2564, 77, 24, 2035, 479, 939, 2740, 5, 3023, 29, 8, 24, 21, 202, 81, 10799, 7, 5, 477, 9, 145, 29747, 24203, 479, 939, 524, 6764, 195, 128, 361, 22, 59, 8325, 2697, 8, 33, 10, 5342, 7174, 28762, 8, 356, 275, 11, 21543, 29, 14, 33, 103, 3989, 479, 114, 47, 101, 10, 7082, 2564, 42, 429, 28, 13, 47, 479, 5, 1468, 16, 33997, 8, 3279, 8, 3473, 479, 939, 74, 3608, 12926, 159, 10, 1836, 479, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}, {'Clothing ID': 870, 'Review Text': \"so unflattering ! really disappointed . made me look 6 month pregnant and i'm a petite size 2 .\", 'input_ids': [0, 2527, 29747, 24203, 27785, 269, 5779, 479, 156, 162, 356, 231, 353, 5283, 8, 939, 437, 10, 4716, 1459, 1836, 132, 479, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}]\n\n\nWe can see that the length of each token list is different from each other\n\nlist(map(len,tdc.main_ddict['train']['input_ids'][:5]))\n\n[91, 24, 79, 82, 121]\n\n\nLet’s apply the collator\n\n# extract only the required keys\ninp_keys = tokenizer.model_input_names\n_inp = [{k:tdc.main_ddict['train'][i][k] for k in inp_keys} for i in range(5)]\n\n\nprint(_inp[:2])\n\n[{'input_ids': [0, 118, 2740, 42, 804, 8, 21, 5779, 19, 5, 2564, 77, 24, 2035, 479, 939, 2740, 5, 3023, 29, 8, 24, 21, 202, 81, 10799, 7, 5, 477, 9, 145, 29747, 24203, 479, 939, 524, 6764, 195, 128, 361, 22, 59, 8325, 2697, 8, 33, 10, 5342, 7174, 28762, 8, 356, 275, 11, 21543, 29, 14, 33, 103, 3989, 479, 114, 47, 101, 10, 7082, 2564, 42, 429, 28, 13, 47, 479, 5, 1468, 16, 33997, 8, 3279, 8, 3473, 479, 939, 74, 3608, 12926, 159, 10, 1836, 479, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {'input_ids': [0, 2527, 29747, 24203, 27785, 269, 5779, 479, 156, 162, 356, 231, 353, 5283, 8, 939, 437, 10, 4716, 1459, 1836, 132, 479, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}]\n\n\n\nout = tdc.data_collator(_inp) # simulation with batch size 5\n\n\nout.keys()\n\ndict_keys(['input_ids', 'attention_mask', 'labels'])\n\n\nNow all token lists have the same length, which is 128: a multiple of 8 and larger than the longest list in the batch (which is 121)\n\nout['input_ids'].shape\n\ntorch.Size([5, 128])\n\n\n\nout['input_ids'][:2,:]\n\ntensor([[    0,   118,  2740,    42,   804,     8,    21,  5779,    19, 50264,\n          2564,    77,    24,  2035,   479,   939,  2740,     5,  3023,    29,\n             8,    24,    21,   202, 50264, 10799,     7,     5,   477, 50264,\n           145, 50264, 24203,   479,   939,   524,  6764,   195,   128,   361,\n            22,    59,  8325,  2697,     8,    33,    10,  5342,  7174, 28762,\n         50264,   356, 50264,    11, 21543,    29,    14,    33,   103, 38941,\n           479,   114,    47,   101,    10,  7082,  2564,    42,   429,    28,\n            13,    47,   479, 50264,  1468, 44089, 33997,     8,  3279,     8,\n          3473,   479,   939,    74,  3608, 12926,   159, 50264,  1836,   479,\n             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1],\n        [    0,  2527, 29747, 24203, 50264,   269,  5779,   479, 50264, 50264,\n           356,   231,   353, 50264,     8, 50264,   437,    10,  4716,  1459,\n          1836, 49943,   479,     2,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1]])\n\n\nThe labels have also been constructed, which shows the “mask” tokens (non -100) in which the model has to predict. To increase the amount of masked tokens, increase the mlm_prob\n\nout['labels'][:2,:]\n\ntensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,     5,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,    81,  -100,  -100,  -100,  -100,     9,\n          -100, 29747,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n             8,  -100,   275,  -100,  -100,  -100,  -100,  -100,  -100,  3989,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,     5,  1468,    16,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,    10,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n        [ -100,  -100,  -100,  -100, 27785,  -100,  -100,  -100,   156,   162,\n          -100,  -100,  -100,  5283,  -100,   939,  -100,  -100,  -100,  -100,\n          -100,   132,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])\n\n\nIf you apply padding in the tokenization step (by adjusting the max_length argument), no matter whether it’s line-by-line tokenization or not, the data collator will skip the padding step\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataLMController(dset,\n                         main_text='Review Text',\n                         filter_dict={'Review Text': lambda x: x is not None},\n                         content_transformations=[text_normalize,str.lower],\n                         cols_to_keep=['Clothing ID','Review Text'],\n                         seed=42,\n                         verbose=False\n                        )\n\n\ntdc.process_and_tokenize(tokenizer,line_by_line=False,max_length=100)\n\n\ntdc.set_data_collator(is_mlm=True,mlm_prob=0.15)\n\n\nlist(map(len,tdc.main_ddict['train']['input_ids'][:5]))\n\n[100, 100, 100, 100, 100]\n\n\nLet’s apply the collator\n\ninp_keys = tokenizer.model_input_names\n_inp = [{k:tdc.main_ddict['train'][i][k] for k in inp_keys} for i in range(5)]\nout = tdc.data_collator(_inp) # simulation with batch size 5\n\n\nout['input_ids'].shape\n\ntorch.Size([5, 100])\n\n\n\nout['input_ids'][:2,:]\n\ntensor([[    0,   118,  2740,    42,   804,     8,    21,  5779,    19, 50264,\n          2564,    77,    24,  2035,   479,   939,  2740,     5,  3023,    29,\n             8,    24,    21,   202,    81, 10799,     7,     5,   477, 50264,\n           145, 50264, 24203,   479,   939,   524,  6764,   195,   128,   361,\n            22,    59,  8325,  2697,     8,    33,    10,  5342,  7174, 28762,\n         50264,   356, 50264,    11, 21543,    29,    14,    33,   103, 41316,\n           479,   114,    47,   101,    10,  7082,  2564,    42,   429,    28,\n            13,    47,   479, 50264, 17204, 50264, 33997,     8,  3279,     8,\n          3473,   479,   939,    74,  3608, 12926,   159, 50264,  1836,   479,\n             2,     0,  2527, 29747, 50264, 27785,   269,  5779,   479, 50264],\n        [  162,   356,   231, 50264,  5283,     8, 50264,   437, 23781,  4716,\n          1459,  1836,   132,   479,     2,     0,   118, 50264,   910,  7474,\n           268,     8,    42,    65,    16,   269, 11962, 50264,   939,  2333,\n          3568,  1836, 50264,    53,   197,    33, 50264, 50264,   158,  2156,\n            24, 50264,   380, 44224,    24,  1302,   350,   251,  2156, 50264,\n           939,   437,   195, 50264,   361,    22,   479,     5, 19553, 11962,\n            53,    10,   410, 50264,   479,   939,  1199,    68, 26498,    61,\n            16,   350,   203,  2156,   187,   939, 50264,    75, 10610, 50264,\n           648,  2156,   939,   197,    33,  9010,    13,    24,     7,   213,\n            15,  1392,   479,     2,     0,   734,     5,  5780,    16,    98]])\n\n\n\nout['labels'][:2,:]\n\ntensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,     5,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,    81,  -100,  -100,  -100,  -100,     9,\n          -100, 29747,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n             8,  -100,   275,  -100,  -100,  -100,  -100,  -100,  -100,  3989,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,     5,  1468,    16,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,    10,  -100,  -100,\n          -100,  -100,  -100,  -100, 24203,  -100,  -100,  -100,  -100,   156],\n        [ -100,  -100,  -100,   353,  -100,  -100,   939,  -100,    10,  -100,\n          -100,  -100,  -100,   479,  -100,  -100,  -100,   657,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,   479,  -100,  -100,\n          -100,  -100,   316,  -100,  -100,  -100,   300,    10,  -100,  -100,\n          -100,  1237,  -100,   479,  -100,  -100,  -100,  -100,  -100,     8,\n          -100,  -100,  -100,   128,  -100,    22,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100, 38596,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  2220,  -100,  -100,    24,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])\n\n\nSince we are using the concatenation-of-tokenization technique, one smart thing that the HuggingFace’s DataCollatorForLanguageModeling (which is the data collator we use) does is to allow maskings at every position, at opposed to to the previous cases (with line-by-line tokenization), there’s no masking near the end tokens of each list, because those end tokens are padding tokens\n\n\nb) For causal language model\n\nfrom transformers import AutoTokenizer\nfrom tokenizers import processors\n\nLet’s define our GPT2 tokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('gpt2')\n\n\ntokenizer\n\nGPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '&lt;|endoftext|&gt;', 'eos_token': '&lt;|endoftext|&gt;', 'unk_token': '&lt;|endoftext|&gt;'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n    50256: AddedToken(\"&lt;|endoftext|&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n\n\nGPT2 does not use start/end-of-sentence token:\n\nprint(tokenizer.convert_ids_to_tokens(tokenizer(\"this is a text. That is a second text.But there's a third one\")['input_ids']))\n\n['this', 'Ġis', 'Ġa', 'Ġtext', '.', 'ĠThat', 'Ġis', 'Ġa', 'Ġsecond', 'Ġtext', '.', 'But', 'Ġthere', \"'s\", 'Ġa', 'Ġthird', 'Ġone']\n\n\nIf you want to perform concatenation-of-token, and you want your causal LM to differentiate between sentences, you can add a special token to separate sentences, as follow:\n\ntokenizer._tokenizer.post_processor = processors.TemplateProcessing(\n    single=\"$A \" + tokenizer.eos_token,\n    special_tokens=[(tokenizer.eos_token, tokenizer.eos_token_id)],\n)\ntokenizer.pad_token = tokenizer.eos_token\n\n\nprint(tokenizer.convert_ids_to_tokens(tokenizer(\"this is a text. That is a second text.But there's a third one\")['input_ids']))\n\n['this', 'Ġis', 'Ġa', 'Ġtext', '.', 'ĠThat', 'Ġis', 'Ġa', 'Ġsecond', 'Ġtext', '.', 'But', 'Ġthere', \"'s\", 'Ġa', 'Ġthird', 'Ġone', '&lt;|endoftext|&gt;']\n\n\nWith this modified tokenizer, let’s perform concatenation-of-tokenization using GPT2\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataLMController(dset,\n                         main_text='Review Text',\n                         filter_dict={'Review Text': lambda x: x is not None},\n                         content_transformations=[text_normalize,str.lower],\n                         seed=42,\n                         verbose=False\n                        )\n\n\ntdc.process_and_tokenize(tokenizer,line_by_line=False,max_length=100)\n\nSince it’s casual language modeling, let’s turn off is_mlm\n\ntdc.set_data_collator(is_mlm=False)\n\n\nlist(map(len,tdc.main_ddict['train']['input_ids'][:5]))\n\n[100, 100, 100, 100, 100]\n\n\nLet’s apply the collator\n\nout = tdc.data_collator([tdc.main_ddict['train'][i] for i in range(5)]) # simulation with batch size 5\n\n\nout['input_ids'].shape\n\ntorch.Size([5, 100])\n\n\n\nout['input_ids'][:2,:]\n\ntensor([[   72,  6149,   428,  2691,   290,   373, 11679,   351,   262,  4197,\n           618,   340,  5284,   764,  1312,  6149,   262,  2124,    82,   290,\n           340,   373,   991,   625,  7857,   284,   262,   966,   286,   852,\n         42880, 16475,   764,  1312,   716,  7331,   642,   705,   860,   366,\n           546, 11323,  8059,   290,   423,   257,  6547,  7888, 28668,   290,\n           804,  1266,   287, 16270,    82,   326,   423,   617,  5485,   764,\n           611,   345,   588,   257,  9155,  4197,   428,  1244,   307,   329,\n           345,   764,   262,  2587,   318, 29175,   290,  5814,   290,  6792,\n           764,  1312,   561,  1950, 16216,   866,   257,  2546,   764, 50256,\n           568, 42880, 16475,  5145,  1107, 11679,   764,   925,   502,   804],\n        [  718,  1227, 10423,   290,  1312,  1101,   257,  4273,   578,  2546,\n           362,   764, 50256,    72,  1842,   374,  3361,   364,   290,   428,\n           530,   318,  1107, 13779,   764,  1312,  3221,  5806,  2546,  1105,\n           475,   815,   423,  1392,   257,   838,   837,   340,  4539,  1263,\n           764,   340,  2331,  1165,   890,   837,   290,  1312,  1101,   642,\n           705,   860,   366,   764,   262, 20842, 13779,   475,   257,  1310,\n         33367,   764,  1312,  3432,   720, 24063,   543,   318,  1165,   881,\n           837,  1201,  1312,  4398,   470, 12666,   340,  1865,   837,  1312,\n           815,   423, 13488,   329,   340,   284,   467,   319,  5466,   764,\n         50256,   986,   262,  3601,   318,   523, 21027,   278,   837,   290]])\n\n\n\nout['labels'][:2,:]\n\ntensor([[   72,  6149,   428,  2691,   290,   373, 11679,   351,   262,  4197,\n           618,   340,  5284,   764,  1312,  6149,   262,  2124,    82,   290,\n           340,   373,   991,   625,  7857,   284,   262,   966,   286,   852,\n         42880, 16475,   764,  1312,   716,  7331,   642,   705,   860,   366,\n           546, 11323,  8059,   290,   423,   257,  6547,  7888, 28668,   290,\n           804,  1266,   287, 16270,    82,   326,   423,   617,  5485,   764,\n           611,   345,   588,   257,  9155,  4197,   428,  1244,   307,   329,\n           345,   764,   262,  2587,   318, 29175,   290,  5814,   290,  6792,\n           764,  1312,   561,  1950, 16216,   866,   257,  2546,   764,  -100,\n           568, 42880, 16475,  5145,  1107, 11679,   764,   925,   502,   804],\n        [  718,  1227, 10423,   290,  1312,  1101,   257,  4273,   578,  2546,\n           362,   764,  -100,    72,  1842,   374,  3361,   364,   290,   428,\n           530,   318,  1107, 13779,   764,  1312,  3221,  5806,  2546,  1105,\n           475,   815,   423,  1392,   257,   838,   837,   340,  4539,  1263,\n           764,   340,  2331,  1165,   890,   837,   290,  1312,  1101,   642,\n           705,   860,   366,   764,   262, 20842, 13779,   475,   257,  1310,\n         33367,   764,  1312,  3432,   720, 24063,   543,   318,  1165,   881,\n           837,  1201,  1312,  4398,   470, 12666,   340,  1865,   837,  1312,\n           815,   423, 13488,   329,   340,   284,   467,   319,  5466,   764,\n          -100,   986,   262,  3601,   318,   523, 21027,   278,   837,   290]])\n\n\nFor CLM, the labels are essentially the same as input_ids. From HuggingFace documentation:\n`DataCollatorForLanguageModeling` will take care of creating the language model labels — in causal language modeling the inputs serve as labels too (just shifted by one element), and this data collator creates them on the fly during training.",
    "crumbs": [
      "5. Text Classes",
      "c. Text Controller For Language Modeling",
      "Text Main For Language Model"
    ]
  },
  {
    "objectID": "text_main_lm.html#save-and-load-textdatacontroller",
    "href": "text_main_lm.html#save-and-load-textdatacontroller",
    "title": "Text Main For Language Model",
    "section": "8. Save and Load TextDataController",
    "text": "8. Save and Load TextDataController\n\nsource\n\nTextDataLMController.save_as_pickles\n\n TextDataLMController.save_as_pickles (fname, parent='pickle_files')\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfname\n\n\nName of the pickle file\n\n\nparent\nstr\npickle_files\nParent folder\n\n\n\n\nsource\n\n\nTextDataController.from_pickle\n\n TextDataController.from_pickle (fname, parent='pickle_files')\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfname\n\n\nName of the pickle file\n\n\nparent\nstr\npickle_files\nParent folder\n\n\n\nTextDataLMController object can be saved and loaded with ease. This is especially useful after text processing and/or tokenization have been done\n\nfrom datasets import disable_caching\n\n\ndisable_caching()\n\n\ntokenizer = AutoTokenizer.from_pretrained('roberta-base')\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataLMController(dset,\n                         main_text='Review Text',\n                         filter_dict={'Review Text': lambda x: x is not None},\n                         content_transformations=[text_normalize,str.lower],\n                         seed=42,\n                         verbose=False\n                        )\n\n\ntdc.process_and_tokenize(tokenizer,line_by_line=True,max_length=-1)\n\ntdc.set_data_collator(is_mlm=True,mlm_prob=0.15)\n\n\ntdc.save_as_pickles('my_lm_tdc')\n\nLoad back our object\n\ntdc2 = TextDataLMController.from_pickle('my_lm_tdc')\n\nYou can still access all its attributes, data, preprocessings, transformations …\n\ntdc2.main_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Review Text', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n        num_rows: 18111\n    })\n    validation: Dataset({\n        features: ['Review Text', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n        num_rows: 4529\n    })\n})\n\n\n\ntdc2.filter_dict,tdc2.content_tfms\n\n({'Review Text': &lt;function __main__.&lt;lambda&gt;(x)&gt;},\n [&lt;function underthesea.pipeline.text_normalize.text_normalize(text, tokenizer='underthesea')&gt;,\n  &lt;method 'lower' of 'str' objects&gt;])",
    "crumbs": [
      "5. Text Classes",
      "c. Text Controller For Language Modeling",
      "Text Main For Language Model"
    ]
  },
  {
    "objectID": "text_main.html",
    "href": "text_main.html",
    "title": "Text Main",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom importlib.machinery import SourceFileLoader\nimport os",
    "crumbs": [
      "1. Quick Starts",
      "Text Main"
    ]
  },
  {
    "objectID": "text_main.html#content-transformation-augmentations-and-tokenization",
    "href": "text_main.html#content-transformation-augmentations-and-tokenization",
    "title": "Text Main",
    "section": "1. Content Transformation, Augmentations, and Tokenization",
    "text": "1. Content Transformation, Augmentations, and Tokenization\n\nsource\n\ntokenizer_explain\n\n tokenizer_explain (inp, tokenizer, split_word=False)\n\nDisplay results from tokenizer\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninp\n\n\nInput sentence\n\n\ntokenizer\n\n\nTokenizer (preferably from HuggingFace)\n\n\nsplit_word\nbool\nFalse\nIs input inp split into list or not\n\n\n\nWe can use this function to show how HuggingFace’s tokenizer works and what its outputs look like\nLet’s try PhoBert tokenizer (for Vietnamese texts). PhoBert tokenizer requires the input to be word-segmented. We will use our built-in function apply_vnmese_word_tokenize to do this\n\nfrom transformers import AutoTokenizer\n\n\n_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\ninp = apply_vnmese_word_tokenize('hội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức')\nprint(inp)\n\nhội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức\n\n\nNow we can use tokenizer_explain to see how our PhoBert-base tokenizer process our input inp\n\ntokenizer_explain(inp,_tokenizer)\n\n        ------- Tokenizer Explained -------\n----- Input -----\nhội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức\n\n----- Tokenized results ----- \n{'input_ids': [0, 1093, 1838, 1574, 3330, 2025, 31, 1574, 2029, 4885, 8554, 25625, 7344, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n----- Results from tokenizer.convert_ids_to_tokens -----\n['&lt;s&gt;', 'hội', 'cư_dân', 'chung_cư', 'sen', 'hồng', '-', 'chung_cư', 'lo@@', 'tus', 'sóng_thần', 'thủ_@@', 'đức', '&lt;/s&gt;']\n\n----- Results from tokenizer.decode ----- \n&lt;s&gt; hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức &lt;/s&gt;\n\n\n\nThe Tokenized results is the outputs of tokenizer, and Results from tokenizer.convert_ids_to_tokens shows us what each token id really is, the newly-added start and end tokens, and even the byte-pair encoding in action\n\nsource\n\n\ntwo_steps_tokenization_explain\n\n two_steps_tokenization_explain (inp, tokenizer, content_tfms=[],\n                                 aug_tfms=[])\n\nDisplay results form each content transformation, then display results from tokenizer\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninp\n\n\nInput sentence\n\n\ntokenizer\n\n\nTokenizer (preferably from HuggingFace)\n\n\ncontent_tfms\nlist\n[]\nA list of text transformations\n\n\naug_tfms\nlist\n[]\nA list of text augmentation\n\n\n\nThis function further showcase how each text transformation and/or text augmentation affect our text input, step by step\nLet’s load Phobert tokenizer one more time to test out this function\n\n_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n\n\nfrom underthesea import text_normalize\n\napply_vnmese_word_tokenize also have an option to normalize text (i.e. standardizing text input, getting rid of extra spaces, normalizing accents for Vietnamese text …)\n\nfrom functools import partial\n\n\ninp = 'Hội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức. Thủ Đức là một huyện trực thuộc thành phố Hồ Chí Minh'\ntwo_steps_tokenization_explain(inp,_tokenizer,content_tfms=[partial(apply_vnmese_word_tokenize,normalize_text=True)])\n\n        ------- Text Transformation Explained -------\n----- Raw sentence -----\nHội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức. Thủ Đức là một huyện trực thuộc thành phố Hồ Chí Minh\n\n----- Content Transformations (on both train and test) -----\n--- apply_vnmese_word_tokenize ---\nHội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức . Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh\n\n\n----- Augmentations (on train only) -----\n\n        ------- Tokenizer Explained -------\n----- Input -----\nHội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức . Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh\n\n----- Tokenized results ----- \n{'input_ids': [0, 792, 1838, 1574, 3330, 2025, 31, 1574, 2029, 4885, 8554, 25625, 7344, 5, 5043, 8, 16, 149, 2850, 214, 784, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n----- Results from tokenizer.convert_ids_to_tokens -----\n['&lt;s&gt;', 'Hội', 'cư_dân', 'chung_cư', 'sen', 'hồng', '-', 'chung_cư', 'lo@@', 'tus', 'sóng_thần', 'thủ_@@', 'đức', '.', 'Thủ_Đức', 'là', 'một', 'huyện', 'trực_thuộc', 'thành_phố', 'Hồ_Chí_Minh', '&lt;/s&gt;']\n\n----- Results from tokenizer.decode ----- \n&lt;s&gt; Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức. Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh &lt;/s&gt;\n\n\n\nLet’s add some text augmentations\n\nimport unidecode\n\n\n# to remove vietnamese accent\nremove_accent = lambda x: unidecode.unidecode(x)\n\nIf you want your function to be printed in with a different name:\n\nremove_accent.__name__ = 'Remove Vietnamese Accent'\n\n\ntwo_steps_tokenization_explain(inp,_tokenizer,\n                               content_tfms=[partial(apply_vnmese_word_tokenize,normalize_text=True)],\n                               aug_tfms=[remove_accent]\n                              )\n\n        ------- Text Transformation Explained -------\n----- Raw sentence -----\nHội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức. Thủ Đức là một huyện trực thuộc thành phố Hồ Chí Minh\n\n----- Content Transformations (on both train and test) -----\n--- apply_vnmese_word_tokenize ---\nHội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức . Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh\n\n\n----- Augmentations (on train only) -----\n--- Remove Vietnamese Accent ---\nHoi cu_dan chung_cu sen hong - chung_cu lotus song_than thu_duc . Thu_Duc la mot huyen truc_thuoc thanh_pho Ho_Chi_Minh\n\n\n        ------- Tokenizer Explained -------\n----- Input -----\nHoi cu_dan chung_cu sen hong - chung_cu lotus song_than thu_duc . Thu_Duc la mot huyen truc_thuoc thanh_pho Ho_Chi_Minh\n\n----- Tokenized results ----- \n{'input_ids': [0, 3021, 1111, 56549, 17386, 22975, 13689, 3330, 27037, 31, 22975, 13689, 2029, 4885, 3227, 9380, 1510, 21605, 6190, 1894, 5, 5770, 4098, 1894, 2644, 3773, 1204, 18951, 2052, 10242, 9835, 1881, 22899, 17366, 10384, 30234, 8470, 1612, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n----- Results from tokenizer.convert_ids_to_tokens -----\n['&lt;s&gt;', 'Ho@@', 'i', 'cu_@@', 'dan', 'chung_@@', 'cu', 'sen', 'hong', '-', 'chung_@@', 'cu', 'lo@@', 'tus', 'so@@', 'ng_th@@', 'an', 'thu_@@', 'du@@', 'c', '.', 'Thu_@@', 'Du@@', 'c', 'la', 'mo@@', 't', 'huy@@', 'en', 'tru@@', 'c_th@@', 'u@@', 'oc', 'thanh_@@', 'pho', 'Ho_@@', 'Chi_@@', 'Minh', '&lt;/s&gt;']\n\n----- Results from tokenizer.decode ----- \n&lt;s&gt; Hoi cu_dan chung_cu sen hong - chung_cu lotus song_than thu_duc. Thu_Duc la mot huyen truc_thuoc thanh_pho Ho_Chi_Minh &lt;/s&gt;\n\n\n\nYou can even be creative with your augmentation functions; let’s say you only want your augmentation to be applied 50% of the time:\n\nimport random\n\n\nrandom.seed(2) # for reproducibility\n\n\nremove_accent = lambda x: unidecode.unidecode(x) if random.random()&lt;0.5 else x\nremove_accent.__name__ = 'Remove Vietnamese Accent with 0.5 prob'\n\n\ntwo_steps_tokenization_explain(inp,_tokenizer,\n                               content_tfms=[partial(apply_vnmese_word_tokenize,normalize_text=True)],\n                               aug_tfms=[remove_accent]\n                              )\n\n        ------- Text Transformation Explained -------\n----- Raw sentence -----\nHội cư dân   chung cư sen hồng- chung cư    lotus sóng thần thủ đức. Thủ Đức là một huyện trực thuộc thành phố Hồ Chí Minh\n\n----- Content Transformations (on both train and test) -----\n--- apply_vnmese_word_tokenize ---\nHội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức . Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh\n\n\n----- Augmentations (on train only) -----\n--- Remove Vietnamese Accent with 0.5 prob ---\nHội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức . Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh\n\n\n        ------- Tokenizer Explained -------\n----- Input -----\nHội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức . Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh\n\n----- Tokenized results ----- \n{'input_ids': [0, 792, 1838, 1574, 3330, 2025, 31, 1574, 2029, 4885, 8554, 25625, 7344, 5, 5043, 8, 16, 149, 2850, 214, 784, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n----- Results from tokenizer.convert_ids_to_tokens -----\n['&lt;s&gt;', 'Hội', 'cư_dân', 'chung_cư', 'sen', 'hồng', '-', 'chung_cư', 'lo@@', 'tus', 'sóng_thần', 'thủ_@@', 'đức', '.', 'Thủ_Đức', 'là', 'một', 'huyện', 'trực_thuộc', 'thành_phố', 'Hồ_Chí_Minh', '&lt;/s&gt;']\n\n----- Results from tokenizer.decode ----- \n&lt;s&gt; Hội cư_dân chung_cư sen hồng - chung_cư lotus sóng_thần thủ_đức. Thủ_Đức là một huyện trực_thuộc thành_phố Hồ_Chí_Minh &lt;/s&gt;\n\n\n\nThere are more examples of interesting augmentation here",
    "crumbs": [
      "1. Quick Starts",
      "Text Main"
    ]
  },
  {
    "objectID": "text_main.html#tokenize-function",
    "href": "text_main.html#tokenize-function",
    "title": "Text Main",
    "section": "2. Tokenize Function",
    "text": "2. Tokenize Function\n\nsource\n\ntokenize_function\n\n tokenize_function (text, tok, max_length=None, is_split_into_words=False,\n                    return_tensors=None, return_special_tokens_mask=False)\n\nThis is a wrapper for Huggingface’ tokenizer, to tokenize and pad your input text, getting them ready for your NLP model\nI will reuse PhoBert’s tokenizer to demonstrate the functionality of this function. For more information about this tokenizer: https://huggingface.co/vinai/phobert-base\n\n_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n\n\nphobert_preprocess = partial(apply_vnmese_word_tokenize,normalize_text=True)\n\n\ntokenize_function(phobert_preprocess('hội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức'),\n                  _tokenizer,max_length=512)\n\n{'input_ids': [0, 1093, 1838, 1574, 3330, 2025, 31, 1574, 2029, 4885, 8554, 25625, 7344, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n\n_inp = ['hội cần mở thẻ tín dụng tại hà nội, đà nẵng, tp. hồ chí minh',\"biti's cao lãnh - đồng tháp\"]\n_inp = [phobert_preprocess(i) for i in _inp]\n_inp\n\n['hội cần mở thẻ_tín_dụng tại hà_nội , đà_nẵng , tp . hồ chí_minh',\n \"biti's cao_lãnh - đồng tháp\"]\n\n\n\ntokenize_function(_inp,_tokenizer,max_length=512)\n\n{'input_ids': [[0, 1093, 115, 548, 10603, 35, 44068, 2151, 4, 62295, 1301, 24931, 4, 1187, 2380, 5, 1005, 43647, 9534, 2], [0, 3907, 2081, 51899, 1118, 10109, 8271, 31, 80, 3186, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n\n\nYou can change the tokenizer outputs’ type, such as pytorch’s tensor, tensorflow objects, or numpy array\n\ntokenize_function(_inp,_tokenizer,max_length=512,return_tensors='pt')\n\n{'input_ids': tensor([[    0,  1093,   115,   548, 10603,    35, 44068,  2151,     4, 62295,\n          1301, 24931,     4,  1187,  2380,     5,  1005, 43647,  9534,     2],\n        [    0,  3907,  2081, 51899,  1118, 10109,  8271,    31,    80,  3186,\n             2,     1,     1,     1,     1,     1,     1,     1,     1,     1]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n\n\n\nresults = tokenize_function(_inp,_tokenizer,max_length=512)\n\n\nprint(_tokenizer.convert_ids_to_tokens(results['input_ids'][0]))\n\n['&lt;s&gt;', 'hội', 'cần', 'mở', 'thẻ_tín_dụng', 'tại', 'hà_@@', 'nội', ',', 'đà_@@', 'n@@', 'ẵng', ',', 't@@', 'p', '.', 'hồ', 'chí_@@', 'minh', '&lt;/s&gt;']\n\n\nYou can change max_length (which allow truncation when sentence length is higher than max_length)\n\nresults = tokenize_function(_inp,_tokenizer,\n                            max_length=5)\n\n\nresults\n\n{'input_ids': [[0, 1093, 115, 548, 2], [0, 3907, 2081, 51899, 2]], 'token_type_ids': [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]}",
    "crumbs": [
      "1. Quick Starts",
      "Text Main"
    ]
  },
  {
    "objectID": "text_main.html#metadatas-processing",
    "href": "text_main.html#metadatas-processing",
    "title": "Text Main",
    "section": "3. Metadatas Processing",
    "text": "3. Metadatas Processing\n\nsource\n\nconcat_metadatas\n\n concat_metadatas (dset:dict, main_text, metadatas, process_metas=True,\n                   sep='.', is_batched=True)\n\nExtract, process (optional) and concatenate metadatas to the front of text\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndset\ndict\n\nHuggingFace Dataset\n\n\nmain_text\n\n\nText feature name\n\n\nmetadatas\n\n\nMetadata (or a list of metadatas)\n\n\nprocess_metas\nbool\nTrue\nWhether apply simple metadata processing, i.e. space strip and lowercase\n\n\nsep\nstr\n.\nSeparator, for multiple metadatas concatenation\n\n\nis_batched\nbool\nTrue\nwhether batching is applied\n\n\n\nThis function allows you to concatenate any text metadatas to the front of your main texts. Adding metadatas to your text might help your model to utilize any extra information for its downstream task",
    "crumbs": [
      "1. Quick Starts",
      "Text Main"
    ]
  },
  {
    "objectID": "text_main.html#load-data-basic-use-case",
    "href": "text_main.html#load-data-basic-use-case",
    "title": "Text Main",
    "section": "1. Load data + Basic use case",
    "text": "1. Load data + Basic use case\n\nsource\n\nTextDataController.from_csv\n\n TextDataController.from_csv (file_path, **kwargs)\n\n\nsource\n\n\nTextDataController.from_df\n\n TextDataController.from_df (df, validate=True, **kwargs)\n\nYou can create a TextDataController from a csv, pandas DataFrame, or directly from a HuggingFace dataset object. Currently, TextDataController is designed for text classification and text regression, as we will explore in this documentation\nWe will load a sample data to prepare for a classification task: which Department Name a comment (Review Text) belongs to\nDataset source: https://www.kaggle.com/datasets/kavita5/review_ecommerce\n\nimport pandas as pd\n\n\ndf = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\n\n\ndf.shape\n\n(23486, 10)\n\n\n\ndf.sample(5)\n\n\n\n\n\n\n\n\n\nClothing ID\nAge\nTitle\nReview Text\nRating\nRecommended IND\nPositive Feedback Count\nDivision Name\nDepartment Name\nClass Name\n\n\n\n\n15253\n831\n31\nGreat work top\nI snagged this top with the 25% off sale and i...\n5\n1\n0\nGeneral Petite\nTops\nBlouses\n\n\n1254\n850\n49\nFlattering, comfy top\nEveryone has said it, so i'll just add my two ...\n4\n1\n0\nGeneral Petite\nTops\nBlouses\n\n\n5105\n824\n38\nAdore this top!\nSaw this one online and when it came it did no...\n5\n1\n2\nGeneral Petite\nTops\nBlouses\n\n\n8611\n920\n29\nGreat spring sweater\nThis sweater is classy and comfortable. it has...\n4\n1\n0\nGeneral\nTops\nFine gauge\n\n\n17574\n1110\n37\nSuper cute!\nI'm not sure why the other reviewers think tha...\n5\n1\n6\nGeneral\nDresses\nDresses\n\n\n\n\n\n\n\n\nYou can create a TextDataController from a dataframe. This also provides a quick input validation check (NaN check and Duplication check)\n\ntdc = TextDataController.from_df(df,\n                                 main_text='Review Text',\n                                 sup_types='classification',\n                                 label_names='Department Name',\n                                )\n\n- Input Validation Precheck -\nData contains missing values!\n-----&gt; List of columns and the number of missing values for each\nTitle              3810\nReview Text         845\nDivision Name        14\nDepartment Name      14\nClass Name           14\ndtype: int64\nData contains duplicated values!\n-----&gt; Number of duplications: 21 rows\n\n\nYou can also create a TextDataController directly from the csv file. The good thing about using HuggingFace Dataset as the main backend of the TextDataController is that you can utilize lots of its useful functionality, such as caching\n\ntdc = TextDataController.from_csv('sample_data/Womens_Clothing_Reviews.csv',\n                                  main_text='Review Text',\n                                  sup_types='classification',\n                                  label_names='Department Name',\n                                 )\n\nYou can also create a TextDataController from a HuggingFace Dataset\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ndset\n\nDataset({\n    features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'],\n    num_rows: 23486\n})\n\n\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         sup_types='classification',\n                         label_names='Department Name',\n                         seed=42\n                        )\n\nIn the “Input Validation Precheck” above, we notice that our dataset has missing values in the text field and the label field. For now, let’s load the data as a Pandas’ DataFrame, perform some cleaning, and create our TextDataController\n\ndf = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\n\n\ndf = df[(~df['Review Text'].isna()) & (~df['Department Name'].isna())].reset_index(drop=True)\n\n\ntdc = TextDataController.from_df(df,\n                                 main_text='Review Text',\n                                 sup_types='classification',\n                                 label_names='Department Name',\n                                )\n\n- Input Validation Precheck -\nData contains missing values!\n-----&gt; List of columns and the number of missing values for each\nTitle    2966\ndtype: int64\nData contains duplicated values!\n-----&gt; Number of duplications: 1 rows\n\n\nAt this point you can start perform 2 important steps on your data\n\nText preprocessings, Label Encoding, Train/Validation Split\nTokenization\n\nWe haven’t provided any preprocessings to the TextDataController; we will see more on how to use preprocessings (step by step) as we progress\n\nddict = tdc.do_all_preprocessing(shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n----- Label Encoding -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 2, which is 0.01% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\nddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Review Text', 'Department Name', 'label'],\n        num_rows: 18099\n    })\n    validation: Dataset({\n        features: ['Review Text', 'Department Name', 'label'],\n        num_rows: 4526\n    })\n})\n\n\nOur DatasetDict now has two split: train and validation. Note that train split is now IterableDataset, for processing efficiency\n\nddict['train'][:3]\n\n{'Review Text': ['I wanted to love this. i have been looking for a poncho-type sweater for our cold midwestern winters. the cream colored one that i really wanted sold out instantly and i missed the window for the xxsp. i ordered this in the xs/sp (smallest size available). i am 5\\'1\" and 108 lbs with small shoulders. the neck opening is huge. my collar bones and a seciton of my upper back were exposed. this would not keep me warm due to so much exposed skin on my neck, back, and shoulders. i suppose i could get a',\n  'Love the movement of the blouse and how it falls. great quality material.',\n  \"Loved these beach pants! i purchased the size medium in the coral. i loved the accents on the ties and the little pom pom details. i did get many compliments on them. the only thing i don't love about them is the material is very thin. i know they are beach pants but i personally would have liked slightly more weight to them. i wore them once with a pair of cropped leggings underneath and i thought it was a very cute way to wear them with some additional substance underneath.\"],\n 'Department Name': ['Tops', 'Tops', 'Intimate'],\n 'label': [4, 4, 2]}\n\n\n\nddict['validation'][:3]\n\n{'Review Text': [\"The raspberry color is really stunning! i have been looking for colored tights for a while and had difficulty finding really rich colors. i was thrilled when i saw these! i've worn them once so far. very comfortable and seem like they will last.\",\n  'I just received this dress and i feel like a goddess in it! it is perfect for graduations, weddings, romantic dinners, tropical va cations....heck, i\\'ll wear it to the grocery store! i love it that much!\\r\\n\\r\\ni am 5\\'7\" with a 34c bust.....i have this dress in a size 4 and it fits very well. this dress is slim cut from the shoulder down to the waist. the dress length hits me at the lower calf, just like the model online. i think the armholes are cut a little high...this being said; this dress would',\n  \"When i saw this top online, i thought i'd love it and immediately ordered it in both colors.  they arrived today and i am soooo disappointed.  i have never seen such drab colors.  the blue is a muddy grayish hue (like an overcast day) and the pink is a dusty shade of peach.  yuck.  and i was hoping the ruffle at the bottom would have a chiffon-like flowy effect.  instead, the ruffle is made of a cheap looking knit.  back these go...\"],\n 'Department Name': ['Intimate', 'Dresses', 'Tops'],\n 'label': [2, 1, 4]}\n\n\nNow we can start with the tokenization\n\nfrom transformers import RobertaTokenizer\n\n\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\nddict = tdc.do_tokenization(tokenizer,max_length=512)\n\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\nddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 18099\n    })\n    validation: Dataset({\n        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 4526\n    })\n})\n\n\n\nprint(ddict['train'][0]['input_ids'][:150])\n\n[0, 100, 770, 7, 657, 42, 4, 939, 33, 57, 546, 13, 10, 181, 261, 11156, 12, 12528, 23204, 13, 84, 2569, 1084, 16507, 31000, 4, 5, 6353, 20585, 65, 14, 939, 269, 770, 1088, 66, 11764, 8, 939, 2039, 5, 2931, 13, 5, 37863, 4182, 4, 939, 2740, 42, 11, 5, 3023, 29, 73, 4182, 36, 23115, 990, 1836, 577, 322, 939, 524, 195, 108, 134, 113, 8, 13955, 23246, 19, 650, 10762, 4, 5, 5397, 1273, 16, 1307, 4, 127, 19008, 12396, 8, 10, 15636, 24899, 9, 127, 2853, 124, 58, 4924, 4, 42, 74, 45, 489, 162, 3279, 528, 7, 98, 203, 4924, 3024, 15, 127, 5397, 6, 124, 6, 8, 10762, 4, 939, 19792, 939, 115, 120, 10, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n\n\n\nprint(ddict['validation'][0]['input_ids'][:150])\n\n[0, 133, 41345, 3195, 16, 269, 5835, 328, 939, 33, 57, 546, 13, 20585, 326, 6183, 13, 10, 150, 8, 56, 9600, 2609, 269, 4066, 8089, 4, 939, 21, 8689, 77, 939, 794, 209, 328, 939, 348, 10610, 106, 683, 98, 444, 4, 182, 3473, 8, 2045, 101, 51, 40, 94, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n\n\nYou can combine Text Processing and Tokenization with 1 method call\n\ntdc = TextDataController.from_df(df,\n                                 main_text='Review Text',\n                                 sup_types='classification',\n                                 label_names='Department Name'\n                                )\n\n- Input Validation Precheck -\nData contains missing values!\n-----&gt; List of columns and the number of missing values for each\nTitle    2966\ndtype: int64\nData contains duplicated values!\n-----&gt; Number of duplications: 1 rows\n\n\n\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n----- Label Encoding -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 4, which is 0.02% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can access the DatasetDict from the instance variable main_ddict\n\ntdc.main_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 18098\n    })\n    validation: Dataset({\n        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 4526\n    })\n})\n\n\nThis DatasetDict is ready to be put into any HuggingFace text model.",
    "crumbs": [
      "1. Quick Starts",
      "Text Main"
    ]
  },
  {
    "objectID": "text_main.html#filtering",
    "href": "text_main.html#filtering",
    "title": "Text Main",
    "section": "2. Filtering",
    "text": "2. Filtering\nThis preprocessing step allow you to filter out certain values of a certain column in your dataset. Let’s say I want to filter out any None value in the column ‘Review Text’\n\ndf = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\ndf[(~df['Review Text'].isna())].isna().sum()\n\nClothing ID                   0\nAge                           0\nTitle                      2966\nReview Text                   0\nRating                        0\nRecommended IND               0\nPositive Feedback Count       0\nDivision Name                13\nDepartment Name              13\nClass Name                   13\ndtype: int64\n\n\nWe will provide a dictionary containing the name of the column and the filtering function to apply on that column. Note that the filtering function will receive an item from the column, and the function should return a boolean\n\ntdc = TextDataController.from_df(df,\n                                 main_text='Review Text',\n                                 sup_types='classification',\n                                 label_names='Department Name',\n                                 filter_dict={'Review Text': lambda x: x is not None},\n                                 seed=42\n                                )\n\n- Input Validation Precheck -\nData contains missing values!\n-----&gt; List of columns and the number of missing values for each\nTitle              3810\nReview Text         845\nDivision Name        14\nDepartment Name      14\nClass Name           14\ndtype: int64\nData contains duplicated values!\n-----&gt; Number of duplications: 21 rows\n\n\n\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\nDone\n----- Label Encoding -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 1, which is 0.01% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntdc.main_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 18111\n    })\n    validation: Dataset({\n        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 4529\n    })\n})\n\n\nLet’s check if we have filtered out all NaN/None value\n\nfor i in tdc.main_ddict['train']['Review Text']:\n    assert i is not None\nfor i in tdc.main_ddict['validation']['Review Text']:\n    assert i is not None\n\nWe can even add multiple filtering functions. Remember from our precheck, there are also None values in our label ‘Department Name’. While we are at it, let’s filter out any rating that is less than 3 (just to showcase what our filtering can do)\n\ndf.Rating.value_counts()\n\nRating\n5    13131\n4     5077\n3     2871\n2     1565\n1      842\nName: count, dtype: int64\n\n\nNote that TextDataController will only keep the text, the labels and the metadatas columns; any other column will be dropped. To keep the ‘Rating’, we need to define the cols_to_keep argument\n\ndf = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\ntdc = TextDataController.from_df(df,\n                                 main_text='Review Text',\n                                 sup_types='classification',\n                                 label_names='Department Name',\n                                 filter_dict={'Review Text': lambda x: x is not None,\n                                              'Department Name': lambda x: x is not None,\n                                              'Rating': lambda x: x&gt;=3\n                                             },\n                                 cols_to_keep=['Review Text','Rating','Department Name'],\n                                 seed=42\n                                )\n\n- Input Validation Precheck -\nData contains missing values!\n-----&gt; List of columns and the number of missing values for each\nTitle              3810\nReview Text         845\nDivision Name        14\nDepartment Name      14\nClass Name           14\ndtype: int64\nData contains duplicated values!\n-----&gt; Number of duplications: 21 rows\n\n\n\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\n----- Do &lt;lambda&gt; on Department Name -----\n----- Do &lt;lambda&gt; on Rating -----\nDone\n----- Label Encoding -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 1, which is 0.01% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor i in tdc.main_ddict['train']['Department Name']:\n    assert i is not None\n\nfor i in tdc.main_ddict['validation']['Department Name']:\n    assert i is not None\n    \nfor i in tdc.main_ddict['validation']['Rating']:\n    assert i &gt;= 3",
    "crumbs": [
      "1. Quick Starts",
      "Text Main"
    ]
  },
  {
    "objectID": "text_main.html#taking-a-sample-from-training-data",
    "href": "text_main.html#taking-a-sample-from-training-data",
    "title": "Text Main",
    "section": "3. Taking a sample from training data",
    "text": "3. Taking a sample from training data\nIf you only want to extract a training sample of your data, you can use the trn_size argument of the method process_and_tokenize (or do_tokenization). Since we use sharding to extract a sample from a DatasetDict, if trn_size is a integer, an approximated size will be returned\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         sup_types='classification',\n                         label_names='Department Name',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         seed=42\n                        )\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True,trn_size=1000)\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\n----- Do &lt;lambda&gt; on Department Name -----\nDone\n----- Label Encoding -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 3, which is 0.02% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntdc.main_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 1006\n    })\n    validation: Dataset({\n        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 4526\n    })\n})\n\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         sup_types='classification',\n                         label_names='Department Name',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         seed=42\n                        )\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True,trn_size=0.1) # return 10% of the data\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\n----- Do &lt;lambda&gt; on Department Name -----\nDone\n----- Label Encoding -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 3, which is 0.02% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\ntdc.main_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 1810\n    })\n    validation: Dataset({\n        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 4526\n    })\n})",
    "crumbs": [
      "1. Quick Starts",
      "Text Main"
    ]
  },
  {
    "objectID": "text_main.html#metadatas-concatenation",
    "href": "text_main.html#metadatas-concatenation",
    "title": "Text Main",
    "section": "4. Metadatas concatenation",
    "text": "4. Metadatas concatenation\nIf we think metadatas can be helpful, we can concatenate them into the front of your text, so that our text classification model is aware of it.\nIn this example, Let’s add ‘Title’ as our metadata\n\ndf = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\ntdc = TextDataController.from_df(df,\n                                 main_text='Review Text',\n                                 sup_types='classification',\n                                 label_names='Department Name',\n                                 filter_dict={'Review Text': lambda x: x is not None,\n                                              'Department Name': lambda x: x is not None,\n                                             },\n                                 metadatas='Title',\n                                 process_metas=True, # to preprocess the metadata (currently it's just empty space stripping and lowercasing),\n                                 seed=42\n                                )\n\n- Input Validation Precheck -\nData contains missing values!\n-----&gt; List of columns and the number of missing values for each\nTitle              3810\nReview Text         845\nDivision Name        14\nDepartment Name      14\nClass Name           14\ndtype: int64\nData contains duplicated values!\n-----&gt; Number of duplications: 21 rows\n\n\n\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\n----- Do &lt;lambda&gt; on Department Name -----\nDone\n----- Metadata Simple Processing & Concatenating to Main Content -----\nDone\n----- Label Encoding -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 1, which is 0.01% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntdc.main_ddict['train']['Review Text'][:5]\n\n['beautiful! . I love this top. it was everything i hoped it would be. it is lined so it is not see through in the chest/back; sleeves are sheer. soft. gorgeous color. love the layers. runs large so definitely size down. i am usually a m and ordered the s. i\\'m 5\\'8\" curvy 32dd',\n 'very flattering . This dress fits to a t! true to size. very flattering. fabric is soft and comfortable.',\n \"the worst . I don't typically write bad reviews, but this dress is so bad and i want to save someone else from buying it. i read the mostly bad reviews and still purchased anyway (my fault i know). the dress is super stiff ( i know denim can be that way and it is possible it would soften up after a few washes). i'm typically a 6/8 and the size small swallowed me, and the xs was big everywhere except through the bust (i ordered both sizes to try). i wouldn't recommend buying this if you are a size 8 or small\",\n \"love this jacket! . I was on the lookout for a denim jacket when i found this beauty on line. i fell in love immediately and didn't think twice about paying full price. i wear it with moss green chinos and it looks really good. the little dots in the jacket are actually a pale green, which gives it extra character. very well made. i was a bit skeptical about the hook and eye fastenings, but they are very secure. \\r\\n\\r\\ni ordered my usual xl and found it roomy enough in the bust and arms. i would definitely call it tru\",\n 'great spring/summer dress. . I am excited for spring so i can wear this. i purchased the orange. it is actually more of a red, but i like it. colorful and flattering fit.']\n\n\n\ntdc.main_ddict['validation']['Review Text'][:5]\n\n[' . Such a fun jacket! great to wear in the spring or to the office as an alternative to a formal blazer. very comfortable!',\n 'simple and elegant . I thought this shirt was really feminine and elegant. only downsides is some of the punched out holes had fabric still attached which you have cut off with scissors- otherwise the shirt will snag. and the second issue of bigger importance are the low armholes. lots of bra showing- not really sure how to get around that so i always wear it with a cardigan. but it would be nice not to have to. \\r\\nother than that it looks nice and pairs nicely with almost anything.',\n 'retro and pretty . This top has a bit of a retro flare but so adorable on. looks really cute with a pair of faded boot cut jeans.',\n 'summer/fall wear . I first spotted this on an retailer employee, she paired it with a peasant top & wore it open w/jeans & boots- so darn cute. love how this peice transitions from summer to fall. i\\'m 5\\'4\" so i had to order the small petite which is perfect. note that this dress is very long! it\\'s just a must have garment. the colors/ print are just beautiful.',\n \"perfect except slip . This is my new favorite dress! my only complaint is the slip is too small and the dress cannot be worn without it. i can't order a size up as the dress would then be huge. not sure what the solution is but the dress itself is stunning.\"]\n\n\nYou can add multiple metadatas. Let’s say ‘Division Name’ is the second metadata.\n\ndf = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\ntdc = TextDataController.from_df(df,\n                                 main_text='Review Text',\n                                 sup_types='classification',\n                                 label_names='Department Name',\n                                 filter_dict={'Review Text': lambda x: x is not None,\n                                              'Department Name': lambda x: x is not None,\n                                             },\n                                 metadatas=['Title','Division Name'],\n                                 process_metas=True,\n                                 seed=42\n                                )\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n\n- Input Validation Precheck -\nData contains missing values!\n-----&gt; List of columns and the number of missing values for each\nTitle              3810\nReview Text         845\nDivision Name        14\nDepartment Name      14\nClass Name           14\ndtype: int64\nData contains duplicated values!\n-----&gt; Number of duplications: 21 rows\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\n----- Do &lt;lambda&gt; on Department Name -----\nDone\n----- Metadata Simple Processing & Concatenating to Main Content -----\nDone\n----- Label Encoding -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 0, which is 0.00% of training set\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntdc.main_ddict['train']['Review Text'][:5]\n\n[\"general petite . meh . This tunic is way over priced for the style and quality. it fit comfortably (runs a size larger) but it's not really flattering, it jut kind of hangs there looking ok. it is a little too deep of a v cut for a work top as well. this top does not support the price at all. it felt like something i could find at department store for way less. i will be returning it.\",\n 'general . awesome buy! . I am so happy i took a chance on this jumpsuit! i am post-baby (six weeks) and although i intend on slimming down more i would say that it is flattering even at my current size. and it will only get better! \\r\\nthe quality and color are great!',\n 'general . warm grey . These are a lovely neutral to slightly warm grey pair of jeans from a great line. i wore my usual size without issues.',\n \"general . loved it, but it didn't work for me. . I wanted this top to work so bad. unfortunately the way the bust of the top is designed it isn't flattering if you aren't flat chested. it squishes on side of your chest and leaves the other side alone. i'm a b cup and had this problem so if you are a b or larger, i don't recommend. however, if you are smaller busted, this piece would be worth the purchase.\",\n \"general . varying feelings and opinions . As you can see, there is an array of differing opinions on here, and i share sentiments on both:\\r\\n_______\\r\\npros:\\r\\n- the texture and feel of this is great; it is very comfortable and is different.\\r\\n- tts for the most part; i normally can wear sizes 10 and 12 (m and l) with most retailer and got the medium and the fit was overall fine but more snug at the hips. if you're more slim/straight, it'll probably fit you like on the model. \\r\\n- good length, not too short or too long.\\r\\n- the mock collar is ni\"]\n\n\n\ntdc.main_ddict['validation']['Review Text'][:5] # The metadata for this text is None\n\n['general petite .  . Such a fun jacket! great to wear in the spring or to the office as an alternative to a formal blazer. very comfortable!',\n 'general petite . simple and elegant . I thought this shirt was really feminine and elegant. only downsides is some of the punched out holes had fabric still attached which you have cut off with scissors- otherwise the shirt will snag. and the second issue of bigger importance are the low armholes. lots of bra showing- not really sure how to get around that so i always wear it with a cardigan. but it would be nice not to have to. \\r\\nother than that it looks nice and pairs nicely with almost anything.',\n 'general . retro and pretty . This top has a bit of a retro flare but so adorable on. looks really cute with a pair of faded boot cut jeans.',\n 'general petite . summer/fall wear . I first spotted this on an retailer employee, she paired it with a peasant top & wore it open w/jeans & boots- so darn cute. love how this peice transitions from summer to fall. i\\'m 5\\'4\" so i had to order the small petite which is perfect. note that this dress is very long! it\\'s just a must have garment. the colors/ print are just beautiful.',\n \"general petite . perfect except slip . This is my new favorite dress! my only complaint is the slip is too small and the dress cannot be worn without it. i can't order a size up as the dress would then be huge. not sure what the solution is but the dress itself is stunning.\"]",
    "crumbs": [
      "1. Quick Starts",
      "Text Main"
    ]
  },
  {
    "objectID": "text_main.html#label-encodings",
    "href": "text_main.html#label-encodings",
    "title": "Text Main",
    "section": "5. Label Encodings",
    "text": "5. Label Encodings\n\nSingle-head prediction\nWe have briefly gone through the simplest case of label encoding, where we only need to predict 1 single label. We call this single head classification\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         sup_types='classification',                         \n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         seed=42\n                        )\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\n----- Do &lt;lambda&gt; on Department Name -----\nDone\n----- Label Encoding -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 3, which is 0.02% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\nAll label names will be saved in instance variable label_lists\n\ntdc.label_lists\n\n[['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']]\n\n\n… and all labels will be encoded\n\ntdc.main_ddict['validation']['label'][:5]\n\n[2, 4, 4, 1, 1]\n\n\nWe also keep the original labeling, for references\n\ntdc.main_ddict['validation']['Department Name'][:5]\n\n['Intimate', 'Tops', 'Tops', 'Dresses', 'Dresses']\n\n\nYou can also do single-head regression. Let’s say we want to predict Rating\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n\n\ndset\n\nDataset({\n    features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'],\n    num_rows: 23486\n})\n\n\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         sup_types='regression',\n                         label_names='Rating',\n                         filter_dict={'Review Text': lambda x: x is not None},\n                         seed=42,\n                        )\n\n\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\nDone\n----- Label Encoding -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 1, which is 0.01% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint(tdc.main_ddict['train']['Rating'][:5])\nprint(tdc.main_ddict['train']['label'][:5])\n\n[3.0, 1.0, 4.0, 3.0, 5.0]\n[3.0, 1.0, 4.0, 3.0, 5.0]\n\n\n\nprint(tdc.main_ddict['validation']['Rating'][:5])\nprint(tdc.main_ddict['validation']['label'][:5])\n\n[5.0, 4.0, 3.0, 5.0, 5.0]\n[5.0, 4.0, 3.0, 5.0, 5.0]\n\n\n\n\nMulti-head prediction\nWhat if we need to predict 2 different labels as once? We call this multi-head classification/regression. For example, let’s define our dataset so that we need to predict both Department Name and Division Name (both as classification)\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names=['Division Name','Department Name'],\n                         sup_types=['classification','classification'],\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         seed=42\n                        )\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\n----- Do &lt;lambda&gt; on Department Name -----\nDone\n----- Label Encoding -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 3, which is 0.02% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntdc.label_lists\n\n[['General', 'General Petite', 'Initmates'],\n ['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']]\n\n\nWe can see that we have two lists, one for label names of Division Name, and one for label names of Department Name\n\ntdc.main_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 18099\n    })\n    validation: Dataset({\n        features: ['Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 4526\n    })\n})\n\n\n\nprint(tdc.main_ddict['validation']['Division Name'][:5])\nprint(tdc.main_ddict['validation']['Department Name'][:5])\nprint(tdc.main_ddict['validation']['label'][:5])\n\n['General Petite', 'General Petite', 'General', 'General Petite', 'General Petite']\n['Intimate', 'Tops', 'Tops', 'Dresses', 'Dresses']\n[[1, 2], [1, 4], [0, 4], [1, 1], [1, 1]]\n\n\nWhat if one label is classification, and another label is regression? We will predict Department Name (classification) and Rating (regression)\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names=['Rating','Department Name'],\n                         sup_types=['regression','classification'],\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         seed=42\n                        )\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\n----- Do &lt;lambda&gt; on Department Name -----\nDone\n----- Label Encoding -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 3, which is 0.02% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint(tdc.main_ddict['validation']['Rating'][:5])\nprint(tdc.main_ddict['validation']['Department Name'][:5])\nprint(tdc.main_ddict['validation']['label'][:5])\n\n[5.0, 5.0, 5.0, 5.0, 4.0]\n['Intimate', 'Tops', 'Tops', 'Dresses', 'Dresses']\n[[5.0, 2.0], [5.0, 4.0], [5.0, 4.0], [5.0, 1.0], [4.0, 1.0]]\n\n\nSince it’s multi-head, you can define multiple classification/regression labels, as many as you want\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names=['Division Name','Rating','Department Name'],\n                         sup_types=['classification','regression','classification'],\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                      'Division Name': lambda x: x is not None,\n                                     },\n                         seed=42\n                        )\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\n----- Do &lt;lambda&gt; on Department Name -----\n----- Do &lt;lambda&gt; on Division Name -----\nDone\n----- Label Encoding -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 3, which is 0.02% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint(tdc.main_ddict['train']['Division Name'][:5])\nprint(tdc.main_ddict['train']['Rating'][:5])\nprint(tdc.main_ddict['train']['Department Name'][:5])\nprint(tdc.main_ddict['train']['label'][:5])\n\n['General Petite', 'General Petite', 'General', 'General', 'General Petite']\n[4.0, 4.0, 5.0, 3.0, 5.0]\n['Tops', 'Tops', 'Tops', 'Tops', 'Dresses']\n[[1.0, 4.0, 4.0], [1.0, 4.0, 4.0], [0.0, 5.0, 4.0], [0.0, 3.0, 4.0], [1.0, 5.0, 1.0]]\n\n\n\nprint(tdc.main_ddict['validation']['Division Name'][:5])\nprint(tdc.main_ddict['validation']['Rating'][:5])\nprint(tdc.main_ddict['validation']['Department Name'][:5])\nprint(tdc.main_ddict['validation']['label'][:5])\n\n['General Petite', 'General Petite', 'General', 'General Petite', 'General Petite']\n[5.0, 5.0, 5.0, 5.0, 4.0]\n['Intimate', 'Tops', 'Tops', 'Dresses', 'Dresses']\n[[1.0, 5.0, 2.0], [1.0, 5.0, 4.0], [0.0, 5.0, 4.0], [1.0, 5.0, 1.0], [1.0, 4.0, 1.0]]\n\n\n\n\nMulti-label classification\nLastly, let’s define a multi-label classification, where a text can have 1 or more label. Our data don’t have such labeling, so we will make a new one, just for demonstration.\n\ndf = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\n\n\ndf['Department Name'].unique()\n\narray(['Intimate', 'Dresses', 'Bottoms', 'Tops', 'Jackets', 'Trend', nan],\n      dtype=object)\n\n\n\ndf['Fake Label'] = [np.random.choice(df['Department Name'].unique()[:-1],size=np.random.randint(2,6),replace=False) for _ in range(len(df))]\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\nClothing ID\nAge\nTitle\nReview Text\nRating\nRecommended IND\nPositive Feedback Count\nDivision Name\nDepartment Name\nClass Name\nFake Label\n\n\n\n\n0\n767\n33\nNaN\nAbsolutely wonderful - silky and sexy and comf...\n4\n1\n0\nInitmates\nIntimate\nIntimates\n[Intimate, Dresses, Trend, Bottoms]\n\n\n1\n1080\n34\nNaN\nLove this dress! it's sooo pretty. i happene...\n5\n1\n4\nGeneral\nDresses\nDresses\n[Trend, Intimate]\n\n\n2\n1077\n60\nSome major design flaws\nI had such high hopes for this dress and reall...\n3\n0\n0\nGeneral\nDresses\nDresses\n[Intimate, Dresses, Bottoms, Trend]\n\n\n3\n1049\n50\nMy favorite buy!\nI love, love, love this jumpsuit. it's fun, fl...\n5\n1\n0\nGeneral Petite\nBottoms\nPants\n[Intimate, Bottoms]\n\n\n4\n847\n47\nFlattering shirt\nThis shirt is very flattering to all due to th...\n5\n1\n6\nGeneral\nTops\nBlouses\n[Trend, Bottoms, Dresses, Intimate, Jackets]\n\n\n\n\n\n\n\n\nYou don’t have to add any extra argument; the controller will determine whether this is for multilabel classification, based on the format of the label values\n\ntdc = TextDataController.from_df(df,\n                                 main_text='Review Text',\n                                 filter_dict={'Review Text': lambda x: x is not None},\n                                 label_names='Fake Label',\n                                 sup_types='classification',\n                                 seed=42,\n                                )\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n\n- Input Validation Precheck -\nData contains missing values!\n-----&gt; List of columns and the number of missing values for each\nTitle              3810\nReview Text         845\nDivision Name        14\nDepartment Name      14\nClass Name           14\ndtype: int64\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\nDone\n----- Label Encoding -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 1, which is 0.01% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntdc.main_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Review Text', 'Fake Label', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 18111\n    })\n    validation: Dataset({\n        features: ['Review Text', 'Fake Label', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 4529\n    })\n})\n\n\n\ntdc.label_lists\n\n[['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']]\n\n\n\ntdc.main_ddict['validation']['Fake Label'][2]\n\n['Trend', 'Intimate', 'Bottoms', 'Dresses']\n\n\nSince this is multilabel classification, the label will be one-hot encoded\n\ntdc.main_ddict['validation']['label'][2]\n\n[1, 1, 1, 0, 0, 1]\n\n\n\ntdc.main_ddict['validation']['label'][:5]\n\n[[0, 1, 1, 0, 1, 0],\n [0, 1, 1, 1, 1, 0],\n [1, 1, 1, 0, 0, 1],\n [1, 1, 0, 1, 0, 0],\n [0, 1, 1, 1, 0, 1]]\n\n\n\n\nNo label\nIf you don’t have a label to define, leave all label-related arguments blank\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         filter_dict={'Review Text': lambda x: x is not None},\n                         seed=42\n                        )\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 1, which is 0.01% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntdc.main_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Review Text', 'input_ids', 'attention_mask'],\n        num_rows: 18111\n    })\n    validation: Dataset({\n        features: ['Review Text', 'input_ids', 'attention_mask'],\n        num_rows: 4529\n    })\n})",
    "crumbs": [
      "1. Quick Starts",
      "Text Main"
    ]
  },
  {
    "objectID": "text_main.html#label-transformation",
    "href": "text_main.html#label-transformation",
    "title": "Text Main",
    "section": "6. Label transformation",
    "text": "6. Label transformation\nSometimes, you want to apply some light transformation to your label(s) before apply label encoding, e.g. there are some typos in your string label (classification), or you want to scale your regression label. TextDataController provides a way for you to do so, via label_tfm_dict argument. For the following example, I will fix the typo ‘Initmates’ in Division Name label, and log scale the Rating\n\nimport math\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names=['Division Name','Rating','Department Name'],\n                         sup_types=['classification','regression','classification'],\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                      'Division Name': lambda x: x is not None,\n                                     },\n                         label_tfm_dict={'Division Name': lambda x: x if x!='Initmates' else 'Intimates',\n                                         'Rating': lambda x: math.log(x)+1},\n                         seed=42,\n                         num_proc=1\n                        )\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\n----- Do &lt;lambda&gt; on Department Name -----\n----- Do &lt;lambda&gt; on Division Name -----\nDone\n-------------------- Label Transformation --------------------\nDone\n----- Label Encoding -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 3, which is 0.02% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice that in the label_lists, the label ‘Initmates’ has been replaced by ‘Intimates’\nAlso, the second empty list corresponds to the label value of Rating, which is for regression, thus results in an empty list\n\ntdc.label_lists\n\n[['General', 'General Petite', 'Intimates'],\n [],\n ['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']]\n\n\n\nprint(tdc.main_ddict['train']['Division Name'][:5])\nprint(tdc.main_ddict['train']['Rating'][:5])\nprint(tdc.main_ddict['train']['Department Name'][:5])\nprint(tdc.main_ddict['train']['label'][:5])\n\n['General Petite', 'General Petite', 'General', 'General', 'General Petite']\n[2.386294361119891, 2.386294361119891, 2.6094379124341005, 2.09861228866811, 2.6094379124341005]\n['Tops', 'Tops', 'Tops', 'Tops', 'Dresses']\n[[1.0, 2.386294361119891, 4.0], [1.0, 2.386294361119891, 4.0], [0.0, 2.6094379124341005, 4.0], [0.0, 2.09861228866811, 4.0], [1.0, 2.6094379124341005, 1.0]]\n\n\n\nprint(tdc.main_ddict['validation']['Division Name'][:5])\nprint(tdc.main_ddict['validation']['Rating'][:5])\nprint(tdc.main_ddict['validation']['Department Name'][:5])\nprint(tdc.main_ddict['validation']['label'][:5])\n\n['General Petite', 'General Petite', 'General', 'General Petite', 'General Petite']\n[2.6094379124341005, 2.6094379124341005, 2.6094379124341005, 2.6094379124341005, 2.386294361119891]\n['Intimate', 'Tops', 'Tops', 'Dresses', 'Dresses']\n[[1.0, 2.6094379124341005, 2.0], [1.0, 2.6094379124341005, 4.0], [0.0, 2.6094379124341005, 4.0], [1.0, 2.6094379124341005, 1.0], [1.0, 2.386294361119891, 1.0]]",
    "crumbs": [
      "1. Quick Starts",
      "Text Main"
    ]
  },
  {
    "objectID": "text_main.html#content-transformation",
    "href": "text_main.html#content-transformation",
    "title": "Text Main",
    "section": "7. Content Transformation",
    "text": "7. Content Transformation\nThis processing allows you to alter the text content in your dataset. You need to define a function that accepts a single string and returns a new, processed string. Note that this transformation will be applied to ALL of your dataset (both train and validation)\nLet’s say we want to normalize our text, because the text might contain some extra spaces between words, or not follow the “single space after a period” rule\n\n_tmp = \"This is a      sentence,which doesn't follow any rule!No single space is provided after period or punctuation marks.    Maybe there are too many spaces!?!   \"\n\n\nfrom underthesea import text_normalize\n\n\ntext_normalize(_tmp)\n\n\"This is a sentence , which doesn't follow any rule ! No single space is provided after period or punctuation marks . Maybe there are too many spaces ! ? !\"\n\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         sup_types='classification',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         content_transformations=text_normalize,\n                         seed=42\n                        )\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\n----- Do &lt;lambda&gt; on Department Name -----\nDone\n----- Label Encoding -----\nDone\n-------------------- Text Transformation --------------------\n----- text_normalize -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 3, which is 0.02% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntdc.main_ddict['train']['Review Text'][0]\n\n\"This sweater is beautiful , but is definitely more for looks than warmth . it's very soft , but very thin . i prefer the way it looks open rather than buttoned . i got the moss green color on sale , and i am glad i didn't pay full price for it--it ' s lovely , but certainly not worth $ 88 .\"\n\n\n\ntdc.main_ddict['validation']['Review Text'][0]\n\n'Such a fun jacket ! great to wear in the spring or to the office as an alternative to a formal blazer . very comfortable !'\n\n\nYou can chain multiple functions. Let’s say after text normalizing, I want to lowercase the text\n\nstr.lower('tHis IS NoT lowerCASE')\n\n'this is not lowercase'\n\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         sup_types='classification',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         content_transformations=[text_normalize,str.lower],\n                         seed=42\n                        )\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\n----- Do &lt;lambda&gt; on Department Name -----\nDone\n----- Label Encoding -----\nDone\n-------------------- Text Transformation --------------------\n----- text_normalize -----\n----- lower -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 3, which is 0.02% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntdc.main_ddict['train']['Review Text'][0]\n\n\"this sweater is beautiful , but is definitely more for looks than warmth . it's very soft , but very thin . i prefer the way it looks open rather than buttoned . i got the moss green color on sale , and i am glad i didn't pay full price for it--it ' s lovely , but certainly not worth $ 88 .\"\n\n\n\ntdc.main_ddict['validation']['Review Text'][0]\n\n'such a fun jacket ! great to wear in the spring or to the office as an alternative to a formal blazer . very comfortable !'\n\n\nYou can even perform some complex transformations, such as removing text inside parentheses, or replacing some texts via a pattern (which is doable using regular expression). Let’s make an example of such transformations, where we remove text inside parentheses, and convert any hashtag into the string ‘hashtag’\n\nimport re\n\n\ndef process_text(s):\n    # Remove texts inside parentheses\n    s = re.sub(r'\\(.*?\\)', '', s)\n    \n    # Convert any hashtag into the string 'hashtag'\n    s = re.sub(r'#\\w+', 'hashtag', s)\n    \n    return s.strip()\n\n\nprocess_text(\"#Promotions There's no way it works (I checked!), however it surprises me #howonearth #mindblowing\")\n\n\"hashtag There's no way it works , however it surprises me hashtag hashtag\"\n\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         sup_types='classification',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         content_transformations=process_text,\n                         seed=42\n                        )\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\n----- Do &lt;lambda&gt; on Department Name -----\nDone\n----- Label Encoding -----\nDone\n-------------------- Text Transformation --------------------\n----- process_text -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 3, which is 0.02% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone",
    "crumbs": [
      "1. Quick Starts",
      "Text Main"
    ]
  },
  {
    "objectID": "text_main.html#trainvalidation-split",
    "href": "text_main.html#trainvalidation-split",
    "title": "Text Main",
    "section": "8. Train/Validation Split",
    "text": "8. Train/Validation Split\nThere are several ways to perform a train/validation split with TextDataController\nThe first way is when you already have a validation split in your HuggingFace’s Dataset. Let’s use the Dataset built-in function train_test_split to simulate this\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\nddict_with_val = dset.train_test_split(test_size=0.1)\n# This will create a 'test' split instead of 'validation', so we will process a bit to have a validation split\nddict_with_val['validation']=ddict_with_val['test']\ndel ddict_with_val['test']\n\n\nddict_with_val\n\nDatasetDict({\n    train: Dataset({\n        features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'],\n        num_rows: 21137\n    })\n    validation: Dataset({\n        features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'],\n        num_rows: 2349\n    })\n})\n\n\n\ntdc = TextDataController(ddict_with_val,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         sup_types='classification',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         seed=42\n                        )\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\n----- Do &lt;lambda&gt; on Department Name -----\nDone\n----- Label Encoding -----\nDone\n-------------------- Train Test Split --------------------\nValidation split already exists\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 1, which is 0.00% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntdc.main_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 20374\n    })\n    validation: Dataset({\n        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 2253\n    })\n})\n\n\nA second way is to split randomly based on a ratio (a float between 0 and 1), or based on the number of data in your validation set\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         sup_types='classification',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         val_ratio=0.15,\n                         seed=42,\n                         verbose=False\n                        )\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\nprint(tdc.main_ddict)\n\nDatasetDict({\n    train: Dataset({\n        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 19231\n    })\n    validation: Dataset({\n        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 3395\n    })\n})\n\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         sup_types='classification',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         val_ratio=5000,\n                         seed=42,\n                         verbose=False\n                        )\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\nprint(tdc.main_ddict)\n\nDatasetDict({\n    train: Dataset({\n        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 17624\n    })\n    validation: Dataset({\n        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 5000\n    })\n})\n\n\nA third way is to do a random stratified split (inspired by sklearn’s). Let’s do a stratified split based on our label ‘Department Name’\n\ndf = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\n\n\ndf['Department Name'].value_counts(normalize=True)\n\nDepartment Name\nTops        0.445978\nDresses     0.269214\nBottoms     0.161852\nIntimate    0.073918\nJackets     0.043967\nTrend       0.005070\nName: proportion, dtype: float64\n\n\n\ntdc = TextDataController.from_df(df,\n                                 main_text='Review Text',\n                                 label_names='Department Name',\n                                 sup_types='classification',\n                                 filter_dict={'Review Text': lambda x: x is not None,\n                                              'Department Name': lambda x: x is not None,\n                                             },\n                                 val_ratio=0.2,\n                                 stratify_cols='Department Name',\n                                 seed=42\n                                )\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n\n- Input Validation Precheck -\nData contains missing values!\n-----&gt; List of columns and the number of missing values for each\nTitle              3810\nReview Text         845\nDivision Name        14\nDepartment Name      14\nClass Name           14\ndtype: int64\nData contains duplicated values!\n-----&gt; Number of duplications: 21 rows\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\n----- Do &lt;lambda&gt; on Department Name -----\nDone\n----- Label Encoding -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio, with stratifying\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 2, which is 0.01% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npd.Series(tdc.main_ddict['train']['Department Name']).value_counts(normalize=True)\n\nTops        0.444033\nDresses     0.271602\nBottoms     0.161878\nIntimate    0.072983\nJackets     0.044309\nTrend       0.005193\nName: proportion, dtype: float64\n\n\n\npd.Series(tdc.main_ddict['validation']['Department Name']).value_counts(normalize=True)\n\nTops        0.444101\nDresses     0.271542\nBottoms     0.161732\nIntimate    0.073133\nJackets     0.044189\nTrend       0.005303\nName: proportion, dtype: float64\n\n\nYou can also use multiple columns for your stratification\n\ntdc = TextDataController.from_df(df,\n                                 main_text='Review Text',\n                                 sup_types='classification',\n                                 label_names='Department Name',\n                                 filter_dict={'Review Text': lambda x: x is not None,\n                                              'Department Name': lambda x: x is not None,\n                                             },\n                                 val_ratio=0.2,\n                                 stratify_cols=['Department Name','Rating'],\n                                 seed=42,\n                                 verbose=False\n                                )\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n\n- Input Validation Precheck -\nData contains missing values!\n-----&gt; List of columns and the number of missing values for each\nTitle              3810\nReview Text         845\nDivision Name        14\nDepartment Name      14\nClass Name           14\ndtype: int64\nData contains duplicated values!\n-----&gt; Number of duplications: 21 rows\n\n\nAnd finally, you can omit any validation split if you specify val_ratio as None\n\ntdc = TextDataController.from_df(df,\n                                 main_text='Review Text',\n                                 label_names='Department Name',\n                                 sup_types='classification',\n                                 filter_dict={'Review Text': lambda x: x is not None,\n                                              'Department Name': lambda x: x is not None,\n                                             },\n                                 val_ratio=None,\n                                 seed=42\n                                )\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\ntdc.main_ddict\n\n- Input Validation Precheck -\nData contains missing values!\n-----&gt; List of columns and the number of missing values for each\nTitle              3810\nReview Text         845\nDivision Name        14\nDepartment Name      14\nClass Name           14\ndtype: int64\nData contains duplicated values!\n-----&gt; Number of duplications: 21 rows\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\n----- Do &lt;lambda&gt; on Department Name -----\nDone\n----- Label Encoding -----\nDone\n-------------------- Train Test Split --------------------\nNo validation split defined\nDone\n-------------------- Dropping unused features --------------------\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['Review Text', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 22628\n    })\n})",
    "crumbs": [
      "1. Quick Starts",
      "Text Main"
    ]
  },
  {
    "objectID": "text_main.html#upsampling",
    "href": "text_main.html#upsampling",
    "title": "Text Main",
    "section": "9. Upsampling",
    "text": "9. Upsampling\nThis is useful when you have an imbalanced dataset and you want to perform some upsampling (oversampling) on the minority class. In TextDataController, you can perform upsampling on any column of the original dataset, and you can even do upsampling on multiple columns at once\nBehind the scene, upsampling contains 2 steps; first, the subset of the data is collected based on the filtering condition, and then this subset is concatenated back into the original data\n\ndf = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig')\n\n\ndf['Department Name'].sample(frac=0.8).value_counts() \n# fraction 0.8 because we only do upsampling on train data, which is 80% of the total data\n\nDepartment Name\nTops        8379\nDresses     5044\nBottoms     3037\nIntimate    1396\nJackets      831\nTrend         92\nName: count, dtype: int64\n\n\n\ndf['Department Name'].sample(frac=0.8).value_counts(normalize=True)\n\nDepartment Name\nTops        0.446876\nDresses     0.269372\nBottoms     0.159823\nIntimate    0.073601\nJackets     0.044736\nTrend       0.005592\nName: proportion, dtype: float64\n\n\nLet’s say I want to upsampling the ‘Trend’ by the factor of 2 (x2 the amount of ‘Trend’ data)\n\ntdc = TextDataController.from_df(df,\n                                 main_text='Review Text',\n                                 label_names='Department Name',\n                                 sup_types='classification',\n                                 filter_dict={'Review Text': lambda x: x is not None,\n                                              'Department Name': lambda x: x is not None,\n                                             },\n                                 val_ratio=0.2,\n                                 stratify_cols='Department Name',\n                                 upsampling_list=[('Department Name',lambda x: x=='Trend')],\n                                 seed=42\n                                )\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n\n- Input Validation Precheck -\nData contains missing values!\n-----&gt; List of columns and the number of missing values for each\nTitle              3810\nReview Text         845\nDivision Name        14\nDepartment Name      14\nClass Name           14\ndtype: int64\nData contains duplicated values!\n-----&gt; Number of duplications: 21 rows\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\n----- Do &lt;lambda&gt; on Department Name -----\nDone\n----- Label Encoding -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio, with stratifying\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 2, which is 0.01% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Upsampling data --------------------\n----- Do &lt;lambda&gt; on Department Name -----\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npd.Series(tdc.main_ddict['train']['Department Name']).value_counts()\n\nTops        8037\nDresses     4916\nBottoms     2930\nIntimate    1321\nJackets      802\nTrend        188\nName: count, dtype: int64\n\n\n\npd.Series(tdc.main_ddict['train']['Department Name']).value_counts(normalize=True)\n\nTops        0.441739\nDresses     0.270199\nBottoms     0.161042\nIntimate    0.072606\nJackets     0.044080\nTrend       0.010333\nName: proportion, dtype: float64\n\n\nThe percenntage of ‘Trend’ data in the train set has approximately doubled (note that we filter some NaN text value so the result is not exactly doubled)\n\npd.Series(tdc.main_ddict['validation']['Department Name']).value_counts(normalize=True)\n\nTops        0.444101\nDresses     0.271542\nBottoms     0.161732\nIntimate    0.073133\nJackets     0.044189\nTrend       0.005303\nName: proportion, dtype: float64\n\n\nSince augmentation is applied only to the train set, the distribution of label in the validation set remains the same\nSimilarly, you can triple the amount of ‘Trend’ by repeating the procedure twice. In the following examples, I will triple the ‘Trend’ and double the ‘Jackets’\n\ntdc = TextDataController.from_df(df,\n                                 main_text='Review Text',\n                                 label_names='Department Name',\n                                 sup_types='classification',\n                                 filter_dict={'Review Text': lambda x: x is not None,\n                                              'Department Name': lambda x: x is not None,\n                                             },\n                                 val_ratio=0.2,\n                                 stratify_cols='Department Name',\n                                 upsampling_list=[('Department Name',lambda x: x=='Trend'),\n                                                  ('Department Name',lambda x: x=='Trend'),\n                                                  ('Department Name',lambda x: x=='Jackets')\n                                                 ],\n                                 # This can be simplified as\n#                                  upsampling_list=[('Department Name',lambda x: x=='Trend' or x=='Jackets'),\n#                                                   ('Department Name',lambda x: x=='Trend')],\n                                 seed=42\n                                )\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n\n- Input Validation Precheck -\nData contains missing values!\n-----&gt; List of columns and the number of missing values for each\nTitle              3810\nReview Text         845\nDivision Name        14\nDepartment Name      14\nClass Name           14\ndtype: int64\nData contains duplicated values!\n-----&gt; Number of duplications: 21 rows\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\n----- Do &lt;lambda&gt; on Department Name -----\nDone\n----- Label Encoding -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio, with stratifying\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 2, which is 0.01% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Upsampling data --------------------\n----- Do &lt;lambda&gt; on Department Name -----\n----- Do &lt;lambda&gt; on Department Name -----\n----- Do &lt;lambda&gt; on Department Name -----\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npd.Series(tdc.main_ddict['train']['Department Name']).value_counts()\n\nTops        8037\nDresses     4916\nBottoms     2930\nJackets     1604\nIntimate    1321\nTrend        282\nName: count, dtype: int64\n\n\nA word of warning: Upsampling is a slow procedure, as it requires multiple dataset concatenation.",
    "crumbs": [
      "1. Quick Starts",
      "Text Main"
    ]
  },
  {
    "objectID": "text_main.html#content-augmentation",
    "href": "text_main.html#content-augmentation",
    "title": "Text Main",
    "section": "10. Content Augmentation",
    "text": "10. Content Augmentation\nSimilarly to Content Transformation, Content Augmentation allows to alter the text content in your dataset. You also need to provide a function accepting a single string, and return a new, processed string. Unlike Content Transformation which is applied to ALL data, the Content Augmentation only applies to your TRAINING data\nOne of the popular library for data augmentation is nlpaug. We will demonstrate how to integrate its augmentation functions into our TextDataController\n\nimport nlpaug.augmenter.char as nac\n\n\n_tmp = \"I like my clothes loose fitting but even for me this ran large, i am 5'7 134b and medium fit in the shoulders but was too big overall\"\n\n\ndef nlp_aug(x,aug=None):\n    results = aug.augment(x)\n    if not isinstance(x,list): return results[0]\n    return results\n\nAugmentation by replacing character with nearby one on the keyboard\n\naug = nac.KeyboardAug(aug_char_max=3,aug_char_p=0.1,aug_word_p=0.07)\nnearby_aug_func = partial(nlp_aug,aug=aug)\n\n\nnearby_aug_func(_tmp)\n\n\"I liMe my c;othes loose fitting but even for me this ran large, i am 5 ' 7 134b and medium fit in the shoulders but was too big overa:l\"\n\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         sup_types='classification',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         content_augmentations=nearby_aug_func,\n                         seed=42,\n                         verbose=True\n                        )\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\n----- Do &lt;lambda&gt; on Department Name -----\nDone\n----- Label Encoding -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 3, which is 0.02% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Text Augmentation --------------------\n----- nlp_aug -----\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\ntdc.main_ddict['train']['Review Text'][:5]\n\n[\"This sweater is beautiful, but is definitely more for looks hhan warmth. it ' s very soft, but vdry thin. i prefer the way it looks open rather than buttoned. i got the moss green color on sale, and i am glad i diFn ' t pay full price for it - - it ' s lovdly, but certainly not wo%th $ 88.\",\n \"I ' m a curCy person, so my review might not be suited to everyone. my standard size in retailer tops is xl, and it is the same for this blouse. - overall: overaol gorgeo8s, wWll made blouse but i wish there was less fabric involved and the burnt out design didn ' t make a horizontal stripe across the back and biceps. this blokse just might not work out as well if you are a full figured person. - pros: g(rgeous blousf high quality unique - cons: i wish the burnt out design didj ' t make a hor\",\n 'This blouse is wonderful. i juwt got and wore the wine Solored blouse today. i received so many compliments. i love it and with the sale price it is so w(rth it.',\n 'When i saw this, i ordered i&lt;medistely thinking it was similar to the popular colorblocked stripe sweater from last yea5. the kgit is sfretchy and textured and fee:s like great quality (would wash w$ll ), but it \\' s pretty lightweight. the fit is huge. .. could easily size Eown. i \\' m 5 \\' 7 \" 128 # and found the small to be loose everywhere, including the arms. the length was at my knees, and the stripe fell awkwardly across my chest. no idea what i \\' d wear this with ev@n if it fit better. sadly, it \\' s goinR',\n \"This dress is a zillion times cuter in real life. it ' s ver% detro - swingy and girlish - it reminds me of something mia farrow would ' ve worn in her rosemary ' s baby era. i havF the black version and i ' ve paired mine with tall black gladiator Eandals for a more sIltry nighttime lo9k and also flip flops for beachy summer days. i think it ' s a total steal at the sale price.\"]\n\n\nAgain, since this is Content Augmentation, the validation set is unmodified.\n\ntdc.main_ddict['validation']['Review Text'][:5]\n\n['Such a fun jacket! great to wear in the spring or to the office as an alternative to a formal blazer. very comfortable!',\n 'I thought this shirt was really feminine and elegant. only downsides is some of the punched out holes had fabric still attached which you have cut off with scissors- otherwise the shirt will snag. and the second issue of bigger importance are the low armholes. lots of bra showing- not really sure how to get around that so i always wear it with a cardigan. but it would be nice not to have to. \\r\\nother than that it looks nice and pairs nicely with almost anything.',\n 'This top has a bit of a retro flare but so adorable on. looks really cute with a pair of faded boot cut jeans.',\n 'I first spotted this on an retailer employee, she paired it with a peasant top & wore it open w/jeans & boots- so darn cute. love how this peice transitions from summer to fall. i\\'m 5\\'4\" so i had to order the small petite which is perfect. note that this dress is very long! it\\'s just a must have garment. the colors/ print are just beautiful.',\n \"This is my new favorite dress! my only complaint is the slip is too small and the dress cannot be worn without it. i can't order a size up as the dress would then be huge. not sure what the solution is but the dress itself is stunning.\"]\n\n\nYou can even apply Content Augmentation stochastically, by adding a random condition in your augmentation function\n\n# def nlp_aug_stochastic(x,aug=None,p=0.5):\n#     results = aug.augment(x)\n#     if not isinstance(x,list): return results[0] if random.random()&lt;p else x\n#     return [a if random.random()&lt;p else b for a,b in zip(results,x)]\n\n\ndef nlp_aug_stochastic(x,aug=None,p=0.5):\n    if not isinstance(x,list): \n        if random.random()&lt;p: return aug.augment(x)[0]\n        return x\n    news=[]\n    originals=[]\n    for _x in x:\n        if random.random()&lt;p: news.append(_x)\n        else: originals.append(_x)\n    # only perform augmentation when needed\n    if len(news): news = aug.augment(news)\n    return news+originals\n\n\naug = nac.KeyboardAug(aug_char_max=3,aug_char_p=0.1,aug_word_p=0.07)\nnearby_aug_func = partial(nlp_aug_stochastic,aug=aug,p=0.3) # nearby_augmentation only applies 30% of the time, with p=0.3\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         sup_types='classification',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         content_augmentations=nearby_aug_func,\n                         seed=42\n                        )\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\n----- Do &lt;lambda&gt; on Department Name -----\nDone\n----- Label Encoding -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 3, which is 0.02% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Text Augmentation --------------------\n----- nlp_aug_stochastic -----\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\ntdc.main_ddict['train']['Review Text'][:10]\n\n[\"This sweater is beautiful, but is definitely more for pooks than warmth. it ' s very soBt, but very thin. i prefer the way it looks opeb rzther than buttoned. i got the moss green color on sale, and i am glad i didn ' t pay full price for it - - it ' s lovely, but certain/y not worth $ 88.\",\n \"I'm a curvy person, so my review might not be suited to everyone. my standard size in retailer tops is xl, and it is the same for this blouse.\\r\\n-\\r\\noverall:\\r\\noverall gorgeous, well made blouse but i wish there was less fabric involved and the burnt out design didn't make a horizontal stripe across the back and biceps. this blouse just might not work out as well if you are a full figured person.\\r\\n-\\r\\npros:\\r\\ngorgeous blouse\\r\\nhigh quality\\r\\nunique\\r\\n-\\r\\ncons:\\r\\ni wish the burnt out design didn't make a hor\",\n 'This blouse is wonderful. i just got and wor$ the wiJe colored blouse today. i received so many compliments. i love it and with the sale priSe it is so worth it.',\n 'When i saw this, i ordered immediately th(nking it was similar to the popular volorGlocked stripe sweater from last year. the knit is stretchy and textured and fe#ls like greaR quality (would wash well ), but it \\' s pretty lightweight. the fit is huge. .. couPd easily size d8wn. i \\' m 5 \\' 7 \" 128 # and found the small to be loose eveGywhere, including the arms. the length was at my knees, and the stripe fell awkwardly across my chest. no idea wtat i \\' d wear tyis with even if it fit better. sadly, it \\' s going',\n \"This dress is a zillion times cuter in real life. it's very retro-swingy and girlish- it reminds me of something mia farrow would've worn in her rosemary's baby era. i have the black version and i've paired mine with tall black gladiator sandals for a more sultry nighttime look and also flip flops for beachy summer days. i think it's a total steal at the sale price.\",\n \"This top is so soft and with a henley neck opening and longer ribbed shirttail hems, it not only feels heavenly against the skin but it gives off a casual chic vibe. it is also great for layering under shorter sweaters and sweatshirts to give my staples a little oomph. it is a bit sheer so cami is a must. i am also not sure how well it will hold up after washings, especially since it's priced quite high. i love it so much that i will most probably end up keeping it it is true to size. i ordered\",\n \"This is my first lair of ag and i loGe them so far. they are not cutfed as shown in the picture. they are long so i had to get them altered (i ' m 5 ' ' 5 ). the color is a rich blue and Ghey have a nice stretch. i haven ' t worn tNem all day yet to see if they keep their shape. usuZlly a 28 or 29 and went with the 28 on these. got them on 20 perc off salf so very happy!\",\n 'I liked this coat but my family said it looked too much like something hilary clinton would wear. i am 54 and i think it made me look a bit dowdy since it runs a bit big.',\n 'I saw a photographer wearing this at a wedding i went to in october. i absolutely fell in love. it is beautiful. i can\\'t wait to wear it for the holidays! i got the small petite and i am 5\\'2\", 125 lbs. fit great. enjoy!',\n \"This dress was adorable & fit great! regrettably, i had to return it since it wasn't lined.\"]\n\n\nOne of the advanced augmentation is “Contexttual Word Embeddings Augmenter” (code example: https://github.com/makcedward/nlpaug/blob/master/example/textual_augmenter.ipynb), where you can insert/substitute words using LLM such as BERT, RoBERTA …\n\nimport nlpaug.augmenter.word as naw\n\n\naug = naw.ContextualWordEmbsAug(model_path='roberta-base', \n                                device='cuda:0', # if you don't have gpu, change to 'cpu'\n                                action=\"substitute\",\n                                top_k=10,\n                               aug_p=0.07)\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\ncontextual_aug_func = partial(nlp_aug,aug=aug)\n\n\n_tmp = \"I like my clothes loose fitting but even for me this ran large, i am 5'7 134b and medium fit in the shoulders but was too big overall\"\n\n\ncontextual_aug_func(_tmp)\n\n\"I kept my clothes slim fitting but even for me this ran large, i am 5'7 134b and medium fit in upper shoulders but was too big overall\"\n\n\n\ncontextual_aug_func([_tmp for i in range(7)])\n\n[\"I like my clothes big enough but even for me this ran large, i am 5'7 134b and medium fit in the shoulders but felt too big overall\",\n \"I like my clothes loose fitting but even for me this ran large, i am 5'7 134b and I fit in the back but still too big overall\",\n \"I like my big loose fitting but even for me this ran large, i stand 5'7 134b and medium light in the shoulders but was too big overall\",\n \"I liked its own loose fitting but even for me this ran large, i am 5'7 134b and medium fit in the shoulders but was too big overall\",\n \"I like my clothes loose fitting but had given me this ran large, i am 5'7 134b is medium fit in the shoulders but was too big overall\",\n \"I made my clothes loose fitting but even for me this ran large, i am 5'7 134b and barely fit over the shoulders but was too big overall\",\n \"I like my clothes loose fitting but honestly for me this ran large, i am 5'7 134b and medium fit in all shoulders it was too big overall\"]\n\n\nFor this type of augmentation, it’s wise to use GPU to minimize processing speed. You also don’t want all of your text to be augmented, so let’s reuse the stochastic augmentation.\n\ncontextual_aug_func = partial(nlp_aug_stochastic,aug=aug,p=0.3)\n\n\n# add these 2 instance variables to your gpu augmentation\ncontextual_aug_func.run_on_gpu=True\ncontextual_aug_func.batch_size=32\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         sup_types='classification',\n                         label_names='Department Name',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         content_augmentations=contextual_aug_func,\n                         seed=42\n                        )\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\n----- Do &lt;lambda&gt; on Department Name -----\nDone\n----- Label Encoding -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 3, which is 0.02% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Text Augmentation --------------------\n----- nlp_aug_stochastic -----\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\ntdc.main_ddict['train']['Review Text'][:10]\n\n['This dress makes me so sad...the textured stretchy fabric, color, length, and overall swingy fit are spot on. as another reviewer noted though, the armholes and neck totally ruin the dress. the neck is tiny, which i could have gotten over once it was on, but the arm holes were just awful - too big all around (too tall of a cut plus too wide of a cut). basically, you could see my bra plus from the front there was unflattering exposure near the armholes. it could have been so good, but alas, but t',\n \"This top is very flattering, the fabric flows and moves. it fits perfectly (slim cut), but hides tummy bulges and other imperfections. and it's slimming too. can be dressed up or down, goes with everything. i ended up buying all three colors, and if there were more, i would buy more!\",\n 'This blouse is wonderful. i just got and wore the wine colored blouse today. i received so many compliments. i love it and with the sale price it is so worth it.',\n 'This top is very versatile. i wore it out to dinner with skinny jeans on a friday night, but it can easily transition to a saturday afternoon stroll around town top.',\n 'This top is so soft and luxuriously comfy! i love wearing it around the house, haven\\'t really \"dressed\" it up yet with jeans or jewelry. it runs slightly big, but if you like the oversized look, this is definitely perfect.',\n \"I was in love with this shirt from the moment i put it on. it is of high fit, with layers to ensure the top isn't sheer. the embroidery is incredibly pretty and the top looks way less grandma in it. i ordered the top xxs and it fits perfectly. i really appreciate that the underarm holes are just the right size and cant show off any of my bra, which sometimes happens with small tops. i can wear it with jeans and boots or with a pencil skirt and heels but is looks great with both outfits. o\",\n \"I read the other review and from the picture it looked as though it may be a little tight, so i ordered up to a large. the medium would have fit, but since i'm in ur mid-40's i felt more comfortable with large. but if people are trim and young or young at heart your usual medium will be fine. live the material and the navy makes it classy and rich looking. could be dressed up or down. have worn it to a cocktail party fundraiser with white crop sleeves and received many reviews. i'm always challenge\",\n \"This skirt is so ladylike and light as air! the cherry red color is beautiful - just as pictured. i can imagine so many opportunities to wear this skirt. with a sweater and tights now, and maybe a striped tee and sandals in the spring.\\ni'm sure i'll have this gorgeous classic in my wardrobe for a very long time to come!\",\n 'Very pretty dress, perfect style for my build, bigger busted, muffin top. the material/pattern is really pretty.',\n \"I purchased this top in the navy. the picture gives the top looking like an interesting blue with some purple in it, but in person the top is just... navy. the lace and fabric are soft. it fits true to me; i almost always wear a small and the small fit me. the wasn't quite my style, but it's a pretty top it will be great for spring and summer.\"]\n\n\nAnd finally, similarly to Content Transformation, you can link multiple augmentation functions together by providing a list of those functions in content_augmentations",
    "crumbs": [
      "1. Quick Starts",
      "Text Main"
    ]
  },
  {
    "objectID": "text_main.html#save-and-load-textdatacontroller",
    "href": "text_main.html#save-and-load-textdatacontroller",
    "title": "Text Main",
    "section": "11. Save and Load TextDataController",
    "text": "11. Save and Load TextDataController\n\nsource\n\nTextDataController.save_as_pickles\n\n TextDataController.save_as_pickles (fname, parent='pickle_files',\n                                     drop_attributes=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfname\n\n\nName of the pickle file\n\n\nparent\nstr\npickle_files\nParent folder\n\n\ndrop_attributes\nbool\nFalse\nWhether to drop large-size attributes\n\n\n\n\nsource\n\n\nTextDataController.from_pickle\n\n TextDataController.from_pickle (fname, parent='pickle_files')\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfname\n\n\nName of the pickle file\n\n\nparent\nstr\npickle_files\nParent folder\n\n\n\nTextDataController object can be saved and loaded with ease. This is especially useful after text processing and/or tokenization have been done\n\nfrom datasets import disable_caching\n\n\ndisable_caching() # disable huggingface caching to see data size\n\n\nfrom underthesea import text_normalize\nimport nlpaug.augmenter.char as nac\nimport nlpaug.augmenter.word as naw\n\n\ndef nlp_aug_stochastic(x,aug=None,p=0.5):\n    if not isinstance(x,list): \n        if random.random()&lt;p: return aug.augment(x)[0]\n        return x\n    news=[]\n    originals=[]\n    for _x in x:\n        if random.random()&lt;p: news.append(_x)\n        else: originals.append(_x)\n    # only perform augmentation when needed\n    if len(news): news = aug.augment(news)\n    return news+originals\n\n\naug2 = naw.ContextualWordEmbsAug(model_path='roberta-base', \n                                device='cuda:0', # if you don't have gpu, change to 'cpu'\n                                action=\"substitute\",\n                                top_k=10,\n                               aug_p=0.07)\n\ncontextual_aug_func = partial(nlp_aug_stochastic,aug=aug2,p=0.1)\n# add these 2 instance variables to your gpu augmentation\ncontextual_aug_func.run_on_gpu=True\ncontextual_aug_func.batch_size=32\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         sup_types='classification',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         content_transformations = [text_normalize,str.lower],\n                         content_augmentations = contextual_aug_func, \n                         process_metas=True,\n                         seed=42\n                        )\ntdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=True)\n\n-------------------- Start Main Text Processing --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\n----- Do &lt;lambda&gt; on Department Name -----\nDone\n----- Metadata Simple Processing & Concatenating to Main Content -----\nDone\n----- Label Encoding -----\nDone\n-------------------- Text Transformation --------------------\n----- text_normalize -----\n----- lower -----\nDone\n-------------------- Train Test Split --------------------\nValidation split based on val_ratio\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 0, which is 0.00% of training set\n-------------------- Text Augmentation --------------------\n----- nlp_aug_stochastic -----\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntdc.main_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Title', 'Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 18102\n    })\n    validation: Dataset({\n        features: ['Title', 'Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 4526\n    })\n})\n\n\n\ntdc.save_as_pickles('my_tdc')\n\nLet’s check the file size\n\nfile_stats = os.stat(Path('pickle_files/my_tdc.pkl'))\nprint(f'File Size in MegaBytes is {round(file_stats.st_size / (1024 * 1024), 3)}')\n\nFile Size in MegaBytes is 479.025\n\n\nLoad back our object\n\ntdc2 = TextDataController.from_pickle('my_tdc')\n\nYou can still access all its attributes, data, preprocessings, transformation/augmentation …\n\ntdc2.main_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Title', 'Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 18102\n    })\n    validation: Dataset({\n        features: ['Title', 'Review Text', 'Division Name', 'Department Name', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 4526\n    })\n})\n\n\n\nfor i,v in enumerate(tdc2.main_ddict['train']):\n    if i==3:break\n    print(f\"Text: {v['Review Text']}\\nLabel: {v['Department Name']} =&gt; {v['label']}\")\n    print('-'*10)\n\nText: general petite . meh . this tunic is way over priced for the style and quality . it fit comfortably ( runs a size larger ) but it's not really flattering , it jut kind of hangs there looking ok . it is a little too deep of a v cut for a work top as well . this top does not support the price at all . it felt like something i could find at department store for way less . i will be returning it .\nLabel: Tops =&gt; 4\n----------\nText: general . awesome buy ! . i am so happy i took a chance on this jumpsuit ! i am post-baby ( six weeks ) and although i intend on slimming down more i would say that it is flattering even at my current size . and it will only get better ! the quality and color are great !\nLabel: Bottoms =&gt; 0\n----------\nText: general petite . snap neck pullover . i love this top . i ordered it in a large thinking it would be a tight rib but it is not so i reordered it in a small . i am 5 ' 7 \" 145 lbs 34 g chest . the small fits perfectly and probably could have taken an xs . it is stretchy but fits wonderfully . i bought the black . i love how the neck snaps and adds a little pizzazz to a simple black turtle neck . i'm wearing it today with straight leg jeans and my leopard print ballet flats . i feel like audrey hepburn ! ! i will not be dry cleaning it . i will wash\nLabel: Bottoms =&gt; 0\n----------\n\n\n\ntdc2.label_lists\n\n[['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']]\n\n\n\ntdc2.filter_dict,tdc2.content_tfms,tdc2.aug_tfms\n\n({'Review Text': &lt;function __main__.&lt;lambda&gt;(x)&gt;,\n  'Department Name': &lt;function __main__.&lt;lambda&gt;(x)&gt;},\n [&lt;function underthesea.pipeline.text_normalize.text_normalize(text, tokenizer='underthesea')&gt;,\n  &lt;method 'lower' of 'str' objects&gt;],\n [functools.partial(&lt;function nlp_aug_stochastic&gt;, aug=&lt;nlpaug.augmenter.word.context_word_embs.ContextualWordEmbsAug object&gt;, p=0.1)])\n\n\nIf you don’t want to store the HuggingFace DatasetDict in your TextDataController, or the augmentation functions (typically when you already have a trained model, and you only use TextDataController to preprocess the test set), you can remove it in the save_as_pickles step\n\ntdc.save_as_pickles('my_lightweight_tdc',drop_attributes=True)\n\nLet’s check the file size\n\nfile_stats = os.stat(Path('pickle_files/my_lightweight_tdc.pkl'))\nprint(f'File Size in MegaBytes is {round(file_stats.st_size / (1024 * 1024), 3)}')\n\nFile Size in MegaBytes is 1.911\n\n\nLoad it back\n\ntdc3 = TextDataController.from_pickle('my_lightweight_tdc')\n\nWe will use this object to demonstrate the Test Set Construction in the next section\n\n\nConstruct a Test Dataset\n\nsource\n\n\nTextDataController.prepare_test_dataset\n\n TextDataController.prepare_test_dataset (test_dset, do_filtering=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntest_dset\n\n\nThe HuggingFace Dataset as Test set\n\n\ndo_filtering\nbool\nFalse\nwhether to perform data filtering on this test set\n\n\n\n\nsource\n\n\nTextDataController.prepare_test_dataset_from_csv\n\n TextDataController.prepare_test_dataset_from_csv (file_path,\n                                                   do_filtering=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\n\n\npath to csv file\n\n\ndo_filtering\nbool\nFalse\nwhether to perform data filtering on this test set\n\n\n\n\nsource\n\n\nTextDataController.prepare_test_dataset_from_df\n\n TextDataController.prepare_test_dataset_from_df (df, validate=True,\n                                                  do_filtering=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\n\n\nPandas Dataframe\n\n\nvalidate\nbool\nTrue\nwhether to perform input data validation\n\n\ndo_filtering\nbool\nFalse\nwhether to perform data filtering on this test set\n\n\n\n\nsource\n\n\nTextDataController.prepare_test_dataset_from_raws\n\n TextDataController.prepare_test_dataset_from_raws (content)\n\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\ncontent\nEither a single sentence, list of sentence or a dictionary with keys are metadata columns and values are list\n\n\n\nLet’s say you have done your preprocessing and tokenization in your training set, and have a nicely trained model, ready to do inference on new data. Here is how you can use TextDataController to apply all the necessary preprocessings to your new data\nWe will reuse the lightweight tdc object we created in the previous section (since we don’t really need all the training data just to construct new data). Also, we will take a small sample of our training data and pretend it is our test data\n\ntdc = TextDataController.from_pickle('my_lightweight_tdc')\n\nLet’s predict a few raw texts\nIf we only provide a raw text as follows\ntdc.prepare_test_dataset_from_raws('This shirt is so comfortable I love it!')\nYou will counter this error:\nValueError: There is/are metadatas in the preprocessing step. Please include a dictionary including these keys for \nmetadatas: ['Title', 'Division Name'], and texture content: Review Text\nSince our preprocessing includes some metadatas, you have to provide a dictionary as follows:\n\nresults = tdc.prepare_test_dataset_from_raws({'Review Text': 'This shirt is so comfortable I love it!',\n                                    'Title': 'Great shirt',\n                                    'Division Name': 'general'\n                                   })\n\n-------------------- Start Test Set Transformation --------------------\n----- Metadata Simple Processing & Concatenating to Main Content -----\nDone\n-------------------- Text Transformation --------------------\n----- text_normalize -----\n----- lower -----\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint(results[0])\n\n{'Review Text': 'general . great shirt . this shirt is so comfortable i love it !', 'Title': 'great shirt', 'Division Name': 'general', 'input_ids': [0, 15841, 479, 372, 6399, 479, 42, 6399, 16, 98, 3473, 939, 657, 24, 27785, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\nLet’s make prediction from a pandas Dataframe\n\ndf_test = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig').sample(frac=0.2,random_state=1)\n# drop NaN values in the label column\ndf_test = df_test[~df_test['Department Name'].isna()].reset_index(drop=True)\ndf_test.shape\n\n(4692, 10)\n\n\nThere are few things to pay attention to when constructing your new test set using TextDataController: - Only a few processings will be applied to your test set: Metadatas concatenation, Filtering (can be omited), Content Transformation, and Tokenization. Therefore, all columns required to perform these processings must exist in your test dataset - You can exclude the label column (e.g. Department Name in this example), since it’s a test set\nTo view all required columns, access the attribute cols_to_keep (you can omit the last column, which is the name of the label column)\n\ntdc.cols_to_keep\n\n['Review Text', 'Title', 'Division Name', 'Department Name']\n\n\nThis test dataset might have some NaN values in the text field (Review Text), thus we will turn on the filtering option to get rid of these NaNs, as this is what we did in the training set. If your test dataset don’t need any filtering, turn off this option\n\ntest_dset = tdc.prepare_test_dataset_from_df(df_test,validate=True,do_filtering=True)\n\n- Input Validation Precheck -\nData contains missing values!\n-----&gt; List of columns and the number of missing values for each\nTitle          758\nReview Text    164\ndtype: int64\nData contains duplicated values!\n-----&gt; Number of duplications: 2 rows\n-------------------- Start Test Set Transformation --------------------\n-------------------- Data Filtering --------------------\n----- Do &lt;lambda&gt; on Review Text -----\n----- Do &lt;lambda&gt; on Department Name -----\nDone\n----- Metadata Simple Processing & Concatenating to Main Content -----\nDone\n-------------------- Text Transformation --------------------\n----- text_normalize -----\n----- lower -----\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_dset\n\nDataset({\n    features: ['Title', 'Review Text', 'Division Name', 'Department Name', 'input_ids', 'attention_mask'],\n    num_rows: 4528\n})\n\n\n\nfor i in range(3):\n    print(f\"Text: {test_dset['Review Text'][i]}\")\n    print(f\"Input_ids: {test_dset['input_ids'][i]}\")\n    print('-'*10)\n\nText: general . perfect for work and play . this shirt works for both going out and going to work , and i can wear it with everything . fits perfect , tucked and untucked , tied and untied . i love it .\nInput_ids: [0, 15841, 479, 1969, 13, 173, 8, 310, 479, 42, 6399, 1364, 13, 258, 164, 66, 8, 164, 7, 173, 2156, 8, 939, 64, 3568, 24, 19, 960, 479, 10698, 1969, 2156, 21222, 8, 7587, 23289, 2156, 3016, 8, 7587, 2550, 479, 939, 657, 24, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n----------\nText: general petite . . i don't know why i had the opposite problem most reviewers had with these ..... i tried on the regular length in the store and found that they were just a bit too short with heels . ( i'm 5 ' 5 ) . i had them ordered in a petite and when they came , they were too short with flats ! maybe it's the way i like to wear them , i like my flare jeans to barely skim the ground . i just exchanged them for regular length and will wear them with a small wedge shoe . aside from the length issues , these are super cute\nInput_ids: [0, 15841, 4716, 1459, 479, 479, 939, 218, 75, 216, 596, 939, 56, 5, 5483, 936, 144, 34910, 56, 19, 209, 29942, 734, 939, 1381, 15, 5, 1675, 5933, 11, 5, 1400, 8, 303, 14, 51, 58, 95, 10, 828, 350, 765, 19, 8872, 479, 36, 939, 437, 195, 128, 195, 4839, 479, 939, 56, 106, 2740, 11, 10, 4716, 1459, 8, 77, 51, 376, 2156, 51, 58, 350, 765, 19, 20250, 27785, 2085, 24, 18, 5, 169, 939, 101, 7, 3568, 106, 2156, 939, 101, 127, 24186, 10844, 7, 6254, 28772, 5, 1255, 479, 939, 95, 11024, 106, 13, 1675, 5933, 8, 40, 3568, 106, 19, 10, 650, 27288, 12604, 479, 4364, 31, 5, 5933, 743, 2156, 209, 32, 2422, 11962, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n----------\nText: general petite . great pants . thes e cords are great--lightweight for fl winters , and the bootcut flare bottom is super cute with ballet flats or booties . i am 5 ' 10 \" and typically a size 8 ; the size 29 fit perfectly . they have a little stretch to them , which is great . very flattering--wish i could order in more colors ! !\nInput_ids: [0, 15841, 4716, 1459, 479, 372, 9304, 479, 5, 29, 364, 37687, 32, 372, 5579, 6991, 4301, 13, 2342, 31000, 2156, 8, 5, 9759, 8267, 24186, 2576, 16, 2422, 11962, 19, 22573, 20250, 50, 9759, 918, 479, 939, 524, 195, 128, 158, 22, 8, 3700, 10, 1836, 290, 25606, 5, 1836, 1132, 2564, 6683, 479, 51, 33, 10, 410, 4140, 7, 106, 2156, 61, 16, 372, 479, 182, 34203, 5579, 605, 1173, 939, 115, 645, 11, 55, 8089, 27785, 27785, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n----------",
    "crumbs": [
      "1. Quick Starts",
      "Text Main"
    ]
  },
  {
    "objectID": "roberta_lm_for_streaming.html",
    "href": "roberta_lm_for_streaming.html",
    "title": "Roberta Language Model for a streamed dataset",
    "section": "",
    "text": "import os\n#This will specify a (or a list) of GPUs for training\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom that_nlp_library.text_main_lm_streaming import *\nfrom that_nlp_library.utils import seed_everything\nfrom that_nlp_library.model_lm_main import *\nfrom underthesea import text_normalize\nfrom functools import partial\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForMaskedLM\nfrom datasets import load_dataset\nimport pandas as pd\nimport numpy as np\nfrom transformers import DataCollatorForLanguageModeling",
    "crumbs": [
      "2. All Use Cases",
      "e. Training A Streamed Dataset",
      "Roberta Language Model for a streamed dataset"
    ]
  },
  {
    "objectID": "roberta_lm_for_streaming.html#create-a-textdatalmcontroller-object",
    "href": "roberta_lm_for_streaming.html#create-a-textdatalmcontroller-object",
    "title": "Roberta Language Model for a streamed dataset",
    "section": "Create a TextDataLMController object",
    "text": "Create a TextDataLMController object\nWe will reuse the data and the preprocessings in this tutorial\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\nddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\nddict_with_val['validation'] = ddict_with_val['test']\nddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\ndel ddict_with_val['test']\n\n\ntdc = TextDataLMControllerStreaming(ddict_with_val,\n                                    main_text='Review Text',\n                                    filter_dict={'Review Text': lambda x: x is not None},\n                                    metadatas=['Title','Division Name'],\n                                    content_transformations=[text_normalize,str.lower],\n                                    cols_to_keep=['Clothing ID','Review Text'],\n                                    seed=42,\n                                    batch_size=1024,\n                                    verbose=False\n                                    )\n\nDefine our tokenizer for Roberta\n\n_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\nProcess and tokenize our dataset (using line-by-line tokenization)\n\ntdc.process_and_tokenize(_tokenizer,line_by_line=True,max_length=-1)\n\n\ntdc.main_ddict\n\nDatasetDict({\n    train: IterableDataset({\n        features: Unknown,\n        n_shards: 1\n    })\n    validation: Dataset({\n        features: ['Clothing ID', 'Review Text', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n        num_rows: 2253\n    })\n})\n\n\nAnd set the data collator\n\ntdc.set_data_collator(is_mlm=True,mlm_prob=0.15)",
    "crumbs": [
      "2. All Use Cases",
      "e. Training A Streamed Dataset",
      "Roberta Language Model for a streamed dataset"
    ]
  },
  {
    "objectID": "roberta_lm_for_streaming.html#initialize-and-train-roberta-language-model",
    "href": "roberta_lm_for_streaming.html#initialize-and-train-roberta-language-model",
    "title": "Roberta Language Model for a streamed dataset",
    "section": "Initialize and train Roberta Language Model",
    "text": "Initialize and train Roberta Language Model\n\n_config = AutoConfig.from_pretrained('roberta-base',\n                                    vocab_size=len(_tokenizer))\n_config\n\nRobertaConfig {\n  \"_name_or_path\": \"roberta-base\",\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.40.1\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n\n\n_model = language_model_init(AutoModelForMaskedLM,\n                             config=_config,\n                             cpoint_path='roberta-base',\n                             seed=42\n                            )\n\nTotal parameters: 124697433\nTotal trainable parameters: 124697433\n\n\nCreate a model controller\n\ncontroller = ModelLMController(_model,data_store=tdc,seed=42)\n\nAnd we can start training our model\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 4\nwarmup_ratio=0.25\ncontroller.fit(epochs,lr,\n               batch_size=bs,\n               weight_decay=wd,\n               warmup_ratio=warmup_ratio,\n               save_checkpoint=False,\n               len_train=20000 # estimation of number of samples in train set\n              )\n\nmax_steps is given, it will override any value given in num_train_epochs\n\n\n\n\n    \n      \n      \n      [1248/1248 07:23, Epoch 3/9223372036854775807]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n0\nNo log\n1.511546\n0.663742\n\n\n1\n1.651100\n1.410441\n0.676708\n\n\n2\n1.651100\n1.279535\n0.697920\n\n\n3\n1.651100\n1.264103\n0.696687\n\n\n\n\n\n\n\n\n    \n      \n      \n      [71/71 00:04]\n    \n    \n\n\nPerplexity on validation set: 3.589\n\n\nFinetuning from a pretrained model results in a massive improvement in terms of metrics\n\ncontroller.trainer.model.save_pretrained('./sample_weights/lm_model')",
    "crumbs": [
      "2. All Use Cases",
      "e. Training A Streamed Dataset",
      "Roberta Language Model for a streamed dataset"
    ]
  },
  {
    "objectID": "roberta_lm_for_streaming.html#fill-mask-using-model",
    "href": "roberta_lm_for_streaming.html#fill-mask-using-model",
    "title": "Roberta Language Model for a streamed dataset",
    "section": "Fill mask using model",
    "text": "Fill mask using model\n\ntrained_model = language_model_init(AutoModelForMaskedLM,\n                                    cpoint_path='./sample_weights/lm_model',\n                                   )\n\nTotal parameters: 124697433\nTotal trainable parameters: 124697433\n\n\n\ncontroller2 = ModelLMController(trained_model,data_store=tdc,seed=42)\n\n\ncontroller2.data_store.tokenizer.mask_token\n\n'&lt;mask&gt;'\n\n\n\ninp1 = {'Clothing ID':1,\n        'Title':'Flattering',\n        'Division Name':'General',\n        'Review Text': \"Love this &lt;mask&gt;. The detail is amazing. Runs small I ordered a 12 I'm usually a 10, but still a little snug\"\n       }\n\n\ncontroller2.predict_raw_text(inp1,print_result=True)\n\nScore: 0.371 &gt;&gt;&gt; general. flattering. love this top. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.281 &gt;&gt;&gt; general. flattering. love this dress. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.119 &gt;&gt;&gt; general. flattering. love this shirt. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.066 &gt;&gt;&gt; general. flattering. love this skirt. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.052 &gt;&gt;&gt; general. flattering. love this sweater. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\n--------------------\n\n\nYou can input several raw texts\n\ninp2 = {'Clothing ID':[1,2],\n        'Title':['Flattering','Lovely, but small'],\n        'Division Name':['General','General'],\n        'Review Text': [\"Love this &lt;mask&gt;. The detail is amazing. Runs small I ordered a 12 I'm usually a 10, but still a little snug\",\n                        \"Love this skirt. The detail is amazing. Runs &lt;mask&gt;, I ordered a 12 I'm usually a 10, but still a little snug\"]\n       }\n\n\ncontroller2.predict_raw_text(inp2,print_result=True)\n\nScore: 0.371 &gt;&gt;&gt; general. flattering. love this top. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.281 &gt;&gt;&gt; general. flattering. love this dress. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.119 &gt;&gt;&gt; general. flattering. love this shirt. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.066 &gt;&gt;&gt; general. flattering. love this skirt. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.052 &gt;&gt;&gt; general. flattering. love this sweater. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\n--------------------\nScore: 0.933 &gt;&gt;&gt; general. lovely, but small. love this skirt. the detail is amazing. runs small, i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.033 &gt;&gt;&gt; general. lovely, but small. love this skirt. the detail is amazing. runs large, i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.009 &gt;&gt;&gt; general. lovely, but small. love this skirt. the detail is amazing. runs big, i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.006 &gt;&gt;&gt; general. lovely, but small. love this skirt. the detail is amazing. runs short, i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.004 &gt;&gt;&gt; general. lovely, but small. love this skirt. the detail is amazing. runs smaller, i ordered a 12 i'm usually a 10, but still a little snug\n--------------------\n\n\n\ncontroller2.predict_raw_text(inp2,print_result=False)\n\n[[{'score': 0.3714928925037384,\n   'token': 299,\n   'token_str': ' top',\n   'sequence': \"general. flattering. love this top. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\"},\n  {'score': 0.28066369891166687,\n   'token': 3588,\n   'token_str': ' dress',\n   'sequence': \"general. flattering. love this dress. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\"},\n  {'score': 0.11859548091888428,\n   'token': 6399,\n   'token_str': ' shirt',\n   'sequence': \"general. flattering. love this shirt. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\"},\n  {'score': 0.06550988554954529,\n   'token': 16576,\n   'token_str': ' skirt',\n   'sequence': \"general. flattering. love this skirt. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\"},\n  {'score': 0.05240405723452568,\n   'token': 23204,\n   'token_str': ' sweater',\n   'sequence': \"general. flattering. love this sweater. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\"}],\n [{'score': 0.9333353638648987,\n   'token': 650,\n   'token_str': ' small',\n   'sequence': \"general. lovely, but small. love this skirt. the detail is amazing. runs small, i ordered a 12 i'm usually a 10, but still a little snug\"},\n  {'score': 0.0329098179936409,\n   'token': 739,\n   'token_str': ' large',\n   'sequence': \"general. lovely, but small. love this skirt. the detail is amazing. runs large, i ordered a 12 i'm usually a 10, but still a little snug\"},\n  {'score': 0.00943901389837265,\n   'token': 380,\n   'token_str': ' big',\n   'sequence': \"general. lovely, but small. love this skirt. the detail is amazing. runs big, i ordered a 12 i'm usually a 10, but still a little snug\"},\n  {'score': 0.006075778976082802,\n   'token': 765,\n   'token_str': ' short',\n   'sequence': \"general. lovely, but small. love this skirt. the detail is amazing. runs short, i ordered a 12 i'm usually a 10, but still a little snug\"},\n  {'score': 0.003932583145797253,\n   'token': 2735,\n   'token_str': ' smaller',\n   'sequence': \"general. lovely, but small. love this skirt. the detail is amazing. runs smaller, i ordered a 12 i'm usually a 10, but still a little snug\"}]]",
    "crumbs": [
      "2. All Use Cases",
      "e. Training A Streamed Dataset",
      "Roberta Language Model for a streamed dataset"
    ]
  },
  {
    "objectID": "roberta_lm_for_streaming.html#extract-hidden-states-from-model",
    "href": "roberta_lm_for_streaming.html#extract-hidden-states-from-model",
    "title": "Roberta Language Model for a streamed dataset",
    "section": "Extract hidden states from model",
    "text": "Extract hidden states from model\n\nFrom raw texts\n\ninp1 = {'Clothing ID':1,\n        'Title':'Flattering',\n        'Division Name': 'General',\n        'Review Text': \"Love this skirt. The detail is amazing. Runs small I ordered a 12 I'm usually a 10, but still a little snug\"\n       }\n\n\n_config = AutoConfig.from_pretrained('./sample_weights/lm_model',output_hidden_states=True)\n\n\ntrained_model = language_model_init(AutoModelForMaskedLM,\n                                    cpoint_path='./sample_weights/lm_model',\n                                    config=_config\n                                   )\n\ncontroller2 = ModelLMController(trained_model,data_store=tdc,seed=42)\n\nTotal parameters: 124697433\nTotal trainable parameters: 124697433\n\n\n\nhidden_from_ip1 = controller2.get_hidden_states_from_raw_text(inp1,\n                                                              state_name='hidden_states',\n                                                              state_idx=[-1,0]\n                                                             )\n\n\nhidden_from_ip1\n\nDataset({\n    features: ['Clothing ID', 'Review Text', 'input_ids', 'attention_mask', 'special_tokens_mask', 'hidden_states'],\n    num_rows: 1\n})\n\n\n\nhidden_from_ip1['hidden_states'].shape\n\n(1, 768)\n\n\n\n\nFrom validation (or even train) set\n\nhidden_from_vals = controller2.get_hidden_states(ds_type='validation',\n                                                 state_name='hidden_states',\n                                                 state_idx=[-1,0]\n                                                )\n\n\nhidden_from_vals\n\nDataset({\n    features: ['Clothing ID', 'Review Text', 'input_ids', 'attention_mask', 'special_tokens_mask', 'hidden_states'],\n    num_rows: 2253\n})\n\n\n\nhidden_from_vals['hidden_states'].shape\n\n(2253, 768)",
    "crumbs": [
      "2. All Use Cases",
      "e. Training A Streamed Dataset",
      "Roberta Language Model for a streamed dataset"
    ]
  },
  {
    "objectID": "roberta_lm_for_streaming.html#create-a-textdatalmcontroller-object-1",
    "href": "roberta_lm_for_streaming.html#create-a-textdatalmcontroller-object-1",
    "title": "Roberta Language Model for a streamed dataset",
    "section": "Create a TextDataLMController object",
    "text": "Create a TextDataLMController object\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\nddict_with_val = dset.train_test_split(test_size=0.1,seed=42)\nddict_with_val['validation'] = ddict_with_val['test']\nddict_with_val['train'] = ddict_with_val['train'].to_iterable_dataset()\ndel ddict_with_val['test']\n\ntdc = TextDataLMControllerStreaming(ddict_with_val,\n                                    main_text='Review Text',\n                                    filter_dict={'Review Text': lambda x: x is not None},\n                                    metadatas=['Title','Division Name'],\n                                    content_transformations=[text_normalize,str.lower],\n                                    cols_to_keep=['Clothing ID','Review Text'],\n                                    seed=42,\n                                    batch_size=1024,\n                                    verbose=False\n                                    )\n\nDefine our tokenizer for Roberta\n\n_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\nProcess and tokenize our dataset (using token concatenation technique)\n\nblock_size=140\ntdc.process_and_tokenize(_tokenizer,line_by_line=False,max_length=block_size)\n\n\ntdc.main_ddict\n\nDatasetDict({\n    train: IterableDataset({\n        features: Unknown,\n        n_shards: 1\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask', 'special_tokens_mask'],\n        num_rows: 1342\n    })\n})\n\n\nAnd set the data collator\n\ntdc.set_data_collator(is_mlm=True,mlm_prob=0.15)",
    "crumbs": [
      "2. All Use Cases",
      "e. Training A Streamed Dataset",
      "Roberta Language Model for a streamed dataset"
    ]
  },
  {
    "objectID": "roberta_lm_for_streaming.html#initialize-and-train-roberta-language-model-1",
    "href": "roberta_lm_for_streaming.html#initialize-and-train-roberta-language-model-1",
    "title": "Roberta Language Model for a streamed dataset",
    "section": "Initialize and train Roberta Language Model",
    "text": "Initialize and train Roberta Language Model\n\n_config = AutoConfig.from_pretrained('roberta-base',\n                                    vocab_size=len(_tokenizer))\n_config\n\nRobertaConfig {\n  \"_name_or_path\": \"roberta-base\",\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.40.1\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n\n\n_model = language_model_init(AutoModelForMaskedLM,\n                             config=_config,\n                             cpoint_path='roberta-base',\n                             seed=42\n                            )\n\nTotal parameters: 124697433\nTotal trainable parameters: 124697433\n\n\nCreate a model controller\n\ncontroller = ModelLMController(_model,data_store=tdc,seed=42)\n\nAnd we can start training our model\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 4\nwarmup_ratio=0.25\ncontroller.fit(epochs,lr,\n               batch_size=bs,\n               weight_decay=wd,\n               warmup_ratio=warmup_ratio,\n               save_checkpoint=False,\n               len_train=20000\n              )\n\n#  [808/808 03:58, Epoch 4/4]\n# Epoch Training Loss   Validation Loss Accuracy\n# 1 No log  1.694216    0.628713\n# 2 1.860100    1.601513    0.642077\n# 3 1.860100    1.515734    0.656354\n# 4 1.561200    1.477700    0.662074\n#  [103/103 00:04]\n# Perplexity on validation set: 4.413\n\nmax_steps is given, it will override any value given in num_train_epochs\n\n\n\n\n    \n      \n      \n      [1248/1248 09:17, Epoch 6/9223372036854775807]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n0\nNo log\n1.613501\n0.643859\n\n\n1\nNo log\n1.542169\n0.650042\n\n\n2\nNo log\n1.441636\n0.667640\n\n\n3\n1.721100\n1.397509\n0.679836\n\n\n4\n1.721100\n1.363855\n0.685481\n\n\n5\n1.721100\n1.326490\n0.688224\n\n\n6\n1.721100\n1.334893\n0.687034\n\n\n\n\n\n\n\n\n    \n      \n      \n      [42/42 00:03]\n    \n    \n\n\nPerplexity on validation set: 3.775\n\n\n\ncontroller.trainer.model.save_pretrained('./sample_weights/lm_model')",
    "crumbs": [
      "2. All Use Cases",
      "e. Training A Streamed Dataset",
      "Roberta Language Model for a streamed dataset"
    ]
  },
  {
    "objectID": "roberta_lm_for_streaming.html#fill-mask-using-model-1",
    "href": "roberta_lm_for_streaming.html#fill-mask-using-model-1",
    "title": "Roberta Language Model for a streamed dataset",
    "section": "Fill mask using model",
    "text": "Fill mask using model\n\ntrained_model = language_model_init(AutoModelForMaskedLM,\n                                    cpoint_path='./sample_weights/lm_model',\n                                   )\n\nTotal parameters: 124697433\nTotal trainable parameters: 124697433\n\n\n\ncontroller2 = ModelLMController(trained_model,data_store=tdc,seed=42)\n\n\ninp1 = {'Clothing ID':1,\n        'Title':'Flattering',\n        'Division Name':'General',\n        'Review Text': \"Love this &lt;mask&gt;. The detail is amazing. Runs small I ordered a 12 I'm usually a 10, but still a little snug\"\n       }\n\n\ncontroller2.predict_raw_text(inp1,print_result=True)\n\nScore: 0.323 &gt;&gt;&gt; general. flattering. love this dress. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.317 &gt;&gt;&gt; general. flattering. love this top. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.116 &gt;&gt;&gt; general. flattering. love this shirt. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.063 &gt;&gt;&gt; general. flattering. love this skirt. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.047 &gt;&gt;&gt; general. flattering. love this sweater. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\n--------------------\n\n\nYou can input several raw texts\n\ninp2 = {'Clothing ID':[1,2],\n        'Title':['Flattering','Lovely, but small'],\n        'Division Name':['General','General'],\n        'Review Text': [\"Love this &lt;mask&gt;. The detail is amazing. Runs small I ordered a 12 I'm usually a 10, but still a little snug\",\n                        \"Love this skirt. The detail is amazing. Runs &lt;mask&gt;, I ordered a 12 I'm usually a 10, but still a little snug\"]\n       }\n\n\ncontroller2.predict_raw_text(inp2,print_result=True)\n\nScore: 0.323 &gt;&gt;&gt; general. flattering. love this dress. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.317 &gt;&gt;&gt; general. flattering. love this top. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.116 &gt;&gt;&gt; general. flattering. love this shirt. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.063 &gt;&gt;&gt; general. flattering. love this skirt. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.047 &gt;&gt;&gt; general. flattering. love this sweater. the detail is amazing. runs small i ordered a 12 i'm usually a 10, but still a little snug\n--------------------\nScore: 0.935 &gt;&gt;&gt; general. lovely, but small. love this skirt. the detail is amazing. runs small, i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.038 &gt;&gt;&gt; general. lovely, but small. love this skirt. the detail is amazing. runs large, i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.009 &gt;&gt;&gt; general. lovely, but small. love this skirt. the detail is amazing. runs big, i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.004 &gt;&gt;&gt; general. lovely, but small. love this skirt. the detail is amazing. runs short, i ordered a 12 i'm usually a 10, but still a little snug\nScore: 0.003 &gt;&gt;&gt; general. lovely, but small. love this skirt. the detail is amazing. runs smaller, i ordered a 12 i'm usually a 10, but still a little snug\n--------------------",
    "crumbs": [
      "2. All Use Cases",
      "e. Training A Streamed Dataset",
      "Roberta Language Model for a streamed dataset"
    ]
  },
  {
    "objectID": "text_augmentation.html",
    "href": "text_augmentation.html",
    "title": "Text Augmentation",
    "section": "",
    "text": "source\n\nremove_vnmese_accent\n\n remove_vnmese_accent (sentence:str, prob=1)\n\nPerform Vietnamese accent removal\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsentence\nstr\n\nInput sentence\n\n\nprob\nint\n1\nProbability that this function is applied to the text\n\n\n\n\ninp = 'hội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức'\n\n\nremove_vnmese_accent(inp)\n\n'hoi cu dan chung cu sen hong - chung cu lotus song than thu duc'\n\n\n\nfor i in range(10):\n    print(remove_vnmese_accent(inp,prob=0.5))\n\nhội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức\nhoi cu dan chung cu sen hong - chung cu lotus song than thu duc\nhội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức\nhội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức\nhội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức\nhội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức\nhội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức\nhoi cu dan chung cu sen hong - chung cu lotus song than thu duc\nhoi cu dan chung cu sen hong - chung cu lotus song than thu duc\nhoi cu dan chung cu sen hong - chung cu lotus song than thu duc\n\n\n\nsource\n\n\nfill_mask_augmentation\n\n fill_mask_augmentation (sentence:str, fillmask_pipeline, prob=1,\n                         random_top_k=1)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsentence\nstr\n\nInput Sentence,\n\n\nfillmask_pipeline\n\n\nHuggingFace fill-mask pipeline\n\n\nprob\nint\n1\nProbability that this function is applied to the text\n\n\nrandom_top_k\nint\n1\nTo select output randomly from top k mask filled\n\n\n\n\nfrom transformers import pipeline\n\n\nfrom that_nlp_library.text_transformation import apply_vnmese_word_tokenize\n\n\nfillmask = pipeline(\"fill-mask\", model='vinai/phobert-base')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\ninp='Tôi vào phòng và mở TV lên. Tìm đến bóng đá là cách duy nhất để tôi thư giãn'\ninp = apply_vnmese_word_tokenize(inp) # because we are using phobert\nprint(inp)\n\nTôi vào phòng và mở TV lên . Tìm đến bóng_đá là cách duy_nhất để tôi thư_giãn\n\n\n\nrandom.seed(1)\n\n\nfill_mask_augmentation(inp,fillmask,random_top_k=1)\n\nCPU times: user 1.11 s, sys: 46.8 ms, total: 1.16 s\nWall time: 102 ms\n\n\n'Tôi vào phòng và mở TV lên. Tìm đến bóng_đá là cách duy_nhất để tôi thư_giãn'\n\n\n\nfill_mask_augmentation(inp,fillmask,random_top_k=10)\n\n'Tôi vào phòng và mở TV lên. Tìm đến bóng_đá là cách duy_nhất để bạn thư_giãn'\n\n\n\nrandom.seed(1)\n\n\nfor i in range(10):\n    print(fill_mask_augmentation(inp,fillmask,random_top_k=5,prob=1))\n\nTôi vào phòng và mở TV lên. Tìm đến bóng_đá là cách duy_nhất để tôi thư_giãn\nTôi vào phòng và mở TV lên. Tìm đến bóng_đá là cách duy_nhất để bạn thư_giãn\nTôi vào phòng và mở TV lên Tìm đến bóng_đá là cách duy_nhất để tôi thư_giãn\nTôi vào phòng và mở TV lên. Tìm đến bóng_đá là cách duy_nhất để tôi thư_giãn\nTôi vào phòng và mở TV lên. Tìm đến bóng_đá là cách duy_nhất để tôi thư_giãn\nTôi vào phòng và mở TV lên. quan_tâm đến bóng_đá là cách duy_nhất để tôi thư_giãn\nTôi vào phòng và mở TV lên. Tìm đến bóng_đá là cách duy_nhất để tôi thư_giãn\nTôi về phòng và mở TV lên. Tìm đến bóng_đá là cách duy_nhất để tôi thư_giãn\nTôi vào phòng và mở TV lên xem Tìm đến bóng_đá là cách duy_nhất để tôi thư_giãn\nTôi vào phòng và mở TV lên. liên_quan đến bóng_đá là cách duy_nhất để tôi thư_giãn",
    "crumbs": [
      "5. Text Classes",
      "a. Text Processing",
      "Text Augmentation"
    ]
  },
  {
    "objectID": "roberta_dhc.html",
    "href": "roberta_dhc.html",
    "title": "Roberta model with Deep Hierarchical Classification",
    "section": "",
    "text": "In this tutorial, we walk through another special case of classification with multiple heads, which is based on this paper: https://arxiv.org/ftp/arxiv/papers/2005/2005.06692.pdf\nimport os\n#This will specify a (or a list) of GPUs for training\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom that_nlp_library.text_main import *\nfrom that_nlp_library.utils import seed_everything\nfrom underthesea import text_normalize\nfrom functools import partial\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport nlpaug.augmenter.char as nac\nfrom datasets import load_dataset\nimport random\nfrom transformers import RobertaTokenizer\nfrom datasets import Dataset",
    "crumbs": [
      "2. All Use Cases",
      "a. Roberta-based models for Supervised Learning",
      "Roberta model with Deep Hierarchical Classification"
    ]
  },
  {
    "objectID": "roberta_dhc.html#build-dhc-conditional-mask",
    "href": "roberta_dhc.html#build-dhc-conditional-mask",
    "title": "Roberta model with Deep Hierarchical Classification",
    "section": "Build DHC Conditional Mask",
    "text": "Build DHC Conditional Mask\n\ntdc.label_names\n\n['Division Name', 'Department Name']\n\n\n\ntdc.label_lists\n\n[['General', 'General Petite', 'Initmates'],\n ['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']]\n\n\n\ndf_trn = tdc.main_ddict['train'].to_pandas()\n\n\ndf_labels = pd.DataFrame(df_trn['label'].tolist())\ndf_labels.columns=tdc.label_names\n\n\ndf_labels.head()\n\n\n\n\n\n\n\n\n\nDivision Name\nDepartment Name\n\n\n\n\n0\n0\n4\n\n\n1\n1\n1\n\n\n2\n1\n1\n\n\n3\n1\n3\n\n\n4\n0\n1\n\n\n\n\n\n\n\n\n\ndhc_mask = build_DHC_conditional_mask(df_labels,*tdc.label_names)\n\n\ndhc_mask.shape\n\ntorch.Size([3, 6])\n\n\n\ndhc_mask\n\ntensor([[1., 1., 0., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [0., 0., 1., 0., 0., 0.]])\n\n\n\ntdc.label_lists\n\n[['General', 'General Petite', 'Initmates'],\n ['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend']]\n\n\nExplain the first row of the mask (for the first label of Division Name: General)\n\ndhc_mask[0]\n\ntensor([1., 1., 0., 1., 1., 1.])\n\n\nSlicing the first portion for Department Name, show string for True mask. The results are the sub-category of Division Name\n\nfor i in torch.where(dhc_mask[0]==True)[0]:\n    print(tdc.label_lists[1][i])\n\nBottoms\nDresses\nJackets\nTops\nTrend\n\n\n\n# let's double check with the original data\nnp.sort(df_trn[df_trn['Division Name']=='General']['Department Name'].unique())\n\narray(['Bottoms', 'Dresses', 'Jackets', 'Tops', 'Trend'], dtype=object)",
    "crumbs": [
      "2. All Use Cases",
      "a. Roberta-based models for Supervised Learning",
      "Roberta model with Deep Hierarchical Classification"
    ]
  },
  {
    "objectID": "roberta_dhc.html#define-and-train-a-custom-roberta-model",
    "href": "roberta_dhc.html#define-and-train-a-custom-roberta-model",
    "title": "Roberta model with Deep Hierarchical Classification",
    "section": "Define and train a custom Roberta model",
    "text": "Define and train a custom Roberta model\n\nFull model\n\nroberta_body = RobertaModel.from_pretrained('roberta-base')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n_model_kwargs={\n    'dhc_mask':dhc_mask,\n    'classifier_dropout':0.1,\n    'last_hidden_size':768,  \n    'linear_l1_size':389,\n    'linear_l2_size':417,\n    'lloss_weight':1.0,\n    'dloss_weight':0.8,\n    'layer2concat':4,\n}\n\nmodel = model_init_classification(model_class = RobertaHSCDHCSequenceClassification,\n                                  cpoint_path = 'roberta-base', \n                                  output_hidden_states=True,\n                                  seed=42,\n                                  body_model=roberta_body,\n                                  model_kwargs = _model_kwargs)\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score]\ncontroller = ModelController(model,tdc,seed=42)\n\nLoading body weights. This assumes the body is the very first block of your custom architecture\nTotal parameters: 127631522\nTotal trainable parameters: 127631522\n\n\nAnd we can start training our model\n\nseed_everything(42)\n\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 3\n\ncontroller.fit(epochs,lr,\n               metric_funcs=metric_funcs,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics_separate_heads,\n              )\n\n\n\n    \n      \n      \n      [849/849 06:06, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1 Score Division name\nAccuracy Score Division name\nF1 Score Department name\nAccuracy Score Department name\n\n\n\n\n1\nNo log\n3.563722\n0.409837\n0.612903\n0.649168\n0.867433\n\n\n2\n3.664900\n3.445055\n0.439022\n0.616880\n0.679065\n0.881573\n\n\n3\n3.664900\n3.447504\n0.450244\n0.620194\n0.681202\n0.883120\n\n\n\n\n\n\n\n\n\nSimpler model\n\nseed_everything(42)\n\n\nroberta_body = RobertaModel.from_pretrained('roberta-base')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n_model_kwargs={\n    'dhc_mask':dhc_mask,\n    'lloss_weight':1.0,\n    'dloss_weight':0.8,\n    'layer2concat':4,\n}\n\nmodel = model_init_classification(model_class = RobertaSimpleHSCDHCSequenceClassification,\n                                  cpoint_path = 'roberta-base', \n                                  output_hidden_states=True,\n                                  seed=42,\n                                  body_model=roberta_body,\n                                  model_kwargs = _model_kwargs)\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score]\ncontroller = ModelController(model,tdc,seed=42)\n\nLoading body weights. This assumes the body is the very first block of your custom architecture\nTotal parameters: 124664112\nTotal trainable parameters: 124664112\n\n\nAnd we can start training our model\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 3\n\ncontroller.fit(epochs,lr,\n               metric_funcs=metric_funcs,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics_separate_heads,\n              )\n\n\n\n    \n      \n      \n      [849/849 04:37, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1 Score Division name\nAccuracy Score Division name\nF1 Score Department name\nAccuracy Score Department name\n\n\n\n\n1\nNo log\n3.566824\n0.430449\n0.610915\n0.627627\n0.859258\n\n\n2\n3.663500\n3.459148\n0.477776\n0.609147\n0.673201\n0.880247\n\n\n3\n3.663500\n3.456095\n0.468276\n0.617985\n0.686397\n0.886655\n\n\n\n\n\n\n\n\ncontroller.trainer.model.save_pretrained('./sample_weights/my_model')",
    "crumbs": [
      "2. All Use Cases",
      "a. Roberta-based models for Supervised Learning",
      "Roberta model with Deep Hierarchical Classification"
    ]
  },
  {
    "objectID": "roberta_dhc.html#make-predictions",
    "href": "roberta_dhc.html#make-predictions",
    "title": "Roberta model with Deep Hierarchical Classification",
    "section": "Make predictions",
    "text": "Make predictions\n\nLoad trained model\n\n_model_kwargs\n\n{'dhc_mask': tensor([[1., 1., 0., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1.],\n         [0., 0., 1., 0., 0., 0.]]),\n 'lloss_weight': 1.0,\n 'dloss_weight': 0.8,\n 'layer2concat': 4}\n\n\n\ntrained_model = model_init_classification(model_class = RobertaSimpleHSCDHCSequenceClassification,\n                                          cpoint_path = Path('./sample_weights/my_model'), \n                                          output_hidden_states=True,\n                                          seed=42,\n                                          model_kwargs = _model_kwargs)\n\ncontroller = ModelController(trained_model,tdc,seed=42)\n\nSome weights of the model checkpoint at sample_weights/my_model were not used when initializing RobertaSimpleHSCDHCSequenceClassification: ['body_model.pooler.dense.bias', 'body_model.pooler.dense.weight']\n- This IS expected if you are initializing RobertaSimpleHSCDHCSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaSimpleHSCDHCSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\nTotal parameters: 124073520\nTotal trainable parameters: 124073520\n\n\n\n\nPredict Train/Validation set\n\ndf_val = controller.predict_ddict(ds_type='validation',are_heads_separated=True)\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_val = df_val.to_pandas()\ndf_val.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nDivision Name\nDepartment Name\nlabel\ninput_ids\nattention_mask\npred_Division Name\npred_prob_Division Name\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\n\n. such a fun jacket ! great to wear in the spr...\nGeneral Petite\nIntimate\n[1, 2]\n[0, 4, 215, 10, 1531, 8443, 27785, 372, 7, 356...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.570708\nJackets\n0.732757\n\n\n1\nsimple and elegant\nsimple and elegant . i thought this shirt was ...\nGeneral Petite\nTops\n[1, 4]\n[0, 41918, 8, 14878, 479, 939, 802, 42, 6399, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.658240\nTops\n0.987063\n\n\n2\nretro and pretty\nretro and pretty . this top has a bit of a ret...\nGeneral\nTops\n[0, 4]\n[0, 4903, 1001, 8, 1256, 479, 42, 299, 34, 10,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.639935\nTops\n0.982640\n\n\n3\nsummer/fall wear\nsummer / fall wear . i first spotted this on a...\nGeneral Petite\nDresses\n[1, 1]\n[0, 18581, 2089, 1589, 1136, 3568, 479, 939, 7...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.526535\nDresses\n0.963429\n\n\n4\nperfect except slip\nperfect except slip . this is my new favorite ...\nGeneral Petite\nDresses\n[1, 1]\n[0, 20473, 4682, 9215, 479, 42, 16, 127, 92, 2...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.574504\nDresses\n0.980230\n\n\n\n\n\n\n\n\nYou can try to get your metric to see if it matches your last traing epoch’s above\n\nf1_score(df_val['Division Name'],df_val['pred_Division Name'],average='macro')\n# 0.45921193659675547\n\n0.46820287596441723\n\n\n\nf1_score(df_val['Department Name'],df_val['pred_Department Name'],average='macro')\n# 0.6824108822326193\n\n0.6863969355585954\n\n\n\n\nPredict Test set\nWe will go through details on how to make a prediction on a completely new and raw dataset using our trained model. For now, let’s reuse the sample csv and pretend it’s our test set\n\ndf_test = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig').sample(frac=0.2,random_state=1)\n# drop NaN values in the label column\ndf_test = df_test[~df_test['Department Name'].isna()].reset_index(drop=True)\n\n# save the label, as we will calculate some metrics later. We also filter out labels with NaN Review Text,\n# as there will be a filtering processing on the test set\ntrue_labels = df_test.loc[~df_test['Review Text'].isna(),'Department Name'].values \n\n# drop the label (you don't need to, but this is necessary to simulate an actual test set)\ndf_test.drop(['Division Name','Department Name'],axis=1,inplace=True)\n\n\n_test_dset = Dataset.from_pandas(df_test)\n_test_dset_predicted = controller.predict_raw_dset(_test_dset,\n                                                   do_filtering=True,\n                                                   are_heads_separated=True,\n                                                  )\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_test_predicted = _test_dset_predicted.to_pandas()\n\n\ndf_test_predicted.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\ninput_ids\nattention_mask\npred_Division Name\npred_prob_Division Name\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\nperfect for work and play\nperfect for work and play . this shirt works f...\n[0, 20473, 13, 173, 8, 310, 479, 42, 6399, 136...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.650245\nTops\n0.986601\n\n\n1\n\n. i don't know why i had the opposite problem ...\n[0, 4, 939, 218, 75, 216, 596, 939, 56, 5, 548...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.750323\nBottoms\n0.985996\n\n\n2\ngreat pants\ngreat pants . thes e cords are great--lightwei...\n[0, 12338, 9304, 479, 5, 29, 364, 37687, 32, 3...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.641696\nBottoms\n0.979315\n\n\n3\nsurprisingly comfy for a button down\nsurprisingly comfy for a button down . i am a ...\n[0, 33258, 3137, 24382, 13, 10, 6148, 159, 479...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.644696\nTops\n0.967189\n\n\n4\nshort and small\nshort and small . the shirt is mostly a thick ...\n[0, 20263, 8, 650, 479, 5, 6399, 16, 2260, 10,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.566876\nTops\n0.929597\n\n\n\n\n\n\n\n\nLet’s quickly check the f1 score to make sure everything works correctly\n\nf1_score(true_labels,df_test_predicted['pred_Department Name'],average='macro')\n\n0.7083328159471795\n\n\nPredict top k results\n\n_test_dset = Dataset.from_pandas(df_test)\n_test_dset_predicted = controller.predict_raw_dset(_test_dset,\n                                                   do_filtering=True,\n                                                   topk=3,\n                                                   are_heads_separated=True\n                                                  )\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_test_predicted = _test_dset_predicted.to_pandas()\n\ndf_test_predicted.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\ninput_ids\nattention_mask\npred_Division Name\npred_prob_Division Name\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\nperfect for work and play\nperfect for work and play . this shirt works f...\n[0, 20473, 13, 173, 8, 310, 479, 42, 6399, 136...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[General, General Petite, Initmates]\n[0.65024525, 0.34086585, 0.008888904]\n[Tops, Intimate, Jackets]\n[0.98660094, 0.010788872, 0.0011576503]\n\n\n1\n\n. i don't know why i had the opposite problem ...\n[0, 4, 939, 218, 75, 216, 596, 939, 56, 5, 548...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[General, General Petite, Initmates]\n[0.7503231, 0.24517073, 0.0045061694]\n[Bottoms, Intimate, Dresses]\n[0.9859962, 0.007048236, 0.0063092075]\n\n\n2\ngreat pants\ngreat pants . thes e cords are great--lightwei...\n[0, 12338, 9304, 479, 5, 29, 364, 37687, 32, 3...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[General, General Petite, Initmates]\n[0.64169586, 0.3475547, 0.010749483]\n[Bottoms, Intimate, Dresses]\n[0.97931457, 0.016918503, 0.002942416]\n\n\n3\nsurprisingly comfy for a button down\nsurprisingly comfy for a button down . i am a ...\n[0, 33258, 3137, 24382, 13, 10, 6148, 159, 479...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[General, General Petite, Initmates]\n[0.6446963, 0.33823726, 0.017066495]\n[Tops, Intimate, Dresses]\n[0.96718866, 0.018242536, 0.0062063746]\n\n\n4\nshort and small\nshort and small . the shirt is mostly a thick ...\n[0, 20263, 8, 650, 479, 5, 6399, 16, 2260, 10,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[General, General Petite, Initmates]\n[0.5668759, 0.363859, 0.06926514]\n[Tops, Intimate, Jackets]\n[0.92959666, 0.06315183, 0.0059652175]\n\n\n\n\n\n\n\n\n\n# Since we have some metadatas (Title), we need to define a dictionary containing those values\nraw_content={'Review Text': 'This shirt is so comfortable I love it!',\n             'Title': 'Great shirt'}\n\n\ncontroller.data_store.num_proc=1\n\n\ndf_result = controller.predict_raw_text(raw_content,are_heads_separated=True,topk=3)\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_result\n\n{'Review Text': ['great shirt . this shirt is so comfortable i love it !'],\n 'Title': ['great shirt'],\n 'input_ids': [[0,\n   12338,\n   6399,\n   479,\n   42,\n   6399,\n   16,\n   98,\n   3473,\n   939,\n   657,\n   24,\n   27785,\n   2]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n 'pred_Division Name': [['General', 'General Petite', 'Initmates']],\n 'pred_prob_Division Name': [[0.6600767374038696,\n   0.33104702830314636,\n   0.008876222185790539]],\n 'pred_Department Name': [['Tops', 'Intimate', 'Jackets']],\n 'pred_prob_Department Name': [[0.9869695901870728,\n   0.010812523774802685,\n   0.0010536855552345514]]}",
    "crumbs": [
      "2. All Use Cases",
      "a. Roberta-based models for Supervised Learning",
      "Roberta model with Deep Hierarchical Classification"
    ]
  },
  {
    "objectID": "model_lm_gpt2_tutorial.html",
    "href": "model_lm_gpt2_tutorial.html",
    "title": "Model Controller Tutorial: Training a GPT2 Language Model",
    "section": "",
    "text": "import os\n#This will specify a (or a list) of GPUs for training\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom that_nlp_library.text_main_lm import *\nfrom that_nlp_library.utils import seed_everything\nfrom that_nlp_library.model_lm_main import *\nfrom that_nlp_library.utils import resize_model_embeddings\nfrom underthesea import text_normalize\nfrom functools import partial\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\nfrom datasets import load_dataset\nimport pandas as pd\nimport numpy as np\nfrom transformers import DataCollatorForLanguageModeling\nfrom tokenizers import processors",
    "crumbs": [
      "2. All Use Cases",
      "d. Causal Language Model Training",
      "Model Controller Tutorial: Training a GPT2 Language Model"
    ]
  },
  {
    "objectID": "model_lm_gpt2_tutorial.html#create-a-textdatalmcontroller-object",
    "href": "model_lm_gpt2_tutorial.html#create-a-textdatalmcontroller-object",
    "title": "Model Controller Tutorial: Training a GPT2 Language Model",
    "section": "Create a TextDataLMController object",
    "text": "Create a TextDataLMController object\nWe will reuse the data and the preprocessings in this tutorial\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataLMController(dset,\n                         main_text='Review Text',\n                         filter_dict={'Review Text': lambda x: x is not None},\n                         metadatas='Title',\n                         content_transformations=[text_normalize,str.lower],\n                         seed=42,\n                         verbose=False\n                        )\n\nDefine our tokenizer for Roberta\n\n_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\nIf you want to perform concatenation-of-token, and you want your causal LM to differentiate between sentences, you can add a special token to separate sentences, as follow:\n\n_tokenizer._tokenizer.post_processor = processors.TemplateProcessing(\n    single=\"$A \" + _tokenizer.eos_token,\n    special_tokens=[(_tokenizer.eos_token, _tokenizer.eos_token_id)],\n)\n_tokenizer.pad_token = _tokenizer.eos_token\n\n\n_tokenizer\n\nGPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '&lt;|endoftext|&gt;', 'eos_token': '&lt;|endoftext|&gt;', 'unk_token': '&lt;|endoftext|&gt;', 'pad_token': '&lt;|endoftext|&gt;'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n    50256: AddedToken(\"&lt;|endoftext|&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n\n\nProcess and tokenize our dataset\n\nblock_size=112\ntdc.process_and_tokenize(_tokenizer,line_by_line=False,max_length=block_size)\n\n\ntdc.main_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'special_tokens_mask'],\n        num_rows: 12741\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask', 'special_tokens_mask'],\n        num_rows: 3235\n    })\n})\n\n\nAnd set the data collator\n\ntdc.set_data_collator(is_mlm=False)",
    "crumbs": [
      "2. All Use Cases",
      "d. Causal Language Model Training",
      "Model Controller Tutorial: Training a GPT2 Language Model"
    ]
  },
  {
    "objectID": "model_lm_gpt2_tutorial.html#initialize-and-train-gpt2-model-from-scratch",
    "href": "model_lm_gpt2_tutorial.html#initialize-and-train-gpt2-model-from-scratch",
    "title": "Model Controller Tutorial: Training a GPT2 Language Model",
    "section": "Initialize and train GPT2 Model from scratch",
    "text": "Initialize and train GPT2 Model from scratch\n\nlen(_tokenizer)\n\n50257\n\n\n\n_tokenizer.bos_token_id,_tokenizer.eos_token_id\n\n(50256, 50256)\n\n\n\n_config = AutoConfig.from_pretrained('gpt2',\n                                     n_ctx=block_size,\n                                     # just in case...\n                                     vocab_size=len(_tokenizer),\n                                     bos_token_id=_tokenizer.bos_token_id,\n                                     eos_token_id=_tokenizer.eos_token_id,\n                                     )\n_config\n\nGPT2Config {\n  \"_name_or_path\": \"gpt2\",\n  \"activation_function\": \"gelu_new\",\n  \"architectures\": [\n    \"GPT2LMHeadModel\"\n  ],\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"initializer_range\": 0.02,\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 112,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 12,\n  \"n_positions\": 1024,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"task_specific_params\": {\n    \"text-generation\": {\n      \"do_sample\": true,\n      \"max_length\": 50\n    }\n  },\n  \"transformers_version\": \"4.40.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 50257\n}\n\n\n\n_model = language_model_init(AutoModelForCausalLM,\n                             config=_config,\n                             cpoint_path=None, # leave this as None to get a non-pretrained model\n                             seed=42\n                            )\n\nInitiate a new language model from scratch\nTotal parameters: 124439808\nTotal trainable parameters: 124439808\n\n\n\n_model\n\nGPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)\n\n\n\n_model = resize_model_embeddings(_model,_tokenizer)\n\nCreate a model controller\n\ncontroller = ModelLMController(_model,data_store=tdc,seed=42)\n\nAnd we can start training our model\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 3\nwarmup_ratio=0.25\ncontroller.fit(epochs,lr,\n               batch_size=bs,\n               weight_decay=wd,\n               warmup_ratio=warmup_ratio,\n               save_checkpoint=False,\n              )\n\n\n\n    \n      \n      \n      [597/597 03:32, Epoch 2/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n0\nNo log\n4.586693\n0.204113\n\n\n2\n5.402300\n3.972607\n0.262826\n\n\n\n\n\n\n\n\n    \n      \n      \n      [102/102 00:06]\n    \n    \n\n\nPerplexity on validation set: 53.123\n\n\n\ncontroller.trainer.model.save_pretrained('./sample_weights/lm_gpt_model')",
    "crumbs": [
      "2. All Use Cases",
      "d. Causal Language Model Training",
      "Model Controller Tutorial: Training a GPT2 Language Model"
    ]
  },
  {
    "objectID": "model_lm_gpt2_tutorial.html#generate-text-using-model",
    "href": "model_lm_gpt2_tutorial.html#generate-text-using-model",
    "title": "Model Controller Tutorial: Training a GPT2 Language Model",
    "section": "Generate text using model",
    "text": "Generate text using model\n\nsentence1 = 'major problem . this is by far one of the '\nsentence2 = 'flattering . this is by far one of the '\n\n\ntrained_model = language_model_init(AutoModelForCausalLM,\n                                    cpoint_path='./sample_weights/lm_gpt_model',\n                                   )\n\nTotal parameters: 124439808\nTotal trainable parameters: 124439808\n\n\n\ncontroller2 = ModelLMController(trained_model,data_store=tdc,seed=42)\n\nYou can input several raw texts\n\ninp = {'Title':['Major Problem','Flattering'],\n        'Review Text': [\"This is by far one of the worst \",\n                        \"This is by far one of the best \"]\n       }\n\n\n# reference for the keyword arguments: \n# https://huggingface.co/docs/transformers/v4.33.2/en/main_classes/text_generation#transformers.GenerationMixin.generate\n\n\ncontroller2.predict_raw_text(inp,print_result=True,\n                             # huggingface text generation kwargs:\n                             num_return_sequences=3,max_new_tokens=50,num_beams=1,do_sample=True\n                            )\n\n&gt;&gt;&gt; major problem . this is by far one of the worst color around or the chest detail, i did not want it to the side. it looks great on me. it's pretty short - definitely be too casual in a size i think it is a little higher length on me. it runs small, but\n&gt;&gt;&gt; major problem . this is by far one of the worst and will be very flattering. i'm 5'7 \" and 120 lbs. it was too small in the regular size 4. this top was huge all the material, so i have looked more like the picture. the fit is the right length and\n&gt;&gt;&gt; major problem . this is by far one of the worst of the top. but the blue is a very low quality and the bottom is very unflattering. overall, the material is beautiful and the cut is a little roomy than i have to pull it in the top, but i did. i am\n--------------------\n&gt;&gt;&gt; flattering . this is by far one of the best. i went to buy it at first but it was huge and to be going back. i am 5'3 \" 140 # and purchased a 6. that it goes with a keeper. i am very thin and it fits perfectly. in person,\n&gt;&gt;&gt; flattering . this is by far one of the best weight than the colors are beautiful. i got the blue which really love it. i can wear it with jeans and it's a great top for spring. i love this!!!!!!!!!! i normally wear a small/\n&gt;&gt;&gt; flattering . this is by far one of the best pair of jeans and comfy look great. love this top! i thought they had to buy again!!! i am wearing a 32 dd and medium or medium but it fits perfect! thes e are great and i think they're so flattering\n--------------------\n\n\n\ncontroller2.predict_raw_text(inp,print_result=False,\n                             # huggingface text generation kwargs:\n                             num_return_sequences=3,max_new_tokens=50,num_beams=1,do_sample=True\n                            )\n\n[[{'generated_text': 'major problem . this is by far one of the worst is very lightweight and made of the sleeves do not be great to wear. the fit is perfect for the spring. the length part has a bit to be. the top of the blue is very pretty but i am in blue. i got the x'},\n  {'generated_text': \"major problem . this is by far one of the worst and the dress was super high-colored green. great shirt that you can wear with a medium bra and it is very flattering. it's soft i had to have to wear it all fall or winter. i ordered the 4 but i love this top\"},\n  {'generated_text': \"major problem . this is by far one of the worst, which is the photo. it is very pretty and i found that it arrived with the top. i took me to return it off. the dress is not a casual style, and the fabric looks as a bit shorter though, it's really pretty\"}],\n [{'generated_text': \"flattering . this is by far one of the best retailer ( so disappointing ). i'm wearing a fan of a medium, but with a large side but i am very sure i did. on the lace, it's not a bit the shoulder and is gorgeous. with this top or no shape.\"},\n  {'generated_text': \"flattering . this is by far one of the best price. it is such an not for the model area, i could have a size small or 8, but i'm not an xs and itchy. the quality of my waistband is lovely, and very cute. it's very comfortable.\"},\n  {'generated_text': 'flattering . this is by far one of the best time i bought it in my regular size. there is very nice and flattering. the material is perfect for fall. i am 5\\'10 \" and the small fit me me perfect. i bought an 8 and i\\'d also usually wear a medium.'}]]",
    "crumbs": [
      "2. All Use Cases",
      "d. Causal Language Model Training",
      "Model Controller Tutorial: Training a GPT2 Language Model"
    ]
  },
  {
    "objectID": "model_lm_gpt2_tutorial.html#create-a-textdatalmcontroller-object-1",
    "href": "model_lm_gpt2_tutorial.html#create-a-textdatalmcontroller-object-1",
    "title": "Model Controller Tutorial: Training a GPT2 Language Model",
    "section": "Create a TextDataLMController object",
    "text": "Create a TextDataLMController object\nWe will reuse the data and the preprocessings in this tutorial\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataLMController(dset,\n                         main_text='Review Text',\n                         filter_dict={'Review Text': lambda x: x is not None},\n                         metadatas='Title',\n                         content_transformations=[text_normalize,str.lower],\n                         seed=42,\n                         verbose=False\n                        )\n\nDefine our tokenizer for Roberta\n\n_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\nIf you want to perform concatenation-of-token, and you want your causal LM to differentiate between sentences, you can add a special token to separate sentences, as follow:\n\n_tokenizer._tokenizer.post_processor = processors.TemplateProcessing(\n    single=\"$A \" + _tokenizer.eos_token,\n    special_tokens=[(_tokenizer.eos_token, _tokenizer.eos_token_id)],\n)\n_tokenizer.pad_token = _tokenizer.eos_token\n\n\n_tokenizer\n\nGPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '&lt;|endoftext|&gt;', 'eos_token': '&lt;|endoftext|&gt;', 'unk_token': '&lt;|endoftext|&gt;', 'pad_token': '&lt;|endoftext|&gt;'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n    50256: AddedToken(\"&lt;|endoftext|&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n\n\nProcess and tokenize our dataset\n\nblock_size=112\ntdc.process_and_tokenize(_tokenizer,line_by_line=False,max_length=block_size)\n\n\ntdc.main_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'special_tokens_mask'],\n        num_rows: 12741\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask', 'special_tokens_mask'],\n        num_rows: 3235\n    })\n})\n\n\nAnd set the data collator\n\ntdc.set_data_collator(is_mlm=False)",
    "crumbs": [
      "2. All Use Cases",
      "d. Causal Language Model Training",
      "Model Controller Tutorial: Training a GPT2 Language Model"
    ]
  },
  {
    "objectID": "model_lm_gpt2_tutorial.html#initialize-and-train-gpt2-model",
    "href": "model_lm_gpt2_tutorial.html#initialize-and-train-gpt2-model",
    "title": "Model Controller Tutorial: Training a GPT2 Language Model",
    "section": "Initialize and train GPT2 Model",
    "text": "Initialize and train GPT2 Model\n\nlen(_tokenizer)\n\n50257\n\n\n\n_tokenizer.bos_token_id,_tokenizer.eos_token_id\n\n(50256, 50256)\n\n\n\n_config = AutoConfig.from_pretrained('gpt2',\n                                     n_ctx=block_size,\n                                     # just in case...\n                                     vocab_size=len(_tokenizer),\n                                     bos_token_id=_tokenizer.bos_token_id,\n                                     eos_token_id=_tokenizer.eos_token_id,\n                                     )\n_config\n\nGPT2Config {\n  \"_name_or_path\": \"gpt2\",\n  \"activation_function\": \"gelu_new\",\n  \"architectures\": [\n    \"GPT2LMHeadModel\"\n  ],\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"initializer_range\": 0.02,\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 112,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 12,\n  \"n_positions\": 1024,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"task_specific_params\": {\n    \"text-generation\": {\n      \"do_sample\": true,\n      \"max_length\": 50\n    }\n  },\n  \"transformers_version\": \"4.40.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 50257\n}\n\n\n\n_model = language_model_init(AutoModelForCausalLM,\n                             config=_config,\n                             cpoint_path='gpt2',\n                             seed=42\n                            )\n\nTotal parameters: 124439808\nTotal trainable parameters: 124439808\n\n\n\n_model = resize_model_embeddings(_model,_tokenizer)\n\nCreate a model controller\n\ncontroller = ModelLMController(_model,data_store=tdc,seed=42)\n\nAnd we can start training our model\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 3\nwarmup_ratio=0.25\ncontroller.fit(epochs,lr,\n               batch_size=bs,\n               weight_decay=wd,\n               warmup_ratio=warmup_ratio,\n               save_checkpoint=False,\n              )\n\n\n\n    \n      \n      \n      [597/597 03:25, Epoch 2/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n0\nNo log\n3.088598\n0.353749\n\n\n2\n3.269600\n2.959936\n0.369057\n\n\n\n\n\n\n\n\n    \n      \n      \n      [102/102 00:06]\n    \n    \n\n\nPerplexity on validation set: 19.297\n\n\n\ncontroller.trainer.model.save_pretrained('./sample_weights/lm_gpt_model')",
    "crumbs": [
      "2. All Use Cases",
      "d. Causal Language Model Training",
      "Model Controller Tutorial: Training a GPT2 Language Model"
    ]
  },
  {
    "objectID": "model_lm_gpt2_tutorial.html#generate-text-using-model-1",
    "href": "model_lm_gpt2_tutorial.html#generate-text-using-model-1",
    "title": "Model Controller Tutorial: Training a GPT2 Language Model",
    "section": "Generate text using model",
    "text": "Generate text using model\n\nsentence1 = 'major problem . this is by far one of the '\nsentence2 = 'flattering . this is by far one of the '\n\n\ntrained_model = language_model_init(AutoModelForCausalLM,\n                                    cpoint_path='./sample_weights/lm_gpt_model',\n                                   )\n\nTotal parameters: 124439808\nTotal trainable parameters: 124439808\n\n\n\ncontroller2 = ModelLMController(trained_model,data_store=tdc,seed=42)\n\nYou can input several raw texts\n\ninp = {'Title':['Major Problem','Flattering'],\n        'Review Text': [\"This is by far one of the worst \",\n                        \"This is by far one of the best \"]\n       }\n\n\n# reference for the keyword arguments: \n# https://huggingface.co/docs/transformers/v4.33.2/en/main_classes/text_generation#transformers.GenerationMixin.generate\n\n\ncontroller2.predict_raw_text(inp,print_result=True,\n                             # huggingface text generation kwargs:\n                             num_return_sequences=3,max_new_tokens=50,num_beams=1,do_sample=True\n                            )\n\n&gt;&gt;&gt; major problem . this is by far one of the worst shorts i've seen in a long time. i want it to work, but what about the other pants. they seem to be washed out and need to be rewashed twice????. i don't have a pair of these yet,\n&gt;&gt;&gt; major problem . this is by far one of the worst pants i have ever owned. i'm 5'7 \" 105 lbs, 34 b with long arms and very short legs. i don't have large hips, so the \" top \" looked a bit wider. at first i was surprised it was so\n&gt;&gt;&gt; major problem . this is by far one of the worst romper i have ever purchased. the cut is low, and the bottom hem is unflattering. overall, this is definitely a dress and the quality is excellent, though a nice touch. for reference, i am 5'6 \", 125 lbs\n--------------------\n&gt;&gt;&gt; flattering . this is by far one of the best top i have bought. it runs large but is easy to pull up and it is thick enough that i am comfortable wearing it with sandals under it. i usually wear an xs in tops, and i ordered a petite. in person,\n&gt;&gt;&gt; flattering . this is by far one of the best buy's ever. the green color is great for winter which means that you can dress it up with boots or wear it wih work all winter long. i love this!!!!! bought this to wear to the office with a nice sweater\n&gt;&gt;&gt; flattering . this is by far one of the best tops i've tried. i love the fabric and design of the material, and the fit. it looks great with skinny jeans or leggings and a pair of white flats. i got the green and the color is stunning. as another reviewer noted\n--------------------",
    "crumbs": [
      "2. All Use Cases",
      "d. Causal Language Model Training",
      "Model Controller Tutorial: Training a GPT2 Language Model"
    ]
  },
  {
    "objectID": "model_lm_gpt2_tutorial.html#create-a-textdatalmcontroller-object-2",
    "href": "model_lm_gpt2_tutorial.html#create-a-textdatalmcontroller-object-2",
    "title": "Model Controller Tutorial: Training a GPT2 Language Model",
    "section": "Create a TextDataLMController object",
    "text": "Create a TextDataLMController object\nWe will reuse the data and the preprocessings in this tutorial\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\ntdc = TextDataLMController(dset,\n                         main_text='Review Text',\n                         filter_dict={'Review Text': lambda x: x is not None},\n                         metadatas='Title',\n                         content_transformations=[text_normalize,str.lower],\n                         seed=42,\n                         verbose=False\n                        )\n\nDefine our tokenizer for Roberta\n\n_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n_tokenizer.pad_token = _tokenizer.eos_token\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\n_tokenizer\n\nGPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '&lt;|endoftext|&gt;', 'eos_token': '&lt;|endoftext|&gt;', 'unk_token': '&lt;|endoftext|&gt;', 'pad_token': '&lt;|endoftext|&gt;'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n    50256: AddedToken(\"&lt;|endoftext|&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n\n\nProcess and tokenize our dataset\n\nblock_size=112\ntdc.process_and_tokenize(_tokenizer,line_by_line=True,max_length=block_size)\n\n\ntdc.main_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Title', 'Review Text', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n        num_rows: 18112\n    })\n    validation: Dataset({\n        features: ['Title', 'Review Text', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n        num_rows: 4529\n    })\n})\n\n\nAnd set the data collator\n\ntdc.set_data_collator(is_mlm=False)",
    "crumbs": [
      "2. All Use Cases",
      "d. Causal Language Model Training",
      "Model Controller Tutorial: Training a GPT2 Language Model"
    ]
  },
  {
    "objectID": "model_lm_gpt2_tutorial.html#initialize-and-train-gpt2-model-1",
    "href": "model_lm_gpt2_tutorial.html#initialize-and-train-gpt2-model-1",
    "title": "Model Controller Tutorial: Training a GPT2 Language Model",
    "section": "Initialize and train GPT2 Model",
    "text": "Initialize and train GPT2 Model\n\nlen(_tokenizer)\n\n50257\n\n\n\n_tokenizer.bos_token_id,_tokenizer.eos_token_id\n\n(50256, 50256)\n\n\n\n_config = AutoConfig.from_pretrained('gpt2',\n                                     n_ctx=block_size,\n                                     # just in case...\n                                     vocab_size=len(_tokenizer),\n                                     bos_token_id=_tokenizer.bos_token_id,\n                                     eos_token_id=_tokenizer.eos_token_id,\n                                     )\n_config\n\nGPT2Config {\n  \"_name_or_path\": \"gpt2\",\n  \"activation_function\": \"gelu_new\",\n  \"architectures\": [\n    \"GPT2LMHeadModel\"\n  ],\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"initializer_range\": 0.02,\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 112,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 12,\n  \"n_positions\": 1024,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"task_specific_params\": {\n    \"text-generation\": {\n      \"do_sample\": true,\n      \"max_length\": 50\n    }\n  },\n  \"transformers_version\": \"4.40.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 50257\n}\n\n\n\n_model = language_model_init(AutoModelForCausalLM,\n                             config=_config,\n                             cpoint_path='gpt2',\n                             seed=42\n                            )\n\nTotal parameters: 124439808\nTotal trainable parameters: 124439808\n\n\n\n_model = resize_model_embeddings(_model,_tokenizer)\n\nCreate a model controller\n\ncontroller = ModelLMController(_model,data_store=tdc,seed=42)\n\nAnd we can start training our model\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 3\nwarmup_ratio=0.25\ncontroller.fit(epochs,lr,\n               batch_size=bs,\n               weight_decay=wd,\n               warmup_ratio=warmup_ratio,\n               save_checkpoint=False,\n              )\n\n\n\n    \n      \n      \n      [849/849 05:07, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\nNo log\n2.985998\n0.250999\n\n\n2\n3.160300\n2.870805\n0.260794\n\n\n3\n3.160300\n2.851692\n0.262393\n\n\n\n\n\n\n\n\n    \n      \n      \n      [142/142 00:08]\n    \n    \n\n\nPerplexity on validation set: 17.317\n\n\n\ncontroller.trainer.model.save_pretrained('./sample_weights/lm_gpt_model')",
    "crumbs": [
      "2. All Use Cases",
      "d. Causal Language Model Training",
      "Model Controller Tutorial: Training a GPT2 Language Model"
    ]
  },
  {
    "objectID": "model_lm_gpt2_tutorial.html#generate-text-using-model-2",
    "href": "model_lm_gpt2_tutorial.html#generate-text-using-model-2",
    "title": "Model Controller Tutorial: Training a GPT2 Language Model",
    "section": "Generate text using model",
    "text": "Generate text using model\n\nsentence1 = 'major problem . this is by far one of the '\nsentence2 = 'flattering . this is by far one of the '\n\n\ntrained_model = language_model_init(AutoModelForCausalLM,\n                                    cpoint_path='./sample_weights/lm_gpt_model',\n                                   )\n\nTotal parameters: 124439808\nTotal trainable parameters: 124439808\n\n\n\ncontroller2 = ModelLMController(trained_model,data_store=tdc,seed=42)\n\nYou can input several raw texts\n\ninp = {'Title':['Major Problem','Flattering'],\n        'Review Text': [\"This is by far one of the worst \",\n                        \"This is by far one of the best \"]\n       }\n\n\ncontroller2.predict_raw_text(inp,print_result=True,\n                             # huggingface text generation kwargs:\n                             num_return_sequences=3,max_new_tokens=50,num_beams=1,do_sample=True\n                            )\n\n&gt;&gt;&gt; major problem . this is by far one of the worst things i've ever bought at retailer. the material can be uncomfortable. i wasn't comfortable with it on the hanger. i ordered a small, which was still tight in all the wrong places. the small was way too big. it fit perfectly\n&gt;&gt;&gt; major problem . this is by far one of the worst quality dress i have ever purchased byron lars and had to return it. i am usually a size six in retailer tops - 6 and this one is just so huge. the length is really good - it was a bit low-cut and it\n&gt;&gt;&gt; major problem . this is by far one of the worst clothes i have bought in years. in my opinion the pattern is just not on par with what the rest of the clothing looks like. the fabric is not even cotton... it was thick, synthetic fabric that made me feel like i was in a hospital\n--------------------\n&gt;&gt;&gt; flattering . this is by far one of the best jeans i've gotten since i was in highschool. i just ordered blue jeans as well. i've been looking for a pair of jeans that fit great and are comfortable. this one is by far, my favorite purchase of the month. the fit\n&gt;&gt;&gt; flattering . this is by far one of the best i have tried on. i love it! i'm 5'4 and 140 pounds and bought the size m. the fabric is really sheer, which i was happy to find. but the fit is flattering and i get compliments every time i wear it\n&gt;&gt;&gt; flattering . this is by far one of the best dresses i've ever owned. it's true to size and runs true to size. i'm 36 dd ( 5'2'and 125 lbs ) and usually wear a 25 in bottoms but with this dress, i could pull off the regular 20\n--------------------",
    "crumbs": [
      "2. All Use Cases",
      "d. Causal Language Model Training",
      "Model Controller Tutorial: Training a GPT2 Language Model"
    ]
  },
  {
    "objectID": "roberta_multihead.html",
    "href": "roberta_multihead.html",
    "title": "Roberta model (Multi Head)",
    "section": "",
    "text": "In this series, we walk through some of the capability of this library: single-head classification, multi-head classification, multi-label classification, and regression. If you want a more detailed tutorial, check this out\nimport os\n#This will specify a (or a list) of GPUs for training\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom that_nlp_library.text_main import *\nfrom that_nlp_library.utils import seed_everything\nfrom underthesea import text_normalize\nfrom functools import partial\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport nlpaug.augmenter.char as nac\nfrom datasets import load_dataset\nimport random\nfrom transformers import RobertaTokenizer\nfrom datasets import Dataset",
    "crumbs": [
      "2. All Use Cases",
      "a. Roberta-based models for Supervised Learning",
      "Roberta model (Multi Head)"
    ]
  },
  {
    "objectID": "roberta_multihead.html#define-and-train-a-custom-roberta-model",
    "href": "roberta_multihead.html#define-and-train-a-custom-roberta-model",
    "title": "Roberta model (Multi Head)",
    "section": "Define and train a custom Roberta model",
    "text": "Define and train a custom Roberta model\n\nfrom transformers.models.roberta.modeling_roberta import RobertaModel\n\n\nnum_classes = [len(tdc.label_lists[0]),len(tdc.label_lists[1])] \nnum_classes\n\n[3, 6]\n\n\n\nroberta_body = RobertaModel.from_pretrained('roberta-base')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n# our model is more complex, so it's best to define some of its arguments\n_model_kwargs={\n    # overall model hyperparams\n    'head_class_sizes':num_classes,\n    'head_class': ConcatHeadSimple,\n    'is_multilabel':tdc.is_multilabel, # False\n    'is_multihead':tdc.is_multihead, # True\n\n#     'head_weights':[1,2], # weights for label 1 and label 2. This means L2's weight is twice as much as L1's\n    # if no `head_weights` is set, default to 1 for all labels\n    \n    # classfication head hyperparams\n    'layer2concat':2, # you can change the number of layers to concat (default is 4, based on the paper)\n    'classifier_dropout':0.1 \n}\n\n\nmodel = model_init_classification(model_class = RobertaHiddenStateConcatForSequenceClassification,\n                                  cpoint_path = 'roberta-base', \n                                  output_hidden_states=True, # since we are using 'hidden layer contatenation' technique\n                                  seed=42,\n                                  body_model=roberta_body,\n                                  model_kwargs = _model_kwargs)\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score]\ncontroller = ModelController(model,tdc,seed=42)\n\nLoading body weights. This assumes the body is the very first block of your custom architecture\nTotal parameters: 124659465\nTotal trainable parameters: 124659465\n\n\nAnd we can start training our model\n\nseed_everything(42)\n\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 3\n\ncontroller.fit(epochs,lr,\n               metric_funcs=metric_funcs,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics,\n              )\n\n\n\n    \n      \n      \n      [849/849 04:59, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1 Score Division name\nAccuracy Score Division name\nF1 Score Department name\nAccuracy Score Department name\n\n\n\n\n1\nNo log\n1.216166\n0.419407\n0.614229\n0.638160\n0.861688\n\n\n2\n1.351000\n1.129738\n0.450979\n0.616659\n0.688072\n0.884887\n\n\n3\n1.351000\n1.127698\n0.463073\n0.620636\n0.686571\n0.885329\n\n\n\n\n\n\n\n\ncontroller.trainer.model.save_pretrained('./sample_weights/my_model1')",
    "crumbs": [
      "2. All Use Cases",
      "a. Roberta-based models for Supervised Learning",
      "Roberta model (Multi Head)"
    ]
  },
  {
    "objectID": "roberta_multihead.html#make-predictions",
    "href": "roberta_multihead.html#make-predictions",
    "title": "Roberta model (Multi Head)",
    "section": "Make predictions",
    "text": "Make predictions\n\nLoad trained model\n\n_model_kwargs\n\n{'head_class_sizes': [3, 6],\n 'head_class': that_nlp_library.models.roberta.classifiers.ConcatHeadSimple,\n 'is_multilabel': False,\n 'is_multihead': True,\n 'layer2concat': 2,\n 'classifier_dropout': 0.1}\n\n\n\ntrained_model = model_init_classification(model_class = RobertaHiddenStateConcatForSequenceClassification,\n                                          cpoint_path = Path('./sample_weights/my_model1'), \n                                          output_hidden_states=True,\n                                          seed=42,\n                                          model_kwargs = _model_kwargs)\n\ncontroller = ModelController(trained_model,tdc,seed=42)\n\nSome weights of the model checkpoint at sample_weights/my_model1 were not used when initializing RobertaHiddenStateConcatForSequenceClassification: ['body_model.pooler.dense.bias', 'body_model.pooler.dense.weight']\n- This IS expected if you are initializing RobertaHiddenStateConcatForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaHiddenStateConcatForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\nTotal parameters: 124068873\nTotal trainable parameters: 124068873\n\n\n\n\nPredict Train/Validation set\n\ndf_val = controller.predict_ddict(ds_type='validation')\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_val = df_val.to_pandas()\ndf_val.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nDivision Name\nDepartment Name\nlabel\ninput_ids\nattention_mask\npred_Division Name\npred_prob_Division Name\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\n\n. such a fun jacket ! great to wear in the spr...\nGeneral Petite\nIntimate\n[1, 2]\n[0, 4, 215, 10, 1531, 8443, 27785, 372, 7, 356...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.557212\nJackets\n0.833067\n\n\n1\nsimple and elegant\nsimple and elegant . i thought this shirt was ...\nGeneral Petite\nTops\n[1, 4]\n[0, 41918, 8, 14878, 479, 939, 802, 42, 6399, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.626316\nTops\n0.987408\n\n\n2\nretro and pretty\nretro and pretty . this top has a bit of a ret...\nGeneral\nTops\n[0, 4]\n[0, 4903, 1001, 8, 1256, 479, 42, 299, 34, 10,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.617450\nTops\n0.987884\n\n\n3\nsummer/fall wear\nsummer / fall wear . i first spotted this on a...\nGeneral Petite\nDresses\n[1, 1]\n[0, 18581, 2089, 1589, 1136, 3568, 479, 939, 7...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.556868\nDresses\n0.982287\n\n\n4\nperfect except slip\nperfect except slip . this is my new favorite ...\nGeneral Petite\nDresses\n[1, 1]\n[0, 20473, 4682, 9215, 479, 42, 16, 127, 92, 2...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.596989\nDresses\n0.983380\n\n\n\n\n\n\n\n\nYou can try to get your metric to see if it matches your last traing epoch’s above\n\nf1_score(df_val['Division Name'],df_val['pred_Division Name'],average='macro')\n\n0.46307304608838856\n\n\n\nf1_score(df_val['Department Name'],df_val['pred_Department Name'],average='macro')\n\n0.6863195959806639\n\n\n\n\nPredict Test set\nWe will go through details on how to make a prediction on a completely new and raw dataset using our trained model. For now, let’s reuse the sample csv and pretend it’s our test set\n\ndf_test = pd.read_csv('sample_data/Womens_Clothing_Reviews.csv',encoding='utf-8-sig').sample(frac=0.2,random_state=1)\n# drop NaN values in the label column\ndf_test = df_test[~df_test['Department Name'].isna()].reset_index(drop=True)\n\n# drop the label (you don't need to, but this is necessary to simulate an actual test set)\ndf_test.drop(['Division Name','Department Name'],axis=1,inplace=True)\n\n\n_test_dset = Dataset.from_pandas(df_test)\n_test_dset_predicted = controller.predict_raw_dset(_test_dset,\n                                                   do_filtering=True, # since we have some text filtering in the processing\n                                                  )\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_test_predicted = _test_dset_predicted.to_pandas()\n\n\ndf_test_predicted.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\ninput_ids\nattention_mask\npred_Division Name\npred_prob_Division Name\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\nperfect for work and play\nperfect for work and play . this shirt works f...\n[0, 20473, 13, 173, 8, 310, 479, 42, 6399, 136...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.628283\nTops\n0.976219\n\n\n1\n\n. i don't know why i had the opposite problem ...\n[0, 4, 939, 218, 75, 216, 596, 939, 56, 5, 548...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.727283\nBottoms\n0.992511\n\n\n2\ngreat pants\ngreat pants . thes e cords are great--lightwei...\n[0, 12338, 9304, 479, 5, 29, 364, 37687, 32, 3...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.685441\nBottoms\n0.979834\n\n\n3\nsurprisingly comfy for a button down\nsurprisingly comfy for a button down . i am a ...\n[0, 33258, 3137, 24382, 13, 10, 6148, 159, 479...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.660995\nTops\n0.962874\n\n\n4\nshort and small\nshort and small . the shirt is mostly a thick ...\n[0, 20263, 8, 650, 479, 5, 6399, 16, 2260, 10,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nGeneral\n0.604513\nTops\n0.943028\n\n\n\n\n\n\n\n\nPredict top k results\n\n_test_dset = Dataset.from_pandas(df_test)\n_test_dset_predicted = controller.predict_raw_dset(_test_dset,\n                                                   do_filtering=True,\n                                                   topk=3\n                                                  )\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_test_predicted = _test_dset_predicted.to_pandas()\n\ndf_test_predicted.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\ninput_ids\nattention_mask\npred_Division Name\npred_prob_Division Name\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\nperfect for work and play\nperfect for work and play . this shirt works f...\n[0, 20473, 13, 173, 8, 310, 479, 42, 6399, 136...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[General, General Petite, Initmates]\n[0.62828326, 0.3495934, 0.022123374]\n[Tops, Intimate, Trend]\n[0.9762191, 0.020559784, 0.0010864006]\n\n\n1\n\n. i don't know why i had the opposite problem ...\n[0, 4, 939, 218, 75, 216, 596, 939, 56, 5, 548...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[General, General Petite, Initmates]\n[0.7272834, 0.2687256, 0.003990893]\n[Bottoms, Intimate, Trend]\n[0.9925114, 0.004263644, 0.002455687]\n\n\n2\ngreat pants\ngreat pants . thes e cords are great--lightwei...\n[0, 12338, 9304, 479, 5, 29, 364, 37687, 32, 3...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[General, General Petite, Initmates]\n[0.68544143, 0.30039048, 0.01416805]\n[Bottoms, Intimate, Trend]\n[0.97983354, 0.018011613, 0.0015884736]\n\n\n3\nsurprisingly comfy for a button down\nsurprisingly comfy for a button down . i am a ...\n[0, 33258, 3137, 24382, 13, 10, 6148, 159, 479...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[General, General Petite, Initmates]\n[0.66099477, 0.3112359, 0.027769288]\n[Tops, Intimate, Dresses]\n[0.9628739, 0.032319617, 0.0018878386]\n\n\n4\nshort and small\nshort and small . the shirt is mostly a thick ...\n[0, 20263, 8, 650, 479, 5, 6399, 16, 2260, 10,...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n[General, General Petite, Initmates]\n[0.6045134, 0.33937424, 0.056112316]\n[Tops, Intimate, Trend]\n[0.94302803, 0.05410044, 0.001068312]\n\n\n\n\n\n\n\n\n\n# Since we have some metadatas (Title), we need to define a dictionary containing those values\nraw_content={'Review Text': 'This shirt is so comfortable I love it!',\n             'Title': 'Great shirt'}\n\n\ndf_result = controller.predict_raw_text(raw_content,topk=3)\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_result\n\n{'Review Text': ['great shirt . this shirt is so comfortable i love it !'],\n 'Title': ['great shirt'],\n 'input_ids': [[0,\n   12338,\n   6399,\n   479,\n   42,\n   6399,\n   16,\n   98,\n   3473,\n   939,\n   657,\n   24,\n   27785,\n   2]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n 'pred_Division Name': [['General', 'General Petite', 'Initmates']],\n 'pred_prob_Division Name': [[0.6413698196411133,\n   0.34388816356658936,\n   0.014742041006684303]],\n 'pred_Department Name': [['Tops', 'Intimate', 'Trend']],\n 'pred_prob_Department Name': [[0.9790199398994446,\n   0.018422359600663185,\n   0.0012304459232836962]]}",
    "crumbs": [
      "2. All Use Cases",
      "a. Roberta-based models for Supervised Learning",
      "Roberta model (Multi Head)"
    ]
  },
  {
    "objectID": "text_main_benchmark.html",
    "href": "text_main_benchmark.html",
    "title": "Text Processing Benchmark",
    "section": "",
    "text": "# !conda list | grep 'datasets\\|transformers\\|torch\\|accelerate'\n# accelerate                0.29.3                   pypi_0    pypi\n# datasets                  2.19.0                   pypi_0    pypi\n# torch                     2.3.0                    pypi_0    pypi\n# transformers              4.40.1                   pypi_0    pypi\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom that_nlp_library.text_main import *\nfrom that_nlp_library.text_main_streaming import *\nfrom datasets import load_dataset,enable_caching,disable_caching\nfrom transformers import RobertaTokenizer\nimport os\nimport time\nfrom underthesea import text_normalize\nimport nlpaug.augmenter.char as nac\nfrom functools import partial\nimport random\nfrom memory_profiler import memory_usage\ndisable_caching() # disable huggingface caching to get a fair benchmark",
    "crumbs": [
      "5. Text Classes",
      "b. Text Controller For Supervised Learning",
      "Text Processing Benchmark"
    ]
  },
  {
    "objectID": "text_main_benchmark.html#benchmark-on-medium-size-dataset-117k-rows",
    "href": "text_main_benchmark.html#benchmark-on-medium-size-dataset-117k-rows",
    "title": "Text Processing Benchmark",
    "section": "1. Benchmark on medium-size dataset (~117k rows)",
    "text": "1. Benchmark on medium-size dataset (~117k rows)\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\nlen(dset)\n\n117430\n\n\n\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\nbs=len(dset)//100\nbs\n\n1174\n\n\n\na) Non-streaming dataset\n\ndef benchmarking(tdc,tokenizer,n=10,shuffle_trn=True,time_list=[]):\n    time1 = time.time()\n    tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=shuffle_trn)\n    time2 = time.time() \n    process_time = round(time2-time1,2)\n    print(f'Time it takes to process + tokenize training texts: {process_time} s')\n    for i,v in enumerate(tdc.main_ddict['train']):\n        if n is not None and i==tdc.batch_size*n: break\n    time3 = time.time()\n    \n    iteration_time = round(time3-time2,2)\n    if n is not None:\n        print(f'Time it takes to go through {n*tdc.batch_size} items: {iteration_time} s')\n    else:\n        print(f'Time it takes to go through all items: {iteration_time} s')\n    \n    total_time = round(time3-time1,2)\n    print(f'Total time: {total_time} s')\n    \n    time_list+=process_time,iteration_time,total_time\n    \ndef benchmarking_and_memory_usage(tdc,tokenizer,n=10,shuffle_trn=True,time_list=[]):\n    mem_usage = memory_usage((benchmarking,[tdc,tokenizer,n,shuffle_trn,time_list]))\n    total_usage = round(max(mem_usage),1)\n    print(f'Maximum memory usage: {total_usage} MiB')\n    time_list.append(total_usage)\n\n\ndef nlp_aug_stochastic(x,aug=None,p=0.5):\n    results = aug.augment(x)\n    if not isinstance(x,list): return results[0] if random.random()&lt;p else x\n    return [a if random.random()&lt;p else b for a,b in zip(results,x)]\n\naug = nac.KeyboardAug(aug_char_max=3,aug_char_p=0.1,aug_word_p=0.07)\nnearby_aug_func = partial(nlp_aug_stochastic,aug=aug,p=0.5)\n\n\nWith filter\n\ntimelist1=[]\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         sup_types='classification',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         val_ratio=None,\n                         batch_size=bs,\n                         seed=42,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,tokenizer,time_list=timelist1)\n\nTime it takes to process + tokenize training texts: 14.37 s\nTime it takes to go through 11740 items: 1.27 s\nTotal time: 15.64 s\nMaximum memory usage: 734.9 MiB\n\n\n\n\nWith filter + metadatas concatenation\n\ntimelist2=[]\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         sup_types='classification',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         val_ratio=None,\n                         batch_size=bs,\n                         seed=42,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,tokenizer,time_list=timelist2)\n\nTime it takes to process + tokenize training texts: 15.26 s\nTime it takes to go through 11740 items: 1.46 s\nTotal time: 16.72 s\nMaximum memory usage: 748.6 MiB\n\n\n\n\nWith filter + metadatas concatenation + content transformation + content augmentation\n\ntimelist3=[]\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         sup_types='classification',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         val_ratio=None,\n                         batch_size=bs,\n                         seed=42,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,tokenizer,time_list=timelist3)\n\nTime it takes to process + tokenize training texts: 35.09 s\nTime it takes to go through 11740 items: 1.52 s\nTotal time: 36.61 s\nMaximum memory usage: 754.7 MiB\n\n\n\n\nWith filter + metadatas concatenation + content transformation + content augmentation + no shuffling\n\ntimelist4=[]\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         sup_types='classification',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         val_ratio=None,\n                         batch_size=bs,\n                         seed=42,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,tokenizer,shuffle_trn=False,time_list=timelist4)\n\nTime it takes to process + tokenize training texts: 34.36 s\nTime it takes to go through 11740 items: 1.47 s\nTotal time: 35.83 s\nMaximum memory usage: 777.3 MiB\n\n\n\n\nWith filter + metadatas concatenation + content transformation + content augmentation + higher batch size\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         sup_types='classification',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         val_ratio=None,\n                         batch_size=bs*3,\n                         seed=42,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,tokenizer)\n\nTime it takes to process + tokenize training texts: 35.7 s\nTime it takes to go through 35220 items: 4.47 s\nTotal time: 40.17 s\nMaximum memory usage: 761.9 MiB\n\n\n\n\nWith filter + metadatas concatenation + content transformation + content augmentation + higher num proc\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         sup_types='classification',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         val_ratio=None,\n                         batch_size=bs,\n                         seed=42,\n                         num_proc=8,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,tokenizer)\n\nTime it takes to process + tokenize training texts: 24.7 s\nTime it takes to go through 11740 items: 1.46 s\nTotal time: 26.16 s\nMaximum memory usage: 754.2 MiB\n\n\n\n\nWith filter + metadatas concatenation + content transformation + content augmentation + iterate the whole dataset (1 epoch)\n\ntimelist5=[]\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         sup_types='classification',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         val_ratio=None,\n                         batch_size=bs,\n                         seed=42,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,tokenizer,n=None,time_list=timelist5)\n\nTime it takes to process + tokenize training texts: 35.34 s\nTime it takes to go through all items: 14.32 s\nTotal time: 49.66 s\nMaximum memory usage: 869.6 MiB\n\n\n\n\n\nb) With streaming\n\ndef benchmarking_streaming(tdc,tokenizer,n=10,time_list=[]):\n    time1 = time.time()\n    tdc.process_and_tokenize(tokenizer,max_length=512,line_by_line=True)\n    time2 = time.time() \n    process_time = round(time2-time1,2)\n    print(f'Time it takes to process + tokenize training texts: {process_time} s')\n    for i,v in enumerate(tdc.main_ddict['train']):\n        if n is not None and i==tdc.batch_size*n: break\n    time3 = time.time()\n    iteration_time = round(time3-time2,2)\n    if n is not None:\n        print(f'Time it takes to go through {n*tdc.batch_size} items: {iteration_time} s')\n    else:\n        print(f'Time it takes to go through all items: {iteration_time} s')\n    \n    total_time = round(time3-time1,2)\n    print(f'Total time: {total_time} s')\n    time_list+=process_time,iteration_time,total_time\ndef benchmarking_and_memory_usage_streaming(tdc,tokenizer,n=10,time_list=[]):\n    mem_usage = memory_usage((benchmarking_streaming,[tdc,tokenizer,n,time_list]))\n    total_usage = round(max(mem_usage),1)\n    print(f'Maximum memory usage: {total_usage} MiB')\n    time_list.append(total_usage)\n\n\ndef nlp_aug_stochastic(x,aug=None,p=0.5):\n    results = aug.augment(x)\n    if not isinstance(x,list): return results[0] if random.random()&lt;p else x\n    return [a if random.random()&lt;p else b for a,b in zip(results,x)]\n\naug = nac.KeyboardAug(aug_char_max=3,aug_char_p=0.1,aug_word_p=0.07)\nnearby_aug_func = partial(nlp_aug_stochastic,aug=aug,p=0.5)\n\n\nWith filter\n\nns_timelist1=[]\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=True)\n\ntdc = TextDataControllerStreaming(dset,\n                                 main_text='Review Text',\n                                 label_names='Department Name',\n                                 sup_types='classification',\n                                 filter_dict={'Review Text': lambda x: x is not None,\n                                              'Department Name': lambda x: x is not None,\n                                             },\n                                 class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n                                 batch_size=bs,\n                                 num_proc=4,\n                                 seed=42,\n                                 verbose=False\n                                )\nbenchmarking_and_memory_usage_streaming(tdc,tokenizer,time_list=ns_timelist1)\n\nTime it takes to process + tokenize training texts: 0.8 s\nTime it takes to go through 11740 items: 4.03 s\nTotal time: 4.82 s\nMaximum memory usage: 743.0 MiB\n\n\n\n\nWith filter + metadatas concatenation\n\nns_timelist2=[]\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=True)\n\ntdc = TextDataControllerStreaming(dset,\n                                 main_text='Review Text',\n                                 label_names='Department Name',\n                                 sup_types='classification',\n                                 filter_dict={'Review Text': lambda x: x is not None,\n                                              'Department Name': lambda x: x is not None,\n                                             },\n                                 class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n                                 metadatas=['Title','Division Name'],\n                                 batch_size=bs,\n                                 num_proc=4,\n                                 seed=42,\n                                 verbose=False\n                                )\nbenchmarking_and_memory_usage_streaming(tdc,tokenizer,time_list=ns_timelist2)\n\nTime it takes to process + tokenize training texts: 0.79 s\nTime it takes to go through 11740 items: 4.43 s\nTotal time: 5.22 s\nMaximum memory usage: 745.9 MiB\n\n\n\n\nWith filter + metadatas concatenation + content transformation + content augmentation\n\nns_timelist3=[]\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=True)\n\ntdc = TextDataControllerStreaming(dset,\n                                 main_text='Review Text',\n                                 label_names='Department Name',\n                                 sup_types='classification',\n                                 filter_dict={'Review Text': lambda x: x is not None,\n                                              'Department Name': lambda x: x is not None,\n                                             },\n                                 class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n                                 metadatas=['Title','Division Name'],\n                                 content_transformations=[text_normalize,str.lower],\n                                 content_augmentations= [nearby_aug_func,str.lower],\n                                 batch_size=bs,\n                                 num_proc=4,\n                                 seed=42,\n                                 verbose=False\n                                )\nbenchmarking_and_memory_usage_streaming(tdc,tokenizer,time_list=ns_timelist3)\n\nTime it takes to process + tokenize training texts: 0.78 s\nTime it takes to go through 11740 items: 12.23 s\nTotal time: 13.01 s\nMaximum memory usage: 743.0 MiB\n\n\n\n\nWith filter + metadatas concatenation + content transformation + content augmentation + higher batch size (not recorded)\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=True)\n\ntdc = TextDataControllerStreaming(dset,\n                                 main_text='Review Text',\n                                 label_names='Department Name',\n                                 sup_types='classification',\n                                 filter_dict={'Review Text': lambda x: x is not None,\n                                              'Department Name': lambda x: x is not None,\n                                             },\n                                 class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n                                 metadatas=['Title','Division Name'],\n                                 content_transformations=[text_normalize,str.lower],\n                                 content_augmentations= [nearby_aug_func,str.lower],\n                                 batch_size=bs*3,\n                                 num_proc=4,\n                                 seed=42,\n                                 verbose=False\n                                )\nbenchmarking_and_memory_usage_streaming(tdc,tokenizer)\n\nTime it takes to process + tokenize training texts: 0.79 s\nTime it takes to go through 35220 items: 36.66 s\nTotal time: 37.45 s\nMaximum memory usage: 887.4 MiB\n\n\n\n\nWith filter + metadatas concatenation + content transformation + content augmentation + iterate the whole dataset (1 epoch)\n\nns_timelist4=[]\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=True)\n\ntdc = TextDataControllerStreaming(dset,\n                                 main_text='Review Text',\n                                 label_names='Department Name',\n                                 sup_types='classification',\n                                 filter_dict={'Review Text': lambda x: x is not None,\n                                              'Department Name': lambda x: x is not None,\n                                             },\n                                 class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n                                 metadatas=['Title','Division Name'],\n                                 content_transformations=[text_normalize,str.lower],\n                                 content_augmentations= [nearby_aug_func,str.lower],\n                                 batch_size=bs,\n                                 num_proc=4,\n                                 seed=42,\n                                 verbose=False\n                                )\nbenchmarking_and_memory_usage_streaming(tdc,tokenizer,n=None,time_list=ns_timelist4)\n\nTime it takes to process + tokenize training texts: 0.8 s\nTime it takes to go through all items: 111.93 s\nTotal time: 112.73 s\nMaximum memory usage: 762.8 MiB",
    "crumbs": [
      "5. Text Classes",
      "b. Text Controller For Supervised Learning",
      "Text Processing Benchmark"
    ]
  },
  {
    "objectID": "text_main_benchmark.html#test-the-effect-of-batch-size-and-num-proc-parallel-process-on-non-streaming-dataset",
    "href": "text_main_benchmark.html#test-the-effect-of-batch-size-and-num-proc-parallel-process-on-non-streaming-dataset",
    "title": "Text Processing Benchmark",
    "section": "2. Test the effect of batch size and num proc (parallel process) on Non-streaming dataset",
    "text": "2. Test the effect of batch size and num proc (parallel process) on Non-streaming dataset\n\ndef benchmarking(tdc,tokenizer,n=10,shuffle_trn=True):\n    time1 = time.time()\n    tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=shuffle_trn)\n    time2 = time.time() \n    print(f'Time it takes to process + tokenize training texts: {(time2-time1):.3f} s')\n    for i,v in enumerate(tdc.main_ddict['train']):\n        if n is not None and i==tdc.batch_size*n: break\n    time3 = time.time()\n    if n is not None:\n        print(f'Time it takes to go through {n*tdc.batch_size} items: {(time3-time2):.3f} s')\n    else:\n        print(f'Time it takes to go through all items: {(time3-time2):.3f} s')\n\n    print(f'Total time: {(time3-time1):.3f} s')\ndef benchmarking_and_memory_usage(tdc,tokenizer,n=10,shuffle_trn=True):\n    mem_usage = memory_usage((benchmarking,[tdc,tokenizer,n,shuffle_trn]))\n    print(f'Maximum memory usage: {max(mem_usage):.3f} MiB')\n\n\ndef nlp_aug_stochastic(x,aug=None,p=0.5):\n    results = aug.augment(x)\n    if not isinstance(x,list): return results[0] if random.random()&lt;p else x\n    return [a if random.random()&lt;p else b for a,b in zip(results,x)]\n\naug = nac.KeyboardAug(aug_char_max=3,aug_char_p=0.1,aug_word_p=0.07)\nnearby_aug_func = partial(nlp_aug_stochastic,aug=aug,p=0.5)\n\nFor non-streaming dataset, text processing + tokenization are the most time-consuming tasks, thus we will check how different batch size and num proc will affect these tasks’ running time\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         sup_types='classification',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         val_ratio=None,\n                         batch_size=100,\n                         num_proc=2,\n                         seed=42,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,tokenizer,n=None)\n\nTime it takes to process + tokenize training texts: 64.098 s\nTime it takes to go through all items: 13.400 s\nTotal time: 77.499 s\nMaximum memory usage: 925.188 MiB\n\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         sup_types='classification',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         val_ratio=None,\n                         batch_size=1000,\n                         num_proc=2,\n                         seed=42,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,tokenizer,n=None)\n\nTime it takes to process + tokenize training texts: 61.297 s\nTime it takes to go through all items: 14.427 s\nTotal time: 75.724 s\nMaximum memory usage: 912.223 MiB\n\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         sup_types='classification',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         val_ratio=None,\n                         batch_size=100,\n                         num_proc=8,\n                         seed=42,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,tokenizer,n=None)\n\nTime it takes to process + tokenize training texts: 25.857 s\nTime it takes to go through all items: 13.776 s\nTotal time: 39.634 s\nMaximum memory usage: 928.574 MiB\n\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         sup_types='classification',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         val_ratio=None,\n                         batch_size=1000,\n                         num_proc=8,\n                         seed=42,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,tokenizer,n=None)\n\nTime it takes to process + tokenize training texts: 24.933 s\nTime it takes to go through all items: 14.271 s\nTotal time: 39.204 s\nMaximum memory usage: 913.266 MiB\n\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         sup_types='classification',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         val_ratio=None,\n                         batch_size=2000,\n                         num_proc=8,\n                         seed=42,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,tokenizer,n=None)\n\nTime it takes to process + tokenize training texts: 25.600 s\nTime it takes to go through all items: 14.465 s\nTotal time: 40.064 s\nMaximum memory usage: 934.883 MiB\n\n\nIncreasing num_proc is more beneficial than increasing processing batch size",
    "crumbs": [
      "5. Text Classes",
      "b. Text Controller For Supervised Learning",
      "Text Processing Benchmark"
    ]
  },
  {
    "objectID": "text_main_benchmark.html#improving-processing-time-with-caching",
    "href": "text_main_benchmark.html#improving-processing-time-with-caching",
    "title": "Text Processing Benchmark",
    "section": "3. Improving processing time with caching",
    "text": "3. Improving processing time with caching\nThe worst processing time is recorded with non-streaming training set, with the following preprocessing: 2-column filtering, 2-column metadatas, 2 content transformations, 2 content augmentation.\nWith caching, we can significantly reduce the preprocessing time. That means, you only need to do all preprocessings once; all subsequent call will take advatages of this cached result.\n\nenable_caching()\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         sup_types='classification',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         val_ratio=None,\n                         batch_size=1000,\n                         num_proc=4,\n                         seed=42,\n                         verbose=False\n                        )\ntdc.process_and_tokenize(tokenizer,max_length=512)\n\nFound cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\nLoading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-0aed8574c094e4fd_*_of_00004.arrow\nLoading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-b781a4a73d06caf5_*_of_00004.arrow\nLoading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-0f85d6db4165d6ef_*_of_00004.arrow\nLoading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-420893192d8b876f_*_of_00004.arrow\nLoading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-ee3f2ca19acd2369_*_of_00004.arrow\nLoading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-27d1b7f9046ec1b4_*_of_00004.arrow\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         sup_types='classification',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         val_ratio=None,\n                         batch_size=1000,\n                         num_proc=4,\n                         seed=42,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,tokenizer,n=None)\n\nFound cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\nLoading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-0aed8574c094e4fd_*_of_00004.arrow\nLoading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-b781a4a73d06caf5_*_of_00004.arrow\nLoading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-0f85d6db4165d6ef_*_of_00004.arrow\nLoading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-420893192d8b876f_*_of_00004.arrow\nLoading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-ee3f2ca19acd2369_*_of_00004.arrow\nLoading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-27d1b7f9046ec1b4_*_of_00004.arrow\nLoading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-57d938bbd364f406_*_of_00004.arrow\nLoading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-10afb2ec3cb12852_*_of_00004.arrow\nLoading cached shuffled indices for dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-5d8840c40fe75896.arrow\nLoading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-030424c28049222f_*_of_00004.arrow\nLoading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-fe4290971e8d1087_*_of_00004.arrow\n\n\nTime it takes to process + tokenize training texts: 0.979 s\nTime it takes to go through all items: 16.824 s\nMaximum memory usage: 823.531 MiB\n\n\nIf you cached, then you only need 0.979s to load the data back from caches, instead of wait for 35s to do the process all over again",
    "crumbs": [
      "5. Text Classes",
      "b. Text Controller For Supervised Learning",
      "Text Processing Benchmark"
    ]
  },
  {
    "objectID": "text_main_benchmark.html#time-and-space-complexity-comparison-as-of-532024",
    "href": "text_main_benchmark.html#time-and-space-complexity-comparison-as-of-532024",
    "title": "Text Processing Benchmark",
    "section": "4. Time and Space Complexity Comparison (as of 5/3/2024)",
    "text": "4. Time and Space Complexity Comparison (as of 5/3/2024)\n\nimport pandas as pd\nimport numpy as np\n\n\nexp1 = [timelist1,ns_timelist1]\nexp2 = [timelist2,ns_timelist2]\nexp3 = [timelist3,ns_timelist3]\nexp4 = [timelist4,[None,None,None,None]] # no shuffling when streaming\nexp5 = [timelist5,ns_timelist4]\n\n\ncol_names=['Filter + Shuffling Train','And 2 metadatas',\n           'And 2 tfms + 2 augs','Same, but without train shuffling',\n           'Time to process 1 epoch']\nidxs=['Non-Streaming','Streaming']\n\n\n_tmp=[]\nfor i in range(2):\n    _tmp.append([l[i][0] for l in [exp1,exp2,exp3,exp4,exp5]])\ndf = pd.DataFrame(np.array(_tmp),columns=col_names)\ndf.index = idxs\ndf.index.name= 'Time (s) to process and tokenize 117k records with batch size 1174'\n\n_tmp=[]\nfor i in range(2):\n    _tmp.append([l[i][1] for l in [exp1,exp2,exp3,exp4,exp5]])\ndf2 = pd.DataFrame(np.array(_tmp),columns=col_names[:-1]+['Time to iterate 1 epoch'])\ndf2.index = idxs\ndf2.index.name= 'Time (s) to iterate 10 batches (11740 items)'\n\n_tmp=[]\nfor i in range(2):\n    _tmp.append([l[i][2] for l in [exp1,exp2,exp3,exp4,exp5]])\ndf3 = pd.DataFrame(np.array(_tmp),columns=col_names[:-1]+['Total time to process + tokenize + iterate 1 epoch'])\ndf3.index = idxs\ndf3.index.name= 'Total time (s) to process + tokenize + iterate 10 batches'\n\n_tmp=[]\nfor i in range(2):\n    _tmp.append([l[i][3] for l in [exp1,exp2,exp3,exp4,exp5]])\ndf4 = pd.DataFrame(np.array(_tmp),columns=col_names[:-1]+['Total memory to process + tokenize + iterate 1 epoch'])\ndf4.index = idxs\ndf4.index.name= 'Total memory (MiB) to process + tokenize + iterate 10 batches'\n\n\ndf\n\n\n\n\n\n\n\n\n\nFilter + Shuffling Train\nAnd 2 metadatas\nAnd 2 tfms + 2 augs\nSame, but without train shuffling\nTime to process 1 epoch\n\n\nTime (s) to process and tokenize 117k records with batch size 1174\n\n\n\n\n\n\n\n\n\nNon-Streaming\n14.37\n15.26\n35.09\n34.36\n35.34\n\n\nStreaming\n0.8\n0.79\n0.78\nNone\n0.8\n\n\n\n\n\n\n\n\n\ndf2\n\n\n\n\n\n\n\n\n\nFilter + Shuffling Train\nAnd 2 metadatas\nAnd 2 tfms + 2 augs\nSame, but without train shuffling\nTime to iterate 1 epoch\n\n\nTime (s) to iterate 10 batches (11740 items)\n\n\n\n\n\n\n\n\n\nNon-Streaming\n1.27\n1.46\n1.52\n1.47\n14.32\n\n\nStreaming\n4.03\n4.43\n12.23\nNone\n111.93\n\n\n\n\n\n\n\n\n\ndf3\n\n\n\n\n\n\n\n\n\nFilter + Shuffling Train\nAnd 2 metadatas\nAnd 2 tfms + 2 augs\nSame, but without train shuffling\nTotal time to process + tokenize + iterate 1 epoch\n\n\nTotal time (s) to process + tokenize + iterate 10 batches\n\n\n\n\n\n\n\n\n\nNon-Streaming\n15.64\n16.72\n36.61\n35.83\n49.66\n\n\nStreaming\n4.82\n5.22\n13.01\nNone\n112.73\n\n\n\n\n\n\n\n\n\ndf4\n\n\n\n\n\n\n\n\n\nFilter + Shuffling Train\nAnd 2 metadatas\nAnd 2 tfms + 2 augs\nSame, but without train shuffling\nTotal memory to process + tokenize + iterate 1 epoch\n\n\nTotal memory (MiB) to process + tokenize + iterate 10 batches\n\n\n\n\n\n\n\n\n\nNon-Streaming\n734.9\n748.6\n754.7\n777.3\n869.6\n\n\nStreaming\n743.0\n745.9\n743.0\nNone\n762.8",
    "crumbs": [
      "5. Text Classes",
      "b. Text Controller For Supervised Learning",
      "Text Processing Benchmark"
    ]
  },
  {
    "objectID": "text_main_benchmark.html#tips-and-tricks",
    "href": "text_main_benchmark.html#tips-and-tricks",
    "title": "Text Processing Benchmark",
    "section": "5. Tips and tricks",
    "text": "5. Tips and tricks\n\nFor non-streaming data, the best way to minimize processing and iteration time is:\n\nTurn on dataset caching, and run the processing step once for it to be cached\n\nThe more content transformations and augmentations added, the slower the process + iteration. This is especially true for streaming data\nFor streaming, be aware of the pros and cons of batch-process and line-by-line process (read more here)",
    "crumbs": [
      "5. Text Classes",
      "b. Text Controller For Supervised Learning",
      "Text Processing Benchmark"
    ]
  },
  {
    "objectID": "evaluations.html",
    "href": "evaluations.html",
    "title": "Model Evaluations",
    "section": "",
    "text": "import os\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom that_nlp_library.text_main import *\nfrom that_nlp_library.utils import seed_everything\nfrom underthesea import text_normalize\nfrom functools import partial\nfrom pathlib import Path\nfrom transformers import RobertaTokenizer\nfrom transformers.models.roberta.modeling_roberta import RobertaForSequenceClassification\nimport nlpaug.augmenter.char as nac\nfrom datasets import load_dataset\nimport random\nimport pandas as pd\nimport numpy as np\nfrom that_nlp_library.model_main import *\nfrom sklearn.metrics import f1_score, accuracy_score",
    "crumbs": [
      "3. Evaluations For Supervised Learning",
      "Model Evaluations"
    ]
  },
  {
    "objectID": "evaluations.html#evaluation-for-single-headmulti-head-classification",
    "href": "evaluations.html#evaluation-for-single-headmulti-head-classification",
    "title": "Model Evaluations",
    "section": "Evaluation for Single-head/Multi-head Classification",
    "text": "Evaluation for Single-head/Multi-head Classification\n\nCreate a TextDataController\n\n# Define the custom augmentation function\n\ndef nlp_aug_stochastic(x,aug=None,p=0.5):\n    if not isinstance(x,list): \n        if random.random()&lt;p: return aug.augment(x)[0]\n        return x\n    news=[]\n    originals=[]\n    for _x in x:\n        if random.random()&lt;p: news.append(_x)\n        else: originals.append(_x)\n    # only perform augmentation when needed\n    if len(news): news = aug.augment(news)\n    return news+originals\n\n\naug = nac.KeyboardAug(aug_char_max=3,aug_char_p=0.1,aug_word_p=0.07)\nnearby_aug_func = partial(nlp_aug_stochastic,aug=aug,p=0.3)\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         sup_types='classification',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         # add \"str.lower\" here because nearby_aug might return uppercase character\n                         val_ratio=0.2,\n                         batch_size=1000,\n                         seed=42,\n                         num_proc=20,\n                         verbose=False\n                        )\n\n\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\ntdc.process_and_tokenize(tokenizer,max_length=100,shuffle_trn=True)\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\n\nDefine and train model\n\nnum_classes = len(tdc.label_lists[0])\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score]\n\n\nseed_everything(42)\nmodel_name='roberta-base'\nmodel = RobertaForSequenceClassification.from_pretrained(model_name,num_labels=num_classes).to('cuda:0')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\ncontroller = ModelController(model,\n                             data_store=tdc,\n                             seed=42)\n\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 3\n\ncontroller.fit(epochs,lr,\n               metric_funcs=metric_funcs,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics,\n              )\n\n\n\n    \n      \n      \n      [849/849 05:01, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1 Score Department name\nAccuracy Score Department name\n\n\n\n\n1\nNo log\n0.316325\n0.734646\n0.913389\n\n\n2\n0.419000\n0.259580\n0.752915\n0.923995\n\n\n3\n0.419000\n0.265681\n0.751789\n0.922890\n\n\n\n\n\n\n\n\n\nGenerate predictions\nWe will do model evaluation on the validation set\n\ndf_val = controller.predict_ddict(ds_type='validation')\n\n-------------------- Start making predictions --------------------\n\n\n\ndf_val = df_val.to_pandas()\ndf_val.head()\n\n\n\n\n\n\n\n\n\nTitle\nReview Text\nDivision Name\nDepartment Name\nlabel\ninput_ids\nattention_mask\npred_Department Name\npred_prob_Department Name\n\n\n\n\n0\n\ngeneral petite . . such a fun jacket ! great t...\ngeneral petite\nIntimate\n2\n[0, 15841, 4716, 1459, 479, 479, 215, 10, 1531...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nJackets\n0.881662\n\n\n1\nsimple and elegant\ngeneral petite . simple and elegant . i though...\ngeneral petite\nTops\n4\n[0, 15841, 4716, 1459, 479, 2007, 8, 14878, 47...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.994637\n\n\n2\nretro and pretty\ngeneral . retro and pretty . this top has a bi...\ngeneral\nTops\n4\n[0, 15841, 479, 11299, 8, 1256, 479, 42, 299, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nTops\n0.994337\n\n\n3\nsummer/fall wear\ngeneral petite . summer / fall wear . i first ...\ngeneral petite\nDresses\n1\n[0, 15841, 4716, 1459, 479, 1035, 1589, 1136, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nDresses\n0.986489\n\n\n4\nperfect except slip\ngeneral petite . perfect except slip . this is...\ngeneral petite\nDresses\n1\n[0, 15841, 4716, 1459, 479, 1969, 4682, 9215, ...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nDresses\n0.986804\n\n\n\n\n\n\n\n\n\n\nStart the model evaluation\n\nsource\n\n\nevaluate_classification_model_metadata\n\n evaluate_classification_model_metadata (df:pandas.core.frame.DataFrame,\n                                         metadatas:str|list,\n                                         label_name:str, pred_name:str,\n                                         metric_funcs:list)\n\n\n\n\n\nType\nDetails\n\n\n\n\ndf\npd.DataFrame\nThe main dataframe containing the predictions\n\n\nmetadatas\nstr | list\nMetadata(s) to perform analysis\n\n\nlabel_name\nstr\nLabel’s column name\n\n\npred_name\nstr\nPrediction’s column name\n\n\nmetric_funcs\nlist\nMetric(s) to calculate\n\n\n\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score]\n\n\nevaluate_classification_model_metadata(df_val,metadatas='Division Name',\n                                       label_name='Department Name',\n                                       pred_name='pred_Department Name',\n                                       metric_funcs=metric_funcs\n                                      )\n\n-------------------- Department Name Analysis on metadata: Division Name --------------------\n---------- Distribution ----------\n                count  proportion\nDivision Name                    \ngeneral          2688    0.593902\ngeneral petite   1546    0.341582\ninitmates         292    0.064516\n\n---------- Metrics for each value in Division Name ----------\n- For all data:\nf1_score: 0.7518 . accuracy_score: 0.9229 . \n- For general:\nf1_score: 0.5958 . accuracy_score: 0.9219 . \n- For general petite:\nf1_score: 0.6386 . accuracy_score: 0.9101 . \n- For initmates:\nf1_score: 1.0000 . accuracy_score: 1.0000 . \n\n\n\nsource\n\n\nshow_top_n_predictions\n\n show_top_n_predictions (df:pandas.core.frame.DataFrame, text_name:str,\n                         label_name:str, pred_name:str, prob_name:str,\n                         is_incorrect=True, ascending=False, n_show=10)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nThe main dataframe containing the predictions\n\n\ntext_name\nstr\n\nText’s column name\n\n\nlabel_name\nstr\n\nLabel’s column name\n\n\npred_name\nstr\n\nPrediction’s column name\n\n\nprob_name\nstr\n\nPrediction probability’s column name\n\n\nis_incorrect\nbool\nTrue\nTo show top correct or incorrect sentences\n\n\nascending\nbool\nFalse\nTo sort by prob_name ascendingly or descendingly\n\n\nn_show\nint\n10\nNumber of sentences to show\n\n\n\n\nshow_top_n_predictions(df_val,text_name='Review Text',\n                      label_name='Department Name',\n                      pred_name='pred_Department Name',\n                      prob_name='pred_prob_Department Name',\n                      is_incorrect=True,\n                      ascending=False,n_show=5)\n\nText: general petite . . it is a beautiful top but runs huge ! ! ! i followed other reviews and ordered a size down hoping it would work but it was totally unflattering ! i wanted so bad to love it but there was no way i could keep it ! !\nTrue label: Dresses, but predict Tops, with confidence 0.9948\n------------------------------------------------------------\nText: general . great \" cardigan \" . if you're very small-framed and / or petite , i would size down in this . unless you want a really slouchy look and fit . ---------- i have very broad shoulders and very large biceps , and typically wear a m / l in retailer tops . sometimes if a top is a pullover style ( as opposed to button-down ) i have to size up to a 12 to get the top to accommodate my shoulder width and biceps . so i was leery the size 8 / italian 44 would fit , but it does , with room to spare , albeit it is not as loose or long in the ar\nTrue label: Jackets, but predict Tops, with confidence 0.9946\n------------------------------------------------------------\nText: general . is this top orange or red . wish this site had \" ask a question about the product \" feature like amazon does . just wondering if anyone has seen this product in person to verify color ? it looks orange but descriptor says red . would appreciate feedback . thank you for your help .\nTrue label: Jackets, but predict Tops, with confidence 0.9944\n------------------------------------------------------------\nText: general petite . . this is absolutely gorgeous cardigan ! the pictures don't do it justice ! it's comfortable and soft and very trendy at the same time ! i bought it in teal and in medium , i am 5 ' 3 \" and usually wear 10-12 ! even though i think it runs tts , but for more fitted look , medium fits better ! i am blond and this color looks great ! i also loved it when unzipped the upper part , it looked even better ! it is perfect with leggings in grey , blue , green and black , also beautiful with aqua flora leggings and with re\nTrue label: Intimate, but predict Tops, with confidence 0.9944\n------------------------------------------------------------\nText: general petite . sweet top . i love this top . i had no issues with fabric . it is soft and light . the wrist details are very pretty . i love the buttons . it fits true to size and it is a good layering top . i got the red which is true to the picture . not looking red but a bright coral color .\nTrue label: Intimate, but predict Tops, with confidence 0.9944\n------------------------------------------------------------\n\n\n\nshow_top_n_predictions(df_val,text_name='Review Text',\n                      label_name='Department Name',\n                      pred_name='pred_Department Name',\n                      prob_name='pred_prob_Department Name',\n                      is_incorrect=True,\n                      ascending=True,n_show=5)\n\nText: general . perfect in all respects . true to size for petite person ; bought in blue - subtle shading , looks great ; paired with your blue straight leg blue \" jeans \" ; perfect outfit for almost all occasions .\nTrue label: Tops, but predict Bottoms, with confidence 0.3470\n------------------------------------------------------------\nText: general . amazing . this fits so perfect ! it is light fabric , not clingy . very flattering ! i am 5 ' 4 120 lbs with athletic build .\nTrue label: Bottoms, but predict Dresses, with confidence 0.3505\n------------------------------------------------------------\nText: general . disappointed . bummed out .. this is super cheaply made and very expensive ... the design is different , the color is beautiful , but the material is a different story .. it should be priced at the most 30 bucks .. not 128 .. i'm returning them . also , they run very big .. !\nTrue label: Bottoms, but predict Tops, with confidence 0.3516\n------------------------------------------------------------\nText: general . tons of compliments . wore this to work and nearly everyone complimented me on it . very comfortable . lots of detailing . perfect for fall . pockets ! i'm usually somewhere between a s and m but this piece was generously cut and a s was perfect . i debated ordering the petite , but actually the regular cut was the perfect length even though i'm only 5 ' 4 \" .\nTrue label: Dresses, but predict Bottoms, with confidence 0.3578\n------------------------------------------------------------\nText: general petite . misnomer in color , stunning on any figure . i'm not sure why they call this \" black \" because what you see in this pictures depicted is pretty much the shade in person . the fabric itself is lovely . it made my wider hips look smaller ! :) ___________ pros : - drapes beautifully on the body , especially for curvier gals . - not black but a deep navy / velvet look and feel . - very forgiving and stretchy . - perfect length in front and back . - hides imperfections wonderfully . - soft , flows well , and unique ! cons : - material is thin , not for winter a\nTrue label: Bottoms, but predict Dresses, with confidence 0.3578\n------------------------------------------------------------\n\n\n\nsource\n\n\nevaluate_classification_model\n\n evaluate_classification_model (df:pandas.core.frame.DataFrame,\n                                text_name:str, label_name:str,\n                                pred_name:str, prob_name:str,\n                                metric_funcs:list=[],\n                                metadatas:str|list=[], n_show=10,\n                                cm_figsize=(20, 20))\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\nThe main dataframe containing the predictions\n\n\ntext_name\nstr\n\nText’s column name\n\n\nlabel_name\nstr\n\nLabel’s column name\n\n\npred_name\nstr\n\nPrediction’s column name\n\n\nprob_name\nstr\n\nPrediction probability’s column name\n\n\nmetric_funcs\nlist\n[]\nMetric(s) to calculate\n\n\nmetadatas\nstr | list\n[]\nMetadata(s) to perform analysis\n\n\nn_show\nint\n10\nNumber of sentences to show\n\n\ncm_figsize\ntuple\n(20, 20)\nConfusion matrix’s figure size\n\n\n\n\nevaluate_classification_model(df_val,text_name='Review Text',\n                              label_name='Department Name',\n                              pred_name='pred_Department Name',\n                              prob_name='pred_prob_Department Name',\n                              metric_funcs=metric_funcs,\n                              metadatas='Division Name',\n                              cm_figsize=(8,8),\n                              n_show=5\n                             )\n\n-------------------- Department Name Analysis --------------------\n\n--------------- Classification Report ---------------\n              precision    recall  f1-score   support\n\n     Bottoms       0.92      0.93      0.92       695\n     Dresses       0.94      0.91      0.93      1248\n    Intimate       0.99      0.88      0.93       341\n     Jackets       0.80      0.79      0.79       213\n        Tops       0.92      0.96      0.94      2010\n       Trend       0.00      0.00      0.00        19\n\n    accuracy                           0.92      4526\n   macro avg       0.76      0.74      0.75      4526\nweighted avg       0.92      0.92      0.92      4526\n\n\n--------------- Confusion Matrix ---------------\n\n-------------------- Department Name Analysis on metadata: Division Name --------------------\n---------- Distribution ----------\n                count  proportion\nDivision Name                    \ngeneral          2688    0.593902\ngeneral petite   1546    0.341582\ninitmates         292    0.064516\n\n---------- Metrics for each value in Division Name ----------\n- For all data:\nf1_score: 0.7518 . accuracy_score: 0.9229 . \n- For general:\nf1_score: 0.5958 . accuracy_score: 0.9219 . \n- For general petite:\nf1_score: 0.6386 . accuracy_score: 0.9101 . \n- For initmates:\nf1_score: 1.0000 . accuracy_score: 1.0000 . \n\n--------------- Let's look at some wrong predictions with high confidence ---------------\nText: general petite . . it is a beautiful top but runs huge ! ! ! i followed other reviews and ordered a size down hoping it would work but it was totally unflattering ! i wanted so bad to love it but there was no way i could keep it ! !\nTrue label: Dresses, but predict Tops, with confidence 0.9948\n------------------------------------------------------------\nText: general . great \" cardigan \" . if you're very small-framed and / or petite , i would size down in this . unless you want a really slouchy look and fit . ---------- i have very broad shoulders and very large biceps , and typically wear a m / l in retailer tops . sometimes if a top is a pullover style ( as opposed to button-down ) i have to size up to a 12 to get the top to accommodate my shoulder width and biceps . so i was leery the size 8 / italian 44 would fit , but it does , with room to spare , albeit it is not as loose or long in the ar\nTrue label: Jackets, but predict Tops, with confidence 0.9946\n------------------------------------------------------------\nText: general . is this top orange or red . wish this site had \" ask a question about the product \" feature like amazon does . just wondering if anyone has seen this product in person to verify color ? it looks orange but descriptor says red . would appreciate feedback . thank you for your help .\nTrue label: Jackets, but predict Tops, with confidence 0.9944\n------------------------------------------------------------\nText: general petite . . this is absolutely gorgeous cardigan ! the pictures don't do it justice ! it's comfortable and soft and very trendy at the same time ! i bought it in teal and in medium , i am 5 ' 3 \" and usually wear 10-12 ! even though i think it runs tts , but for more fitted look , medium fits better ! i am blond and this color looks great ! i also loved it when unzipped the upper part , it looked even better ! it is perfect with leggings in grey , blue , green and black , also beautiful with aqua flora leggings and with re\nTrue label: Intimate, but predict Tops, with confidence 0.9944\n------------------------------------------------------------\nText: general petite . sweet top . i love this top . i had no issues with fabric . it is soft and light . the wrist details are very pretty . i love the buttons . it fits true to size and it is a good layering top . i got the red which is true to the picture . not looking red but a bright coral color .\nTrue label: Intimate, but predict Tops, with confidence 0.9944\n------------------------------------------------------------\n\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))",
    "crumbs": [
      "3. Evaluations For Supervised Learning",
      "Model Evaluations"
    ]
  },
  {
    "objectID": "models.roberta.classifiers.html",
    "href": "models.roberta.classifiers.html",
    "title": "Roberta Classifiers",
    "section": "",
    "text": "source\n\n\n\n ConcatHeadExtended (config, classifier_dropout=0.1, last_hidden_size=768,\n                     layer2concat=4, num_labels=None, **kwargs)\n\nConcatenated head for Roberta Classification Model. This head takes the last n hidden states of [CLS], and concatenate them before passing through the classifier head\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nconfig\n\n\nHuggingFace model configuration\n\n\nclassifier_dropout\nfloat\n0.1\nDropout ratio (for dropout layer right before the last nn.Linear)\n\n\nlast_hidden_size\nint\n768\nLast hidden size (before the last nn.Linear)\n\n\nlayer2concat\nint\n4\nnumber of hidden layer to concatenate (counting from top)\n\n\nnum_labels\nNoneType\nNone\nNumber of label output. Overwrite config.num_labels\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\n\n\n ConcatHeadSimple (config, classifier_dropout=0.1, layer2concat=4,\n                   num_labels=None, **kwargs)\n\nConcatenated head for Roberta Classification Model, the simpler version (no hidden linear layer) This head takes the last n hidden states of [CLS], and concatenate them before passing through the classifier head\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nconfig\n\n\nHuggingFace model configuration\n\n\nclassifier_dropout\nfloat\n0.1\nDropout ratio (for dropout layer right before the last nn.Linear)\n\n\nlayer2concat\nint\n4\nnumber of hidden layer to concatenate (counting from top)\n\n\nnum_labels\nNoneType\nNone\nNumber of label output. Overwrite config.num_labels\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\n\n\n RobertaClassificationHeadCustom (config, classifier_dropout=0.1,\n                                  num_labels=None, **kwargs)\n\n*Same as RobertaClassificationHead, but you can freely adjust dropout\nReference: https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py#L1424*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nconfig\n\n\nHuggingFace model configuration\n\n\nclassifier_dropout\nfloat\n0.1\nDropout ratio (for dropout layer right before the last nn.Linear)\n\n\nnum_labels\nNoneType\nNone\nNumber of label output. Overwrite config.num_labels\n\n\nkwargs",
    "crumbs": [
      "6. Model Classes",
      "c. Roberta-based classification Model",
      "Roberta Classifiers"
    ]
  },
  {
    "objectID": "models.roberta.classifiers.html#classification-head",
    "href": "models.roberta.classifiers.html#classification-head",
    "title": "Roberta Classifiers",
    "section": "",
    "text": "source\n\n\n\n ConcatHeadExtended (config, classifier_dropout=0.1, last_hidden_size=768,\n                     layer2concat=4, num_labels=None, **kwargs)\n\nConcatenated head for Roberta Classification Model. This head takes the last n hidden states of [CLS], and concatenate them before passing through the classifier head\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nconfig\n\n\nHuggingFace model configuration\n\n\nclassifier_dropout\nfloat\n0.1\nDropout ratio (for dropout layer right before the last nn.Linear)\n\n\nlast_hidden_size\nint\n768\nLast hidden size (before the last nn.Linear)\n\n\nlayer2concat\nint\n4\nnumber of hidden layer to concatenate (counting from top)\n\n\nnum_labels\nNoneType\nNone\nNumber of label output. Overwrite config.num_labels\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\n\n\n ConcatHeadSimple (config, classifier_dropout=0.1, layer2concat=4,\n                   num_labels=None, **kwargs)\n\nConcatenated head for Roberta Classification Model, the simpler version (no hidden linear layer) This head takes the last n hidden states of [CLS], and concatenate them before passing through the classifier head\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nconfig\n\n\nHuggingFace model configuration\n\n\nclassifier_dropout\nfloat\n0.1\nDropout ratio (for dropout layer right before the last nn.Linear)\n\n\nlayer2concat\nint\n4\nnumber of hidden layer to concatenate (counting from top)\n\n\nnum_labels\nNoneType\nNone\nNumber of label output. Overwrite config.num_labels\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\n\n\n RobertaClassificationHeadCustom (config, classifier_dropout=0.1,\n                                  num_labels=None, **kwargs)\n\n*Same as RobertaClassificationHead, but you can freely adjust dropout\nReference: https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py#L1424*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nconfig\n\n\nHuggingFace model configuration\n\n\nclassifier_dropout\nfloat\n0.1\nDropout ratio (for dropout layer right before the last nn.Linear)\n\n\nnum_labels\nNoneType\nNone\nNumber of label output. Overwrite config.num_labels\n\n\nkwargs",
    "crumbs": [
      "6. Model Classes",
      "c. Roberta-based classification Model",
      "Roberta Classifiers"
    ]
  },
  {
    "objectID": "models.roberta.classifiers.html#main-classification-architecture",
    "href": "models.roberta.classifiers.html#main-classification-architecture",
    "title": "Roberta Classifiers",
    "section": "Main classification architecture",
    "text": "Main classification architecture\n\nsource\n\nRobertaBaseForSequenceClassification\n\n RobertaBaseForSequenceClassification (config, is_multilabel=False,\n                                       is_multihead=False,\n                                       head_class_sizes=[],\n                                       head_weights=[], head_class=None,\n                                       **head_class_kwargs)\n\n*Base Roberta Architecture for Sequence Classification task\nBased on: https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py#L1155C35-L1155C35*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nconfig\n\n\nHuggingFace model configuration\n\n\nis_multilabel\nbool\nFalse\nWhether this is a multilabel classification\n\n\nis_multihead\nbool\nFalse\nWhether this is a multihead (multi-level) classification\n\n\nhead_class_sizes\nlist\n[]\nClass size for each head\n\n\nhead_weights\nlist\n[]\nloss weight for each head. This will be multiplied to the loss of each head’s output\n\n\nhead_class\nNoneType\nNone\nThe class object of the head. You can use RobertaClassificationHeadCustom as default\n\n\nhead_class_kwargs\n\n\n\n\n\n\n\nsource\n\n\nRobertaHiddenStateConcatForSequenceClassification\n\n RobertaHiddenStateConcatForSequenceClassification (config,\n                                                    layer2concat=4,\n                                                    is_multilabel=False,\n                                                    is_multihead=False,\n                                                    head_class_sizes=[],\n                                                    head_weights=[],\n                                                    head_class=None,\n                                                    **head_class_kwargs)\n\nRoberta Architecture with Hidden-State-Concatenation for Sequence Classification task\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nconfig\n\n\nHuggingFace model configuration\n\n\nlayer2concat\nint\n4\nnumber of hidden layer to concatenate (counting from top)\n\n\nis_multilabel\nbool\nFalse\nWhether this is a multilabel classification\n\n\nis_multihead\nbool\nFalse\nWhether this is a multihead (multi-level) classification\n\n\nhead_class_sizes\nlist\n[]\nClass size for each head\n\n\nhead_weights\nlist\n[]\nloss weight for each head. This will be multiplied to the loss of each head’s output\n\n\nhead_class\nNoneType\nNone\nThe class object of the head. You can use ConcatHeadSimple or ConcatHeadExtended\n\n\nhead_class_kwargs",
    "crumbs": [
      "6. Model Classes",
      "c. Roberta-based classification Model",
      "Roberta Classifiers"
    ]
  },
  {
    "objectID": "models.roberta.conditional_prob_classifiers.html",
    "href": "models.roberta.conditional_prob_classifiers.html",
    "title": "Conditional Probability Classifiers",
    "section": "",
    "text": "import pandas as pd\n\n\nsource\n\nbuild_standard_condition_mask\n\n build_standard_condition_mask (df_labels, label1, label2)\n\n\n_df_labels=pd.DataFrame({\n    'col_1':[0,0,0,1,1,2,2,2],\n    'col_2':[0,1,2,3,4,5,6,7]\n})\n_df_labels\n\n# 0 -&gt; (0,1,2), 1 -&gt; (3,4), 2-&gt; (5,6,7)\n\n\n\n\n\n\n\n\n\ncol_1\ncol_2\n\n\n\n\n0\n0\n0\n\n\n1\n0\n1\n\n\n2\n0\n2\n\n\n3\n1\n3\n\n\n4\n1\n4\n\n\n5\n2\n5\n\n\n6\n2\n6\n\n\n7\n2\n7\n\n\n\n\n\n\n\n\n\nprint(build_standard_condition_mask(_df_labels,'col_1','col_2'))\n\ntensor([[ True, False, False,  True,  True,  True, False, False, False, False,\n         False],\n        [False,  True, False, False, False, False,  True,  True, False, False,\n         False],\n        [False, False,  True, False, False, False, False, False,  True,  True,\n          True]])\n\n\n\nsource\n\n\nRobertaHSCCProbSequenceClassification\n\n RobertaHSCCProbSequenceClassification (config, size_l1=None,\n                                        size_l2=None, standard_mask=None,\n                                        layer2concat=4, device=None,\n                                        head_class=None,\n                                        **head_class_kwargs)\n\nRoberta Conditional Probability Architecture with Hidden-State-Concatenation for Sequence Classification task\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nconfig\n\n\nHuggingFace model configuration\n\n\nsize_l1\nNoneType\nNone\nNumber of classes for head 1\n\n\nsize_l2\nNoneType\nNone\nNumber of classes for head 2\n\n\nstandard_mask\nNoneType\nNone\nMask for conditional probability\n\n\nlayer2concat\nint\n4\nnumber of hidden layer to concatenate (counting from top)\n\n\ndevice\nNoneType\nNone\nCPU or GPU\n\n\nhead_class\nNoneType\nNone\nThe class object of the head. You can use RobertaClassificationHeadCustom as default\n\n\nhead_class_kwargs",
    "crumbs": [
      "6. Model Classes",
      "c. Roberta-based classification Model",
      "Conditional Probability Classifiers"
    ]
  },
  {
    "objectID": "deprecated/model_main_envibert_dhc-copy1.html",
    "href": "deprecated/model_main_envibert_dhc-copy1.html",
    "title": "Model Controller Tutorial: EnviBert model with Deep Hierarchical Classification",
    "section": "",
    "text": "We will walk through other cases of classification: multi-head and multi-label. Since we will showcase the capabiilty of this label in these cases, there won’t be as detailed as this tutorial"
  },
  {
    "objectID": "deprecated/model_main_envibert_dhc-copy1.html#load-data",
    "href": "deprecated/model_main_envibert_dhc-copy1.html#load-data",
    "title": "Model Controller Tutorial: EnviBert model with Deep Hierarchical Classification",
    "section": "Load data",
    "text": "Load data\n\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom that_nlp_library.text_main import *\n\n\nfrom underthesea import text_normalize\nfrom functools import partial\nfrom pathlib import Path\nfrom importlib.machinery import SourceFileLoader\nfrom transformers import DataCollatorWithPadding\nimport pandas as pd\nimport numpy as np\n\nDefine some necessary text augmentations and text transformations\n\nFor Text Transformation\n\n\ntxt_tfms=[text_normalize]\n\n\nFor Text Augmentation\n\n\nover_nonown_tfm = partial(sampling_with_condition,query='Source==\"non owned\"',frac=0.5,seed=42,apply_to_all=False)\nover_nonown_tfm.__name__ = 'Oversampling Non Owned'\n\nover_own_tfm = partial(sampling_with_condition,query='Source==\"owned\"',frac=2,seed=42,apply_to_all=False)\nover_own_tfm.__name__ = 'Oversampling Owned'\n\nover_hc_tfm = partial(sampling_with_condition,query='Source==\"hc search\"',frac=2.5,seed=42,apply_to_all=False)\nover_hc_tfm.__name__ = 'Oversampling HC search'\n\nremove_accent_tfm = partial(remove_vnmese_accent,frac=1,seed=42,apply_to_all=True)\nremove_accent_tfm.__name__ = 'Add No-Accent Text'\n\naug_tfms = [over_nonown_tfm,over_own_tfm,over_hc_tfm,remove_accent_tfm]\n\nLet’s load and preprocess our data\n\nDATA_PATH = Path('secret_data')\n\n\ndf = TextDataMain.from_csv(DATA_PATH/'buyer_listening_with_all_raw_data_w2223.csv',return_df=True)\n\n----- Input Validation Precheck -----\nDataFrame contains missing values!\n-----&gt; List of columns and the number of missing values for each\nis_valid    65804\ndtype: int64\nDataFrame contains duplicated values!\n-----&gt; Number of duplications: 7 rows\n\n\n\ndf.head(3)\n\n\n\n\n\n\n\n\n\nWeek\nGroup\nSource\nContent\nL1\nL2\nL3\nL4\nis_valid\niteration\n\n\n\n\n0\n1.0\nGoogle Play\nGoogle Play\nTại sao cứ hiện thông báo\nServices\nShopee communication channels\nAnnoying pop-up ads\nNon-tech\nNaN\n1\n\n\n1\n1.0\nGoogle Play\nGoogle Play\nMlem\nOthers\nCannot defined\n-\n-\nNaN\n1\n\n\n2\n1.0\nGoogle Play\nGoogle Play\n1 số sản phẩm trong giỏ hàng vừa đc cập nhật t...\nFeature\nCart & Order\nCart issues/suggestions\nTech\nNaN\n1\n\n\n\n\n\n\n\n\nQuick preprocess of data and train/validation split. Due to custom logic, we will sample our data here instead of using the train_ratio from the to_datasetdict function\n\ndf_rare = df[df.L2.isin(['Chatbot', 'Commercial Others'])].copy()\n\ndf_final = pd.concat([df.query('iteration==1').sample(500,random_state=42),\n                      df.query('iteration&gt;=7 & iteration&lt;13').sample(1200,random_state=42),\n                      df_rare,\n                      df.query('iteration&gt;=13'),\n                     ],axis=0).reset_index(drop=True)\n\nval_idxs = df_final[df_final.iteration&gt;=13].index.values # from week 9\n\n\ntdm = TextDataMain(df_final,\n                    main_content='Content',\n                    metadatas='Source',\n                    label_names=['L1','L2'],\n                    val_ratio=val_idxs,\n                    split_cols='L2',\n                    content_tfms = txt_tfms,\n                    aug_tfms = aug_tfms,\n                    process_metadatas=True,\n                    seed=42,\n                    shuffle_trn=True)\n\n----- Input Validation Precheck -----\nDataFrame contains missing values!\n-----&gt; List of columns and the number of missing values for each\nis_valid    498\ndtype: int64\n\n\nDefine our tokenizer for EnviBert\n\ncache_dir=Path('./envibert_tokenizer')\ntokenizer = SourceFileLoader(\"envibert.tokenizer\", \n                             str(cache_dir/'envibert_tokenizer.py')).load_module().RobertaTokenizer(cache_dir)\n\nCreate our DatasetDict from TextDataMain (as our ModelController class can also work with DatasetDict)\n\nmain_ddict= tdm.to_datasetdict(tokenizer,\n                               max_length=512,\n                               )\n\n-------------------- Start Main Text Processing --------------------\n----- Metadata Simple Processing & Concatenating to Main Content -----\n----- Label Encoding -----\n-------------------- Text Transformation --------------------\n----- text_normalize -----\n-------------------- Train Test Split --------------------\nPrevious Validation Percentage: 74.101%\n- Before leak check\nSize: 4927\n- After leak check\nSize: 4885\n- Number of rows leaked: 42, or 0.85% of the original validation (or test) data\nCurrent Validation Percentage: 73.47%\n-------------------- Text Augmentation --------------------\nTrain data size before augmentation: 1764\n----- Oversampling Non Owned -----\nTrain data size after THIS augmentation: 2229\n----- Oversampling Owned -----\nTrain data size after THIS augmentation: 2789\n----- Oversampling HC search -----\nTrain data size after THIS augmentation: 2904\n----- Add No-Accent Text -----\nTrain data size after THIS augmentation: 5808\nTrain data size after ALL augmentation: 5808\n-------------------- Map Tokenize Function --------------------\n\n\n100%|█████████████████████████████████████| 6649/6649 [00:01&lt;00:00, 3560.77it/s]\n100%|████████████████████████████████████| 2904/2904 [00:00&lt;00:00, 10469.09it/s]\n\n\n\n\n\n\n\n\n\nmain_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 5808\n    })\n    validation: Dataset({\n        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 4885\n    })\n})\n\n\n\nmain_ddict['validation']['label'][:5]\n\n[[5, 12], [5, 12], [5, 12], [2, 23], [3, 5]]"
  },
  {
    "objectID": "deprecated/model_main_envibert_dhc-copy1.html#build-dhc-conditional-mask",
    "href": "deprecated/model_main_envibert_dhc-copy1.html#build-dhc-conditional-mask",
    "title": "Model Controller Tutorial: EnviBert model with Deep Hierarchical Classification",
    "section": "Build DHC Conditional Mask",
    "text": "Build DHC Conditional Mask\n\ndf_labels = tdm.df[tdm.label_names]\n\n\ndf_labels.head()\n\n\n\n\n\n\n\n\n\nL1\nL2\n\n\n\n\n0\n1\n59\n\n\n1\n5\n12\n\n\n2\n1\n59\n\n\n3\n1\n43\n\n\n4\n5\n12\n\n\n\n\n\n\n\n\n\ndhc_mask = build_DHC_conditional_mask(df_labels,*tdm.label_names)\n\n\ndhc_mask.shape\n\ntorch.Size([10, 66])\n\n\nExplain the first row of the mask (for level 1 label “Buyer Complained Seller”)\n\ndhc_mask[0]\n\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\nSlicing the first portion for level 2, show string for True mask\n\nfor i in torch.where(dhc_mask[0]==1)[0]:\n    print(tdm.label_lists[1][i])\n\nCustomer service (didn't respond/impolite)\nIllegal/counterfeit products\nProduct description\nProduct quality\nSellers cancelled order without any advanced notice/reason\nSellers cheated Buyers (Sellers tried to reach me outside of Shopee App)\nSellers packed fake orders"
  },
  {
    "objectID": "deprecated/model_main_envibert_dhc-copy1.html#train-envibert-with-hidden-layer-concatenation-using-tdm",
    "href": "deprecated/model_main_envibert_dhc-copy1.html#train-envibert-with-hidden-layer-concatenation-using-tdm",
    "title": "Model Controller Tutorial: EnviBert model with Deep Hierarchical Classification",
    "section": "Train EnviBert (with hidden layer concatenation), using TDM",
    "text": "Train EnviBert (with hidden layer concatenation), using TDM\n\nmodel_name='nguyenvulebinh/envibert'\nenvibert_body = RobertaModel.from_pretrained(model_name)\n\nSome weights of the model checkpoint at nguyenvulebinh/envibert were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\n_model_kwargs={\n    'dhc_mask':dhc_mask,\n    'classifier_dropout':0.1,\n    'last_hidden_size':768,  \n    'linear_l1_size':389,\n    'linear_l2_size':417,\n    'lloss_weight':1.0,\n    'dloss_weight':0.8,\n    'layer2concat':4,\n}\n\nmodel = model_init_classification(model_class = RobertaHSCDHCSequenceClassification,\n                                  cpoint_path = model_name, \n                                  output_hidden_states=True, # since we are not using 'hidden layer contatenation' technique\n                                  seed=42,\n                                  body_model=envibert_body,\n                                  model_kwargs = _model_kwargs)\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score]\ncontroller = ModelController(model,tdm,metric_funcs)\n\nLoading body weights. This assumes the body is the very first first-layer block of your custom architecture\n\n\nAnd we can start training our model\n\nlr = 9e-5\nbs=4\nwd=0.01\nepochs= 4\n\ncontroller.fit(epochs,lr,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics_separate_singleheads,\n              )\n\n/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\n\n\n\n    \n      \n      \n      [2904/2904 03:21, Epoch 4/4]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1 Score L1\nAccuracy Score L1\nF1 Score L2\nAccuracy Score L2\n\n\n\n\n1\nNo log\n9.726516\n0.240784\n0.536540\n0.032214\n0.232958\n\n\n2\n7.853000\n9.363424\n0.409587\n0.610645\n0.067087\n0.328147\n\n\n3\n7.853000\n9.608892\n0.463691\n0.668987\n0.092133\n0.390993\n\n\n4\n6.093800\n9.744572\n0.473580\n0.663664\n0.099278\n0.415353\n\n\n\n\n\n\n\n\ncontroller.trainer.model.save_pretrained('./sample_weights/my_model')"
  },
  {
    "objectID": "deprecated/model_main_envibert_dhc-copy1.html#predict-using-trained-model-using-tdm",
    "href": "deprecated/model_main_envibert_dhc-copy1.html#predict-using-trained-model-using-tdm",
    "title": "Model Controller Tutorial: EnviBert model with Deep Hierarchical Classification",
    "section": "Predict using trained model, using TDM",
    "text": "Predict using trained model, using TDM\n\nLoad trained model\n\n_model_kwargs\n\n{'dhc_mask': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n          0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n          0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n         [0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.]]),\n 'classifier_dropout': 0.1,\n 'last_hidden_size': 768,\n 'linear_l1_size': 389,\n 'linear_l2_size': 417,\n 'lloss_weight': 1.0,\n 'dloss_weight': 0.8,\n 'layer2concat': 4}\n\n\n\ntrained_model = model_init_classification(model_class = RobertaHSCDHCSequenceClassification,\n                                          cpoint_path = Path('./sample_weights/my_model'), \n                                          output_hidden_states=True,\n                                          seed=42,\n                                          model_kwargs = _model_kwargs)\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score]\ncontroller = ModelController(trained_model,tdm,metric_funcs)\n\nSome weights of the model checkpoint at sample_weights/my_model were not used when initializing RobertaHSCDHCSequenceClassification: ['body_model.pooler.dense.weight', 'body_model.pooler.dense.bias']\n- This IS expected if you are initializing RobertaHSCDHCSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaHSCDHCSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\n\nPredict Train/Validation set\nMake prediction on all validation set\n\ndf_val = controller.predict_ddict(ds_type='validation',is_dhc=True,batch_size=8)\n\n-------------------- Start making predictions --------------------\n\n\n\n\n\n\ndf_val.head()\n\n\n\n\n\n\n\n\n\ntext\nlabel\nSource\npred_L1\npred_prob_L1\npred_L2\npred_prob_L2\n\n\n\n\n0\ngoogle play - lam phien\n[5, 12]\ngoogle play\nFeature\n0.880263\nApp performance\n0.696242\n\n\n1\ngoogle play - .. t . À mà họ nữ ưu m\n[5, 12]\ngoogle play\nFeature\n0.781973\nApp performance\n0.684697\n\n\n2\ngoogle play - Cc lùa dao\n[5, 12]\ngoogle play\nOthers\n0.986321\nCannot defined\n0.979061\n\n\n3\ngoogle play - Mặt hàng sp mình đều nhỡ với Gia...\n[2, 23]\ngoogle play\nDelivery\n0.910066\nDelivery time\n0.616661\n\n\n4\ngoogle play - Chưa tối ưu tốt cho Android Oppo...\n[3, 5]\ngoogle play\nOthers\n0.588300\nCannot defined\n0.617693\n\n\n\n\n\n\n\n\nTo convert the label index to string, we can use the label_lists attribute of tdm\n\nimport pandas as pd\n\n\ndf_val[['label_L1','label_L2']] = pd.DataFrame(df_val.label.tolist(), index= df_val.index)\n\n\ndf_val.head()\n\n\n\n\n\n\n\n\n\ntext\nlabel\nSource\npred_L1\npred_prob_L1\npred_L2\npred_prob_L2\nlabel_L1\nlabel_L2\n\n\n\n\n0\ngoogle play - lam phien\n[5, 12]\ngoogle play\nFeature\n0.880263\nApp performance\n0.696242\n5\n12\n\n\n1\ngoogle play - .. t . À mà họ nữ ưu m\n[5, 12]\ngoogle play\nFeature\n0.781973\nApp performance\n0.684697\n5\n12\n\n\n2\ngoogle play - Cc lùa dao\n[5, 12]\ngoogle play\nOthers\n0.986321\nCannot defined\n0.979061\n5\n12\n\n\n3\ngoogle play - Mặt hàng sp mình đều nhỡ với Gia...\n[2, 23]\ngoogle play\nDelivery\n0.910066\nDelivery time\n0.616661\n2\n23\n\n\n4\ngoogle play - Chưa tối ưu tốt cho Android Oppo...\n[3, 5]\ngoogle play\nOthers\n0.588300\nCannot defined\n0.617693\n3\n5\n\n\n\n\n\n\n\n\n\ndf_val['label_L1']= df_val['label_L1'].apply(lambda x: tdm.label_lists[0][x]).values\ndf_val['label_L2']= df_val['label_L2'].apply(lambda x: tdm.label_lists[1][x]).values\n\n\nf1_score(df_val.label_L1,df_val.pred_L1,average='macro'),f1_score(df_val.label_L2,df_val.pred_L2,average='macro')\n\n(0.47357980728239585, 0.09925006584798883)\n\n\n\n\nPredict Test set\nWe will go through details on how to make a prediction on a completely new and raw dataset using our trained model. For now, let’s reuse the sample csv and pretend it’s our test set\n\ndf_test = TextDataMain.from_csv(Path('sample_data')/'sample_large.csv',return_df=True)\n\n----- Input Validation Precheck -----\nDataFrame contains duplicated values!\n-----&gt; Number of duplications: 16 rows\n\n\nWe added the required columns as we defined in training process, and remove all the labels\n\ndf_test = df_test.drop(['L1','L2'],axis=1)\n\n\ndf_test.head()\n\n\n\n\n\n\n\n\n\nSource\nContent\n\n\n\n\n0\nGoogle Play\nApp ncc lúc nào cx lag đơ, phần tìm kiếm thì v...\n\n\n1\nNon Owned\n..❗️ GÓC THANH LÝ Tính ra rẻ hơn cả mua #Shope...\n\n\n2\nGoogle Play\nMắc gì người ta đặt hàng toàn lỗi 😃????\n\n\n3\nOwned\n#GhienShopeePayawardT8 Khi bạn chơi shopee quá...\n\n\n4\nGoogle Play\nRất bức xúc khi dùng . mã giảm giá người dùng ...\n\n\n\n\n\n\n\n\nWe will create a DatasetDict for this test dataframe\n\ntest_ddict = tdm.get_test_datasetdict_from_df(df_test)\n\n-------------------- Getting Test Set --------------------\n----- Input Validation Precheck -----\nDataFrame contains duplicated values!\n-----&gt; Number of duplications: 19 rows\n-------------------- Start Test Set Transformation --------------------\n----- Metadata Simple Processing & Concatenating to Main Content -----\n-------------------- Text Transformation --------------------\n----- text_normalize -----\n-------------------- Test Leak Checking --------------------\n- Before leak check\nSize: 2269\n- After leak check\nSize: 2080\n- Number of rows leaked: 189, or 8.33% of the original validation (or test) data\n-------------------- Construct DatasetDict --------------------\n\n\n100%|█████████████████████████████████████| 2269/2269 [00:00&lt;00:00, 3940.94it/s]\n\n\n\n\n\n\ntest_ddict\n\nDatasetDict({\n    test: Dataset({\n        features: ['text', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 2269\n    })\n})\n\n\nOur test data has been processed + transformed (but not augmented) the same way as the validation set. Now we can start making the prediction\n\ncontroller = ModelController(model,tdm)\ndf_result = controller.predict_ddict(ddict=test_ddict,ds_type='test',is_dhc=True)\n\n-------------------- Start making predictions --------------------\n\n\n\n\n\n\ndf_result.head()\n\n\n\n\n\n\n\n\n\ntext\nSource\npred_L1\npred_prob_L1\npred_L2\npred_prob_L2\n\n\n\n\n0\ngoogle play - App ncc lúc nào cx lag đơ , phần...\ngoogle play\nFeature\n0.903318\nApp performance\n0.674870\n\n\n1\nnon owned - .. ❗ ️ GÓC THANH LÝ Tính ra rẻ hơn...\nnon owned\nOthers\n0.998537\nCannot defined\n0.995626\n\n\n2\ngoogle play - Mắc gì người ta đặt hàng toàn lỗ...\ngoogle play\nFeature\n0.715968\nApp performance\n0.529486\n\n\n3\nowned - # GhienShopeePayawardT8 Khi bạn chơi s...\nowned\nCommercial\n0.989516\nShopee Programs\n0.993086\n\n\n4\ngoogle play - Rất bức xúc khi dùng . mã giảm g...\ngoogle play\nFeature\n0.840763\nApply Voucher\n0.310494\n\n\n\n\n\n\n\n\nWe can even predict top k results\n\ndf_result = controller.predict_ddict(ddict=test_ddict,ds_type='test',topk=3,is_dhc=True)\ndf_result.head()\n\n-------------------- Start making predictions --------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\ntext\nSource\npred_L1\npred_prob_L1\npred_L2\npred_prob_L2\npred_L1_top1\npred_L1_top2\npred_L1_top3\npred_prob_L1_top1\npred_prob_L1_top2\npred_prob_L1_top3\npred_L2_top1\npred_L2_top2\npred_L2_top3\npred_prob_L2_top1\npred_prob_L2_top2\npred_prob_L2_top3\n\n\n\n\n0\ngoogle play - App ncc lúc nào cx lag đơ , phần...\ngoogle play\n[3, 9, 4]\n[0.90331787, 0.070163645, 0.013683925]\n[5, 6, 64]\n[0.6748701, 0.12654907, 0.05017495]\nFeature\nShopee account\nOrder/Item\n0.903318\n0.070164\n0.013684\nApp performance\nApply Voucher\nSign up/Log in\n0.674870\n0.126549\n0.050175\n\n\n1\nnon owned - .. ❗ ️ GÓC THANH LÝ Tính ra rẻ hơn...\nnon owned\n[5, 3, 2]\n[0.9985366, 0.0007172561, 0.00040264206]\n[12, 9, 5]\n[0.99562645, 0.0017475911, 0.0010105681]\nOthers\nFeature\nDelivery\n0.998537\n0.000717\n0.000403\nCannot defined\nBranding\nApp performance\n0.995626\n0.001748\n0.001011\n\n\n2\ngoogle play - Mắc gì người ta đặt hàng toàn lỗ...\ngoogle play\n[3, 5, 4]\n[0.7159677, 0.2576615, 0.010411925]\n[5, 12, 6]\n[0.52948564, 0.31033903, 0.07395215]\nFeature\nOthers\nOrder/Item\n0.715968\n0.257661\n0.010412\nApp performance\nCannot defined\nApply Voucher\n0.529486\n0.310339\n0.073952\n\n\n3\nowned - # GhienShopeePayawardT8 Khi bạn chơi s...\nowned\n[1, 6, 0]\n[0.9895163, 0.009675543, 0.00036532234]\n[59, 63, 29]\n[0.993086, 0.003024094, 0.0012311569]\nCommercial\nPayment\nBuyer complained seller\n0.989516\n0.009676\n0.000365\nShopee Programs\nShopeePay\nGames/Minigames\n0.993086\n0.003024\n0.001231\n\n\n4\ngoogle play - Rất bức xúc khi dùng . mã giảm g...\ngoogle play\n[3, 9, 4]\n[0.8407633, 0.102363825, 0.034727756]\n[6, 5, 64]\n[0.3104941, 0.3058795, 0.09680278]\nFeature\nShopee account\nOrder/Item\n0.840763\n0.102364\n0.034728\nApply Voucher\nApp performance\nSign up/Log in\n0.310494\n0.305880\n0.096803\n\n\n\n\n\n\n\n\nIf we just want to make a prediction on a small amount of data (single sentence, or a few sentences), we can use ModelController.predict_raw_text\n\n# Since we have some metadatas, we need to define a dictionary (to imitate a DatasetDict)\nraw_content={\n    'Source': 'Google play',\n    'Content':'Tôi không thích Shopee.Tại vì dùng app rất chậm,lag banh nhà lầu, thậm chí log in còn không đc'\n}\n\nIf we don’t use metadata, we can use something like this:\nraw_content='Tôi không thích Shopee.Tại vì dùng app rất chậm,lag banh nhà lầu, thậm chí log in còn không đc'\n\ndf_result = controller.predict_raw_text(raw_content,topk=1,is_dhc=True)\ndf_result\n\n100%|███████████████████████████████████████████| 1/1 [00:00&lt;00:00, 4777.11it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntext\nSource\npred_L1\npred_prob_L1\npred_L2\npred_prob_L2\n\n\n\n\n0\ngoogle play - Tôi không thích Shopee . Tại vì ...\ngoogle play\nFeature\n0.975475\nApp performance\n0.919566"
  },
  {
    "objectID": "deprecated/text_main_benchmark[deprecated].html",
    "href": "deprecated/text_main_benchmark[deprecated].html",
    "title": "Text Processing Benchmark",
    "section": "",
    "text": "!conda list | grep 'datasets\\|transformers\\|torch'\n\ndatasets                  2.14.4                   pypi_0    pypi\npytorch-ignite            0.4.11                   pypi_0    pypi\npytorch-lightning         2.0.1.post0              pypi_0    pypi\ntorch                     2.0.1+cu118              pypi_0    pypi\ntorchaudio                2.0.2+cu118              pypi_0    pypi\ntorchmetrics              1.1.1                    pypi_0    pypi\ntorchvision               0.15.2+cu118             pypi_0    pypi\ntransformers              4.31.0                   pypi_0    pypi\n# !conda list | grep 'datasets\\|transformers'\n# datasets                  2.11.0                   pypi_0    pypi\n# transformers              4.28.1                   pypi_0    pypi\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom that_nlp_library.text_main import *\nfrom importlib.machinery import SourceFileLoader\nfrom datasets import load_dataset,enable_caching,disable_caching\nfrom transformers import RobertaTokenizer\nimport os\nimport time\nfrom underthesea import text_normalize\nimport nlpaug.augmenter.char as nac\nfrom functools import partial\nimport random\nfrom memory_profiler import memory_usage\ndisable_caching() # disable huggingface caching to get a fair benchmark\ndef benchmarking(tdc,bs,tokenizer,n=10,shuffle_trn=True):\n    time1 = time.time()\n    tdc.process_and_tokenize(tokenizer,max_length=512,shuffle_trn=shuffle_trn)\n    time2 = time.time() \n    print(f'Time it takes to process + tokenize training texts: {(time2-time1):.3f} s')\n    for i,v in enumerate(tdc.main_ddict['train']):\n        if n is not None and i==bs*n: break\n    time3 = time.time()\n    if n is not None:\n        print(f'Time it takes to go through {n*bs} items: {(time3-time2):.3f} s')\n    else:\n        print(f'Time it takes to go through all items: {(time3-time2):.3f} s')\n\n#     print(f'Total time: {(time3-time1):.3f} s')\ndef benchmarking_and_memory_usage(tdc,bs,tokenizer,n=10,shuffle_trn=True):\n    mem_usage = memory_usage((benchmarking,[tdc,bs,tokenizer,n,shuffle_trn]))\n    print(f'Maximum memory usage: {max(mem_usage):.3f} MiB')\ndef nlp_aug_stochastic(x,aug=None,p=0.5):\n    results = aug.augment(x)\n    if not isinstance(x,list): return results[0] if random.random()&lt;p else x\n    return [a if random.random()&lt;p else b for a,b in zip(results,x)]\n\naug = nac.KeyboardAug(aug_char_max=3,aug_char_p=0.1,aug_word_p=0.07)\nnearby_aug_func = partial(nlp_aug_stochastic,aug=aug,p=0.5)"
  },
  {
    "objectID": "deprecated/text_main_benchmark[deprecated].html#benchmark-on-medium-size-dataset-117k-rows",
    "href": "deprecated/text_main_benchmark[deprecated].html#benchmark-on-medium-size-dataset-117k-rows",
    "title": "Text Processing Benchmark",
    "section": "Benchmark on medium-size dataset (~117k rows)",
    "text": "Benchmark on medium-size dataset (~117k rows)\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\nlen(dset)\n\nFound cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n\n\n117430\n\n\n\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\n\nbs=128\n\n\nWithout iterable dataset\nWith filter\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         val_ratio=None,\n                         batch_size=bs,\n                         seed=42,\n                         convert_training_to_iterable=False,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,bs,tokenizer)\n\nFound cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime it takes to process + tokenize training texts: 14.940 s\nTime it takes to go through 1280 items: 0.155 s\nMaximum memory usage: 825.723 MiB\n\n\nWith filter + metadatas concatenation\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         val_ratio=None,\n                         batch_size=bs,\n                         seed=42,\n                         convert_training_to_iterable=False,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,bs,tokenizer)\n\nFound cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime it takes to process + tokenize training texts: 15.741 s\nTime it takes to go through 1280 items: 0.168 s\nMaximum memory usage: 857.930 MiB\n\n\nWith filter + metadatas concatenation + content transformation + content augmentation\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         val_ratio=None,\n                         batch_size=bs,\n                         seed=42,\n                         convert_training_to_iterable=False,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,bs,tokenizer)\n\nFound cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime it takes to process + tokenize training texts: 35.980 s\nTime it takes to go through 1280 items: 0.176 s\nMaximum memory usage: 893.555 MiB\n\n\nWith filter + metadatas concatenation + content transformation + content augmentation + no shuffling\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         val_ratio=None,\n                         batch_size=bs,\n                         seed=42,\n                         convert_training_to_iterable=False,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,bs,tokenizer,shuffle_trn=False)\n\nFound cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime it takes to process + tokenize training texts: 35.534 s\nTime it takes to go through 1280 items: 0.180 s\nMaximum memory usage: 892.668 MiB\n\n\nWith filter + metadatas concatenation + content transformation + content augmentation + higher batch size\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         val_ratio=None,\n                         batch_size=512,\n                         seed=42,\n                         convert_training_to_iterable=False,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,512,tokenizer)\n\nFound cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime it takes to process + tokenize training texts: 35.427 s\nTime it takes to go through 5120 items: 0.746 s\nMaximum memory usage: 794.441 MiB\n\n\n\n\nWith iterable dataset\nWith filter\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         val_ratio=None,\n                         batch_size=bs,\n                         seed=42,\n                         convert_training_to_iterable=True,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,bs,tokenizer)\n\nFound cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n\n\n\n\n\n\n\n\n\n\n\nTime it takes to process + tokenize training texts: 2.888 s\nTime it takes to go through 1280 items: 0.571 s\nMaximum memory usage: 752.379 MiB\n\n\nWith filter + metadatas concatenation\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         val_ratio=None,\n                         batch_size=bs,\n                         seed=42,\n                         convert_training_to_iterable=True,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,bs,tokenizer)\n\nFound cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime it takes to process + tokenize training texts: 2.615 s\nTime it takes to go through 1280 items: 0.547 s\nMaximum memory usage: 804.832 MiB\n\n\nWith filter + metadatas concatenation + content transformation + content augmentation\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         val_ratio=None,\n                         batch_size=bs,\n                         seed=42,\n                         convert_training_to_iterable=True,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,bs,tokenizer)\n\nFound cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime it takes to process + tokenize training texts: 22.078 s\nTime it takes to go through 1280 items: 0.606 s\nMaximum memory usage: 857.551 MiB\n\n\nWith filter + metadatas concatenation + content transformation + content augmentation + no shuffling\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         val_ratio=None,\n                         batch_size=bs,\n                         seed=42,\n                         convert_training_to_iterable=True,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,bs,tokenizer,shuffle_trn=False)\n\nFound cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime it takes to process + tokenize training texts: 22.369 s\nTime it takes to go through 1280 items: 0.543 s\nMaximum memory usage: 857.930 MiB\n\n\nWith filter + metadatas concatenation + content transformation + content augmentation + higher batch size\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         val_ratio=None,\n                         batch_size=512,\n                         seed=42,\n                         convert_training_to_iterable=True,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,512,tokenizer)\n\nFound cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime it takes to process + tokenize training texts: 21.482 s\nTime it takes to go through 5120 items: 2.150 s\nMaximum memory usage: 752.199 MiB\n\n\n\n\nWith streaming (v1)\nWith filter\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=True)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n                         val_ratio=None,\n                         batch_size=bs,\n                         seed=42,\n                         convert_training_to_iterable=True,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,bs,tokenizer)\n\nTime it takes to process + tokenize training texts: 0.002 s\nTime it takes to go through 1280 items: 1.327 s\nMaximum memory usage: 686.387 MiB\n\n\nWith filter + metadatas concatenation\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=True)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n                         metadatas=['Title','Division Name'],\n                         val_ratio=None,\n                         batch_size=bs,\n                         seed=42,\n                         convert_training_to_iterable=True,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,bs,tokenizer)\n\nTime it takes to process + tokenize training texts: 0.002 s\nTime it takes to go through 1280 items: 1.470 s\nMaximum memory usage: 803.281 MiB\n\n\nWith filter + metadatas concatenation + content transformation + content augmentation\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=True)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         val_ratio=None,\n                         batch_size=bs,\n                         seed=42,\n                         convert_training_to_iterable=True,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,bs,tokenizer)\n\nTime it takes to process + tokenize training texts: 0.082 s\nTime it takes to go through 1280 items: 95.631 s\nMaximum memory usage: 6908.953 MiB\n\n\nWith filter + metadatas concatenation + content transformation + content augmentation + no shuffling\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=True)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         val_ratio=None,\n                         batch_size=bs,\n                         seed=42,\n                         convert_training_to_iterable=True,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,bs,tokenizer,shuffle_trn=False)\n\nTime it takes to process + tokenize training texts: 0.078 s\nTime it takes to go through 1280 items: 11.870 s\nMaximum memory usage: 6892.258 MiB\n\n\nWith filter + metadatas concatenation + content transformation + content augmentation + higher batch size\n\n# dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n#                     split='train',\n#                     streaming=True)\n\n# tdc = TextDataController(dset,\n#                          main_text='Review Text',\n#                          label_names='Department Name',\n#                          filter_dict={'Review Text': lambda x: x is not None,\n#                                       'Department Name': lambda x: x is not None,\n#                                      },\n#                          class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n#                          metadatas=['Title','Division Name'],\n#                          content_transformations=[text_normalize,str.lower],\n#                          content_augmentations= [nearby_aug_func,str.lower], \n#                          val_ratio=None,\n#                          batch_size=512,\n#                          seed=42,\n#                          convert_training_to_iterable=True,\n#                          verbose=False\n#                         )\n# benchmarking_and_memory_usage(tdc,512,tokenizer,shuffle_trn=False)\n\n\n\nWith streaming (v2)\n\ndef benchmarking(tdc,bs,tokenizer,n=10):\n    time1 = time.time()\n    tdc.process_and_tokenize(tokenizer,max_length=512)\n    time2 = time.time() \n    print(f'Time it takes to process + tokenize training texts: {(time2-time1):.3f} s')\n    for i,v in enumerate(tdc.main_ddict['train']):\n        if n is not None and i==bs*n: break\n    time3 = time.time()\n    if n is not None:\n        print(f'Time it takes to go through {n*bs} items: {(time3-time2):.3f} s')\n    else:\n        print(f'Time it takes to go through all items: {(time3-time2):.3f} s')\n\ndef benchmarking_and_memory_usage(tdc,bs,tokenizer,n=10):\n    mem_usage = memory_usage((benchmarking,[tdc,bs,tokenizer,n]))\n    print(f'Maximum memory usage: {max(mem_usage):.3f} MiB')\n\nWith filter\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=True)\n\ntdc = TextDataControllerStreaming(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n                         batch_size=bs,\n                         seed=42,\n                        )\nbenchmarking_and_memory_usage(tdc,bs,tokenizer)\n\n\nbenchmarking_and_memory_usage(tdc,bs,tokenizer)\n\nTime it takes to process + tokenize training texts: 0.808 s\nTime it takes to go through 1280 items: 0.639 s\nMaximum memory usage: 672.266 MiB\n\n\nWith filter + metadatas concatenation\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=True)\n\ntdc = TextDataControllerStreaming(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n                         metadatas=['Title','Division Name'],\n                         batch_size=bs,\n                         seed=42,\n                        )\nbenchmarking_and_memory_usage(tdc,bs,tokenizer)\n\nTime it takes to process + tokenize training texts: 0.818 s\nTime it takes to go through 1280 items: 0.568 s\nMaximum memory usage: 679.590 MiB\n\n\nWith filter + metadatas concatenation + content transformation + content augmentation\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=True)\n\ntdc = TextDataControllerStreaming(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         batch_size=bs,\n                         seed=42,\n                        )\nbenchmarking_and_memory_usage(tdc,bs,tokenizer)\n\nTime it takes to process + tokenize training texts: 0.826 s\nTime it takes to go through 1280 items: 1.599 s\nMaximum memory usage: 679.723 MiB\n\n\nWith filter + metadatas concatenation + content transformation + content augmentation + higher batch size\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=True)\n\ntdc = TextDataControllerStreaming(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         class_names_predefined=['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'],\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         batch_size=512,\n                         seed=42,\n                        )\nbenchmarking_and_memory_usage(tdc,512,tokenizer)\n\nTime it takes to process + tokenize training texts: 0.835 s\nTime it takes to go through 5120 items: 5.734 s\nMaximum memory usage: 677.559 MiB\n\n\n\n\nTest the effect of batch size and num_proc\nText processing + tokenization are the most time-consuming tasks, thus we will check how different batch size and num proc will affect these tasks’ running time\n\nbs=16\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         val_ratio=None,\n                         batch_size=bs,\n                         seed=42,\n                         convert_training_to_iterable=False,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,bs,tokenizer,n=None,shuffle_trn=False)\n\nFound cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime it takes to process + tokenize training texts: 71.757 s\nTime it takes to go through all items: 15.128 s\nMaximum memory usage: 1041.410 MiB\n\n\n\nbs=128\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         val_ratio=None,\n                         batch_size=bs,\n                         seed=42,\n                         convert_training_to_iterable=False,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,bs,tokenizer,n=None,shuffle_trn=False)\n\nFound cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime it takes to process + tokenize training texts: 60.165 s\nTime it takes to go through all items): 15.950 s\nMaximum memory usage: 831.129 MiB\n\n\n\nbs=128*10\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         val_ratio=None,\n                         batch_size=bs,\n                         seed=42,\n                         convert_training_to_iterable=False,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,bs,tokenizer,n=None,shuffle_trn=False)\n\nFound cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime it takes to process + tokenize training texts: 58.748 s\nTime it takes to go through all items: 16.631 s\nMaximum memory usage: 845.074 MiB\n\n\n\nbs=128*10\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         val_ratio=None,\n                         batch_size=bs,\n                         seed=42,\n                         convert_training_to_iterable=False,\n                         num_proc=16,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,bs,tokenizer,n=None,shuffle_trn=False)\n\nFound cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime it takes to process + tokenize training texts: 47.417 s\nTime it takes to go through all items: 16.684 s\nMaximum memory usage: 1009.738 MiB\n\n\nConclusion: increase BOTH batch size and num_proc can help decrease the processing + tokenization time, but the relationship between batch size, num_proc and running time are not linear"
  },
  {
    "objectID": "deprecated/text_main_benchmark[deprecated].html#improving-processing-time-with-caching",
    "href": "deprecated/text_main_benchmark[deprecated].html#improving-processing-time-with-caching",
    "title": "Text Processing Benchmark",
    "section": "Improving processing time with caching",
    "text": "Improving processing time with caching\nThe worst processing time is recorded with an non-iterable training set, with the following preprocessing: 2-column filtering, 2-column metadatas, 2 content transformations, 2 content augmentation; the total preprocessing time is ~62s for 117k dataset. However, this results in the best data iteration time: 0.183s for going through 1280 items.\nWith caching, we can significantly reduce the preprocessing time. That means, you only need to do all preprocessings once; all subsequent call will take advatages of this cached result.\n\nenable_caching()\n\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv' for i in range(5)],\n                    split='train',\n                    streaming=False)\n\ntdc = TextDataController(dset,\n                         main_text='Review Text',\n                         label_names='Department Name',\n                         filter_dict={'Review Text': lambda x: x is not None,\n                                      'Department Name': lambda x: x is not None,\n                                     },\n                         metadatas=['Title','Division Name'],\n                         content_transformations=[text_normalize,str.lower],\n                         content_augmentations= [nearby_aug_func,str.lower], \n                         val_ratio=None,\n                         batch_size=bs,\n                         seed=42,\n                         convert_training_to_iterable=False,\n                         verbose=False\n                        )\nbenchmarking_and_memory_usage(tdc,bs,tokenizer)\n\nFound cached dataset csv (/home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\nLoading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-a8e48b2fdcc1675b_*_of_00004.arrow\nLoading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-7f67ed2247bad412_*_of_00004.arrow\nLoading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-8895dee11a0750d6_*_of_00004.arrow\nLoading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-7cb19b4a6f7bb9bc_*_of_00004.arrow\nLoading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-8d6c7bf742ea98e4_*_of_00004.arrow\nLoading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-48a8c757ecb89ade_*_of_00004.arrow\nLoading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-896ddc2616d23c5e_*_of_00004.arrow\nLoading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-e0a802a1bedd79b5_*_of_00004.arrow\nLoading cached shuffled indices for dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-eae3d3101ec08ba5.arrow\nLoading cached processed dataset at /home/quan/.cache/huggingface/datasets/csv/sample_data-b5f53892a1b938ad/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-baf7b5167460c6dd.arrow\n\n\nTime it takes to process + tokenize training texts: 1.471 s\nTime it takes to go through 1280 items: 0.176 s\nMaximum memory usage: 874.715 MiB"
  },
  {
    "objectID": "deprecated/text_main_benchmark[deprecated].html#conclusion",
    "href": "deprecated/text_main_benchmark[deprecated].html#conclusion",
    "title": "Text Processing Benchmark",
    "section": "Conclusion",
    "text": "Conclusion\nWith CPU batch size of 128, and data iteration of 1280 items (10 batches)\n\nTime to process + tokenize. Unit: seconds\n\n\n\n\n\n\n\n\n\n\n\n\nFiltering\n+ 2-column metadatas\n+ 2 tfms and 2 augs\n+ no train shuffling\n\n\n\n\nno iterable training\n37.038\n40.147\n62.309\n59.452\n\n\niterable training\n2.85\n2.623\n22.31\n22.421\n\n\nstreaming\n0.002\n0.002\n0.084\n0.08\n\n\n\n\nTime to loop through 1280 items (10 batches). Unit: seconds\n\n\n\n\n\n\n\n\n\n\n\n\nFiltering\n+ 2-column metadatas\n+ 2 tfms and 2 augs\n+ no train shuffling\n\n\n\n\nno iterable training\n0.155\n0.181\n0.183\n0.184\n\n\niterable training\n0.464\n0.544\n0.562\n0.474\n\n\nstreaming\n1.244\n1.365\n95.443\n11.529\n\n\n\n\nMaximum memory usage. Unit: megabytes\n\n\n\n\n\n\n\n\n\n\n\n\nFiltering\n+ 2-column metadatas\n+ 2 tfms and 2 augs\n+ no train shuffling\n\n\n\n\nno iterable training\n762.734\n806.473\n859.008\n867.031\n\n\niterable training\n799.742\n838.613\n891.176\n892\n\n\nstreaming\n752.238\n829.074\n6955.02\n6841.391"
  },
  {
    "objectID": "deprecated/text_main_benchmark[deprecated].html#tips-and-tricks",
    "href": "deprecated/text_main_benchmark[deprecated].html#tips-and-tricks",
    "title": "Text Processing Benchmark",
    "section": "Tips and tricks",
    "text": "Tips and tricks\n\nFor non-streaming data, the best way to minimize processing and iteration time is:\n\nUse non-iterable training (which means don’t turn training set into an Iterable Dataset)\nTurn on dataset caching, and run the processing step once for it to be cached\n\nIf caching is not an option, then use iterable training (turn trainingset into an Iterable Dataset)\nThe more content transformations and augmentations added, the slower the process + iteration. This is especially true for streaming data\nFor streaming data, which might be the slowest option, here are a few things to speed up the whole pipeline:\n\nTry to define and create a validation set split in your dataset; don’t use the validation split functionality of `TextDataController\nMinimize the amount of content transformation and content augmentation\nTurn off shuffle_trn\nSet a smaller CPU batch size. E.g. in my 64gb RAM machine, and this dataset of 117k rows, I can only set batch size up to 200 to avoid memory error"
  },
  {
    "objectID": "deprecated/textdatacontroller[deprecated].html",
    "href": "deprecated/textdatacontroller[deprecated].html",
    "title": "Text Data Controller",
    "section": "",
    "text": "class TextDataController():\n    def __init__(self,\n                 inp, # HuggingFainpce Dataset or DatasetDict\n                 main_text:str, # Name of the main text column\n                 label_names=None, # Names of the label (dependent variable) columns\n                 class_names_predefined=None, # List of names associated with the labels (same index order)\n                 filter_dict={}, # A dictionary: {feature: filtering_function_based_on_the_feature}\n                 metadatas=[], # Names of the metadata columns\n                 process_metas=True, # Whether to do simple text processing on the chosen metadatas\n                 content_transformations=[], # A list of text transformations\n                 val_ratio:int|float|None=0.2, # Ratio of data for validation set\n                 stratify_cols=[], # Column(s) needed to do stratified shuffle split\n                 upsampling_list={}, # A list of tuple. Each tuple: (feature,upsampling_function_based_on_the_feature)\n                 content_augmentations=[], # A list of text augmentations\n                 seed=None, # Random seed\n                 batch_size=1000, # CPU batch size\n                 num_proc=4, # Number of process for multiprocessing\n                 cols_to_keep=None, # Columns to keep after all processings\n                 buffer_size=10000, # For shuffling data\n                 num_shards=64, # Number of shards. Stream datasets can be made out of multiple shards\n                 convert_training_to_iterable=True, # Whether to convert training Dataset to IterableDataset\n                 verbose=True, # Whether to print processing information\n                ):\n            \n        self.main_text = main_text\n        self.metadatas = val2iterable(metadatas)\n        self.process_metas = process_metas\n        self.label_names = val2iterable(label_names) if label_names is not None else None\n        self.label_lists = class_names_predefined\n        self.filter_dict = filter_dict\n        self.content_tfms = val2iterable(content_transformations)\n        self.upsampling_list = upsampling_list\n        self.aug_tfms = val2iterable(content_augmentations)\n        self.val_ratio = val_ratio\n        self.stratify_cols = val2iterable(stratify_cols)\n        self.seed = seed\n        self.is_batched = batch_size&gt;1\n        self.batch_size = batch_size\n        self.num_proc = num_proc\n        self.cols_to_keep = cols_to_keep\n        self.buffer_size = buffer_size\n        self.num_shards = num_shards\n        self.ddict_rest = DatasetDict()\n        self.convert_training_to_iterable = convert_training_to_iterable\n        self.verbose = verbose\n        self.verboseprint = print if verbose else lambda *a, **k: None\n        \n        if hasattr(inp,'keys'):\n            if 'train' in inp.keys(): # is datasetdict\n                self.ddict_rest = inp\n                self.dset = self.ddict_rest.pop('train')\n            else:\n                raise ValueError('The given DatasetDict has no \"train\" split')\n        else: # is dataset\n            self.dset = inp\n            \n        self.is_streamed=isinstance(self.dset,IterableDataset)\n        \n        self.all_cols = get_dset_col_names(self.dset)\n        if self.is_streamed and self.label_names is not None and self.label_lists is None:\n            raise ValueError('All class labels must be provided when streaming')\n        \n        if self.is_streamed and len(self.upsampling_list):\n            warnings.warn(\"Upsampling requires dataset concatenation, which can be extremely slow (x2) for streamed dataset\")\n            \n        self._processed_call=False\n        \n        self._determine_multihead_multilabel()\n        \n            \n    @classmethod\n    def from_csv(cls,file_path,**kwargs):\n        file_path = Path(file_path)\n        ds = load_dataset(str(file_path.parent),\n                                  data_files=file_path.name,\n                                  split='train')\n        return TextDataController(ds,**kwargs)\n        \n    \n    @classmethod\n    def from_df(cls,df,validate=True,**kwargs):\n        if validate:\n            check_input_validation(df)\n        ds = Dataset.from_pandas(df)\n        return TextDataController(ds,**kwargs)\n    \n    @classmethod\n    def from_pickle(cls,\n                    fname, # Name of the pickle file\n                    parent='pickle_files' # Parent folder\n                   ):\n        return load_pickle(fname,parent=parent)\n    \n    def _determine_multihead_multilabel(self):\n        self.is_multilabel=False\n        self.is_multihead=False\n        if self.label_names is None: return\n        \n        if len(self.label_names)&gt;1:\n            self.is_multihead=True\n        # get label of first row\n        first_label = self.dset[self.label_names[0]][0] if not self.is_streamed else next(iter(self.dset))[self.label_names[0]]\n        if isinstance(first_label,(list,set,tuple)):\n            # This is multi-label. Ignore self.label_names[1:]\n            self.label_names = [self.label_names[0]]\n            self.is_multihead=False\n            self.is_multilabel=True\n                     \n    def validate_input(self):\n        if self.is_streamed:\n            self.verboseprint('Input validation check is disabled when data is streamed')\n            return\n        _df = self.dset.to_pandas()\n        check_input_validation(_df)\n    \n    \n    \n    def save_as_pickles(self,\n                        fname, # Name of the pickle file\n                        parent='pickle_files', # Parent folder\n                        drop_data_attributes=False # Whether to drop all large-size data attributes\n                       ):\n        if drop_data_attributes:\n            if hasattr(self, 'main_ddict'):\n                del self.main_ddict\n            if hasattr(self, 'ddict_rest'):\n                del self.ddict_rest\n        save_to_pickle(self,fname,parent=parent)\n    \n        \n    def _check_validation_leaking(self):\n        if self.val_ratio is None or self.is_streamed:\n            return\n        \n        trn_txt = self.main_ddict['train'][self.main_text]\n        val_txt = self.main_ddict['validation'][self.main_text]        \n        val_txt_leaked = check_text_leaking(trn_txt,val_txt,verbose=self.verbose)\n        \n        if len(val_txt_leaked)==0: return\n        \n        # filter train dataset to get rid of leaks\n        self.verboseprint('Filtering leaked data out of training set...')\n        _func = partial(lambda_batch,\n                        feature=self.main_text,\n                        func=lambda x: x.strip().lower() not in val_txt_leaked,\n                        is_batched=self.is_batched)\n        self.main_ddict['train'] = hf_filter_dset(self.main_ddict['train'],_func,self.is_batched,self.batch_size,self.num_proc)   \n        self.verboseprint('Done')\n           \n    def _train_test_split(self):\n        print_msg('Train Test Split',20,verbose=self.verbose)\n        val_key = list(set(self.ddict_rest.keys()) & set(['val','validation','valid']))\n        if len(val_key)==1: # val split exists\n            self.verboseprint('Validation split already exists')\n            self.main_ddict=DatasetDict({'train':self.dset,\n                                         'validation':self.ddict_rest.pop(val_key[0])})\n            \n    \n        elif self.val_ratio is None: # use all data\n            self.verboseprint('No validation split defined')\n            self.main_ddict=DatasetDict({'train':self.dset})\n            \n        elif (isinstance(self.val_ratio,float) or isinstance(self.val_ratio,int)) and not len(self.stratify_cols):\n            self.verboseprint('Validation split based on val_ratio')\n            if self.is_streamed:\n                if isinstance(self.val_ratio,float):\n                    warnings.warn(\"Length of streamed dataset is unknown to use float validation ratio. Default to the first 1000 data points for validation\")\n                    self.val_ratio=1000  \n                trn_dset = self.dset.skip(self.val_ratio)  \n                val_datas = list(self.dset.take(self.val_ratio))\n                val_dict={k: [v[k] for v in val_datas] for k in val_datas[0].keys()}   \n                val_dset = Dataset.from_dict(val_dict) \n                self.main_ddict=DatasetDict({'train':trn_dset,\n                                         'validation':val_dset})\n            else:\n                # train val split\n                self.main_ddict = self.dset.train_test_split(test_size=self.val_ratio,shuffle=True,seed=self.seed)\n                self.main_ddict['validation']=self.main_ddict['test']\n                del self.main_ddict['test']\n        \n        else: # val_ratio split with stratifying\n            if self.is_streamed: raise ValueError('Stratified split is not supported for streamed data')                \n            if self.is_multilabel and self.label_names[0] in self.stratify_cols:\n                raise ValueError('For MultiLabel classification, you cannot choose the label as your stratified column')\n            self.verboseprint('Validation split based on val_ratio, with stratifying')\n            # Create a new feature 'stratified', which is a concatenation of values in stratify_cols\n            if self.is_batched:\n                stratified_creation = lambda x: {'stratified':\n                                     ['_'.join(list(map(str,[x[v][i] for v in self.stratify_cols]))) \n                                      for i in range(len(x[self.stratify_cols[0]]))]}\n            else:\n                stratified_creation = lambda x: {'stratified':\n                                     '_'.join(list(map(str,[x[v] for v in self.stratify_cols]))) \n                                      }\n            self.dset = self.dset.map(stratified_creation,\n                                      batched=self.is_batched,\n                                      batch_size=self.batch_size,\n                                      num_proc=self.num_proc)\n            self.dset=self.dset.class_encode_column(\"stratified\")\n            # train val split\n            self.main_ddict = self.dset.train_test_split(test_size=self.val_ratio,\n                                                         shuffle=True,seed=self.seed,\n                                                        stratify_by_column='stratified')\n            self.main_ddict['validation']=self.main_ddict['test']\n            del self.main_ddict['test']\n            self.main_ddict=self.main_ddict.remove_columns(['stratified'])\n            \n        \n        del self.dset\n        self.verboseprint('Done')\n\n                             \n    def _create_label_mapping_func(self,encoder_classes):\n        if self.is_multihead:\n            label2idxs = [{v:i for i,v in enumerate(l_classes)} for l_classes in encoder_classes]\n                    \n            _func = lambda inp: {'label': [[label2idxs[i][v] for i,v in enumerate(vs)] for vs in zip(*[inp[l] for l in self.label_names])] \\\n                                    if self.is_batched else [label2idxs[i][v] for i,v in enumerate([inp[l] for l in self.label_names])]\n                              }\n            \n        else:\n            label2idx = {v:i for i,v in enumerate(encoder_classes[0])}\n            _func = partial(lambda_map_batch,\n                           feature=self.label_names[0],\n                           func=lambda x: label2idx[x],\n                           output_feature='label',\n                           is_batched=self.is_batched)\n        return _func\n        \n    def _encode_labels(self):\n        if self.label_names is None: return\n        print_msg('Label Encoding',verbose=self.verbose)\n        \n        if self.label_lists is not None and not isinstance(self.label_lists[0],list):\n            self.label_lists = [self.label_lists]\n                    \n        encoder_classes=[]\n        if not self.is_multilabel:\n            for idx,l in enumerate(self.label_names):\n                if self.label_lists is None:\n                    l_encoder = LabelEncoder()\n                    _ = l_encoder.fit(self.dset[l])\n                    l_classes = list(l_encoder.classes_)\n                else:\n                    l_classes = sorted(list(self.label_lists[idx]))\n                encoder_classes.append(l_classes)\n            \n            _func = self._create_label_mapping_func(encoder_classes)\n                \n            self.dset = hf_map_dset(self.dset,_func,self.is_batched,self.batch_size,self.num_proc)\n\n            val_key = list(set(self.ddict_rest.keys()) & set(['val','validation','valid']))\n            if len(val_key)&gt;1: raise ValueError('Your DatasetDict has more than 1 validation split')\n            if len(val_key)==1:\n                val_key=val_key[0]\n                self.ddict_rest[val_key] = hf_map_dset(self.ddict_rest[val_key],_func,\n                                                       self.is_batched,self.batch_size,self.num_proc)\n                    \n        else:\n            # For MultiLabel, we transform the label itself to one-hot (or actually, few-hot)\n            if self.label_lists is None:\n                l_encoder = MultiLabelBinarizer()\n                _ = l_encoder.fit(self.dset[self.label_names[0]])\n                l_classes = list(l_encoder.classes_)\n            else:\n                l_classes = sorted(list(self.label_lists[0]))\n            \n            encoder_classes.append(l_classes)\n            \n            l_encoder = MultiLabelBinarizer(classes=encoder_classes[0])\n            _ = l_encoder.fit(None)\n            _func = partial(lambda_map_batch,\n                            feature=self.label_names[0],\n                            func=lambda x: l_encoder.transform(x),\n                            output_feature='label',\n                            is_batched=self.is_batched,\n                            is_func_batched=True)\n            self.dset = hf_map_dset(self.dset,_func,self.is_batched,self.batch_size,self.num_proc)                                                 \n            \n            val_key = list(set(self.ddict_rest.keys()) & set(['val','validation','valid']))\n            if len(val_key)&gt;1: raise ValueError('Your DatasetDict has more than 1 validation dataset')\n            if len(val_key)==1:\n                val_key=val_key[0]\n                self.ddict_rest[val_key] = hf_map_dset(self.ddict_rest[val_key],_func,\n                                                       self.is_batched,self.batch_size,self.num_proc)\n            \n        self.label_lists = encoder_classes\n        self.verboseprint('Done')\n        \n    def _process_metadatas(self,dset,ddict_rest=None):\n        if len(self.metadatas)&gt;0:\n            print_msg('Metadata Simple Processing & Concatenating to Main Content',verbose=self.verbose)\n            map_func = partial(concat_metadatas,\n                               main_text=self.main_text,\n                               metadatas=self.metadatas,\n                               process_metas=self.process_metas,\n                               is_batched=self.is_batched)\n            dset = hf_map_dset(dset,map_func,self.is_batched,self.batch_size,self.num_proc)\n            if ddict_rest is not None:\n                ddict_rest = hf_map_dset(ddict_rest,map_func,self.is_batched,self.batch_size,self.num_proc)\n            self.verboseprint('Done')\n        return dset if ddict_rest is None else (dset,ddict_rest)\n            \n            \n    \n    def _simplify_ddict(self):\n        print_msg('Dropping unused features',20,verbose=self.verbose)\n        if self.cols_to_keep is None:\n            self.cols_to_keep= [self.main_text] + self.metadatas\n            if self.label_names is not None: self.cols_to_keep+=self.label_names\n        cols_to_remove = set(self.all_cols) - set(self.cols_to_keep)\n        self.main_ddict['train']=self.main_ddict['train'].remove_columns(list(cols_to_remove))\n        if 'validation' in self.main_ddict.keys():\n            self.main_ddict['validation']=self.main_ddict['validation'].remove_columns(list(cols_to_remove))\n        self.verboseprint('Done')\n    \n    def _do_transformation(self,dset,ddict_rest=None):\n        if len(self.content_tfms):\n            print_msg('Text Transformation',20,verbose=self.verbose)\n            for tfm in self.content_tfms:\n                print_msg(callable_name(tfm),verbose=self.verbose)\n                _func = partial(lambda_map_batch,\n                               feature=self.main_text,\n                               func=tfm,\n                               is_batched=self.is_batched)\n                dset = hf_map_dset(dset,_func,self.is_batched,self.batch_size,self.num_proc)\n                if ddict_rest is not None:\n                    ddict_rest = hf_map_dset(ddict_rest,_func,self.is_batched,self.batch_size,self.num_proc)\n            self.verboseprint('Done')\n        return dset if ddict_rest is None else (dset,ddict_rest)\n \n    def _do_filtering(self,dset,ddict_rest=None):\n        if len(self.filter_dict):\n            print_msg('Data Filtering',20,verbose=self.verbose)\n            col_names = get_dset_col_names(dset)\n            for f,tfm in self.filter_dict.items():\n                if f in col_names:\n                    print_msg(f'Do {callable_name(tfm)} on {f}',verbose=self.verbose)\n                    _func = partial(lambda_batch,\n                                    feature=f,\n                                    func=tfm,\n                                    is_batched=self.is_batched)\n                    dset = hf_filter_dset(dset,_func,self.is_batched,self.batch_size,self.num_proc)\n                if ddict_rest is not None: # assuming ddict_rest has the column to filter, always\n                    ddict_rest = hf_filter_dset(ddict_rest,_func,self.is_batched,self.batch_size,self.num_proc)\n            self.verboseprint('Done')\n        return dset if ddict_rest is None else (dset,ddict_rest)\n    \n    def _upsampling(self):\n        if len(self.upsampling_list):\n            print_msg('Upsampling data',20,verbose=self.verbose)\n            results=[]\n            for f,tfm in self.upsampling_list:\n                print_msg(f'Do {callable_name(tfm)} on {f}',verbose=self.verbose)\n                _func = partial(lambda_batch,\n                                feature=f,\n                                func=tfm,\n                                is_batched=self.is_batched)\n                new_dset = hf_filter_dset(self.main_ddict['train'],_func,self.is_batched,self.batch_size,self.num_proc)\n                results.append(new_dset)\n            # slow concatenation for iterable dataset    \n            self.main_ddict['train'] = concatenate_datasets(results+[self.main_ddict['train']])\n            self.verboseprint('Done')\n      \n    def _do_augmentation(self):\n        \n        if len(self.aug_tfms):\n            print_msg('Text Augmentation',20,verbose=self.verbose)\n\n            seed_notorch(self.seed)\n#             self.main_ddict['train'] = self.main_ddict['train'].with_transform(partial(augmentation_helper,\n#                                                                    text_name=self.main_text,\n#                                                                    func=partial(func_all,functions=self.aug_tfms)))  \n            if not self.is_streamed:  \n                for tfm in self.aug_tfms:\n                    print_msg(callable_name(tfm),verbose=self.verbose)\n            \n                    bs = self.batch_size\n                    is_func_batched=False\n                    num_proc = self.num_proc\n                    is_batched = self.is_batched\n                    if hasattr(tfm, \"run_on_gpu\") and getattr(tfm,'run_on_gpu')==True:\n                        bs = 32 if not hasattr(tfm, \"batch_size\") else getattr(tfm,'batch_size')\n                        is_func_batched=True\n                        is_batched=True\n                        num_proc=1\n                        \n                    _func = partial(lambda_map_batch,\n                                   feature=self.main_text,\n                                   func=tfm,\n                                   is_batched=is_batched,\n                                   is_func_batched=is_func_batched\n                                   )\n                    self.main_ddict['train'] = hf_map_dset(self.main_ddict['train'],_func,\n                                                              is_batched=is_batched,\n                                                              batch_size=bs,\n                                                              num_proc=num_proc\n                                                             )\n\n            else: \n                self.main_ddict['train'] = IterableDataset.from_generator(augmentation_stream_generator,\n#                                                features = self.main_ddict['train'].features,\n                                               gen_kwargs={'dset': self.main_ddict['train'],\n                                                           'text_name':self.main_text,\n                                                           'func':partial(func_all,functions=self.aug_tfms)\n                                                          })\n            self.verboseprint('Done')\n        \n    def _convert_to_iterable(self):\n        if (not self.is_streamed) and self.convert_training_to_iterable:\n            print_msg('Converting train set to iterable',20,verbose=self.verbose)\n            self.main_ddict['train'] = self.main_ddict['train'].to_iterable_dataset(num_shards=self.num_shards)\n            self.is_streamed=True\n            self.verboseprint('Done')\n\n            \n    def _do_train_shuffling(self):\n        print_msg('Shuffling train set',20,verbose=self.verbose)\n        if self.is_streamed:\n            self.main_ddict['train'] = self.main_ddict['train'].shuffle(seed=self.seed, buffer_size=self.buffer_size)\n        else:\n            self.main_ddict['train'] = self.main_ddict['train'].shuffle(seed=self.seed)\n        self.verboseprint('Done')\n        \n    def do_all_preprocessing(self,\n                             shuffle_trn=True # To shuffle the train set before tokenization\n                            ):\n        if self._processed_call:\n            warnings.warn('Your dataset has already been processed. Returning the previous processed DatasetDict...')\n            return self.main_ddict\n            \n        print_msg('Start Main Text Processing',20,verbose=self.verbose)\n        \n        # Filtering\n        self.dset,self.ddict_rest = self._do_filtering(self.dset,self.ddict_rest)\n        \n        # Process metadatas\n        self.dset,self.ddict_rest = self._process_metadatas(self.dset,self.ddict_rest)\n        \n        # Process labels\n        self._encode_labels()\n        \n        # Content transformation\n        self.dset,self.ddict_rest = self._do_transformation(self.dset,self.ddict_rest)\n         \n        # Train Test Split.\n        ### self.main_ddict is created here\n        self._train_test_split()\n        \n        # Dropping unused columns\n        self._simplify_ddict()\n        \n        # Check validation leaking\n        self._check_validation_leaking()\n        \n        ### The rest of these functions applies only to the train dataset\n        # Upsampling\n        self._upsampling()\n        \n        # Augmentation\n        self._do_augmentation()\n           \n        # Convert train set to iterable\n        self._convert_to_iterable()\n        \n        # Shuffle train\n        if shuffle_trn:\n            self._do_train_shuffling()\n        \n        self._processed_call=True\n        \n        return self.main_ddict\n    \n        \n    def do_tokenization(self,\n                        tokenizer, # Tokenizer (preferably from HuggingFace)\n                        max_length=None, # pad to model's allowed max length (default is max_sequence_length)\n                        trn_size=None, # The number of training data to be tokenized\n                       ):\n        print_msg('Tokenization',20,verbose=self.verbose)\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        tok_func = partial(tokenize_function,tok=tokenizer,max_length=max_length)\n        _func = partial(lambda_map_batch,\n                        feature=self.main_text,\n                        func=tok_func,\n                        output_feature=None,\n                        is_batched=self.is_batched)\n        \n        if trn_size is not None:\n            self.main_ddict['train'] = self.main_ddict['train'].take(trn_size)\n        \n        for k in self.main_ddict.keys():\n            self.main_ddict[k] = hf_map_dset(self.main_ddict[k],_func,self.is_batched,self.batch_size,self.num_proc)\n\n        self.verboseprint('Done')\n        return self.main_ddict\n        \n    def process_and_tokenize(self,\n                             tokenizer, # Tokenizer (preferably from HuggingFace)\n                             max_length=None, # pad to model's allowed max length (default is max_sequence_length)\n                             trn_size=None, # The number of training data to be tokenized\n                             shuffle_trn=True, # To shuffle the train set before tokenization\n                            ):\n        \"\"\"\n        This will perform `do_all_processing` then `do_tokenization`\n        \"\"\"\n        _ = self.do_all_preprocessing(shuffle_trn)\n        _ = self.do_tokenization(tokenizer,max_length,trn_size)\n        \n    \n    def set_data_collator(self,data_collator):\n        self.data_collator = data_collator\n        \n    \n    def prepare_test_dataset_from_csv(self,\n                                      file_path, # path to csv file\n                                      do_filtering=False # whether to perform data filtering on this test set\n                                     ):\n        file_path = Path(file_path)\n        ds = load_dataset(str(file_path.parent),\n                          data_files=file_path.name,\n                          split='train')\n        return self.prepare_test_dataset(ds,do_filtering)\n    \n    def prepare_test_dataset_from_df(self,\n                                     df, # Pandas Dataframe\n                                     validate=True, # whether to perform input data validation\n                                     do_filtering=False # whether to perform data filtering on this test set \n                                    ):\n        if validate:\n            check_input_validation(df)\n        ds = Dataset.from_pandas(df)\n        return self.prepare_test_dataset(ds,do_filtering)\n    \n    def prepare_test_dataset_from_raws(self,\n                                       content, # Either a single sentence, list of sentence or a dictionary with keys are metadata columns and values are list\n                                      ):\n        if len(self.metadatas)!=0 and not isinstance(content,dict):\n            raise ValueError(f'There is/are metadatas in the preprocessing step. Please include a dictionary including these keys for metadatas: {self.metadatas}, and texture content: {self.main_text}')\n            \n        _dic = {self.main_text:[content]} if isinstance(content,str) else content\n        for k in _dic.keys():\n            _dic[k] = val2iterable(_dic[k])\n        \n        test_dict = Dataset.from_dict(_dic)\n        return self.prepare_test_dataset(test_dict,do_filtering=False)\n    \n    def prepare_test_dataset(self,\n                             test_dset, # The HuggingFace Dataset as Test set\n                             do_filtering=False # whether to perform data filtering on this test set\n                            ):\n        test_cols = set(get_dset_col_names(test_dset))\n        test_cols = test_cols - set(self.label_names)\n        missing_cols = set(self.cols_to_keep) - set(self.label_names) - set(test_cols)\n        if len(missing_cols):\n            raise ValueError(f'Test set does not have these columns required for preprocessings: {missing_cols}')\n            \n        print_msg('Start Test Set Transformation',20,verbose=self.verbose)\n\n        # Filtering\n        if do_filtering:\n            test_dset = self._do_filtering(test_dset)\n        \n        # Process metadatas\n        test_dset = self._process_metadatas(test_dset)\n        \n        # Content transformation\n        test_dset = self._do_transformation(test_dset)\n        \n        # Drop unused columns\n        cols_to_remove = test_cols - set(self.cols_to_keep)\n        test_dset=test_dset.remove_columns(list(cols_to_remove))\n        \n        # Tokenization\n        print_msg('Tokenization',20,verbose=self.verbose)\n        _func = partial(lambda_batch,\n                        feature=self.main_text,\n                        func=partial(tokenize_function,tok=self.tokenizer,max_length=self.max_length),\n                        output_feature=None,\n                        is_batched=self.is_batched)\n        \n        test_dset = hf_map_dset(test_dset,_func,self.is_batched,self.batch_size,self.num_proc)\n        \n        self.verboseprint('Done')\n        return test_dset"
  },
  {
    "objectID": "deprecated/model_main_envibert[deprecated].html",
    "href": "deprecated/model_main_envibert[deprecated].html",
    "title": "Model Controller Tutorial: EnviBert model",
    "section": "",
    "text": "We will reuse the sample data in this tutorial to experiment with the models\n\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom that_nlp_library.text_main import *\n\n\nfrom underthesea import text_normalize\nfrom functools import partial\nfrom pathlib import Path\nfrom importlib.machinery import SourceFileLoader\nimport os\nfrom transformers import DataCollatorWithPadding\nimport torch\n\nDefine some necessary text augmentations and text transformations\n\nFor Text Transformation\n\n\ntxt_tfms=[text_normalize]\n\n\nFor Text Augmentation\n\n\nover_nonown_tfm = partial(sampling_with_condition,query='Source==\"non owned\"',frac=0.5,seed=42,apply_to_all=False)\nover_nonown_tfm.__name__ = 'Oversampling Non Owned'\n\nover_own_tfm = partial(sampling_with_condition,query='Source==\"owned\"',frac=2,seed=42,apply_to_all=False)\nover_own_tfm.__name__ = 'Oversampling Owned'\n\nover_hc_tfm = partial(sampling_with_condition,query='Source==\"hc search\"',frac=2.5,seed=42,apply_to_all=False)\nover_hc_tfm.__name__ = 'Oversampling HC search'\n\nremove_accent_tfm = partial(remove_vnmese_accent,frac=1,seed=42,apply_to_all=True)\nremove_accent_tfm.__name__ = 'Add No-Accent Text'\n\naug_tfms = [over_nonown_tfm,over_own_tfm,over_hc_tfm,remove_accent_tfm]\n\nCreate a TextDataMain object\n\nDATA_PATH = Path('sample_data')\n\n\ntdm = TextDataMain.from_csv(DATA_PATH/'sample_large.csv',\n                            return_df=False,\n                            main_content='Content',\n                            metadatas='Source',\n                            label_names='L1',\n                            val_ratio=0.2,\n                            split_cols='L1',\n                            content_tfms = txt_tfms,\n                            aug_tfms = aug_tfms,\n                            process_metadatas=True,\n                            seed=42,\n                            shuffle_trn=True)\n\n----- Input Validation Precheck -----\nDataFrame contains duplicated values!\n-----&gt; Number of duplications: 16 rows\n\n\n\ntdm.df.tail()\n\n\n\n\n\n\n\n\n\nSource\nContent\nL1\nL2\n\n\n\n\n2264\nGoogle Play\nĐăng xuất tài khoản thì không đăng nhập lại được.\nShopee account\nSign up/Log in\n\n\n2265\nNon Owned\nTriển lãm Thương mại Điện tử Việt Nam với sự t...\nOthers\nBranding\n\n\n2266\nGoogle Play\nNhư cứtttttttt\nOthers\nCannot defined\n\n\n2267\nHC search\náo khoác\nOthers\nCannot defined\n\n\n2268\nNon Owned\n[https://shopee.vn/jocastore.vn](https://shope...\nOthers\nCannot defined\n\n\n\n\n\n\n\n\nDefine our tokenizer for EnviBert\n\ncache_dir=Path('./envibert_tokenizer')\ntokenizer = SourceFileLoader(\"envibert.tokenizer\", \n                             str(cache_dir/'envibert_tokenizer.py')).load_module().RobertaTokenizer(cache_dir)\n\n\n# # EnviBert a data collator to work. We will save this as an attribute in TDM\n# data_collator = DataCollatorWithPadding(tokenizer,padding=True,max_length=512)\n# tdm.set_data_collator(data_collator)\n\nCreate our DatasetDict from TextDataMain (as our ModelController class can also work with DatasetDict)\n\nmain_ddict= tdm.to_datasetdict(tokenizer,\n                                max_length=512,\n                              )\n\n-------------------- Start Main Text Processing --------------------\n----- Metadata Simple Processing & Concatenating to Main Content -----\n----- Label Encoding -----\n-------------------- Text Transformation --------------------\n----- text_normalize -----\n-------------------- Train Test Split --------------------\nPrevious Validation Percentage: 20.009%\n- Before leak check\nSize: 454\n- After leak check\nSize: 447\n- Number of rows leaked: 7, or 1.54% of the original validation (or test) data\nCurrent Validation Percentage: 19.7%\n-------------------- Text Augmentation --------------------\nTrain data size before augmentation: 1822\n----- Oversampling Non Owned -----\nTrain data size after THIS augmentation: 2020\n----- Oversampling Owned -----\nTrain data size after THIS augmentation: 2248\n----- Oversampling HC search -----\nTrain data size after THIS augmentation: 2390\n----- Add No-Accent Text -----\nTrain data size after THIS augmentation: 4780\nTrain data size after ALL augmentation: 4780\n-------------------- Map Tokenize Function --------------------\n\n\n100%|█████████████████████████████████████| 2269/2269 [00:00&lt;00:00, 3928.47it/s]\n100%|████████████████████████████████████| 2390/2390 [00:00&lt;00:00, 19058.57it/s]\n\n\n\n\n\n\n\n\n\nmain_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 4780\n    })\n    validation: Dataset({\n        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 447\n    })\n})\n\n\nLet’s see some examples of outputs the TDM produces:\n\ntwo_steps_tokenization_explain('Tôi đặt hàng mà chẳng thấy giao 1 năm rồi.Làm với chả ăn,chán 🤬 🤬 🤬 🤬 🤬 🤬',\n                               tokenizer,content_tfms=[text_normalize])\n\n----- Text Transformation Explained -----\n--- Raw sentence ---\nTôi đặt hàng mà chẳng thấy giao 1 năm rồi.Làm với chả ăn,chán 🤬 🤬 🤬 🤬 🤬 🤬\n--- text_normalize ---\nTôi đặt hàng mà chẳng thấy giao 1 năm rồi . Làm với chả ăn , chán 🤬 🤬 🤬 🤬 🤬 🤬\n\n----- Tokenizer Explained -----\n--- Input ---\nTôi đặt hàng mà chẳng thấy giao 1 năm rồi . Làm với chả ăn , chán 🤬 🤬 🤬 🤬 🤬 🤬\n\n--- Tokenized results --- \n{'input_ids': [0, 842, 642, 114, 145, 1371, 289, 363, 139, 93, 539, 5, 3798, 39, 7225, 380, 4, 5925, 3529, 3, 3529, 3, 3529, 3, 3529, 3, 3529, 3, 3529, 3, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n--- Results from tokenizer.convert_ids_to_tokens ---\n['&lt;s&gt;', '▁Tôi', '▁đặt', '▁hàng', '▁mà', '▁chẳng', '▁thấy', '▁giao', '▁1', '▁năm', '▁rồi', '▁.', '▁Làm', '▁với', '▁chả', '▁ăn', '▁,', '▁chán', '▁', '&lt;unk&gt;', '▁', '&lt;unk&gt;', '▁', '&lt;unk&gt;', '▁', '&lt;unk&gt;', '▁', '&lt;unk&gt;', '▁', '&lt;unk&gt;', '&lt;/s&gt;']\n\n--- Results from tokenizer.decode --- \n&lt;s&gt; ▁Tôi ▁đặt ▁hàng ▁mà ▁chẳng ▁thấy ▁giao ▁1 ▁năm ▁rồi ▁. ▁Làm ▁với ▁chả ▁ăn ▁, ▁chán ▁ &lt;unk&gt; ▁ &lt;unk&gt; ▁ &lt;unk&gt; ▁ &lt;unk&gt; ▁ &lt;unk&gt; ▁ &lt;unk&gt; &lt;/s&gt;\n\n\n\n\ntdm.tokenizer_explain_single(tokenizer) # Pick a random text in train set to explain\n\n----- Tokenizer Explained -----\n--- Input ---\nowned - # ShopeePaychamguibaiSS3 # ShopeePay1111 link : https://bit.ly/UuDaiShopeePay11-11 Username : chipheo2306 Thương hiệu l’oreal rất hay chạy sale những ngày lễ lớn và nhất ngày 11.11 có thể sẽ có voucher giảm 50 % giảm tối đa 100 k nếu săn được voucher mình rất muốn sản phẩm ￼ Nước tẩy trang cho mọi loại da L'Oreal Paris 3 in1 Micellar Water 400 ml bởi sản phẩm giá thành bình dân nếu săn sale nữa thì càng rẻ cho dung tích lớn 400 ml lận và sản phẩm có chứa công nghệ mi-xen đột phá Chiết xuất thảo dược và Glycerin bổ sung độ ẩm cho da Hút tất cả bụi bẩn , cặn dơ của lớp make-up mà không gây khô da . Với công nghệ mới , mang đến các tẩy trang , làm sạch , giữ ẩm và dưỡng mềm da đồng thời chỉ trong một sản phẩm . Mình luôn ưu tiên thanh toán qua ShopeePay để thanh toán được tiện lợi và nhanh chóng\n\n--- Tokenized results --- \n{'input_ids': [0, 3507, 13, 2481, 1888, 51603, 53097, 1501, 1509, 10976, 4020, 2327, 31682, 1245, 2481, 1888, 51603, 53097, 4894, 4894, 1599, 46, 47048, 5513, 5713, 428, 428, 12651, 244, 1142, 428, 2057, 1114, 1477, 2327, 17803, 51603, 53097, 4894, 7343, 10578, 7103, 6460, 46, 6613, 58185, 54439, 1603, 2332, 325, 1546, 3, 47666, 173, 292, 1065, 4331, 53, 125, 1204, 300, 17, 134, 125, 704, 10890, 22, 61, 75, 22, 25734, 455, 918, 213, 455, 876, 930, 825, 1817, 513, 4738, 34, 25734, 177, 173, 452, 165, 280, 3529, 3, 4460, 5540, 430, 35, 584, 342, 680, 748, 427, 2187, 21633, 2759, 186, 11, 1147, 18100, 27237, 6499, 3198, 20332, 688, 165, 280, 157, 119, 565, 191, 513, 4738, 4331, 685, 132, 840, 1617, 35, 936, 448, 300, 3198, 20332, 10166, 17, 165, 280, 22, 1880, 59, 552, 5297, 142, 1780, 883, 2198, 1167, 18729, 228, 1425, 3789, 17, 31062, 12035, 829, 1334, 1790, 276, 3075, 35, 680, 29574, 781, 206, 4199, 5749, 4, 16823, 29324, 20, 1028, 364, 142, 4281, 145, 38, 533, 2557, 680, 5, 816, 59, 552, 183, 4, 434, 62, 29, 5540, 430, 4, 83, 1610, 4, 679, 3075, 17, 1187, 1794, 680, 122, 155, 127, 37, 40, 165, 280, 5, 4910, 612, 1261, 597, 656, 1081, 204, 1888, 51603, 53097, 58, 656, 1081, 34, 957, 579, 17, 687, 1519, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n--- Results from tokenizer.convert_ids_to_tokens ---\n['&lt;s&gt;', '▁owned', '▁-', '▁#', '▁Sh', 'opee', 'Pay', 'ch', 'am', 'gu', 'ib', 'ai', 'SS', '3', '▁#', '▁Sh', 'opee', 'Pay', '11', '11', '▁link', '▁:', '▁htt', 'ps', ':', '/', '/', 'bit', '.', 'ly', '/', 'U', 'u', 'D', 'ai', 'Sh', 'opee', 'Pay', '11', '-11', '▁Us', 'ern', 'ame', '▁:', '▁chip', 'heo', '230', '6', '▁Thương', '▁hiệu', '▁l', '&lt;unk&gt;', 'oreal', '▁rất', '▁hay', '▁chạy', '▁sale', '▁những', '▁ngày', '▁lễ', '▁lớn', '▁và', '▁nhất', '▁ngày', '▁11', '.11', '▁có', '▁thể', '▁sẽ', '▁có', '▁voucher', '▁giảm', '▁50', '▁%', '▁giảm', '▁tối', '▁đa', '▁100', '▁k', '▁nếu', '▁săn', '▁được', '▁voucher', '▁mình', '▁rất', '▁muốn', '▁sản', '▁phẩm', '▁', '&lt;unk&gt;', '▁Nước', '▁tẩy', '▁trang', '▁cho', '▁mọi', '▁loại', '▁da', '▁L', \"'\", 'O', 'real', '▁Paris', '▁3', '▁in', '1', '▁Mic', 'ellar', '▁Water', '▁400', '▁ml', '▁bởi', '▁sản', '▁phẩm', '▁giá', '▁thành', '▁bình', '▁dân', '▁nếu', '▁săn', '▁sale', '▁nữa', '▁thì', '▁càng', '▁rẻ', '▁cho', '▁dung', '▁tích', '▁lớn', '▁400', '▁ml', '▁lận', '▁và', '▁sản', '▁phẩm', '▁có', '▁chứa', '▁công', '▁nghệ', '▁mi', '-', 'x', 'en', '▁đột', '▁phá', '▁Chiết', '▁xuất', '▁thảo', '▁dược', '▁và', '▁Gly', 'cer', 'in', '▁bổ', '▁sung', '▁độ', '▁ẩm', '▁cho', '▁da', '▁Hút', '▁tất', '▁cả', '▁bụi', '▁bẩn', '▁,', '▁cặn', '▁dơ', '▁của', '▁lớp', '▁make', '-', 'up', '▁mà', '▁không', '▁gây', '▁khô', '▁da', '▁.', '▁Với', '▁công', '▁nghệ', '▁mới', '▁,', '▁mang', '▁đến', '▁các', '▁tẩy', '▁trang', '▁,', '▁làm', '▁sạch', '▁,', '▁giữ', '▁ẩm', '▁và', '▁dưỡng', '▁mềm', '▁da', '▁đồng', '▁thời', '▁chỉ', '▁trong', '▁một', '▁sản', '▁phẩm', '▁.', '▁Mình', '▁luôn', '▁ưu', '▁tiên', '▁thanh', '▁toán', '▁qua', '▁Sh', 'opee', 'Pay', '▁để', '▁thanh', '▁toán', '▁được', '▁tiện', '▁lợi', '▁và', '▁nhanh', '▁chóng', '&lt;/s&gt;']\n\n--- Results from tokenizer.decode --- \n&lt;s&gt; ▁owned ▁- ▁# ▁Sh opee Pay ch am gu ib ai SS 3 ▁# ▁Sh opee Pay 11 11 ▁link ▁: ▁htt ps : / / bit. ly / U u D ai Sh opee Pay 11 -11 ▁Us ern ame ▁: ▁chip heo 230 6 ▁Thương ▁hiệu ▁l &lt;unk&gt; oreal ▁rất ▁hay ▁chạy ▁sale ▁những ▁ngày ▁lễ ▁lớn ▁và ▁nhất ▁ngày ▁11.11 ▁có ▁thể ▁sẽ ▁có ▁voucher ▁giảm ▁50 ▁% ▁giảm ▁tối ▁đa ▁100 ▁k ▁nếu ▁săn ▁được ▁voucher ▁mình ▁rất ▁muốn ▁sản ▁phẩm ▁ &lt;unk&gt; ▁Nước ▁tẩy ▁trang ▁cho ▁mọi ▁loại ▁da ▁L'O real ▁Paris ▁3 ▁in 1 ▁Mic ellar ▁Water ▁400 ▁ml ▁bởi ▁sản ▁phẩm ▁giá ▁thành ▁bình ▁dân ▁nếu ▁săn ▁sale ▁nữa ▁thì ▁càng ▁rẻ ▁cho ▁dung ▁tích ▁lớn ▁400 ▁ml ▁lận ▁và ▁sản ▁phẩm ▁có ▁chứa ▁công ▁nghệ ▁mi - x en ▁đột ▁phá ▁Chiết ▁xuất ▁thảo ▁dược ▁và ▁Gly cer in ▁bổ ▁sung ▁độ ▁ẩm ▁cho ▁da ▁Hút ▁tất ▁cả ▁bụi ▁bẩn ▁, ▁cặn ▁dơ ▁của ▁lớp ▁make - up ▁mà ▁không ▁gây ▁khô ▁da ▁. ▁Với ▁công ▁nghệ ▁mới ▁, ▁mang ▁đến ▁các ▁tẩy ▁trang ▁, ▁làm ▁sạch ▁, ▁giữ ▁ẩm ▁và ▁dưỡng ▁mềm ▁da ▁đồng ▁thời ▁chỉ ▁trong ▁một ▁sản ▁phẩm ▁. ▁Mình ▁luôn ▁ưu ▁tiên ▁thanh ▁toán ▁qua ▁Sh opee Pay ▁để ▁thanh ▁toán ▁được ▁tiện ▁lợi ▁và ▁nhanh ▁chóng &lt;/s&gt;"
  },
  {
    "objectID": "deprecated/model_main_envibert[deprecated].html#train-envibert-using-tdm",
    "href": "deprecated/model_main_envibert[deprecated].html#train-envibert-using-tdm",
    "title": "Model Controller Tutorial: EnviBert model",
    "section": "Train EnviBert using TDM",
    "text": "Train EnviBert using TDM\n\ntdm.label_lists\n\n[['Buyer complained seller',\n  'Commercial',\n  'Delivery',\n  'Feature',\n  'Order/Item',\n  'Others',\n  'Payment',\n  'Return/Refund',\n  'Services',\n  'Shopee account']]\n\n\n\nnum_classes = len(tdm.label_lists[0]) # 10\nnum_classes\n\n10\n\n\nLet’s define our model and model controller. First, we will initialize the pretrained body model\n\nfrom transformers.models.roberta.modeling_roberta import RobertaModel\n\n\nmodel_name='nguyenvulebinh/envibert'\n\n\nenvibert_body = RobertaModel.from_pretrained(model_name)\n\nSome weights of the model checkpoint at nguyenvulebinh/envibert were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\nThen we can define a simple class as the head for our classification task, something like this:\n\nclass SimpleClassificationHead(torch.nn.Module):\n    def __init__(self,\n                 config, # HuggingFace model configuration\n                 classifier_dropout=0.1, # Dropout ratio (for dropout layer right before the last nn.Linear)\n                 num_labels=None, # Number of label output. Every classification class must have this exact variable\n                ):\n        super().__init__()\n        self.dropout = torch.nn.Dropout(classifier_dropout)\n        self.out_proj = torch.nn.Linear(config.hidden_size, num_labels)\n    def forward(self, inp, **kwargs):\n        x = inp\n        x = self.dropout(x)\n        x = self.out_proj(x)\n        return x\n\n\n_model_kwargs={\n    # overall model hyperparams\n    'is_multilabel':tdm.is_multilabel, # False\n    'is_multihead':tdm.is_multihead, # False\n    'head_class_sizes':num_classes,\n    'head_class': SimpleClassificationHead,\n    # classfication head hyperparams\n    'classifier_dropout':0.1 \n}\n\nmodel = model_init_classification(model_class = RobertaBaseForSequenceClassification,\n                                  cpoint_path = 'nguyenvulebinh/envibert', \n                                  output_hidden_states=False, # since we are not using 'hidden layer contatenation' technique\n                                  seed=42,\n                                  body_model=envibert_body,\n                                  model_kwargs = _model_kwargs)\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score] # we will use both f1_macro and accuracy score as metrics\ncontroller = ModelController(model,tdm,metric_funcs)\n\nLoading body weights. This assumes the body is the very first first-layer block of your custom architecture\n\n\nAnd we can start training our model\n\nlr = 8e-5\nbs=4\nwd=0.01\nepochs= 2\n\ncontroller.fit(epochs,lr,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics_classification,\n              )\n\n/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\n\n\n\n    \n      \n      \n      [1194/1194 01:04, Epoch 1/2]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1 Score L1\nAccuracy Score L1\n\n\n\n\n0\nNo log\n1.059704\n0.350748\n0.677852\n\n\n1\nNo log\n1.007712\n0.462641\n0.697987\n\n\n\n\n\n\n\n\nLogging your training\nYou can log your training using HuggingFace:\n\nSupported platforms are “azure_ml”, “comet_ml”, “mlflow”, “neptune”, “tensorboard”,“clearml” and “wandb”\nReferences:\n\nhttps://huggingface.co/docs/transformers/v4.28.0/en/main_classes/trainer#transformers.TrainingArguments\nhttps://docs.wandb.ai/guides/integrations/huggingface#:~:text=Logging%20your%20Hugging%20Face%20model,every%20save_steps%20in%20the%20TrainingArguments%20.\n\n\ncontroller.fit(epochs,lr,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics_classification,\n               hf_report_to='wandb'\n              )\nYou can save your model weights at the end of your training\ncontroller.trainer.model.save_pretrained('./sample_weights/model_progress')\nOr you can save your weights at every epochs during your training\ncontroller.fit(epochs,lr,\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=True,\n               o_dir='sample_weights',\n               compute_metrics=compute_metrics_classification,\n              )"
  },
  {
    "objectID": "deprecated/model_main_envibert[deprecated].html#train-envibert-with-tokenized-datasetdict",
    "href": "deprecated/model_main_envibert[deprecated].html#train-envibert-with-tokenized-datasetdict",
    "title": "Model Controller Tutorial: EnviBert model",
    "section": "Train EnviBert with tokenized DatasetDict",
    "text": "Train EnviBert with tokenized DatasetDict\nThis part assumes you already have your tokenized datasetdict. You must have your tokenizer as well\n\ntokenizer\n\nRobertaTokenizer(name_or_path='', vocab_size=59993, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '&lt;s&gt;', 'eos_token': '&lt;/s&gt;', 'unk_token': '&lt;unk&gt;', 'sep_token': '&lt;/s&gt;', 'pad_token': '&lt;pad&gt;', 'cls_token': '&lt;s&gt;', 'mask_token': '&lt;mask&gt;'}, clean_up_tokenization_spaces=True)\n\n\nNote that your DatasetDict must contain tokens besides raw text (which typically includes ‘input_ids’, ‘token_type_ids’, ‘attention_mask’)\n\nmain_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 4780\n    })\n    validation: Dataset({\n        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 447\n    })\n})\n\n\n\nnum_classes = 10\n\n\nmodel_name='nguyenvulebinh/envibert'\n_model_kwargs={\n    # overall model hyperparams\n    'is_multilabel':False, # False\n    'is_multihead':False, # False\n    'head_class_sizes':num_classes,\n    'head_class': SimpleClassificationHead,\n    # classfication head hyperparams\n    'classifier_dropout':0.1 \n}\n\n\nenvibert_body = RobertaModel.from_pretrained(model_name)\n\n\nmodel = model_init_classification(model_class = RobertaBaseForSequenceClassification,\n                                  cpoint_path = 'nguyenvulebinh/envibert', \n                                  output_hidden_states=False, # since we are not using 'hidden layer contatenation' technique\n                                  seed=42,\n                                  body_model=envibert_body,\n                                  model_kwargs = _model_kwargs)\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score] # we will use both f1_macro and accuracy score as metrics\ncontroller = ModelController(model,\n                             metric_funcs=metric_funcs)\n\nSome weights of the model checkpoint at nguyenvulebinh/envibert were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\nLoading body weights. This assumes the body is the very first first-layer block of your custom architecture\n\n\n\nlr = 8e-5\nbs=4\nwd=0.01\nepochs= 2\n\ncontroller.fit(epochs,lr,\n               ddict=main_ddict, # Put in your tokenized datasetdict here\n               batch_size=bs,\n               weight_decay=wd,\n               save_checkpoint=False,\n               compute_metrics=compute_metrics_classification,\n               tokenizer=tokenizer,\n               label_names='L1'\n              )\n\n/home/quan/anaconda3/envs/fastai_v2/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\n\n\n\n    \n      \n      \n      [1194/1194 01:06, Epoch 1/2]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1 Score L1\nAccuracy Score L1\n\n\n\n\n0\nNo log\n1.059704\n0.350748\n0.677852\n\n\n1\nNo log\n1.007712\n0.462641\n0.697987\n\n\n\n\n\n\n\n\ncontroller.trainer.model.save_pretrained('./sample_weights/model_progress')"
  },
  {
    "objectID": "deprecated/model_main_envibert[deprecated].html#predict-using-trained-model-using-tdm",
    "href": "deprecated/model_main_envibert[deprecated].html#predict-using-trained-model-using-tdm",
    "title": "Model Controller Tutorial: EnviBert model",
    "section": "Predict using trained model, using TDM",
    "text": "Predict using trained model, using TDM\n\nLoad trained model\n\n_model_kwargs\n\n{'is_multilabel': False,\n 'is_multihead': False,\n 'head_class_sizes': 10,\n 'head_class': __main__.SimpleClassificationHead,\n 'classifier_dropout': 0.1}\n\n\n\ntrained_model = model_init_classification(model_class = RobertaBaseForSequenceClassification,\n                                          cpoint_path = Path('./sample_weights/model_progress'), \n                                          output_hidden_states=False,\n                                          seed=42,\n                                          model_kwargs = _model_kwargs)\n\nSome weights of the model checkpoint at sample_weights/model_progress were not used when initializing RobertaBaseForSequenceClassification: ['body_model.pooler.dense.weight', 'body_model.pooler.dense.bias']\n- This IS expected if you are initializing RobertaBaseForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaBaseForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\ntrained_model\n\nRobertaBaseForSequenceClassification(\n  (body_model): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(59993, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-5): 6 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=1024, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=1024, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classification_head): SimpleClassificationHead(\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=10, bias=True)\n  )\n)\n\n\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score] # we will use both f1_macro and accuracy score as metrics\ncontroller = ModelController(trained_model,tdm,metric_funcs)\n\n\n\nPredict Train/Validation set\nMake prediction on all validation set\n\ndf_val = controller.predict_ddict(ds_type='validation')\n\n-------------------- Start making predictions --------------------\n\n\n\n\n\n\ndf_val.head()\n\n\n\n\n\n\n\n\n\ntext\nlabel\nSource\npred_L1\npred_prob_L1\n\n\n\n\n0\ngoogle play - Chơi gam rất là lác\n3\ngoogle play\nCommercial\n0.837496\n\n\n1\ngoogle play - Zq\n5\ngoogle play\nOthers\n0.927602\n\n\n2\nnon owned - Làn sóng kỹ thuật số và sự lựa chọ...\n5\nnon owned\nOthers\n0.918241\n\n\n3\ngoogle play - Hàng quốc tế không còn ship COD ...\n6\ngoogle play\nDelivery\n0.804539\n\n\n4\ngoogle play - Quá tệ . Giao hàng chậm như rùa ...\n2\ngoogle play\nDelivery\n0.758327\n\n\n\n\n\n\n\n\nTo convert the label index to string, we can use the label_lists attribute of tdm\n\ndf_val['label']= df_val['label'].apply(lambda x: tdm.label_lists[0][x]).values\n\ndf_val.head()\n\n\n\n\n\n\n\n\n\ntext\nlabel\nSource\npred_L1\npred_prob_L1\n\n\n\n\n0\ngoogle play - Chơi gam rất là lác\nFeature\ngoogle play\nCommercial\n0.837496\n\n\n1\ngoogle play - Zq\nOthers\ngoogle play\nOthers\n0.927602\n\n\n2\nnon owned - Làn sóng kỹ thuật số và sự lựa chọ...\nOthers\nnon owned\nOthers\n0.918241\n\n\n3\ngoogle play - Hàng quốc tế không còn ship COD ...\nPayment\ngoogle play\nDelivery\n0.804539\n\n\n4\ngoogle play - Quá tệ . Giao hàng chậm như rùa ...\nDelivery\ngoogle play\nDelivery\n0.758327\n\n\n\n\n\n\n\n\nYou can try to get your metric to see if it matches your last traing epoch’s above\n\nf1_score(df_val.label,df_val.pred_L1,average='macro')\n\n0.4634417008698494\n\n\nYou can also make predictions on all training set, by changing argument ds_type to “train”\n\n\nPredict Test set\nWe will go through details on how to make a prediction on a completely new and raw dataset using our trained model. For now, let’s reuse the sample csv and pretend it’s our test set\n\ndf_test = TextDataMain.from_csv(Path('sample_data')/'sample_large.csv',return_df=True)\n\n----- Input Validation Precheck -----\nDataFrame contains duplicated values!\n-----&gt; Number of duplications: 16 rows\n\n\nWe will remove all the labels and unnecessary columns\n\ntrue_label = df_test['L1'].values\n\n\ndf_test = df_test.drop(['L1','L2'],axis=1)\n\n\ndf_test.head()\n\n\n\n\n\n\n\n\n\nSource\nContent\n\n\n\n\n0\nGoogle Play\nApp ncc lúc nào cx lag đơ, phần tìm kiếm thì v...\n\n\n1\nNon Owned\n..❗️ GÓC THANH LÝ Tính ra rẻ hơn cả mua #Shope...\n\n\n2\nGoogle Play\nMắc gì người ta đặt hàng toàn lỗi 😃????\n\n\n3\nOwned\n#GhienShopeePayawardT8 Khi bạn chơi shopee quá...\n\n\n4\nGoogle Play\nRất bức xúc khi dùng . mã giảm giá người dùng ...\n\n\n\n\n\n\n\n\nWe will create a DatasetDict for this test dataframe\n\ntest_ddict = tdm.get_test_datasetdict_from_df(df_test)\n\n-------------------- Getting Test Set --------------------\n----- Input Validation Precheck -----\nDataFrame contains duplicated values!\n-----&gt; Number of duplications: 19 rows\n-------------------- Start Test Set Transformation --------------------\n----- Metadata Simple Processing & Concatenating to Main Content -----\n-------------------- Text Transformation --------------------\n----- text_normalize -----\n-------------------- Test Leak Checking --------------------\n- Before leak check\nSize: 2269\n- After leak check\nSize: 0\n- Number of rows leaked: 2269, or 100.00% of the original validation (or test) data\n-------------------- Construct DatasetDict --------------------\n\n\n100%|█████████████████████████████████████| 2269/2269 [00:00&lt;00:00, 3954.30it/s]\n\n\n\n\n\nRemember the Leak Check we did in TextDataMain? Our df_test only has 70 rows, and it also shows that 70 rows of our data is leaked (100%), which is correct because this test dataset is actually a small sample of the training data.\n\ntest_ddict\n\nDatasetDict({\n    test: Dataset({\n        features: ['text', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 2269\n    })\n})\n\n\nOur test data has been processed + transformed (but not augmented) the same way as the validation set. Now we can start making the prediction\n\ncontroller = ModelController(trained_model,tdm)\ndf_result = controller.predict_ddict(ddict=test_ddict,ds_type='test')\n\n-------------------- Start making predictions --------------------\n\n\n\n\n\n\ndf_result.head()\n\n\n\n\n\n\n\n\n\ntext\nSource\npred_L1\npred_prob_L1\n\n\n\n\n0\ngoogle play - App ncc lúc nào cx lag đơ , phần...\ngoogle play\nFeature\n0.878221\n\n\n1\nnon owned - .. ❗ ️ GÓC THANH LÝ Tính ra rẻ hơn...\nnon owned\nOthers\n0.930981\n\n\n2\ngoogle play - Mắc gì người ta đặt hàng toàn lỗ...\ngoogle play\nFeature\n0.849374\n\n\n3\nowned - # GhienShopeePayawardT8 Khi bạn chơi s...\nowned\nCommercial\n0.915552\n\n\n4\ngoogle play - Rất bức xúc khi dùng . mã giảm g...\ngoogle play\nShopee account\n0.571471\n\n\n\n\n\n\n\n\nLet’s quickly check the f1 score to make sure everything works correctly\n\nf1_score(true_label,df_result.pred_L1,average='macro')\n\n0.5303012712104336\n\n\nSince we are getting the predictions on the entire training+validation set, the F1 score is expected to be slightly higher than validation’s F1 score.\nWe can even predict top k results\n\ndf_result = controller.predict_ddict(ddict=test_ddict,ds_type='test',topk=3)\ndf_result.head()\n\n-------------------- Start making predictions --------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\ntext\nSource\npred_L1\npred_prob_L1\npred_L1_top1\npred_L1_top2\npred_L1_top3\npred_prob_L1_top1\npred_prob_L1_top2\npred_prob_L1_top3\n\n\n\n\n0\ngoogle play - App ncc lúc nào cx lag đơ , phần...\ngoogle play\n[3, 5, 9]\n[0.87822074, 0.023822138, 0.02159522]\nFeature\nOthers\nShopee account\n0.878221\n0.023822\n0.021595\n\n\n1\nnon owned - .. ❗ ️ GÓC THANH LÝ Tính ra rẻ hơn...\nnon owned\n[5, 1, 0]\n[0.9309808, 0.015578598, 0.009805982]\nOthers\nCommercial\nBuyer complained seller\n0.930981\n0.015579\n0.009806\n\n\n2\ngoogle play - Mắc gì người ta đặt hàng toàn lỗ...\ngoogle play\n[3, 5, 9]\n[0.8493735, 0.050054528, 0.021759989]\nFeature\nOthers\nShopee account\n0.849374\n0.050055\n0.021760\n\n\n3\nowned - # GhienShopeePayawardT8 Khi bạn chơi s...\nowned\n[1, 6, 7]\n[0.9155516, 0.01255093, 0.010521941]\nCommercial\nPayment\nReturn/Refund\n0.915552\n0.012551\n0.010522\n\n\n4\ngoogle play - Rất bức xúc khi dùng . mã giảm g...\ngoogle play\n[9, 3, 1]\n[0.57147133, 0.25687057, 0.03061041]\nShopee account\nFeature\nCommercial\n0.571471\n0.256871\n0.030610\n\n\n\n\n\n\n\n\nIf we just want to make a prediction on a small amount of data (single sentence, or a few sentences), we can use ModelController.predict_raw_text\n\n# Since we have some metadatas, we need to define a dictionary (to imitate a DatasetDict)\nraw_content={\n    'Source': 'Google play',\n    'Content':'Tôi không thích Shopee.Tại vì dùng app rất chậm,lag banh nhà lầu, thậm chí log in còn không đc'\n}\n\nIf we don’t use metadata, we can use something like this:\nraw_content='Tôi không thích Shopee.Tại vì dùng app rất chậm,lag banh nhà lầu, thậm chí log in còn không đc'\n\ndf_result = controller.predict_raw_text(raw_content,topk=1)\ndf_result\n\n100%|███████████████████████████████████████████| 1/1 [00:00&lt;00:00, 4718.00it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntext\nSource\npred_L1\npred_prob_L1\n\n\n\n\n0\ngoogle play - Tôi không thích Shopee . Tại vì ...\ngoogle play\nFeature\n0.876843"
  },
  {
    "objectID": "deprecated/model_main_envibert[deprecated].html#predict-using-trained-model-using-tokenized-datasetdict",
    "href": "deprecated/model_main_envibert[deprecated].html#predict-using-trained-model-using-tokenized-datasetdict",
    "title": "Model Controller Tutorial: EnviBert model",
    "section": "Predict using trained model, using tokenized DatasetDict",
    "text": "Predict using trained model, using tokenized DatasetDict\n\nLoad trained model\n\nnum_classes = 10\n\nmodel_name='nguyenvulebinh/envibert'\n_model_kwargs={\n    # overall model hyperparams\n    'is_multilabel':False, # False\n    'is_multihead':False, # False\n    'head_class_sizes':num_classes,\n    'head_class': SimpleClassificationHead,\n    # classfication head hyperparams\n    'classifier_dropout':0.1 \n}\n\n\ntrained_model = model_init_classification(model_class = RobertaBaseForSequenceClassification,\n                                          cpoint_path = Path('./sample_weights/model_progress'), \n                                          output_hidden_states=False,\n                                          seed=42,\n                                          model_kwargs = _model_kwargs)\n\n\nmetric_funcs = [partial(f1_score,average='macro'),accuracy_score]\ncontroller = ModelController(trained_model,metric_funcs) # notice that we don't use tdm here\n\nSome weights of the model checkpoint at sample_weights/model_progress were not used when initializing RobertaBaseForSequenceClassification: ['body_model.pooler.dense.weight', 'body_model.pooler.dense.bias']\n- This IS expected if you are initializing RobertaBaseForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaBaseForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\n\nPredict validation set\n\nmain_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 4780\n    })\n    validation: Dataset({\n        features: ['text', 'label', 'Source', 'input_ids', 'token_type_ids', 'attention_mask', 'pred_L1', 'pred_prob_L1'],\n        num_rows: 447\n    })\n})\n\n\n\nmy_label_name = 'L1'\nmy_class_predefined = ['Buyer complained seller',\n 'Commercial',\n 'Delivery',\n 'Feature',\n 'Order/Item',\n 'Others',\n 'Payment',\n 'Return/Refund',\n 'Services',\n 'Shopee account']\n\n\ndf_val = controller.predict_ddict(main_ddict,\n                                  ds_type='validation',\n                                  is_multilabel=False,\n                                  tokenizer=tokenizer,\n                                  label_names = my_label_name,\n                                  class_names_predefined=my_class_predefined\n                                  )\n\n-------------------- Start making predictions --------------------\n\n\n\n\n\n\ndf_val.head()\n\n\n\n\n\n\n\n\n\ntext\nlabel\nSource\npred_L1\npred_prob_L1\n\n\n\n\n0\ngoogle play - Chơi gam rất là lác\n3\ngoogle play\nCommercial\n0.837496\n\n\n1\ngoogle play - Zq\n5\ngoogle play\nOthers\n0.927602\n\n\n2\nnon owned - Làn sóng kỹ thuật số và sự lựa chọ...\n5\nnon owned\nOthers\n0.918241\n\n\n3\ngoogle play - Hàng quốc tế không còn ship COD ...\n6\ngoogle play\nDelivery\n0.804539\n\n\n4\ngoogle play - Quá tệ . Giao hàng chậm như rùa ...\n2\ngoogle play\nDelivery\n0.758327\n\n\n\n\n\n\n\n\n\n\nPredict test set\n\ntest_ddict\n\nDatasetDict({\n    test: Dataset({\n        features: ['text', 'Source', 'input_ids', 'token_type_ids', 'attention_mask', 'pred_L1', 'pred_prob_L1'],\n        num_rows: 2269\n    })\n})\n\n\nIt would be cumbersome to preprocess your test data the same way you preprocess your validation set, without the use of tdm (which stores the preprocess pipeline). In short, you need to produce the test datasetdict test_ddict containing processed 'input_ids', 'token_type_ids', 'attention_mask', then call\ndf_results = controller.predict_ddict(ddict=test_ddict,\n                                      ds_type='test',\n                                      is_multilabel=False,\n                                      tokenizer=tokenizer,\n                                      label_names = my_label_name,\n                                      class_names_predefined=my_class_predefined     \n                                     )"
  },
  {
    "objectID": "deprecated/text_augmentation[deprecated].html",
    "href": "deprecated/text_augmentation[deprecated].html",
    "title": "Text Augmentation",
    "section": "",
    "text": "from nbdev.showdoc import *\n\n\nfrom __future__ import annotations\nfrom functools import partial, wraps\nimport unidecode\nimport numpy as np\nfrom tqdm import tqdm\nfrom that_nlp_library.utils import val2iterable\nimport pandas as pd\n\n\ndef _remove_kwargs(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        kwargs.pop(\"apply_to_all\", True)\n        return func(*args, **kwargs)\n    return wrapper\n\ndef _sampling_content(content,frac=1,seed=42,others=None):\n    replace=frac&gt;1\n    rng = np.random.default_rng(seed)\n    _len = len(content)\n    idxs = rng.choice(list(range(_len)),int(frac*_len),replace=replace)\n    content = content[idxs]\n    if others is not None:\n        others = others.iloc[idxs]\n    return content,others\n\n\n@_remove_kwargs\ndef remove_vnmese_accent(content:np.ndarray|list, # A list or Numpy array of string\n                         frac=1, # Fraction of the content to perform augmentation\n                         seed=42, # Random seed\n                         others=None # Metadata associating with the content\n                        ):\n    \"Perform Vietnamese accent removal\"\n    content = val2iterable(content,t='nparray')\n    if isinstance(content,list):\n        content = np.array(content)\n    content,others = _sampling_content(content,frac=frac,seed=seed,others=others)\n    content = np.array([unidecode.unidecode(c) for c in tqdm(content)])\n    if others is None:\n        return content\n    return content,others\n\n\nsource\n\nremove_vnmese_accent\n\n remove_vnmese_accent (content:numpy.ndarray|list, frac=1, seed=42,\n                       others=None)\n\nPerform Vietnamese accent removal\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncontent\nnp.ndarray | list\n\nA list or Numpy array of string\n\n\nfrac\nint\n1\nFraction of the content to perform augmentation\n\n\nseed\nint\n42\nRandom seed\n\n\nothers\nNoneType\nNone\nMetadata associating with the content\n\n\n\n\nremove_vnmese_accent('hội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức')\n\n100%|████████████████████████████████████████| 1/1 [00:00&lt;00:00, 1934.64it/s]\n\n\narray(['hoi cu dan chung cu sen hong - chung cu lotus song than thu duc'],\n      dtype='&lt;U63')\n\n\n\ntexts=[\n     'hội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức',\n     'This is the recommended way to make a Python package importable from anywhere',\n     'hội cần mở thẻ tín dụng tại hà nội, đà nẵng, tp. hồ chí minh',\n     \"biti's cao lãnh - đồng tháp\",\n     'chợ phòng trọ + việc làm...khu lĩnh nam - vĩnh hưng - mai động (hoàng mai)'\n ]\nremove_vnmese_accent(texts)\n\n100%|███████████████████████████████████████| 5/5 [00:00&lt;00:00, 45003.26it/s]\n\n\narray(['cho phong tro + viec lam...khu linh nam - vinh hung - mai dong (hoang mai)',\n       'This is the recommended way to make a Python package importable from anywhere',\n       'hoi cu dan chung cu sen hong - chung cu lotus song than thu duc',\n       \"biti's cao lanh - dong thap\",\n       'hoi can mo the tin dung tai ha noi, da nang, tp. ho chi minh'],\n      dtype='&lt;U77')\n\n\n\nremove_vnmese_accent(texts,frac=0.5)\n\n100%|███████████████████████████████████████| 2/2 [00:00&lt;00:00, 28532.68it/s]\n\n\narray(['hoi cu dan chung cu sen hong - chung cu lotus song than thu duc',\n       \"biti's cao lanh - dong thap\"], dtype='&lt;U63')\n\n\n\n@_remove_kwargs\ndef sampling_with_condition(content:np.ndarray|list, # Numpy array of string\n                              query:str, # Pandas query string for query method\n                              frac=1, # Fraction of the content to perform augmentation\n                              seed=42, # Random seed\n                              others:pd.DataFrame=None, # Metadata (as dataframe) that you can query on\n                           ):\n    \"\"\"\n    Can perform oversampling/undersampling based on dataframe query\n    \n    For more information about dataframe query: https://pandas.pydata.org/docs/user_guide/indexing.html#indexing-query\n    \"\"\"\n    if isinstance(content,list):\n        content = np.array(content)\n    idx_to_sample = others.query(query).index.values\n    others_to_sample = others.loc[idx_to_sample].copy()\n    content_to_sample=content[idx_to_sample].copy()\n    return _sampling_content(content_to_sample,frac,seed,others=others_to_sample)\n\n\nsource\n\n\nsampling_with_condition\n\n sampling_with_condition (content:numpy.ndarray|list, query:str, frac=1,\n                          seed=42,\n                          others:pandas.core.frame.DataFrame=None)\n\nCan perform oversampling/undersampling based on dataframe query\nFor more information about dataframe query: https://pandas.pydata.org/docs/user_guide/indexing.html#indexing-query\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncontent\nnp.ndarray | list\n\nNumpy array of string\n\n\nquery\nstr\n\nPandas query string for query method\n\n\nfrac\nint\n1\nFraction of the content to perform augmentation\n\n\nseed\nint\n42\nRandom seed\n\n\nothers\npd.DataFrame\nNone\nMetadata (as dataframe) that you can query on\n\n\n\n\ntexts=[\n     'hội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức',\n     'This is the recommended way to make a Python package importable from anywhere',\n     'hội cần mở thẻ tín dụng tại hà nội, đà nẵng, tp. hồ chí minh',\n     \"biti's cao lãnh - đồng tháp\",\n     'chợ phòng trọ + việc làm...khu lĩnh nam - vĩnh hưng - mai động (hoàng mai)'\n      ]\ndf = pd.DataFrame({'text':texts,\n                  'value 1': [1,2,1,3,4],\n                  'value_2': ['vnm','eng','vnm','vnm','vnm']\n                  })\n\n\ndf\n\n\n\n\n\n\n\n\n\ntext\nvalue 1\nvalue_2\n\n\n\n\n0\nhội cư dân chung cư sen hồng - chung cư lotus ...\n1\nvnm\n\n\n1\nThis is the recommended way to make a Python p...\n2\neng\n\n\n2\nhội cần mở thẻ tín dụng tại hà nội, đà nẵng, t...\n1\nvnm\n\n\n3\nbiti's cao lãnh - đồng tháp\n3\nvnm\n\n\n4\nchợ phòng trọ + việc làm...khu lĩnh nam - vĩnh...\n4\nvnm\n\n\n\n\n\n\n\n\n\ndf_new,others = sampling_with_condition(df['text'].values,\n                                        query='`value 1` == 1',\n                                        frac=1,\n                                        others=df[['value 1','value_2']]\n                                       )\n\n\nprint(df_new)\ndisplay(others)\n\n['hội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức'\n 'hội cần mở thẻ tín dụng tại hà nội, đà nẵng, tp. hồ chí minh']\n\n\n\n\n\n\n\n\n\n\nvalue 1\nvalue_2\n\n\n\n\n0\n1\nvnm\n\n\n2\n1\nvnm\n\n\n\n\n\n\n\n\n\ndf_new,others = sampling_with_condition(df['text'].values,\n                                        query='`value 1`&gt;2 and `value 1`&lt;4',\n                                        frac=2,\n                                        others=df[['value 1','value_2']]\n                                       )\n\n\nprint(df_new)\ndisplay(others)\n\n[\"biti's cao lãnh - đồng tháp\" \"biti's cao lãnh - đồng tháp\"]\n\n\n\n\n\n\n\n\n\n\nvalue 1\nvalue_2\n\n\n\n\n3\n3\nvnm\n\n\n3\n3\nvnm\n\n\n\n\n\n\n\n\n\ndf_new,others = sampling_with_condition(df['text'].values,\n                                        query='value_2==\"vnm\"',\n                                        frac=0.5,\n                                        others=df[['value 1','value_2']]\n                                       )\n\n\nprint(df_new)\ndisplay(others)\n\n['hội cư dân chung cư sen hồng - chung cư lotus sóng thần thủ đức'\n 'chợ phòng trọ + việc làm...khu lĩnh nam - vĩnh hưng - mai động (hoàng mai)']\n\n\n\n\n\n\n\n\n\n\nvalue 1\nvalue_2\n\n\n\n\n0\n1\nvnm\n\n\n4\n4\nvnm"
  },
  {
    "objectID": "hidden_states.html",
    "href": "hidden_states.html",
    "title": "Text Feature Extraction From Hidden States",
    "section": "",
    "text": "import os\n#This will specify a (or a list) of GPUs for training\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\nfrom that_nlp_library.text_transformation import *\nfrom that_nlp_library.text_augmentation import *\nfrom that_nlp_library.text_main_lm import *\nfrom that_nlp_library.utils import seed_everything\nfrom that_nlp_library.model_lm_main import *\nfrom underthesea import text_normalize\nfrom functools import partial\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForMaskedLM\nfrom datasets import load_dataset\nimport pandas as pd\nimport numpy as np\nfrom transformers import DataCollatorForLanguageModeling",
    "crumbs": [
      "4. Hidden States Extraction",
      "Text Feature Extraction From Hidden States"
    ]
  },
  {
    "objectID": "hidden_states.html#create-a-textdatalmcontroller-object",
    "href": "hidden_states.html#create-a-textdatalmcontroller-object",
    "title": "Text Feature Extraction From Hidden States",
    "section": "Create a TextDataLMController object",
    "text": "Create a TextDataLMController object\nWe will reuse the data and the preprocessings in this tutorial\nIn order to extract a feature vector from a review sentence in the dataset, we can directly use pretrained models such as Roberta, GPT2, … But if our dataset is vastly different from the datasets these pretrained models are trained on, we can finetune these pretrained models on our dataset before extracting the feature vector. And that’s exactly what we are going to do now.\n\ndset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\nddict = dset.train_test_split(test_size=0.2,seed=42)\nddict['validation'] = ddict['test']\ndel ddict['test']\n\n\n# For now, we will filter missing review text rows here instead of relying the data controler\nddict = ddict.filter(lambda x: x['Review Text'] is not None)\n\n\nddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'],\n        num_rows: 18111\n    })\n    validation: Dataset({\n        features: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'],\n        num_rows: 4530\n    })\n})\n\n\n\ntdc = TextDataLMController(ddict,\n                         main_text='Review Text',\n                         metadatas='Title',\n                         content_transformations=[text_normalize,str.lower],\n                         seed=42,\n                         verbose=True\n                        )\n\nDefine our tokenizer for Roberta\n\n_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n\n/home/quan/anaconda3/envs/nlp_dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\nProcess and tokenize our dataset (using line-by-line tokenization)\n\nblock_size=112\ntdc.process_and_tokenize(_tokenizer,line_by_line=True,max_length=block_size) \n# set max_length=-1 if you want the data collator to pad\n\n-------------------- Start Main Text Processing --------------------\n----- Metadata Simple Processing & Concatenating to Main Content -----\nDone\n-------------------- Text Transformation --------------------\n----- text_normalize -----\n----- lower -----\nDone\n-------------------- Train Test Split --------------------\nValidation split already exists\nDone\n-------------------- Dropping unused features --------------------\nDone\n- Number of rows leaked: 2, which is 0.01% of training set\nFiltering leaked data out of training set...\nDone\n-------------------- Shuffling and flattening train set --------------------\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntdc.main_ddict\n\nDatasetDict({\n    train: Dataset({\n        features: ['Title', 'Review Text', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n        num_rows: 18109\n    })\n    validation: Dataset({\n        features: ['Title', 'Review Text', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n        num_rows: 4530\n    })\n})\n\n\nAnd set the data collator\n\ntdc.set_data_collator(is_mlm=True,mlm_prob=0.15)",
    "crumbs": [
      "4. Hidden States Extraction",
      "Text Feature Extraction From Hidden States"
    ]
  },
  {
    "objectID": "hidden_states.html#initialize-and-train-roberta-language-model",
    "href": "hidden_states.html#initialize-and-train-roberta-language-model",
    "title": "Text Feature Extraction From Hidden States",
    "section": "Initialize and train Roberta Language Model",
    "text": "Initialize and train Roberta Language Model\n\n_config = AutoConfig.from_pretrained('roberta-base',vocab_size=len(_tokenizer))\n\n\n_model = language_model_init(AutoModelForMaskedLM,\n                             config=_config,\n                             cpoint_path='roberta-base',\n                             seed=42\n                            )\n\nTotal parameters: 124697433\nTotal trainable parameters: 124697433\n\n\nCreate a model controller\n\ncontroller = ModelLMController(_model,data_store=tdc,seed=42)\n\nAnd we can start training our model\n\nlr = 1e-4\nbs=32\nwd=0.01\nepochs= 6\nwarmup_ratio=0.25\ncontroller.fit(epochs,lr,\n               batch_size=bs,\n               weight_decay=wd,\n               warmup_ratio=warmup_ratio,\n               save_checkpoint=False,\n              )\n\n\n\n    \n      \n      \n      [1698/1698 11:05, Epoch 6/6]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\nNo log\n1.502509\n0.657702\n\n\n2\n1.706200\n1.509666\n0.656255\n\n\n3\n1.706200\n1.423058\n0.671800\n\n\n4\n1.476100\n1.344976\n0.684443\n\n\n5\n1.476100\n1.309737\n0.691151\n\n\n6\n1.308800\n1.293480\n0.694945\n\n\n\n\n\n\n\n\n    \n      \n      \n      [142/142 00:07]\n    \n    \n\n\nPerplexity on validation set: 3.634\n\n\nFinetuning from a pretrained model results in a massive improvement in terms of metrics\n\ncontroller.trainer.model.save_pretrained('./sample_weights/roberta_lm_model')",
    "crumbs": [
      "4. Hidden States Extraction",
      "Text Feature Extraction From Hidden States"
    ]
  },
  {
    "objectID": "hidden_states.html#extract-hidden-states-from-model",
    "href": "hidden_states.html#extract-hidden-states-from-model",
    "title": "Text Feature Extraction From Hidden States",
    "section": "Extract hidden states from model",
    "text": "Extract hidden states from model\n\nFrom raw texts\nWe can extract a feature vector from a raw text\n\n# including the `Title` entry, because we have it as our metadata in the data controller\ninp1 = {'Title':'Flattering',\n        'Review Text': \"Love this skirt. The detail is amazing. Runs small I ordered a 12 I'm usually a 10, but still a little snug\"\n       }\n\nThere’s a crucial step we have to do: set output_hidden_states to be True, so that the model can return them back for us to extract\n\n_config = AutoConfig.from_pretrained('./sample_weights/roberta_lm_model',output_hidden_states=True)\n\n\ntrained_model = language_model_init(AutoModelForMaskedLM,\n                                    cpoint_path='./sample_weights/roberta_lm_model',\n                                    config=_config\n                                   )\n\ncontroller2 = ModelLMController(trained_model,data_store=tdc,seed=42)\n\nTotal parameters: 124697433\nTotal trainable parameters: 124697433\n\n\nWhen output_hidden_states is set to True, the model will return a variable called hidden_states, which construct of hidden states of each layer in RoBERTa-base model. We only want the last layer’s hidden states (index -1), and we want the hidden vector of the first token of this layer (the [CLS] token)\n\nhidden_from_ip1 = controller2.get_hidden_states_from_raw_text(inp1,\n                                                              state_name='hidden_states',\n                                                              state_idx=[-1,0]\n                                                             )\n\n-------------------- Start Test Set Transformation --------------------\n----- Metadata Simple Processing & Concatenating to Main Content -----\nDone\n-------------------- Text Transformation --------------------\n----- text_normalize -----\n----- lower -----\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhidden_from_ip1\n\nDataset({\n    features: ['Title', 'Review Text', 'input_ids', 'attention_mask', 'special_tokens_mask', 'hidden_states'],\n    num_rows: 1\n})\n\n\nThe lenght of the hidden vector (our feature vector) from the first token of the last layer of RoBERTa is 768\n\nhidden_from_ip1['hidden_states'].shape\n\n(1, 768)\n\n\n\n\nFrom train (or validation) set\nSimilarly, we can extract feature vectors for all sentences in our existing set. We will do some experiments on the validation set for now. As an exercise, you can repeat all the experiments with the training set (All you have to do is to change ds_type to \"train\")\n\n_config = AutoConfig.from_pretrained('./sample_weights/roberta_lm_model',output_hidden_states=True)\n\ntrained_model = language_model_init(AutoModelForMaskedLM,\n                                    cpoint_path='./sample_weights/roberta_lm_model',\n                                    config=_config\n                                   )\n\ncontroller2 = ModelLMController(trained_model,data_store=tdc,seed=42)\n\nTotal parameters: 124697433\nTotal trainable parameters: 124697433\n\n\n\nhidden_from_val = controller2.get_hidden_states(ds_type='validation',\n                                                 state_name='hidden_states',\n                                                 state_idx=[-1,0]\n                                                )\n\n\n\n\n\nhidden_from_val\n\nDataset({\n    features: ['Title', 'Review Text', 'input_ids', 'attention_mask', 'special_tokens_mask', 'hidden_states'],\n    num_rows: 4530\n})\n\n\n\nhidden_from_val['hidden_states'].shape\n\n(4530, 768)",
    "crumbs": [
      "4. Hidden States Extraction",
      "Text Feature Extraction From Hidden States"
    ]
  },
  {
    "objectID": "hidden_states.html#dimensionality-reduction",
    "href": "hidden_states.html#dimensionality-reduction",
    "title": "Text Feature Extraction From Hidden States",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction\nTypically for dimensionality reduction, the two main downstream tasks we can perform on the reduced-dimension data is either for visualization, or as an input of simpler machine learning models (regression, tree-related algorithms)\nFor dimensionality reduction, it’s important to determine the target number of lower dimensions that we will perform on the original data. We will use a popular algorithm called PCA, thus those dimensions are the principal components (PC). Thus, we can use the explained variance ratio (calculated from PC’s’ eigenvalue) to determine the amount of PC needed for our dimension reduction task\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndef plot_cummulative_variance(X,figsize=(6,4),dim_to_visualize=None):\n    if dim_to_visualize is None: dim_to_visualize = X.shape[1]\n    _pca = PCA(svd_solver='full',random_state=42)\n    _tmp = _pca.fit_transform(X)\n    fig,ax = plt.subplots(figsize=figsize)\n    ax.bar(1+np.arange(dim_to_visualize),_pca.explained_variance_ratio_[:dim_to_visualize],color='maroon')\n    ax.set_ylabel('Explained Variance')\n\n    \n    ax2 = ax.twinx()\n    ax2.plot(1+np.arange(dim_to_visualize),np.cumsum(_pca.explained_variance_ratio_[:dim_to_visualize]),label='Cummulative line',color='blue')\n    ax2.set_xlabel('n_components')\n    ax2.set_ylabel('Cummulative Explained Variance')\n    ax2.legend(loc='upper right')\n    plt.show()\n\n\nplot_cummulative_variance(hidden_from_val['hidden_states'],dim_to_visualize=55)\n\n\n\n\n\n\n\n\nwe can achieve roughly 80% total variance with only ~55 components (out of 768, which is only ~7% of data). The first 3 PC are the most important PC with the highest explained variance.\nLet’s try to visualize the first 3 PCs\n\n_pca = PCA(svd_solver='full',n_components=3,random_state=42)\n_pca_coord = _pca.fit_transform(hidden_from_val['hidden_states'])\n\n\npca_df = pd.DataFrame(_pca_coord,columns=['pc1','pc2','pc3'])\n\n\npca_df['DN']=ddict['validation']['Department Name']\npca_df['DN'].fillna('Tops',inplace=True)\n\n/tmp/ipykernel_51605/3688443385.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  pca_df['DN'].fillna('Tops',inplace=True)\n\n\n\nfig,axs = plt.subplots(3,1,figsize=(10,14))\nsns.scatterplot(data=pca_df,x='pc1',y='pc2',hue='DN',ax=axs[0],s=10)\nsns.scatterplot(data=pca_df,x='pc1',y='pc3',hue='DN',ax=axs[1],s=10)\nsns.scatterplot(data=pca_df,x='pc2',y='pc3',hue='DN',ax=axs[2],s=10)\n\n\n\n\n\n\n\n\nWe can somewhat distinguish a clear cluster when visualizing PC1 and PC3 on the top right. Combining with the Department Name, this cluster contains mostly “Bottoms” and “Intimiate” clothing items. Therefore, using these PCs, we can build a simple classification model to classify Department Name",
    "crumbs": [
      "4. Hidden States Extraction",
      "Text Feature Extraction From Hidden States"
    ]
  },
  {
    "objectID": "hidden_states.html#sentence-similarity",
    "href": "hidden_states.html#sentence-similarity",
    "title": "Text Feature Extraction From Hidden States",
    "section": "Sentence Similarity",
    "text": "Sentence Similarity\nWhen each sentence is represented by a dense feature vector, we can perform one of the popular task of sentence similarity: which sentences are the most similar to a given sentence? In this experiment, we will use FAISS library, which is designed specifically for efficient similarity search and clustering of dense vectors\n\nimport faiss\nimport time\nimport pandas as pd\nimport numpy as np\nfrom datasets import load_from_disk\n\n\ndef faiss_find_NN(index,X,k):\n    tic = time.perf_counter()\n    D, I = index.search(X, k=k)  \n    toc = time.perf_counter()\n    print(f\"Execution time: {toc - tic:0.4f} seconds\")\n    return I,D\n\ndef faiss_prepare_gpu_index(X):\n    res = faiss.StandardGpuResources()    \n    index = faiss.IndexFlatL2(X.shape[1])\n    gpu_index = faiss.index_cpu_to_gpu(res, 0, index)\n    gpu_index.add(X)\n    return gpu_index\n\n    \n\ndef faiss_prepare_gpu_index_fast(X,nlist,nprobe=1):\n    \"\"\"\n    https://github.com/facebookresearch/faiss/wiki/Faster-search\n    - nlist parameter in the IndexIVFFlat index in Faiss refers to the number of Voronoi cells, \n    or clusters, that the index uses to partition the vector space.\n    \n    - nprobe parameter specifies the number of cells to visit during the search, \n    can be used in conjunction with nlist to further fine-tune the performance of the index. \n    \n    For example, you can increase nlist to speed up the search \n    and then increase nprobe to compensate for the potential decrease in accuracy.\n    \"\"\"\n    res = faiss.StandardGpuResources()\n    d = X.shape[1]\n    quantizer = faiss.IndexFlatL2(d)\n    index = faiss.IndexIVFFlat(quantizer,d,nlist)\n    assert not index.is_trained\n    index.train(X)\n    assert index.is_trained\n    \n    index.add(X)\n    index.nprobe = nprobe\n    gpu_index = faiss.index_cpu_to_gpu(res,0,index)\n    return gpu_index\n\nWe will produce the feature vectors for the entire dataset\n\nfull_dset = load_dataset('sample_data',data_files=['Womens_Clothing_Reviews.csv'],split='train')\n# For now, we will filter missing review text rows here instead of relying the data controler\nfull_dset = full_dset.filter(lambda x: x['Review Text'] is not None)\n\n\n_config = AutoConfig.from_pretrained('./sample_weights/roberta_lm_model',output_hidden_states=True)\n\ntrained_model = language_model_init(AutoModelForMaskedLM,\n                                    cpoint_path='./sample_weights/roberta_lm_model',\n                                    config=_config\n                                   )\n\ncontroller2 = ModelLMController(trained_model,data_store=tdc,seed=42)\n\nTotal parameters: 124697433\nTotal trainable parameters: 124697433\n\n\n\nall_hiddens = controller2.get_hidden_states_from_raw_dset(dset=full_dset,\n                                                          state_name='hidden_states',\n                                                          state_idx=[-1,0])\n\n-------------------- Start Test Set Transformation --------------------\n----- Metadata Simple Processing & Concatenating to Main Content -----\nDone\n-------------------- Text Transformation --------------------\n----- text_normalize -----\n----- lower -----\nDone\n-------------------- Tokenization --------------------\nDone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# all_hiddens = load_from_disk(\"./sample_weights/all_hiddens\")\n\n\nall_hiddens['hidden_states'].shape\n\n(22641, 768)\n\n\n\ngpu_index = faiss_prepare_gpu_index(all_hiddens['hidden_states'])\n\n\nidxs,D = faiss_find_NN(gpu_index,all_hiddens['hidden_states'],k=5) # find 20 nearest neighbors\n\nExecution time: 0.0659 seconds\n\n\n\ni=1\nprint(f\"1. Original Sentence: {all_hiddens['Review Text'][i]}\\n\")\nprint('2. Neighbors:')\nfor a,b in enumerate(idxs[i]):\n    print(f\"  - Sentence: {all_hiddens['Review Text'][b]}\")\n    print(f\"  - Distance: {D[i][a]}\")\n    print('-'*10)\n\n1. Original Sentence: . love this dress ! it's sooo pretty . i happened to find it in a store , and i'm glad i did bc i never would have ordered it online bc it's petite . i bought a petite and am 5 ' 8 \" . i love the length on me - hits just a little below the knee . would definitely be a true midi on someone who is truly petite .\n\n2. Neighbors:\n  - Sentence: . love this dress ! it's sooo pretty . i happened to find it in a store , and i'm glad i did bc i never would have ordered it online bc it's petite . i bought a petite and am 5 ' 8 \" . i love the length on me - hits just a little below the knee . would definitely be a true midi on someone who is truly petite .\n  - Distance: 3.0517578125e-05\n----------\n  - Sentence: this dress is beautiful . when i received the dress , it was true to size on the bust and length . i didn't purchase the petite , but i wish i did since i am 5 ' 2 . i tried to wear my wedges , but it's still long . i love the dress and would definitely recommend it .\n  - Distance: 0.445709228515625\n----------\n  - Sentence: pretty everyday dress . this is a great everyday dress . i bought it in black and it looks cute with wedges . it's a little boxy but that also makes it super comfy so i don't mind that too much . definitely order one-size down if you're between sizes . you may want to consider petite , if it's available , becuase i found it to be a bit long for my taste . ( i'm 5 ' 5 \" and it hits at the knee ) . i've already worn it a few times and have had received several compliments .\n  - Distance: 0.4746246337890625\n----------\n  - Sentence: adorable ! . i ran into retailer to actually return a dress and saw this hanging in the petite section and had to try it . at almost 5 ' 4 \" i can sometimes go between regular and petite sizing . and thankfully i could grab the petite with this dress . ( they didn't have the regular size ) it's airy and summery , i love the design of the swing . it's loose , but not tent like . the length came to about 3 inches above my knee . it's not see though at all ! i had black bottoms on and you couldn't see them . another plus it tha\n  - Distance: 0.479583740234375\n----------\n  - Sentence: love this dress ! . i'm 5 ft 3 inches , 130 lbs . this dress fits perfect ! i bought a 4 r . the petite 4 probably would have fit and been a inch or two shorter but i don't the regular looked bad . it hits an inch or two below my knee . medium thickness of fabric is forgiving even though the dress hugs in all the right places . beautiful dress ! !\n  - Distance: 0.48602294921875\n----------\n\n\n\ni=2\nprint(f\"1. Original Sentence: {all_hiddens['Review Text'][i]}\\n\")\nprint('2. Neighbors:')\nfor a,b in enumerate(idxs[i]):\n    print(f\"  - Sentence: {all_hiddens['Review Text'][b]}\")\n    print(f\"  - Distance: {D[i][a]}\")\n    print('-'*10)\n\n1. Original Sentence: some major design flaws . i had such high hopes for this dress and really wanted it to work for me . i initially ordered the petite small ( my usual size ) but i found this to be outrageously small . so small in fact that i could not zip it up ! i reordered it in petite medium , which was just ok . overall , the top half was comfortable and fit nicely , but the bottom half had a very tight under layer and several somewhat cheap ( net ) over layers . imo , a major design flaw was the net over layer sewn directly into the zipper - it c\n\n2. Neighbors:\n  - Sentence: some major design flaws . i had such high hopes for this dress and really wanted it to work for me . i initially ordered the petite small ( my usual size ) but i found this to be outrageously small . so small in fact that i could not zip it up ! i reordered it in petite medium , which was just ok . overall , the top half was comfortable and fit nicely , but the bottom half had a very tight under layer and several somewhat cheap ( net ) over layers . imo , a major design flaw was the net over layer sewn directly into the zipper - it c\n  - Distance: 0.0\n----------\n  - Sentence: wanted to love . . i was so excited for this dress to arrive , but when it did , it was a major disappointment . there is no lining on the skirt portion of the dress leaving it sheer . also , the top part along the chest puckered in a weird unflattering way . the colors were beautiful , but not enough to to make up for the other design flaws . i am 5 ' 6 \" 140 lb and the small fit tts . will be sending it back\n  - Distance: 0.29474639892578125\n----------\n  - Sentence: sloppy fit . tried this dress in a small . i am 5 ' 6 \" , 135 lbs , 34 c . the small was huge - the sash helped some , but looked like a potato sack on me . also , i noticed the fabric was piling slighly in the store . i had high hopes for this dress but left this one in the store .\n  - Distance: 0.34699249267578125\n----------\n  - Sentence: gorgeous but ... . this dress caught my eye online and i had to have it . when it arrived it was still just as pretty as i remembered . beautiful , vibrant colors and a unique oversized floral on a flirty , swishy fabric . unfortunately the cut was really strange . after reading several reviews i went with an xs , but i'm not sure a s would have solved all the fit problems i was having . while everything from the waist down fit perfectly , the arms were cut too tight , the chest was huge and yet i struggled to zip it up . an\n  - Distance: 0.35202789306640625\n----------\n  - Sentence: wanted to love it , but ... . i saw the colors and texture of this top and wanted to love it however when i put it on it just did not work . the cut was so strange . it runs very small in the shoulders and then flairs out so much at the waist . i even ordered up a size based on reviews . i ordered an 8 when i normally where a small ( 6 ) in things . the shoulders were still too small and it was way too big every where else .\n  - Distance: 0.35425567626953125\n----------",
    "crumbs": [
      "4. Hidden States Extraction",
      "Text Feature Extraction From Hidden States"
    ]
  },
  {
    "objectID": "model_main.html",
    "href": "model_main.html",
    "title": "Model Main Functions and Controller",
    "section": "",
    "text": "source\n\nmodel_init_classification\n\n model_init_classification (model_class, cpoint_path,\n                            output_hidden_states:bool, device=None,\n                            config=None, seed=None, body_model=None,\n                            model_kwargs={})\n\n*To initialize a classification (or regression) model, either from an existing HuggingFace model or custom architecture\nCan be used for binary, multi-class single-head, multi-class multi-head, multi-label clasisifcation, and regression*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel_class\n\n\nModel’s class object, e.g. RobertaHiddenStateConcatForSequenceClassification\n\n\ncpoint_path\n\n\nEither model string name on HuggingFace, or the path to model checkpoint\n\n\noutput_hidden_states\nbool\n\nTo whether output the model hidden states or not. Useful when you try to build a custom classification head\n\n\ndevice\nNoneType\nNone\nDevice to train on\n\n\nconfig\nNoneType\nNone\nModel config. If not provided, AutoConfig is used to load config from cpoint_path\n\n\nseed\nNoneType\nNone\nRandom seed\n\n\nbody_model\nNoneType\nNone\nIf not none, we use this to initialize model’s body. If you only want to load the model checkpoint in cpoint_path, leave this as none\n\n\nmodel_kwargs\ndict\n{}\nKeyword arguments for model (both head and body)\n\n\n\n\nsource\n\n\ncompute_metrics\n\n compute_metrics (pred, metric_funcs=[], metric_types=[], head_sizes=[],\n                  label_names=[], is_multilabel=False,\n                  multilabel_threshold=0.5)\n\n*Return a dictionary of metric name and its values.\nReference: https://github.com/huggingface/transformers/blob/main/src/transformers/trainer_utils.py#L107C16-L107C16*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npred\n\n\nAn EvalPrediction object from HuggingFace (which is a named tuple with predictions and label_ids attributes)\n\n\nmetric_funcs\nlist\n[]\nA list of metric functions to evaluate\n\n\nmetric_types\nlist\n[]\nType of metric (‘classification’ or ‘regression’) for each metric functions above\n\n\nhead_sizes\nlist\n[]\nClass size for each head. Regression head will have head size 1\n\n\nlabel_names\nlist\n[]\nNames of the label (dependent variable) columns\n\n\nis_multilabel\nbool\nFalse\nWhether this is a multilabel classification\n\n\nmultilabel_threshold\nfloat\n0.5\nThreshold for multilabel (&gt;= threshold is positive)\n\n\n\n\nsource\n\n\ncompute_metrics_separate_heads\n\n compute_metrics_separate_heads (pred, metric_funcs=[], label_names=[],\n                                 **kwargs)\n\n*Return a dictionary of metric name and its values. This is used in Deep Hierarchical Classification (special case of multi-head classification)\nThis metric function is mainly used when you have a separate logit output for each head (instead of the typical multi-head logit output: all heads’ logits are concatenated)*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npred\n\n\nAn EvalPrediction object from HuggingFace (which is a named tuple with predictions and label_ids attributes)\n\n\nmetric_funcs\nlist\n[]\nA list of metric functions to evaluate\n\n\nlabel_names\nlist\n[]\nNames of the label (dependent variable) columns\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nloss_for_classification\n\n loss_for_classification (logits, labels, is_multilabel=False,\n                          is_multihead=False, head_sizes=[],\n                          head_weights=[])\n\n*The general loss function for classification\n\nIf is_multilabel is False and is_multihead is False: Single-Head Classification, e.g. You predict 1 out of n class\nIf is_multilabel is False and is_multihead is True: Multi-Head Classification, e.g. You predict 1 out of n classes at Level 1, and 1 out of m classes at Level 2\nIf is_multilabel is True and is_multihead is False: Single-Head Multi-Label Classification, e.g. You predict x out of n class (x&gt;=0)\nIf is_multilabel is True and is_multihead is True: Not supported*\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlogits\n\n\noutput of the last linear layer, before any softmax/sigmoid. Size: (bs,class_size)\n\n\nlabels\n\n\ndetermined by your datasetdict. Size: (bs,number_of_head)\n\n\nis_multilabel\nbool\nFalse\nWhether this is a multilabel classification\n\n\nis_multihead\nbool\nFalse\nWhether this is a multihead classification\n\n\nhead_sizes\nlist\n[]\nClass size for each head. Regression head will have head size 1\n\n\nhead_weights\nlist\n[]\nloss weight for each head. Default to 1 for each head\n\n\n\n\nsource\n\n\nfinetune\n\n finetune (lr, bs, wd, epochs, ddict, tokenizer, o_dir='./tmp_weights',\n           save_checkpoint=False, model=None, model_init=None,\n           data_collator=None, compute_metrics=None, grad_accum_steps=2,\n           lr_scheduler_type='cosine', warmup_ratio=0.1, no_valid=False,\n           val_bs=None, seed=None, report_to='none', trainer_class=None,\n           len_train=None)\n\nThe main model training/finetuning function\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlr\n\n\nLearning rate\n\n\nbs\n\n\nBatch size\n\n\nwd\n\n\nWeight decay\n\n\nepochs\n\n\nNumber of epochs\n\n\nddict\n\n\nThe HuggingFace datasetdict\n\n\ntokenizer\n\n\nHuggingFace tokenizer\n\n\no_dir\nstr\n./tmp_weights\nDirectory to save weights\n\n\nsave_checkpoint\nbool\nFalse\nWhether to save weights (checkpoints) to o_dir\n\n\nmodel\nNoneType\nNone\nNLP model\n\n\nmodel_init\nNoneType\nNone\nA function to initialize model\n\n\ndata_collator\nNoneType\nNone\nHuggingFace data collator\n\n\ncompute_metrics\nNoneType\nNone\nA function to compute metric, e.g. compute_metrics\n\n\ngrad_accum_steps\nint\n2\nThe batch at each step will be divided by this integer and gradient will be accumulated over gradient_accumulation_steps steps.\n\n\nlr_scheduler_type\nstr\ncosine\nThe scheduler type to use. Including: linear, cosine, cosine_with_restarts, polynomial, constant, constant_with_warmup\n\n\nwarmup_ratio\nfloat\n0.1\nThe warmup ratio for some lr scheduler\n\n\nno_valid\nbool\nFalse\nWhether there is a validation set or not\n\n\nval_bs\nNoneType\nNone\nValidation batch size\n\n\nseed\nNoneType\nNone\nRandom seed\n\n\nreport_to\nstr\nnone\nThe list of integrations to report the results and logs to. Supported platforms are “azure_ml”, “comet_ml”, “mlflow”, “neptune”, “tensorboard”,“clearml” and “wandb”. Use “all” to report to all integrations installed, “none” for no integrations.\n\n\ntrainer_class\nNoneType\nNone\nYou can include the class name of your custom trainer here\n\n\nlen_train\nNoneType\nNone\nestimated number of samples in the whole training set (for streaming dataset only)\n\n\n\n\nsource\n\n\nModelController\n\n ModelController (model, data_store=None, seed=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel\n\n\nNLP model\n\n\ndata_store\nNoneType\nNone\na TextDataController/TextDataControllerStreaming object\n\n\nseed\nNoneType\nNone\nRandom seed\n\n\n\n\nsource\n\n\nModelController.fit\n\n ModelController.fit (epochs, learning_rate, ddict=None,\n                      metric_funcs=[&lt;function accuracy_score at\n                      0x7f896fe39820&gt;], metric_types=[], batch_size=16,\n                      val_batch_size=None, weight_decay=0.01,\n                      lr_scheduler_type='cosine', warmup_ratio=0.1,\n                      o_dir='./tmp_weights', save_checkpoint=False,\n                      hf_report_to='none', compute_metrics=&lt;function\n                      compute_metrics&gt;, grad_accum_steps=2,\n                      tokenizer=None, label_names=None, head_sizes=None,\n                      trainer_class=None, len_train=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nepochs\n\n\nNumber of epochs\n\n\nlearning_rate\n\n\nLearning rate\n\n\nddict\nNoneType\nNone\nDatasetDict to fit (will override data_store)\n\n\nmetric_funcs\nlist\n[&lt;function accuracy_score at 0x7f896fe39820&gt;]\nA list of metric functions (can be from Sklearn)\n\n\nmetric_types\nlist\n[]\nA list of metric types (classification or regression) that matches with the metric function list\n\n\nbatch_size\nint\n16\nBatch size\n\n\nval_batch_size\nNoneType\nNone\nValidation batch size. Set to batch_size if None\n\n\nweight_decay\nfloat\n0.01\nWeight decay\n\n\nlr_scheduler_type\nstr\ncosine\nThe scheduler type to use. Including: linear, cosine, cosine_with_restarts, polynomial, constant, constant_with_warmup\n\n\nwarmup_ratio\nfloat\n0.1\nThe warmup ratio for some lr scheduler\n\n\no_dir\nstr\n./tmp_weights\nDirectory to save weights\n\n\nsave_checkpoint\nbool\nFalse\nWhether to save weights (checkpoints) to o_dir\n\n\nhf_report_to\nstr\nnone\nThe list of HuggingFace-allowed integrations to report the results and logs to\n\n\ncompute_metrics\nfunction\ncompute_metrics\nA function to compute metric, e.g. compute_metrics which utilizes the given metric_funcs\n\n\ngrad_accum_steps\nint\n2\nGradient will be accumulated over gradient_accumulation_steps steps.\n\n\ntokenizer\nNoneType\nNone\nTokenizer (to override one in data_store)\n\n\nlabel_names\nNoneType\nNone\nNames of the label (dependent variable) columns (to override one in data_store)\n\n\nhead_sizes\nNoneType\nNone\nClass size for each head (to override one in model)\n\n\ntrainer_class\nNoneType\nNone\nYou can include the class name of your custom trainer here\n\n\nlen_train\nNoneType\nNone\nNumber of samples in the whole training set (for streaming dataset only)\n\n\n\n\nsource\n\n\nModelController.predict_raw_text\n\n ModelController.predict_raw_text (content:Union[dict,list,str],\n                                   is_multilabel=None,\n                                   multilabel_threshold=0.5, topk=1,\n                                   are_heads_separated=False)\n\n\nsource\n\n\nModelController.predict_raw_dset\n\n ModelController.predict_raw_dset (dset, batch_size=16,\n                                   do_filtering=False, is_multilabel=None,\n                                   multilabel_threshold=0.5, topk=1,\n                                   are_heads_separated=False)\n\n\nsource\n\n\nModelController.predict_ddict\n\n ModelController.predict_ddict (ddict:Union[datasets.dataset_dict.DatasetD\n                                ict,datasets.arrow_dataset.Dataset]=None,\n                                ds_type='test', batch_size=16,\n                                is_multilabel=None,\n                                multilabel_threshold=0.5, topk=1,\n                                tokenizer=None, label_names=None,\n                                class_names_predefined=None,\n                                are_heads_separated=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nddict\nDatasetDict | Dataset\nNone\nA processed and tokenized DatasetDict/Dataset (will override one in data_store)\n\n\nds_type\nstr\ntest\nThe split of DatasetDict to predict\n\n\nbatch_size\nint\n16\nBatch size for making prediction on GPU\n\n\nis_multilabel\nNoneType\nNone\nIs this a multilabel classification?\n\n\nmultilabel_threshold\nfloat\n0.5\nThreshold for multilabel classification\n\n\ntopk\nint\n1\nNumber of labels to return for each head\n\n\ntokenizer\nNoneType\nNone\nTokenizer (to override one in data_store)\n\n\nlabel_names\nNoneType\nNone\nNames of the label (dependent variable) columns (to override one in data_store)\n\n\nclass_names_predefined\nNoneType\nNone\nList of names associated with the labels (same index order) (to override one in data_store)\n\n\nare_heads_separated\nbool\nFalse\nAre outputs (of model) separate heads?",
    "crumbs": [
      "6. Model Classes",
      "a. Model Controller For Supervised Learning",
      "Model Main Functions and Controller"
    ]
  }
]